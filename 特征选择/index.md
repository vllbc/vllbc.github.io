# 特征选择

`特征选择`是`特征工程`里的一个重要问题，其目标是**寻找最优特征子集**。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，**提高模型精确度，减少运行时间的目的**。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。

根据特征选择的形式，可分为三大类：
-   Filter(过滤法)：按照`发散性`或`相关性`对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选
-   Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征
-   Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）
## 过滤式
基本想法是：分别对每个特征 $x_i$ ，计算 $x_i$ 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征$x_i$ 。
-   Pearson相关系数
-   卡方验证
-   互信息和最大信息系数
-   距离相关系数
-   方差选择法
### 

## 包裹式

## 嵌入式


