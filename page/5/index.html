<!DOCTYPE html>
<html lang="zh-CN">
    <head>
	<meta name="generator" content="Hugo 0.145.0">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="vllbc02&#39;s blogs">
  <meta property="og:description" content="vllbc&#39;s blog">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="website">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="vllbc02&#39;s blogs">
  <meta name="twitter:description" content="vllbc&#39;s blog">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="vllbc02&#39;s blogs">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="vllbc02&#39;s blogs"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/blog.vllbc.top\/","inLanguage": "zh-CN","description": "vllbc's blog","image": {
                "@type": "ImageObject",
                "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                "width":  3200 ,
                "height":  2048 
            },"thumbnailUrl": "https:\/\/blog.vllbc.top\/images\/screenshot.png","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","name": "vllbc02's blogs"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper">
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="page home" data-home="posts"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="所有文章"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/avatar.png"
        data-srcset="/images/avatar.png, /images/avatar.png 1.5x, /images/avatar.png 2x"
        data-sizes="auto"
        alt="/images/avatar.png"
        title="/images/avatar.png" width="512" height="512" /></a></div><h1 class="home-title">vllbc</h1><div class="home-subtitle"><div id="id-1" class="typeit"></div></div><div class="links"><a href="https://github.com/vllbc" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github fa-fw" aria-hidden="true"></i></a><a href="https://steamcommunity.com/id/vllbc" title="Steam" target="_blank" rel="noopener noreffer me"><i class="fab fa-steam fa-fw" aria-hidden="true"></i></a><a href="tel:18265090197" title="Phone" rel="me"><i class="fas fa-phone fa-fw" aria-hidden="true"></i></a><a href="mailto:vllbc02@163.com" title="Email" rel="me"><i class="far fa-envelope fa-fw" aria-hidden="true"></i></a><a href="/index.xml" title="RSS" target="_blank" rel="noopener noreffer me"><i class="fas fa-rss fa-fw" aria-hidden="true"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/remove_padding/">remove_padding</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-08">2025-07-08</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/infra/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Infra</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a></span></div><div class="content"><p>即
packing，将不同长度的序列紧凑存储，避免填充，减少不必要的计算和存储，提升效率。</p>
<h2 id="动机">动机</h2>
<p>sft进行微调，因为gpu是并行计算的，所以如果一个batch里面的数据，每条数据长度不相等，就需要对数据进行truncation（截断）和padding（pad数据到相同的seq_length）。显然，如果使用了padding，那么一个batch里面，就会有很多的pad_token，这些pad_token输入进入到了模型，但是却没有样本训练，造成了计算量的浪费。</p></div><div class="post-footer">
        <a href="/remove_padding/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/infra/">Infra</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/ulysses_sequence_parallel/">ulysses_sequence_parallel</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-08">2025-07-08</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>大模型分布式</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a></span></div><div class="content"><p>一句话：在sequence维度上进行切分</p>
<ul>
<li>将输入序列 X (长度 N) 沿序列维度切分为 SP 块，每个 GPU 分配到 N/SP
长度的子序列。
<ul>
<li>对于非注意力层 (如 MLP)，计算是完全局部的，每个 GPU
处理自己的子序列即可。
<ul>
<li>token 之间独立，token-level projection</li>
<li>Ulysses
SP的核心复杂性在于Attention层。为了让每个token在计算注意力时能够考虑到全局序列信息（或者说，让每个head在计算时能看到完整的序列，即使这个head只在当前rank计算），Attention模块前后需要进行两次精密的all-to-all数据重排。MLP层则没有这样的需求，数据在进入MLP时已经是按序列分片好的，可以直接进行本地计算。</li>
</ul></li>
<li>对于注意力层:
<ul>
<li>步骤 1 (计算 Q, K, V): 每个 GPU 基于其本地子序列计算出本地的
Q_local, K_local, V_local (维度约为 N/SP x d，d 是隐藏维度)。</li>
<li>步骤 2 (全局 K, V 收集 - 关键):
使用 <strong>All-to-All</strong> 通信操作（All-Gather??）。每个 GPU
将自己的 K_local, V_local 发送给所有其他 GPU，并接收来自所有其他 GPU 的
K, V 块。执行后，<strong>每个 GPU 拥有完整的全局 K 和 V 矩阵 (维度 N x
d)</strong>，但仍然只拥有本地的 Q_local (维度 N/SP x d)。
<ul>
<li><a
href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</a></li>
</ul></li>
<li>步骤 3 (本地注意力计算): 每个 GPU 使用其 Q_local 和完整的全局 K, V
计算其负责的那部分注意力输出 O_local (维度 N/SP x d)。计算公式为
Attention(Q_local, K_global, V_global)。这一步的计算量是 (N/SP) * N *
d，内存瓶颈在于存储临时的注意力分数矩阵，大小约为 <strong>(N/SP) *
N</strong>。相比原始的 **N*N**，内存显著降低。</li>
<li>步骤 4 (可选的输出重组):
如果后续层需要按序列拼接的完整输出，可能需要另一次通信（如 All-Gather
或另一次 All-to-All 的变种）来组合 O_local。但在 DeepSpeed
实现中，通常保持分布式状态，直接输入到下一个同样按序列并行的层。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="verl中的序列并行">verl中的序列并行</h2>
<p>在verl中，一般与remove_padding一起使用，即</p></div><div class="post-footer">
        <a href="/ulysses_sequence_parallel/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/">大模型分布式</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/entropyreasoning/">entropy(reasoning)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/reasoning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Reasoning</a></span></div><div class="content"><h2 id="熵坍塌">熵坍塌</h2>
<h3
id="ulorlan-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models-reasoning-abilities"><a
href="../../reading/Reasoning/UloRL：An%20Ultra-Long%20Output%20Reinforcement%20Learning%20Approach%20for%20Advancing%20Large%20Language%20Models’%20Reasoning%20Abilities.md">UloRL：An
Ultra-Long Output Reinforcement Learning Approach for Advancing Large
Language Models’ Reasoning Abilities</a></h3>
<p>UloRL的核心创新 <strong>动态掩码熟练掌握正向词元（Dynamic Masking of
well-Mastered Positive Tokens,
DMMPTs）</strong>。论文作者敏锐地指出，熵坍塌的根源并非“训练正样本”，而是“过度训练已经熟练掌握的正向词元（MPTs）”。MPTs指的是那些在正确答案中，且模型已经能以极高概率（如&gt;99%）预测出来的词元。DMMPTs为此设计了一个“熵值恒温器”：
1. 设定一个理想的“目标熵值”。 2.
在训练时，如果模型的当前熵低于这个目标值，说明模型开始变得“僵化”了。此时，系统会自动“屏蔽”（mask）掉那些MPTs，不再对它们进行训练，迫使模型关注那些还未掌握好的部分。
3. 如果模型熵值高于或等于目标值，则正常进行训练。</p></div><div class="post-footer">
        <a href="/entropyreasoning/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/reasoning/">Reasoning</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/fsdp/">fsdp</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>大模型分布式</a></span></div><div class="content"><h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/1929115059113693341">RL
系统深思：FSDP 训练后端</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/694288870">PyTorch FSDP
设计解读</a></p></div><div class="post-footer">
        <a href="/fsdp/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/">大模型分布式</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/">transformer参数量分析</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/infra/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Infra</a></span></div><div class="content"><p>进入大模型时代，基本上所有大模型都使用 decoder 部分，因此本文只分析
decoder 部分的参数量。 Transformer 的 decoder 每一层由 attention 和 mlp
组成，一般有 l 层。</p>
<h2 id="self-attention">Self-attention</h2>
<p>Self-attention 层由 <span class="math inline">\(W_{Q}\)</span>
、<span class="math inline">\(W_{K}\)</span>、<span
class="math inline">\(W_{V}\)</span> 和输出矩阵 <span
class="math inline">\(W_{O}\)</span> 和它们的偏置组成，权重矩阵的形状为
<span class="math inline">\([h,h]\)</span>，偏置形状为 <span
class="math inline">\([h]\)</span>，则 self-attention 部分的参数量为
<span class="math inline">\(4h^2+4h\)</span></p></div><div class="post-footer">
        <a href="/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/infra/">Infra</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/">思维链压缩</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/reasoning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Reasoning</a></span></div><div class="content">
</div><div class="post-footer">
        <a href="/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/reasoning/">Reasoning</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/">显存占用计算</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/infra/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Infra</a></span></div><div class="content"><h2 id="训练时">训练时</h2>
<ul>
<li><strong>模型参数</strong>：我们模型的可学习权重。</li>
<li><strong>Optimizer
states（优化器状态</strong>）：您需要跟踪的确切状态取决于您使用的优化器;例如，如果您使用的是 <a
href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?ref=jeremyjordan.me">AdamW</a>，则除了模型参数之外，您还需要跟踪第一和第二动量估计值。</li>
<li><strong>模型激活值</strong>：这将根据您的网络架构和批处理大小而有所不同，但会显著影响内存使用。<a
href="https://www.jeremyjordan.me/neural-networks-training">反向传播</a>需要此信息，以便我们能够有效地计算梯度。</li>
<li><strong>梯度</strong>：为模型的每个参数存储，与模型参数相同的内存占用。</li>
<li><strong>Input data</strong>：要传递给模型的 Importing
数据批次，内存占用取决于正在建模的数据的大小和类型。</li>
</ul>
<p>图示： <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20250710001336.png"
alt="image.png" /> 具体数值： 对于一个 transformer
来说，参数量可以由以下公式给出（详见 <a
href="transformer参数量分析.md">transformer参数量分析</a>）：</p></div><div class="post-footer">
        <a href="/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/infra/">Infra</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E9%95%BF%E5%BA%A6%E6%89%A9%E5%B1%95%E5%A4%96%E6%8E%A8/">长度扩展（外推）</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-06">2025-07-06</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/long-context/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Long Context</a></span></div><div class="content">
</div><div class="post-footer">
        <a href="/%E9%95%BF%E5%BA%A6%E6%89%A9%E5%B1%95%E5%A4%96%E6%8E%A8/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/long-context/">Long-Context</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/%E9%9D%A2%E7%BB%8F2025/">面经2025</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-07-02">2025-07-02</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E9%9D%A2%E7%BB%8F/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>面经</a></span></div><div class="content"><h1 id="常见问题总结">常见问题总结</h1>
<h2 id="多轮对话sft样本怎么构造">多轮对话sft样本怎么构造？</h2>
<ul>
<li><p>[大模型微调样本构造
trick]（https://zhuanlan.zhihu.com/p/641562439)</p>
<ul>
<li><p>多轮对话的传统组织方式：将多轮对话拆分为多条独立的训练样本，如
Q1A1/Q2A2/Q3A3 可拆分为 Q1—&gt;A1， Q1A1Q2-&gt;A2， Q1A1Q2A2Q3-&gt;A3
三条样本。</p></div><div class="post-footer">
        <a href="/%E9%9D%A2%E7%BB%8F2025/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E9%9D%A2%E7%BB%8F/">面经</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/world_model/">world_model</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-06-30">2025-06-30</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a>&nbsp;<a href="/categories/agent/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Agent</a>&nbsp;<a href="/categories/world-model/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>World-Model</a></span></div><div class="content"><p>我理解的agent中的world
model即可以预测采取某个action之后state的变化，这样做的好处是可以降低试错带来的时间成本或者是其它潜在的成本、风险。</p></div><div class="post-footer">
        <a href="/world_model/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/agent/">Agent</a>,&nbsp;<a href="/tags/world-model/">World-Model</a></div></div>
</article><ul class="pagination"><li class="page-item ">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/3/">3</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/4/">4</a>
                    </span>
                </li><li class="page-item active">
                    <span class="page-link">
                        <a href="/page/5/">5</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/6/">6</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/7/">7</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/23/">23</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script>window.config={"data":{"id-1":"NLP、任务规划、世界模型"},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};</script><script src="/js/theme.min.js"></script></body>
</html>
