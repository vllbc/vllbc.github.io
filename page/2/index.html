<!DOCTYPE html>
<html lang="zh-CN">
    <head>
	<meta name="generator" content="Hugo 0.145.0">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="vllbc02&#39;s blogs">
  <meta property="og:description" content="vllbc&#39;s blog">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="website">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="vllbc02&#39;s blogs">
  <meta name="twitter:description" content="vllbc&#39;s blog">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="vllbc02&#39;s blogs">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="vllbc02&#39;s blogs"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/blog.vllbc.top\/","inLanguage": "zh-CN","description": "vllbc's blog","image": {
                "@type": "ImageObject",
                "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                "width":  3200 ,
                "height":  2048 
            },"thumbnailUrl": "https:\/\/blog.vllbc.top\/images\/screenshot.png","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","name": "vllbc02's blogs"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper">
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="page home" data-home="posts"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="所有文章"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/avatar.png"
        data-srcset="/images/avatar.png, /images/avatar.png 1.5x, /images/avatar.png 2x"
        data-sizes="auto"
        alt="/images/avatar.png"
        title="/images/avatar.png" width="512" height="512" /></a></div><h1 class="home-title">vllbc</h1><div class="home-subtitle"><div id="id-1" class="typeit"></div></div><div class="links"><a href="https://github.com/vllbc" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github fa-fw" aria-hidden="true"></i></a><a href="https://steamcommunity.com/id/vllbc" title="Steam" target="_blank" rel="noopener noreffer me"><i class="fab fa-steam fa-fw" aria-hidden="true"></i></a><a href="tel:18265090197" title="Phone" rel="me"><i class="fas fa-phone fa-fw" aria-hidden="true"></i></a><a href="mailto:vllbc02@163.com" title="Email" rel="me"><i class="far fa-envelope fa-fw" aria-hidden="true"></i></a><a href="/index.xml" title="RSS" target="_blank" rel="noopener noreffer me"><i class="fas fa-rss fa-fw" aria-hidden="true"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/asnumpy/">asnumpy</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-01-08">2025-01-08</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/einops/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Einops</a></span></div><div class="content"><blockquote>
<p>Convert a tensor of an imperative framework
(i.e. numpy/cupy/torch/jax/etc.) to <code>numpy.ndarray</code></p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20250108154828.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure></div><div class="post-footer">
        <a href="/asnumpy/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/einops/">Einops</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/parse_shape/">parse_shape</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2025-01-08">2025-01-08</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/einops/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Einops</a></span></div><div class="content"><blockquote>
<p>Parse a tensor shape to dictionary mapping axes names to their
lengths.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use underscore to skip the dimension in parsing. </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> np.zeros([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>]) <span class="op">&gt;&gt;&gt;</span> parse_shape(x, <span class="st">&#39;batch _ h w&#39;</span>) {<span class="st">&#39;batch&#39;</span>: <span class="dv">2</span>, <span class="st">&#39;h&#39;</span>: <span class="dv">5</span>, <span class="st">&#39;w&#39;</span>: <span class="dv">7</span>} </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `parse_shape` output can be used to specify axes_lengths for other operations: </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> np.zeros([<span class="dv">700</span>]) </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> rearrange(y, <span class="st">&#39;(b c h w) -&gt; b c h w&#39;</span>, <span class="op">**</span>parse_shape(x, <span class="st">&#39;b _ h w&#39;</span>)).shape </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">7</span>)</span></code></pre></div>
<p>也就是把维度的维数映射到对应的命名。与数据无关，只看得到维度。</p></div><div class="post-footer">
        <a href="/parse_shape/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/einops/">Einops</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/5%E7%A7%8D%E6%96%B9%E6%B3%95/">5种方法</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-12-20">2024-12-20</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/task-planning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Task Planning</a>&nbsp;<a href="/categories/survey/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Survey</a></span></div><div class="content"><p>该工作主要梳理了LLM-based Agent
中的规划（planning）能力，原文链接：</p>
<p><a href="​arxiv.org/abs/2402.02716">Understanding the planning of LLM
agents: A survey</a></p>
<p>文章中，作者将planning能力进一步细分为了五个维度：</p></div><div class="post-footer">
        <a href="/5%E7%A7%8D%E6%96%B9%E6%B3%95/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/task_planning/">Task_planning</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/gather%E5%92%8Cscatter/">gather和scatter</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-12-20">2024-12-20</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/coding/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Coding</a>&nbsp;<a href="/categories/torch/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Torch</a></span></div><div class="content"><h2 id="gather">gather</h2>
<p>参数：</p>
<ul>
<li><strong>input</strong> (<a
href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/tensors.html%23torch.Tensor">Tensor</a>)
– the source tensor</li>
<li><strong>dim</strong> (<a
href="https://link.zhihu.com/?target=https%3A//docs.python.org/3/library/functions.html%23int">int</a>)
– the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – the indices of
elements to gather</li>
<li><strong>out</strong> (<a
href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/tensors.html%23torch.Tensor">Tensor</a>_,__optional_)
– the destination tensor</li>
<li><strong>sparse_grad</strong> (<a
href="https://link.zhihu.com/?target=https%3A//docs.python.org/3/library/functions.html%23bool">bool</a><em>,optional</em>)
– If <code>True</code>, gradient w.r.t. <code>input</code> will be a
sparse tensor. &gt;
gather操作是scatter操作的<strong>逆操作</strong>，如果说scatter是根据index和src求self(<em>input</em>)，那么gather操作是根据self(input)和index求src。具体来说gather操作是根据index指出的索引，沿dim指定的轴收集input的值。</li>
</ul>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>out[i][j][k] <span class="op">=</span> <span class="bu">input</span>[index[i][j][k]][j][k]  <span class="co"># if dim == 0</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>out[i][j][k] <span class="op">=</span> <span class="bu">input</span>[i][index[i][j][k]][k]  <span class="co"># if dim == 1</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>out[i][j][k] <span class="op">=</span> <span class="bu">input</span>[i][j][index[i][j][k]]  <span class="co"># if dim == 2</span></span></code></pre></div>
<p>对于gather操作来说，有三个约束需要满足：</p></div><div class="post-footer">
        <a href="/gather%E5%92%8Cscatter/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/coding/">Coding</a>,&nbsp;<a href="/tags/gather/">Gather</a>,&nbsp;<a href="/tags/scatter/">Scatter</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/llama%E7%B3%BB%E5%88%97/">llama系列</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-09-26">2024-09-26</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a>&nbsp;<a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div><div class="content"><h1 id="llama介绍">LLaMA介绍</h1>
<p>LLaMA 是目前为止，效果最好的开源 LLM 之一。</p>
<blockquote>
<p><strong>论文的核心思想：相比于GPT，更小的模型+更多的训练数据</strong>**也可以获得可比的效果</p>
</blockquote>
<p>基于更多 tokens
的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为
<code>LLaMA</code>，参数范围从 7B 到 65B 不等，与现有最佳 LLM
相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于
GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM
的使用和研究平民化，因为它可以在单个 GPU
上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如
Chinchilla 或 PaLM-540B）相竞争的能力。</p></div><div class="post-footer">
        <a href="/llama%E7%B3%BB%E5%88%97/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/llama/">Llama</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/generate%E7%9B%B8%E5%85%B3/">frequency_penalty&presence_penalty</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-09-05">2024-09-05</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a></span></div><div class="content"><p>LLM解码时采用的自回归采样，其过程如下：</p>
<ol type="1">
<li>小模型使用前缀作为输入，将输出结果处理+归一化成<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83&amp;zhida_source=entity">概率分布</a>后，采样生成下一个token。</li>
<li>将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。</li>
</ol>
<p>将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=top-k&amp;zhida_source=entity">top-k</a>和top-n等
# 贪心搜索
直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复
# 随机采样
按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
# beam search 维护一个大小为 k
的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k
个单词，然后保留总概率最高的 k
个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。
# top-k <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240927155937.png"
alt="image.png" /></p></div><div class="post-footer">
        <a href="/generate%E7%9B%B8%E5%85%B3/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/rwkv/">rwkv</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-09-04">2024-09-04</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a></span></div><div class="content"><h1 id="线性transformer">线性Transformer</h1>
<p><span class="math display">\[V_i&#39;=\frac{\sum_{j=1}^N
sim(Q_i,K_j)V_j}{\sum_{j=1}^N sim(Q_i,K_j)}\]</span> 注意下标i。
其中</p>
<p><span
class="math display">\[sim(Q_{i},K_{j})=\phi(Q_{i},K_{j})\]</span></p>
<p>此时有：</p>
<p><span
class="math display">\[V_{i}^{\prime}=\frac{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}V_{j}}{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}}\]</span></p>
<p>注意可以将<span
class="math inline">\(\phi(Q_{i})\)</span>提出来。</p>
<p>原始Transformer的计算复杂度随序列长N呈二次方增长，这是因为attention的计算包含两层for循环，外层是对于每一个Query，我们需要计算它对应token的新表征；内层for循环是为了计算每一个Query对应的新表征，需要让该Query与每一个Key进行计算。
所以外层是 for q in Queries，内层是 for k in
Keys。Queries数量和Keys数量都是N，所以复杂度是 O(N^2) 。而Linear
Transformer，它只有外层for q in
Queries这个循环了。因为求和项的计算与i无关，所以所有的 Qi
可以共享求和项的值。换言之，求和项的值可以只计算一次，然后存在内存中供所有
Qi 去使用。所以Linear Transformer的计算复杂度是O(N) 。</p></div><div class="post-footer">
        <a href="/rwkv/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a>,&nbsp;<a href="/tags/rwkv/">Rwkv</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/rope/">rope</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-08-31">2024-08-31</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/llm/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>LLM</a></span></div><div class="content"><h1 id="证明">证明</h1>
<p>核心思想就是找到一个转换，可以通过点积操作将位置信息注入，即： <span
class="math display">\[&lt;f_q\left(x_m,m\right),f_k\left(x_n,n\right)&gt;=g\left(x_m,x_n,m-n\right)\]</span>
而通过复数的一些性质，找到了满足上述操作的转换：</p>
<p><span class="math display">\[\begin{aligned}
&amp;f_{q}\left(\boldsymbol{x}_{m},m\right)=\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)e^{im\theta}
\\
&amp;f_{k}\left(\boldsymbol{x}_{n},n\right)=\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)e^{in\theta}
\\
&amp;g\left(\boldsymbol{x}_{m},\boldsymbol{x}_{n},m-n\right)=\mathrm{Re}\left[\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)^{*}e^{i(m-n)\theta}\right]
\end{aligned}\]</span> 可以发现g函数中存在相对位置信息。 欧拉公式：<span
class="math inline">\(e^{ix}=\cos x+i\sin x\)</span></p></div><div class="post-footer">
        <a href="/rope/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/llm/">LLM</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/data-engineering-for-scaling-language-models-to-128k-context/">Data Engineering for Scaling Language Models to 128K Context</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-08-08">2024-08-08</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>文献和源码阅读</a></span></div><div class="content"><h1
id="data-engineering-for-scaling-language-models-to-128k-context">Data
Engineering for Scaling Language Models to 128K Context</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Data Engineering for Scaling
Language Models to 128K Context</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Yao Fu; Rameswar Panda;
Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao
Peng</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2024-02-15</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2402.10171" rel="noopener noreferrer nofollow">10.48550/arXiv.2402.10171</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_Z5AQISDH" rel="noopener noreferrer nofollow">Fu
et al_2024_Data Engineering for Scaling Language Models to 128K
Context.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。</span></span></p></div><div class="post-footer">
        <a href="/data-engineering-for-scaling-language-models-to-128k-context/">阅读全文</a><div class="post-tags">
                <i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%96%87%E7%8C%AE/">文献</a>,&nbsp;<a href="/tags/llm/">LLM</a></div></div>
</article><article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/kv-cache/">KV cache</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-publish">发布于 <time datetime="2024-08-07">2024-08-07</time></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a></span></div><div class="content"><h1 id="kv-cache">KV cache</h1>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916121501.png"
alt="image.png" /> <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916121505.png"
alt="image.png" /> <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916121510.png"
alt="image.png" /></p>
<p>LLM推理过程分为Prefill和Decode两个阶段，其中Prefill阶段会对Prompt中所有的token做<a
href="https://zhida.zhihu.com/search?q=%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97&amp;zhida_source=entity&amp;is_preview=1">并行计算</a>，得到Prompt中所有Tokens的KV
Cache以及计算得到首Token。Prompt阶段Token计算得到的KV
Cache会保存下来，留给Decode阶段复用，Decode阶段是一个自回归过程，每decode一个新的Token，都需要用到所有之前计算得到的KV
Cache来计算当前query
token的Attention。因此，当输出长度越来越大或者context很长时，KV
Cache将会占用大量的显存。<strong>如何优化KV
Cache的显存占用，一直都是LLM推理的核心主题之一。</strong></p></div><div class="post-footer">
        <a href="/kv-cache/">阅读全文</a></div>
</article><ul class="pagination"><li class="page-item ">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item active">
                    <span class="page-link">
                        <a href="/page/2/">2</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/3/">3</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/4/">4</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/24/">24</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script>window.config={"data":{"id-1":"NLP、任务规划、机器学习"},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};</script><script src="/js/theme.min.js"></script></body>
</html>
