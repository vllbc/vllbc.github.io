<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - æ ‡ç­¾ - vllbc02</title>
        <link>https://vllbc.top/tags/transformer/</link>
        <description>Transformer - æ ‡ç­¾ - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[Transformer Feed-Forward Layers Are Key-Value Memories ðŸ’¡ Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 æœŸåˆŠæ ‡ç­¾ DOI 10.48550/arXiv.2012.14913 é™„ä»¶ Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf ðŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®çš„ å‰é¦ˆå±‚å æ®äº† Transformer æ¨¡åž‹å‚æ•°çš„ä¸‰åˆ†]]></description>
</item>
<item>
    <title>trainer</title>
    <link>https://vllbc.top/trainer/</link>
    <pubDate>Tue, 16 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/trainer/</guid>
    <description><![CDATA[åŸºæœ¬ç”¨æ³• ä¸‹é¢æ˜¯ä½¿ç”¨çš„ä¸€ä¸ªä¾‹å­ï¼Œé‡ç‚¹æ˜¯TrainingArgå’Œdata_collatorã€‚ dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=&#39;./text.txt&#39;, block_size=512) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) training_args = TrainingArguments( output_dir=&#39;./outputs/&#39;, overwrite_output_dir=True, num_train_epochs=100, per_device_train_batch_size=16, save_steps=5000, ) trainer = Trainer( model=model, args=training_args,]]></description>
</item>
<item>
    <title>Transformer</title>
    <link>https://vllbc.top/transformer/</link>
    <pubDate>Wed, 08 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/transformer/</guid>
    <description><![CDATA[Transformer \[ -\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_i^{+}\right) / \tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{+}\right) / \tau})+\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right))} \] èƒŒæ™¯ å…ˆä»Žword2vecå¼€å§‹è¯´èµ·ï¼Œword2vecå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡åž‹ï¼Œä½†æ˜¯å®ƒæœ‰ä¸ªé—®é¢˜å°±æ˜¯å®ƒæ²¡æœ‰åŠžæ³•è§£å†³ä¸€è¯]]></description>
</item>
</channel>
</rss>
