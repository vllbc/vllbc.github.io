<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/transformer/</link>
        <description>Transformer - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[<h1
id="transformer-feed-forward-layers-are-key-value-memories">Transformer
Feed-Forward Layers Are Key-Value Memories</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Transformer Feed-Forward
Layers Are Key-Value Memories</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Mor Geva; Roei Schuster;
Jonathan Berant; Omer Levy</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2021-09-05</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2012.14913" rel="noopener noreferrer nofollow">10.48550/arXiv.2012.14913</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_NUWXXUEK" rel="noopener noreferrer nofollow">Geva
et al_2021_Transformer Feed-Forward Layers Are Key-Value
Memories.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">前馈层占据了 Transformer
模型参数的三分之二，但其在网络中的作用尚未被充分探索。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。前馈层的输出是其记忆的组合，并通过模型层的残差连接逐步细化，以产生最终的输出分布。</span></span></p>]]></description>
</item>
<item>
    <title>trainer</title>
    <link>https://blog.vllbc.top/trainer/</link>
    <pubDate>Tue, 16 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/trainer/</guid>
    <description><![CDATA[<h1 id="基本用法">基本用法</h1>
<p>下面是使用的一个例子，重点是TrainingArg和data_collator。</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> LineByLineTextDataset(tokenizer<span class="op">=</span>tokenizer, file_path<span class="op">=</span><span class="st">&#39;./text.txt&#39;</span>, block_size<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling( tokenizer<span class="op">=</span>tokenizer, mlm<span class="op">=</span><span class="va">True</span>, mlm_probability<span class="op">=</span><span class="fl">0.15</span> ) </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments( output_dir<span class="op">=</span><span class="st">&#39;./outputs/&#39;</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                                  overwrite_output_dir<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                                  num_train_epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                                  per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>, </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                                  save_steps<span class="op">=</span><span class="dv">5000</span>, ) </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer( model<span class="op">=</span>model, </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                  args<span class="op">=</span>training_args, </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                  data_collator<span class="op">=</span>data_collator, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                  train_dataset<span class="op">=</span>dataset, ) </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>trainer.train() </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>trainer.save_model(<span class="st">&#39;./outputs/&#39;</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p>的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其它的附图。
[0089]
图1为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的整体流程图； [0090]
图2为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的Decoder-only模型架构示意图； [0091]
图3为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的利用现有大语言模型训练流程图； [0092]
图4为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的预训练大语言模型流程图； [0093]
图5为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的推理流程图； [0094]
图6为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的虚拟字符检索流程图； [0095]
图7为本发明第二个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的各个模型的推理性能对比图； [0096]
图8为本发明第二个实施例提供的一种基于大语言模型自身对上下文进行压缩的
方法的部分压缩示例图。</p>]]></description>
</item>
<item>
    <title>Transformer</title>
    <link>https://blog.vllbc.top/transformer/</link>
    <pubDate>Wed, 08 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer/</guid>
    <description><![CDATA[<h1 id="transformer">Transformer</h1>
<p><span class="math display">\[
-\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_i^{+}\right) /
\tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{+}\right) /
\tau})+\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{-}\right) / \tau}\right))}
\]</span></p>
<h2 id="背景">背景</h2>
<p>先从word2vec开始说起，word2vec可以看作是一个<a
href="预训练模型.md">预训练模型</a>，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用<a
href="语言模型.md">语言模型</a>训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的<a
href="Word%20Embedding.md">word embedding</a>空间里去。</p>
<p>而<a href="ELMo.md">ELMo</a>就解决了这个问题，它使用了双向的<a
href="../Deep%20Learning/循环神经网络系列/LSTM.md">LSTM</a>，具体的可以看<a
href="ELMo.md">ELMo</a>,总之使用<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>作为特征提取器，解决了多义词的问题，但现在来看，<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>和<a
href="../Deep%20Learning/卷积神经网络系列/CNN.md">CNN</a>的地位，成为了最流行的特征提取器，大火的<a
href="GPT.md">GPT</a>和<a
href="BERT.md">BERT</a>都与Transformer离不开关系。拿bank为例，<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>在读取整个句子之前不会理解bank的含义，也就是<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer
的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-<a
href="Attention.md">attention</a>），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6
个）中。</p>]]></description>
</item>
</channel>
</rss>
