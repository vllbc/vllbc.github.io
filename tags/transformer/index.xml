<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - 标签 - vllbc02</title>
        <link>https://vllbc.top/tags/transformer/</link>
        <description>Transformer - 标签 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[Transformer Feed-Forward Layers Are Key-Value Memories 💡 Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 期刊标签 DOI 10.48550/arXiv.2012.14913 附件 Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf 📜 研究背景 &amp; 基础 &amp; 目的 前馈层占据了 Transformer 模型参数的三分]]></description>
</item>
<item>
    <title>trainer</title>
    <link>https://vllbc.top/trainer/</link>
    <pubDate>Tue, 16 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/trainer/</guid>
    <description><![CDATA[基本用法 下面是使用的一个例子，重点是TrainingArg和data_collator。 dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=&#39;./text.txt&#39;, block_size=512) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) training_args = TrainingArguments( output_dir=&#39;./outputs/&#39;, overwrite_output_dir=True, num_train_epochs=100, per_device_train_batch_size=16, save_steps=5000, ) trainer = Trainer( model=model, args=training_args,]]></description>
</item>
<item>
    <title>Transformer</title>
    <link>https://vllbc.top/transformer/</link>
    <pubDate>Wed, 08 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/transformer/</guid>
    <description><![CDATA[Transformer \[ -\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_i^{+}\right) / \tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{+}\right) / \tau})+\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right))} \] 背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词]]></description>
</item>
</channel>
</rss>
