<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/transformer/</link>
        <description>Transformer - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Peri-LN：Revisiting Normalization Layer in the Transformer Architecture</title>
    <link>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</link>
    <pubDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇关于
Transformer 架构中归一化策略的优秀论文——《Peri-LN: Revisiting
Normalization Layer in the Transformer Architecture》。</p>]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward
Network,
FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：<strong>FFN层在功能上等同于一个键值记忆（Key-Value
Memory）系统</strong>。</p>]]></description>
</item>
<item>
    <title>Transformer</title>
    <link>https://blog.vllbc.top/transformer/</link>
    <pubDate>Wed, 08 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer/</guid>
    <description><![CDATA[<h1 id="transformer">Transformer</h1>
<p><span class="math display">\[
-\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_i^{+}\right) /
\tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{+}\right) /
\tau})+\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{-}\right) / \tau}\right))}
\]</span></p>
<h2 id="背景">背景</h2>
<p>先从word2vec开始说起，word2vec可以看作是一个<a
href="预训练模型.md">预训练模型</a>，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用<a
href="语言模型.md">语言模型</a>训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的<a
href="Word%20Embedding.md">word embedding</a>空间里去。</p>
<p>而<a href="ELMo.md">ELMo</a>就解决了这个问题，它使用了双向的<a
href="../../../Deep%20Learning/循环神经网络系列/LSTM.md">LSTM</a>，具体的可以看<a
href="ELMo.md">ELMo</a>,总之使用<a
href="../../../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>作为特征提取器，解决了多义词的问题，但现在来看，<a
href="../../../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了<a
href="../../../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>和<a
href="../Deep%20Learning/卷积神经网络系列/CNN.md">CNN</a>的地位，成为了最流行的特征提取器，大火的<a
href="GPT.md">GPT</a>和<a
href="BERT.md">BERT</a>都与Transformer离不开关系。拿bank为例，<a
href="../../../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>在读取整个句子之前不会理解bank的含义，也就是<a
href="../../../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer
的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-<a
href="Attention.md">attention</a>），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6
个）中。</p>]]></description>
</item>
</channel>
</rss>
