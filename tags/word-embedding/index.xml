<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Word Embedding - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/word-embedding/</link>
        <description>Word Embedding - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 16 Jul 2021 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/word-embedding/" rel="self" type="application/rss+xml" /><item>
    <title>Word Embedding</title>
    <link>https://blog.vllbc.top/word-embedding/</link>
    <pubDate>Fri, 16 Jul 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/word-embedding/</guid>
    <description><![CDATA[<h1 id="词嵌入">词嵌入</h1>
<h2 id="介绍">介绍</h2>
<blockquote>
<p><strong>词嵌入</strong>是<a
href="https://baike.baidu.com/item/自然语言处理">自然语言处理</a>（NLP）中<a
href="https://baike.baidu.com/item/语言模型">语言模型</a>与<a
href="https://baike.baidu.com/item/表征学习">表征学习</a>技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续<a
href="https://baike.baidu.com/item/向量空间">向量空间</a>中，每个单词或词组被映射为实数<a
href="https://baike.baidu.com/item/域">域</a>上的向量。</p>
<p>词嵌入的方法包括<a
href="https://baike.baidu.com/item/人工神经网络">人工神经网络</a>、对词语同现矩阵<a
href="https://baike.baidu.com/item/降维">降维</a>、<a
href="https://baike.baidu.com/item/概率模型">概率模型</a>以及单词所在上下文的显式表示等。</p>
<p>在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中<a
href="https://baike.baidu.com/item/语法分析器">语法分析器</a>和<a
href="https://baike.baidu.com/item/文本情感分析">文本情感分析</a>等的效果。</p>
</blockquote>
<p>以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep
contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word
representation)，可以粗略得把它分为三个阶段：</p>]]></description>
</item>
</channel>
</rss>
