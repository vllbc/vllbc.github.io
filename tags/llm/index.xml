<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/llm/</link>
        <description>LLM - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>MHA</title>
    <link>https://blog.vllbc.top/mha/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mha/</guid>
    <description><![CDATA[<h2 id="self-attention">Self-attention</h2>
<p>首先介绍一下最主要的 self-attention，可以说是 self-attention
实现了上述的 token 之间交互的功能。</p>
<p>自注意力是模型的关键组成部分之一。注意和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。</p>]]></description>
</item>
<item>
    <title>dapo</title>
    <link>https://blog.vllbc.top/dapo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dapo/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>grpo</title>
    <link>https://blog.vllbc.top/grpo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/grpo/</guid>
    <description><![CDATA[<h1 id="grpo-trl-库">GRPO (trl 库)</h1>
<h2 id="重要参数">重要参数</h2>
<ul>
<li>Num_generations: <strong>Number of generations to sample. The
effective batch size (num_processes * per_device_batch_size *
gradient_accumulation_steps) must be evenly divisible by this
value.</strong></li>
<li>generation_batch_size: <strong>Batch size to use for generation. If
<code>None</code>, it defaults to the effective training batch size:
<code>per_device_train_batch_size * num_processes * steps_per_generation</code>.</strong></li>
<li>steps_per_generation: Number of optimization steps per generation.
If <code>None</code>, it defaults to gradient_accumulation_steps.</li>
<li>Num_iterations: Number of iterations per batch (denoted as μ in the
algorithm).</li>
<li>Per_device_train_batch_size</li>
<li>Num_processes (world_size)</li>
</ul>
<p>trl 库的重要参数比较少。其中根据官方文档，generation_batch_size =
`per_device_train_batch_size * num_processes * steps_per_generation
Gradient_accumulation_steps 一般就是 steps_per_generation (对应 verl
中的 mini_batch_size / n_gpus /
ppo_micro_batch_size_per_gpu)，可以理解为 per_device_train_bs (对应 verl
中的 ppo_micro_batch_size_per_gpu) 是使用梯度累计后的 bs，乘 gpu
数，再乘梯度累计的 steps 就是总的 batch_size（对应 verl 中的
train_batch_size * rollout. N）。所以注意，总的 batch_size
(generation_batch_size) 是已经 rollout 采样后的 bs，除以 num_generations
才是针对 prompts 的 bs（verl 中的 train_batch_size）。
下面是_get_train_sampler 方法的注释，对每一个 prompt 重复
num_generations 是该方法实现的。</p>]]></description>
</item>
<item>
    <title>paged attention</title>
    <link>https://blog.vllbc.top/paged-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/paged-attention/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>ppo</title>
    <link>https://blog.vllbc.top/ppo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ppo/</guid>
    <description><![CDATA[<h2 id="ppo-openrlhf-库">PPO (openrlhf 库)</h2>
<p>重点记录一下 experience 的采集过程。训练其实很简单。Actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
<item>
    <title>Reinforcing General Reasoning without Verifiers</title>
    <link>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</guid>
    <description><![CDATA[<h3
id="一论文的研究目标与意义"><strong>一、论文的研究目标与意义</strong></h3>
<h4 id="研究目标与待解决问题"><strong>研究目标与待解决问题</strong></h4>
<p>论文的核心研究目标是：<strong>将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。</strong></p>]]></description>
</item>
<item>
    <title>REINFORECE&#43;&#43;</title>
    <link>https://blog.vllbc.top/reinforece/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforece/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>ReMAX（REINFORCE argmax）</title>
    <link>https://blog.vllbc.top/remaxreinforce-argmax/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/remaxreinforce-argmax/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>RLOO</title>
    <link>https://blog.vllbc.top/rloo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rloo/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>MLA</title>
    <link>https://blog.vllbc.top/mla/</link>
    <pubDate>Fri, 11 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mla/</guid>
    <description><![CDATA[<p>前置阅读：<a href="MHA.md">MHA</a> 和 <a
href="../rope.md">rope</a></p>]]></description>
</item>
</channel>
</rss>
