<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>文献 - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/%E6%96%87%E7%8C%AE/</link>
        <description>文献 - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/%E6%96%87%E7%8C%AE/" rel="self" type="application/rss+xml" /><item>
    <title>Data Engineering for Scaling Language Models to 128K Context</title>
    <link>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</link>
    <pubDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以专家的身份，与您一同深入探讨这篇在长上下文（Long
Context）领域具有重要影响力的论文——《Data Engineering for Scaling
Language Models to 128K Context》。</p>]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward
Network,
FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：<strong>FFN层在功能上等同于一个键值记忆（Key-Value
Memory）系统</strong>。</p>]]></description>
</item>
</channel>
</rss>
