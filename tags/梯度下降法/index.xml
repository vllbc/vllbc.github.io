<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>梯度下降法 - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
        <description>梯度下降法 - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 04 Jan 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" rel="self" type="application/rss+xml" /><item>
    <title>梯度下降法</title>
    <link>https://blog.vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
    <pubDate>Tue, 04 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
    <description><![CDATA[<h1 id="梯度下降法">梯度下降法</h1>
<h2 id="简介">简介</h2>
<p></p>
<h2 id="批度梯度下降">批度梯度下降</h2>
<p>其实就是一次将整个数据集进行梯度下降的迭代 
## 随机梯度下降
就是对样本进行循环，每循环一个样本就更新一次参数，但是不容易收敛 </p>
<h2 id="小批量梯度下降">小批量梯度下降</h2>
<p>大多数用于深度学习的梯度下降算法介于以上两者之间，<strong>使用一个以上而又不是全部的训练样本</strong>。传统上，这些会被称为小批量(mini-batch)或小批量随机(mini-batch
stochastic)方法，现在通常将它们简单地成为随机(stochastic)方法。对于<strong>深度学习</strong>模型而言，人们所说的“<strong>随机梯度下降,
SGD</strong>”，其实就是基于<strong>小批量（mini-batch）的随机梯度下降</strong>。</p>]]></description>
</item>
</channel>
</rss>
