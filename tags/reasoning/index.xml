<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reasoning - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/reasoning/</link>
        <description>Reasoning - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/reasoning/" rel="self" type="application/rss+xml" /><item>
    <title>Towards Effective Code-Integrated Reasoning</title>
    <link>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇名为《Towards
Effective Code-Integrated Reasoning》的论文
(arXiv:2505.24480v1)。这篇论文系统性地探讨了一个在当前大模型研究中至关重要的前沿方向：如何让模型更稳定、更有效地利用外部工具（特别是代码解释器）来完成复杂的推理任务。</p>]]></description>
</item>
<item>
    <title>BRiTE：Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning</title>
    <link>https://blog.vllbc.top/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇富有洞见的论文《BRITE:
Bootstrapping Reinforced Thinking Process to Enhance Language Model
Reasoning》。这篇论文确实触及了当前大模型领域一个核心且棘手的问题：如何让模型不仅能生成流畅的文本，更能进行可靠、严谨的逻辑推理。</p>]]></description>
</item>
<item>
    <title>PROCESS REINFORCEMENT THROUGH IMPLICIT REWARDS</title>
    <link>https://blog.vllbc.top/process-reinforcement-through-implicit-rewards/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/process-reinforcement-through-implicit-rewards/</guid>
    <description><![CDATA[<h3 id="论文深入解读">论文深入解读</h3>
<p>这篇名为《Process Reinforcement through Implicit
Rewards》(通过隐式奖励进行过程强化)
的论文，由来自清华大学、上海人工智能实验室、UIUC
等顶尖机构的研究者共同完成，为大语言模型（LLM）的强化学习（RL）领域带来了一个极具价值和创新性的解决方案——<strong>PRIME</strong>
框架。其核心贡献在于，它成功地将<strong>过程监督 (Process
Supervision)</strong> 的高效率与<strong>结果监督 (Outcome
Supervision)</strong>
的低成本相结合，解决了在复杂推理任务（如数学和编程）中应用强化学习时面临的关键瓶颈。</p>]]></description>
</item>
<item>
    <title>First Return, Entropy-Eliciting Explore</title>
    <link>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，非常荣幸能与您一同深入探讨这篇富有启发性的论文——《First
Return, Entropy-Eliciting Explore》。这篇由字节跳动、M-A-P
及曼彻斯特大学的研究者们共同完成的工作，直面了当前大模型在复杂推理任务中通过强化学习进行优化时的一个核心痛点。</p>]]></description>
</item>
<item>
    <title>entropy(reasoning)</title>
    <link>https://blog.vllbc.top/entropyreasoning/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/entropyreasoning/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>思维链压缩</title>
    <link>https://blog.vllbc.top/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>MCTS和PRM</title>
    <link>https://blog.vllbc.top/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[<h2 id="核心总结">核心总结</h2>
<ul>
<li><strong>PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果</strong>。例如，
<ul>
<li>单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。</li>
<li>单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。</li>
</ul></li>
<li><strong>PRM 和 MCTS
既可以应用于优化训练数据，也可以用来预测用</strong>
<ul>
<li>用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward
model训练数据。</li>
<li>用于推理：很简单，推理用MCTS的方式把 test-scaling
做上来，再结合PRM的方式从众多路径中挑选最佳答案。</li>
</ul></li>
<li><strong>PRM和MCTS的缺点</strong><br />
这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。</li>
<li>Process Reward Model(PRM) 在实际应用中有三大局限：
<ul>
<li>第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。</li>
<li>第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。</li>
<li>第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy
model，但反而更多时间去优化 reward model 去了。</li>
</ul></li>
<li>对MCTS的看法：
<ul>
<li>文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。</li>
<li>MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。</li>
</ul></li>
</ul>
<h2 id="参考">参考</h2>
<p>https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs Can
Master Math Reasoning with Self-Evolved Deep Thinking</p>]]></description>
</item>
</channel>
</rss>
