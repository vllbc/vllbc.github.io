<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>hinge loss - 标签 - vllbc02</title>
        <link>https://vllbc.top/tags/hinge-loss/</link>
        <description>hinge loss - 标签 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/hinge-loss/" rel="self" type="application/rss+xml" /><item>
    <title>hinge loss</title>
    <link>https://vllbc.top/hinge-loss/</link>
    <pubDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/hinge-loss/</guid>
    <description><![CDATA[在机器学习中，hinge loss是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为： 其中 \(\hat{y}\) 表]]></description>
</item>
</channel>
</rss>
