<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Ensemble Learning - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/ensemble-learning/</link>
        <description>Ensemble Learning - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 09 Jul 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/ensemble-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Ensemble Learning</title>
    <link>https://blog.vllbc.top/ensemble-learning/</link>
    <pubDate>Sat, 09 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ensemble-learning/</guid>
    <description><![CDATA[<h1 id="集成学习">集成学习</h1>
<p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。
集成学习在各个规模的数据集上都有很好的策略。
数据集大：划分成多个小数据集，学习多个模型进行组合
数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</p>]]></description>
</item>
</channel>
</rss>
