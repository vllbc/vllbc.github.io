<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Attention - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/attention/</link>
        <description>Attention - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/attention/" rel="self" type="application/rss+xml" /><item>
    <title>MHA</title>
    <link>https://blog.vllbc.top/mha/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mha/</guid>
    <description><![CDATA[<h2 id="self-attention">Self-attention</h2>
<p>首先介绍一下最主要的 self-attention，可以说是 self-attention
实现了上述的 token 之间交互的功能。</p>
<p>自注意力是模型的关键组成部分之一。注意和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。</p>]]></description>
</item>
<item>
    <title>flash attention</title>
    <link>https://blog.vllbc.top/flash-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/flash-attention/</guid>
    <description><![CDATA[<p>Safe softmax 并没有 1-pass 算法，那么 Attention
会不会有呢？有！这就是 FlashAttention！</p>
<p>在使用 online attention 的情况下，从头开始计算 attention score
的过程如下： <span
class="math inline">\(\operatorname{NOTATIONS}\)</span></p>
<p><span class="math inline">\(Q[k,:]:\)</span> the <span
class="math inline">\(k\)</span> -th row vector of <span
class="math inline">\(Q\)</span> matrix. <span
class="math inline">\(\begin{aligned}O[k,:]:\mathrm{~the~}k\text{-th row
of output }O\mathrm{~matrix.}\\\mathbf{V}[i,i]:\mathrm{~the~}k\text{-th
row of output }O\mathrm{~matrix.}\end{aligned}\)</span> <span
class="math inline">\(V[i,:]{:\text{ the }i\text{-th row of }V\text{
matrix}}.\)</span> <span
class="math inline">\(\{\boldsymbol{o}_i\}{:}\sum_{j=1}^ia_jV[j,:]\)</span>,
a row vector storing partial aggregation result <span
class="math inline">\(A[k,:i]\times V[:i,:]\)</span> BODY</p>]]></description>
</item>
<item>
    <title>GQA</title>
    <link>https://blog.vllbc.top/gqa/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gqa/</guid>
    <description><![CDATA[<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>如上图所示，GQA 就是在 MHA 和 MQA 之间做了一个平衡。对 query heads
进行分组，分成几组就对应多少个 kv heads，然后每一组内的 query Heads
共享相同的 KV head。 GQA 可以在减少计算量和 KV Cache
同时确保模型效果不受到大的影响。</p>]]></description>
</item>
<item>
    <title>online attention</title>
    <link>https://blog.vllbc.top/online-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/online-attention/</guid>
    <description><![CDATA[<h3 id="pass">3-pass</h3>
<p><span class="math inline">\(\mathsf{NO}\)</span> TATIONS</p>
<p><span
class="math inline">\(\{m_i\}{:}\max_{j=1}^i\left\{x_j\right\}\)</span>,
with initial value <span class="math inline">\(m_0=-\infty.\)</span>
<span class="math inline">\(\{d_i\}{:}\sum_{j=1}^ie^{x_j-m_N}\)</span>,
with initial value <span class="math inline">\(d_0=0,d_N\)</span> is the
denominator of safe softmax. <span class="math inline">\(\{a_i\}{:\text{
the final softmax value}}.\)</span></p>
<p>BODY <span class="math inline">\(\textbf{for }i\leftarrow 1,
N\textbf{ do}\)</span> <span
class="math display">\[m_i\leftarrow\max\left(m_{i-1},x_i\right)\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span class="math display">\[d_i\leftarrow
d_{i-1}+e^{x_i-m_N}\]</span> <span
class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span
class="math display">\[a_i\leftarrow\frac{e^{x_i-m_N}}{d_N}\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p>这是 3 step 计算 attention
的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于 sram
中没有足够的存储空间，因此需要多次访存。 ### Online attention <span
class="math display">\[\begin{aligned}
d_i^{\prime}&amp; =\sum_{j=1}^ie^{x_j-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1} e^{x_j-m_i}\right)+e^{x_i-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1}
e^{x_j-m_{i-1}}\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
&amp;= d_{i-1}&#39; e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}\]</span> 找到迭代式之后就可以从 3 step 降到 2 step <span
class="math display">\[\begin{aligned}&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;m_i&amp;&amp;\leftarrow&amp;\max\left(m_{i-1},x_i\right)\\&amp;&amp;&amp;d_i^{\prime}&amp;&amp;\leftarrow&amp;d_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\&amp;\mathbf{end}\\&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;a_i\leftarrow&amp;&amp;\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&amp;\mathbf{end}\end{aligned}\]</span>
好像 FLOPs
计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的 scale</p>]]></description>
</item>
<item>
    <title>paged attention</title>
    <link>https://blog.vllbc.top/paged-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/paged-attention/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/691038809">#
图解大模型计算加速系列之：vLLM核心技术PagedAttention原理</a></p>]]></description>
</item>
<item>
    <title>MLA</title>
    <link>https://blog.vllbc.top/mla/</link>
    <pubDate>Fri, 11 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mla/</guid>
    <description><![CDATA[<p>前置阅读：<a href="MHA.md">MHA</a> 、<a href="MQA.md">MQA</a>、<a
href="GQA.md">GQA</a> 和 <a href="../basic/rope.md">rope</a></p>
<p>在标准的 Transformer中，多头注意力（Multi-Head Attention,
MHA）机制通过并行计算多个注意力头来捕捉输入序列中的不同特征。每个注意力头都有自己的查询（Query,
Q）、键（Key, K）和值（Value, V）矩阵，计算过程如下：</p>]]></description>
</item>
<item>
    <title>Attention</title>
    <link>https://blog.vllbc.top/attention/</link>
    <pubDate>Tue, 23 Feb 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/attention/</guid>
    <description><![CDATA[<h1 id="seq2seq中的attention">Seq2Seq中的Attention</h1>
<h2 id="缺陷">缺陷</h2>
<p>在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。</p>]]></description>
</item>
</channel>
</rss>
