<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Attention - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/attention/</link>
        <description>Attention - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 23 Feb 2021 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/attention/" rel="self" type="application/rss+xml" /><item>
    <title>Attention</title>
    <link>https://blog.vllbc.top/attention/</link>
    <pubDate>Tue, 23 Feb 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/attention/</guid>
    <description><![CDATA[<h1 id="seq2seq中的attention">Seq2Seq中的Attention</h1>
<h2 id="缺陷">缺陷</h2>
<p>在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。</p>]]></description>
</item>
</channel>
</rss>
