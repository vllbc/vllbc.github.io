<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Model - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/model/</link>
        <description>Model - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 08 Aug 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/model/" rel="self" type="application/rss+xml" /><item>
    <title>qwen</title>
    <link>https://blog.vllbc.top/qwen/</link>
    <pubDate>Fri, 08 Aug 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/qwen/</guid>
    <description><![CDATA[<pre class="python3"><code>------------------------------------------------------------------------------------------------------
# Qwen-1
+ Embedding and output projection. (Untied embedding for input embedding and output projection)
+ ROPE
+ QKV bias required
+ Pre-Norm &amp; RMSNorm
+ SwiGLU

------------------------------------------------------------------------------------------------------
# Qwen-2
- Multi-Head Attention

+ MoE
+ Grouped Query Attention
+ Dual Chunk Attention
+ YARN
+ Expert Granularity
+ Expert Routing
+ Expert Initialization
+ Shared Experts

------------------------------------------------------------------------------------------------------
# Qwen-2.5
+ More control tokens. 3 -&gt; 22

------------------------------------------------------------------------------------------------------
# Qwen-3
- QKV bias
- Shared experts

+ QK-Norm
------------------------------------------------------------------------------------------------------</code></pre>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/1933558376689804480">Qwen
各版本主要结构变化</a></p>]]></description>
</item>
<item>
    <title>K2</title>
    <link>https://blog.vllbc.top/k2/</link>
    <pubDate>Wed, 30 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/k2/</guid>
    <description><![CDATA[<h2 id="预训练">预训练</h2>
<h3 id="muon-clip">Muon-clip</h3>
<p>详见<a href="../../Deep%20Learning/优化器/Muon.md">Muon</a></p>
<p>通过裁剪权重解决Max-Logit问题，实现稳定训练。</p>
<h3 id="数据增强">数据增强</h3>
<p>通过改写句子来提高token效率，避免重复使用token造成的过拟合，改写方法如下：</p>]]></description>
</item>
<item>
    <title>Gemma</title>
    <link>https://blog.vllbc.top/gemma/</link>
    <pubDate>Tue, 29 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gemma/</guid>
    <description><![CDATA[<h2 id="gemma-3">Gemma 3</h2>
<h3 id="qk-norm">QK-Norm</h3>
<p>简单来说就是在Q和K矩阵上进行RMSNorm，即：</p>
<p><span class="math display">\[
\begin{aligned}
O &amp;= softmax(\bar{Q}\bar{K}^T)V \\
\bar{Q} &amp;=RMSNorm(Q) \\
\bar{K} &amp;=RMSNorm(V)
\end{aligned}\
\]</span></p>
<p>但这种方法的问题是不适用MLA的推理阶段，因为推理阶段的MLA将Wk吸取到了Q中，具体见<a
href="../Attention/MLA.md">MLA</a></p>]]></description>
</item>
</channel>
</rss>
