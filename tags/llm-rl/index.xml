<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM-RL - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/llm-rl/</link>
        <description>LLM-RL - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/llm-rl/" rel="self" type="application/rss+xml" /><item>
    <title>Group Sequence Policy Optimization</title>
    <link>https://blog.vllbc.top/group-sequence-policy-optimization/</link>
    <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/group-sequence-policy-optimization/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献是提出了一种名为 <strong>组序列策略优化 (Group
Sequence Policy Optimization, GSPO)</strong>
的新型强化学习（RL）算法，旨在解决在训练大型语言模型（特别是<strong>混合专家模型,
Mixture-of-Experts,
MoE</strong>）时普遍存在的训练不稳定甚至模型崩溃的痛点。这不仅仅是一次微小的算法改进，而是一次对现有主流RL优化范式的根本性反思与重构，其核心思想是
<strong>“将优化的基本单元与奖励的基本单元对齐”</strong>。</p>]]></description>
</item>
</channel>
</rss>
