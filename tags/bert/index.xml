<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>BERT - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/bert/</link>
        <description>BERT - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 08 Apr 2021 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/bert/" rel="self" type="application/rss+xml" /><item>
    <title>BERT</title>
    <link>https://blog.vllbc.top/bert/</link>
    <pubDate>Thu, 08 Apr 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/bert/</guid>
    <description><![CDATA[<h1 id="bert">Bert</h1>
<p>BERT 的模型架构非常简单，你已经知道它是如何工作的：它只是 Transformer
的编码器。新的是训练目标和 BERT 用于下游任务的方式。</p>
<p>我们如何使用纯文本训练（双向）编码器？我们只知道从左到右的语言建模目标，但它仅适用于每个标记只能使用以前的标记（并且看不到未来）的解码器。BERT
的作者提出了其他未标记数据的训练目标。在讨论它们之前，让我们先看看 BERT
作为 Transformer 编码器的输入。</p>]]></description>
</item>
</channel>
</rss>
