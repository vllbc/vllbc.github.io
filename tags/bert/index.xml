<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>bert - æ ‡ç­¾ - vllbc02</title>
        <link>https://vllbc.top/tags/bert/</link>
        <description>bert - æ ‡ç­¾ - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 17 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/bert/" rel="self" type="application/rss+xml" /><item>
    <title>bert</title>
    <link>https://vllbc.top/bert/</link>
    <pubDate>Fri, 17 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/bert/</guid>
    <description><![CDATA[bert ðŸ’¡ Meta Data Title BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Journal () Authors Pub.date 2019-05-24 ðŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®çš„ ðŸ“Š ç ”ç©¶å†…å®¹ ðŸš© ç ”ç©¶ç»“è®º ðŸ“Œ æ„Ÿæƒ³ &amp; ç–‘é—® ðŸ”¬ ç†è®ºæŽ¨å¯¼]]></description>
</item>
<item>
    <title>BERT</title>
    <link>https://vllbc.top/bert/</link>
    <pubDate>Thu, 08 Apr 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/bert/</guid>
    <description><![CDATA[Bert BERT çš„æ¨¡åž‹æž¶æž„éžå¸¸ç®€å•ï¼Œä½ å·²ç»çŸ¥é“å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šå®ƒåªæ˜¯ Transformer çš„ç¼–ç å™¨ã€‚æ–°çš„æ˜¯è®­ç»ƒç›®æ ‡å’Œ BERT ç”¨äºŽä¸‹æ¸¸ä»»åŠ¡çš„æ–¹å¼ã€‚ æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨çº¯æ–‡æœ¬è®­ç»ƒï¼ˆåŒå‘ï¼‰ç¼–ç ]]></description>
</item>
</channel>
</rss>
