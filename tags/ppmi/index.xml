<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>PPMI - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/ppmi/</link>
        <description>PPMI - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 22 Feb 2021 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/ppmi/" rel="self" type="application/rss+xml" /><item>
    <title>PPMI</title>
    <link>https://blog.vllbc.top/ppmi/</link>
    <pubDate>Mon, 22 Feb 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ppmi/</guid>
    <description><![CDATA[<h2 id="pmi">PMI</h2>
<p>点互信息 对于两个单词之间的PMI来说，可以这样计算：</p>
<p><span class="math display">\[
PMI(w,c) = \log \frac{p(w,c)}{p(w)p(c)} = \log \frac{N(w,c)
|w,c|}{N(w)N(c)}
\]</span> ## MI 在概率论和信息论中，两个随机变量的互信息（Mutual
Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布
p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息(Mutual
Information)是度量两个事件集合之间的相关性(mutual
dependence)。互信息最常用的单位是bit。</p>]]></description>
</item>
</channel>
</rss>
