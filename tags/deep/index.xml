<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/deep/</link>
        <description>Deep - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/deep/" rel="self" type="application/rss+xml" /><item>
    <title>Layer Norm</title>
    <link>https://blog.vllbc.top/layer-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/layer-norm/</guid>
    <description><![CDATA[<h2 id="pre-norm">pre-norm</h2>
<p>Pre-norm:<span
class="math inline">\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\)</span></p>
<p><span class="math inline">\(先来看Pre-norm^{+},递归展开：\)</span>
<span class="math display">\[X_{t+1}=X_t+F_t(Norm(X_t))\]</span> <span
class="math inline">\(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\)</span>
其中，展开<span class="math inline">\(^{+}\)</span>后的每一项( <span
class="math inline">\(F_{1}( Norm( X_{1}) ) , \ldots\)</span>, <span
class="math inline">\(F_{t- 1}( Norm( X_{t- 1}) )\)</span>, <span
class="math inline">\(F_{t}( Norm( X_{t})
)\)</span>)之间都是同一量级的， 所以<span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\)</span>和 <span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))\)</span>之间的区别就像t和t-1的区别一样，我们可以将
其记为<span class="math inline">\(X_t+ 1= \mathscr{O} ( t+ 1)\)</span> .
这种特性就导致当t足够大的时候，<span
class="math inline">\(X_{t+1}\)</span>和<span
class="math inline">\(X_t\)</span>之间区别可以忽略不计（直觉上），那么就有：</p>]]></description>
</item>
<item>
    <title>对抗训练</title>
    <link>https://blog.vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[<h1 id="min-max公式">Min-Max公式</h1>
<p>$$ <em>{} </em>{(x,y) }U[<em>{r</em>{adv}}L(,x+r_{adv},y)]</p>
<p>$$</p>
<ol type="1">
<li>内部max是为了找到worst-case的扰动，也就是攻击，其中， <span
class="math inline">\(L\)</span>为损失函数， <span
class="math inline">\(\mathbb{S}\)</span> 为扰动的范围空间。</li>
<li>外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 <span
class="math inline">\(\mathbb{D}\)</span> 是输入样本的分布。
简单理解就是<strong>在输入上进行梯度上升(增大loss)，在参数上进行梯度下降(减小loss)</strong></li>
</ol>
<h1 id="加入扰动后的损失函数">加入扰动后的损失函数</h1>
<p>$$ <em>{} -P(y |x+r</em>{adv};)</p>]]></description>
</item>
<item>
    <title>Adam算法</title>
    <link>https://blog.vllbc.top/adam/</link>
    <pubDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adam/</guid>
    <description><![CDATA[<h2 id="moment矩">moment(矩)</h2>
<p>矩在数学中的定义，一阶矩(first moment)就是样本的均值(mean),
二阶矩就是方差（variance）。 ## 滑动平均 滑动平均(exponential moving
average)，或者叫做指数加权平均(exponentially weighted moving
average)，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。在时间序列预测中也常用。</p>]]></description>
</item>
</channel>
</rss>
