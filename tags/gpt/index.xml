<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>GPT - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/gpt/</link>
        <description>GPT - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 27 Jul 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/gpt/" rel="self" type="application/rss+xml" /><item>
    <title>GPT</title>
    <link>https://blog.vllbc.top/gpt/</link>
    <pubDate>Wed, 27 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gpt/</guid>
    <description><![CDATA[<h1 id="gpt">GPT</h1>
<h2 id="预训练从左到右的-transformer-语言模型">预训练(从左到右的
Transformer 语言模型)</h2>
<p>GPT 是一种基于 Transformer 的从左到右的语言模型。该架构是一个 12 层的
Transformer 解码器（没有解码器-编码器）。 ## 模型架构 </p>
<p>就是12层的transformer-decoder。其中只使用了transformer模型中的decoder部分，并且把decoder里面的encoder-decoder
attention部分去掉了，只保留了masked
self-attention，再加上feed-forward部分。再提一句，masked
self-attention保证了GPT模型是一个单向的语言模型。 另外，作者在position
encoding上做了调整，使用了可学习的位置编码，不同于transformer的三角函数位置编码。</p>]]></description>
</item>
</channel>
</rss>
