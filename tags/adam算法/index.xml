<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Adam算法 - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/adam%E7%AE%97%E6%B3%95/</link>
        <description>Adam算法 - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/adam%E7%AE%97%E6%B3%95/" rel="self" type="application/rss+xml" /><item>
    <title>Adam算法</title>
    <link>https://blog.vllbc.top/adam/</link>
    <pubDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adam/</guid>
    <description><![CDATA[<h1 id="adam算法">Adam算法</h1>
<h2 id="背景">背景</h2>
<p>作为机器学习的初学者必然会接触梯度下降算法以及SGD，基本上形式如下：</p>
<p><span class="math display">\[
\theta_t = \theta_{t-1} - \alpha \;g(\theta)
\]</span> 其中<span class="math inline">\(\alpha\)</span>为学习率，<span
class="math inline">\(g(\theta)\)</span>为梯度。</p>]]></description>
</item>
</channel>
</rss>
