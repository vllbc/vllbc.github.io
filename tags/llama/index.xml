<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Llama - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/llama/</link>
        <description>Llama - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/llama/" rel="self" type="application/rss+xml" /><item>
    <title>llama系列</title>
    <link>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</link>
    <pubDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</guid>
    <description><![CDATA[<h1 id="llama介绍">LLaMA介绍</h1>
<p>LLaMA 是目前为止，效果最好的开源 LLM 之一。</p>
<blockquote>
<p><strong>论文的核心思想：相比于GPT，更小的模型+更多的训练数据</strong>**也可以获得可比的效果</p>
</blockquote>
<p>基于更多 tokens
的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为
<code>LLaMA</code>，参数范围从 7B 到 65B 不等，与现有最佳 LLM
相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于
GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM
的使用和研究平民化，因为它可以在单个 GPU
上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如
Chinchilla 或 PaLM-540B）相竞争的能力。</p>]]></description>
</item>
</channel>
</rss>
