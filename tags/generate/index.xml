<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Generate - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/generate/</link>
        <description>Generate - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 05 Sep 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/generate/" rel="self" type="application/rss+xml" /><item>
    <title>frequency_penalty&amp;presence_penalty</title>
    <link>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</link>
    <pubDate>Thu, 05 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</guid>
    <description><![CDATA[<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>LLM解码时采用的自回归采样，其过程如下：</p>
<ol type="1">
<li>小模型使用前缀作为输入，将输出结果处理+归一化成<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83&amp;zhida_source=entity">概率分布</a>后，采样生成下一个token。</li>
<li>将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。</li>
</ol>
<p>将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=top-k&amp;zhida_source=entity">top-k</a>和top-n等
# 贪心搜索
直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复
# 随机采样
按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
# beam search 维护一个大小为 k
的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k
个单词，然后保留总概率最高的 k
个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。
# top-k </p>]]></description>
</item>
</channel>
</rss>
