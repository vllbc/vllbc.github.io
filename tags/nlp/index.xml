<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - 标签 - vllbc02</title>
        <link>https://vllbc.top/tags/nlp/</link>
        <description>NLP - 标签 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>ICL</title>
    <link>https://vllbc.top/icl/</link>
    <pubDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/icl/</guid>
    <description><![CDATA[ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot learning，允许输入数条示例和一则任务说明； - One-shot learnin]]></description>
</item>
<item>
    <title>T5</title>
    <link>https://vllbc.top/t5/</link>
    <pubDate>Sat, 09 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/t5/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>继续预训练</title>
    <link>https://vllbc.top/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Fri, 12 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[领域自适应之继续预训练]]></description>
</item>
<item>
    <title>Prompt</title>
    <link>https://vllbc.top/prompt/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/prompt/</guid>
    <description><![CDATA[参考 NLP新宠——浅谈Prompt的前世今生 - 知乎 (zhihu.com)]]></description>
</item>
<item>
    <title>seq2seq</title>
    <link>https://vllbc.top/seq2seq/</link>
    <pubDate>Wed, 09 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/seq2seq/</guid>
    <description><![CDATA[Seq2Seq （本文只介绍最原始的seq2seq，带有注意力在attention文章中） RNN 有关RNN Seq2Seq是典型的Encoder-decoder]]></description>
</item>
<item>
    <title>tokenization</title>
    <link>https://vllbc.top/tokenization/</link>
    <pubDate>Mon, 17 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/tokenization/</guid>
    <description><![CDATA[Tokenization技术 本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子]]></description>
</item>
<item>
    <title>CoVe</title>
    <link>https://vllbc.top/cove/</link>
    <pubDate>Sun, 09 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/cove/</guid>
    <description><![CDATA[CoVe Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器， 模型训练 主要假设是，为了翻译一个]]></description>
</item>
<item>
    <title>Quick-Thought</title>
    <link>https://vllbc.top/quick-thought/</link>
    <pubDate>Mon, 03 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/quick-thought/</guid>
    <description><![CDATA[这是一种句向量的表示方式，即sentence2vec，实际上是对skip thought的改进，]]></description>
</item>
<item>
    <title>BM25</title>
    <link>https://vllbc.top/bm25/</link>
    <pubDate>Sun, 07 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/bm25/</guid>
    <description><![CDATA[BM25算法 BM25算法，通常用来作搜索相关性平分。一句话概况其主要思想：对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，]]></description>
</item>
<item>
    <title>GPT</title>
    <link>https://vllbc.top/gpt/</link>
    <pubDate>Wed, 27 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/gpt/</guid>
    <description><![CDATA[GPT 预训练(从左到右的 Transformer 语言模型) GPT 是一种基于 Transformer 的从左到右的语言模型。该架构是一个 12 层的 Transformer 解码器（没有解码器-编码器）。 ## 模型架构 就是12层的]]></description>
</item>
</channel>
</rss>
