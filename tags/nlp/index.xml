<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/nlp/</link>
        <description>NLP - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 18 May 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>RLHF</title>
    <link>https://blog.vllbc.top/rlhf/</link>
    <pubDate>Sun, 18 May 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rlhf/</guid>
    <description><![CDATA[<p>基础部分看猛猿大佬的<a
href="https://zhuanlan.zhihu.com/p/7461863937">人人都能看懂的RL-PPO理论知识</a>即可，通俗易懂，我写不出来比这个更好的了。本文是各RL算法笔记。</p>
<h1 id="ppo-openrlhf库">PPO (openrlhf库)</h1>
<p>重点记录一下experience的采集过程。训练其实很简单。actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
<item>
    <title>MCTS和PRM</title>
    <link>https://blog.vllbc.top/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[<h2 id="核心总结">核心总结</h2>
<ul>
<li><strong>PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果</strong>。例如，
<ul>
<li>单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。</li>
<li>单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。</li>
</ul></li>
<li><strong>PRM 和 MCTS
既可以应用于优化训练数据，也可以用来预测用</strong>
<ul>
<li>用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward
model训练数据。</li>
<li>用于推理：很简单，推理用MCTS的方式把 test-scaling
做上来，再结合PRM的方式从众多路径中挑选最佳答案。</li>
</ul></li>
<li><strong>PRM和MCTS的缺点</strong><br />
这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。</li>
<li>Process Reward Model(PRM) 在实际应用中有三大局限：
<ul>
<li>第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。</li>
<li>第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。</li>
<li>第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy
model，但反而更多时间去优化 reward model 去了。</li>
</ul></li>
<li>对MCTS的看法：
<ul>
<li>文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。</li>
<li>MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。</li>
</ul></li>
</ul>
<h2 id="参考">参考</h2>
<p>https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs Can
Master Math Reasoning with Self-Evolved Deep Thinking</p>]]></description>
</item>
<item>
    <title>llama系列</title>
    <link>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</link>
    <pubDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</guid>
    <description><![CDATA[<h1 id="llama介绍">LLaMA介绍</h1>
<p>LLaMA 是目前为止，效果最好的开源 LLM 之一。</p>
<blockquote>
<p><strong>论文的核心思想：相比于GPT，更小的模型+更多的训练数据</strong>**也可以获得可比的效果</p>
</blockquote>
<p>基于更多 tokens
的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为
<code>LLaMA</code>，参数范围从 7B 到 65B 不等，与现有最佳 LLM
相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于
GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM
的使用和研究平民化，因为它可以在单个 GPU
上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如
Chinchilla 或 PaLM-540B）相竞争的能力。</p>]]></description>
</item>
<item>
    <title>ICL</title>
    <link>https://blog.vllbc.top/icl/</link>
    <pubDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/icl/</guid>
    <description><![CDATA[<p>ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot
learning，允许输入数条示例和一则任务说明； - One-shot
learning，只允许输入一条示例和一则任务说明； - Zero-shot
learning，不允许输入任何示例，只允许输入一则任务说明。 </p>]]></description>
</item>
<item>
    <title>T5</title>
    <link>https://blog.vllbc.top/t5/</link>
    <pubDate>Sat, 09 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/t5/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>继续预训练</title>
    <link>https://blog.vllbc.top/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Fri, 12 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[<h1 id="领域自适应之继续预训练">领域自适应之继续预训练</h1>]]></description>
</item>
<item>
    <title>Prompt</title>
    <link>https://blog.vllbc.top/prompt/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/prompt/</guid>
    <description><![CDATA[<h1 id="参考">参考</h1>
<p><a
href="https://zhuanlan.zhihu.com/p/399295895">NLP新宠——浅谈Prompt的前世今生
- 知乎 (zhihu.com)</a></p>]]></description>
</item>
<item>
    <title>seq2seq</title>
    <link>https://blog.vllbc.top/seq2seq/</link>
    <pubDate>Wed, 09 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/seq2seq/</guid>
    <description><![CDATA[<h1 id="seq2seq">Seq2Seq</h1>
<p>（本文只介绍最原始的seq2seq，带有注意力在attention文章中）</p>
<h2 id="rnn">RNN</h2>
<p>有关RNN</p>
<p>Seq2Seq是典型的Encoder-decoder框架的模型，其中编码器和解码器都采用的RNN模型或者RNN模型的变体：GRU、LSTM等。</p>]]></description>
</item>
<item>
    <title>tokenization</title>
    <link>https://blog.vllbc.top/tokenization/</link>
    <pubDate>Mon, 17 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/tokenization/</guid>
    <description><![CDATA[<h1 id="tokenization技术">Tokenization技术</h1>
<p>本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子中经常会有此类问题，并且都是外国学校的，说明外国学校还是比较注重这一块的基础的。
首先明确一个概念：token可以理解为一个符号，就代表一个语言单位，tokenize的意思就是把一个句子或语料分成token.</p>]]></description>
</item>
<item>
    <title>CoVe</title>
    <link>https://blog.vllbc.top/cove/</link>
    <pubDate>Sun, 09 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/cove/</guid>
    <description><![CDATA[<h1 id="cove">CoVe</h1>
<p>Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器，</p>
<h2 id="模型训练">模型训练</h2>
<p>主要假设是，为了翻译一个句子，NMT编码器学会理解句子。
因此来自编码器的向量包含有关单词上下文的信息。</p>]]></description>
</item>
</channel>
</rss>
