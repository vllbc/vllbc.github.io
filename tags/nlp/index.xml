<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - 标签 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/tags/nlp/</link>
        <description>NLP - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>MCTS和PRM</title>
    <link>http://localhost:1313/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[核心总结 PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果。例如， 单独使用PRM：我]]></description>
</item>
<item>
    <title>llama系列</title>
    <link>http://localhost:1313/llama%E7%B3%BB%E5%88%97/</link>
    <pubDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/llama%E7%B3%BB%E5%88%97/</guid>
    <description><![CDATA[LLaMA介绍 LLaMA 是目前为止，效果最好的开源 LLM 之一。 论文的核心思想：相比于GPT，更小的模型+更多的训练数据**也可以获得可比的效果 基于更多 tokens]]></description>
</item>
<item>
    <title>ICL</title>
    <link>http://localhost:1313/icl/</link>
    <pubDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/icl/</guid>
    <description><![CDATA[ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot learning，允许输入数条示例和一则任务说明； - One-shot learnin]]></description>
</item>
<item>
    <title>T5</title>
    <link>http://localhost:1313/t5/</link>
    <pubDate>Sat, 09 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/t5/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>继续预训练</title>
    <link>http://localhost:1313/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Fri, 12 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[领域自适应之继续预训练]]></description>
</item>
<item>
    <title>Prompt</title>
    <link>http://localhost:1313/prompt/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/prompt/</guid>
    <description><![CDATA[参考 NLP新宠——浅谈Prompt的前世今生 - 知乎 (zhihu.com)]]></description>
</item>
<item>
    <title>seq2seq</title>
    <link>http://localhost:1313/seq2seq/</link>
    <pubDate>Wed, 09 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/seq2seq/</guid>
    <description><![CDATA[Seq2Seq （本文只介绍最原始的seq2seq，带有注意力在attention文章中） RNN 有关RNN Seq2Seq是典型的Encoder-decoder]]></description>
</item>
<item>
    <title>tokenization</title>
    <link>http://localhost:1313/tokenization/</link>
    <pubDate>Mon, 17 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/tokenization/</guid>
    <description><![CDATA[Tokenization技术 本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子]]></description>
</item>
<item>
    <title>CoVe</title>
    <link>http://localhost:1313/cove/</link>
    <pubDate>Sun, 09 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/cove/</guid>
    <description><![CDATA[CoVe Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器， 模型训练 主要假设是，为了翻译一个]]></description>
</item>
<item>
    <title>Quick-Thought</title>
    <link>http://localhost:1313/quick-thought/</link>
    <pubDate>Mon, 03 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/quick-thought/</guid>
    <description><![CDATA[这是一种句向量的表示方式，即sentence2vec，实际上是对skip thought的改进，]]></description>
</item>
</channel>
</rss>
