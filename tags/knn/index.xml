<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>KNN - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/knn/</link>
        <description>KNN - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 25 Jun 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/knn/" rel="self" type="application/rss+xml" /><item>
    <title>KNN</title>
    <link>https://blog.vllbc.top/knn/</link>
    <pubDate>Sat, 25 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/knn/</guid>
    <description><![CDATA[<h1 id="knn">KNN</h1>
<p>参考：<a
href="https://cuijiahua.com/blog/2017/11/ml_1_knn.html">https://cuijiahua.com/blog/2017/11/ml_1_knn.html</a></p>
<p>《统计学习方法》李航（kd树）</p>
<h2 id="简介">简介</h2>
<p>k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart
P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>]]></description>
</item>
</channel>
</rss>
