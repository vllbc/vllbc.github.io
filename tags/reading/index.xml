<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reading - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/reading/</link>
        <description>Reading - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/reading/" rel="self" type="application/rss+xml" /><item>
    <title>First Return, Entropy-Eliciting Explore</title>
    <link>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，非常荣幸能与您一同深入探讨这篇富有启发性的论文——《First
Return, Entropy-Eliciting Explore》。这篇由字节跳动、M-A-P
及曼彻斯特大学的研究者们共同完成的工作，直面了当前大模型在复杂推理任务中通过强化学习进行优化时的一个核心痛点。</p>]]></description>
</item>
<item>
    <title>Reinforcing General Reasoning without Verifiers</title>
    <link>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</guid>
    <description><![CDATA[<h3
id="一论文的研究目标与意义"><strong>一、论文的研究目标与意义</strong></h3>
<h4 id="研究目标与待解决问题"><strong>研究目标与待解决问题</strong></h4>
<p>论文的核心研究目标是：<strong>将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。</strong></p>]]></description>
</item>
<item>
    <title>GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS</title>
    <link>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇具有开创性意义的论文《Generalist
Reward Models: Found Inside Large Language
Models》。这篇论文提出了一种颠覆性的思想：我们或许不再需要耗费巨资去训练一个独立的奖励模型（Reward
Model），因为一个强大的通用奖励模型，早已“内生”于任何一个通过标准方式（next-token
prediction）训练的语言模型之中。</p>]]></description>
</item>
<item>
    <title>RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS</title>
    <link>https://blog.vllbc.top/rlprextrapolating-rlvr-to-general-domains-without-verifiers/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rlprextrapolating-rlvr-to-general-domains-without-verifiers/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇具有重要价值的论文《RLPR:
Extrapolating RLVR to General Domains without Verifiers》。</p>
<p>这篇论文的核心贡献在于，它巧妙地绕开了现有强化学习方法在提升大模型通用推理能力时遇到的一个核心瓶颈——<strong>验证器（Verifier）</strong>，从而为更广泛、更低成本地提升大模型能力开辟了一条新路径。</p>]]></description>
</item>
<item>
    <title>LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks</title>
    <link>https://blog.vllbc.top/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/</link>
    <pubDate>Mon, 07 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，非常荣幸能与您一同深入探讨这篇富有洞见的论文《PLAN-AND-ACT:
Improving Planning of Agents for Long-Horizon
Tasks》。这篇论文聚焦于提升大型语言模型（LLM）在执行复杂、长远任务时的规划能力，这是一个在构建真正自主
AI 智能体（Agent）过程中的核心挑战。</p>]]></description>
</item>
<item>
    <title>WEB AGENTS WITH WORLD MODELS ：LEARNING AND LEVERAGING ENVIRONMENT DYNAMICS IN WEB NAVIGATION</title>
    <link>https://blog.vllbc.top/web-agents-with-world-models-learning-and-leveraging-environment-dynamics-in-web-navigation/</link>
    <pubDate>Sat, 05 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/web-agents-with-world-models-learning-and-leveraging-environment-dynamics-in-web-navigation/</guid>
    <description><![CDATA[<p>这篇名为《Web Agents with World Models: Learning and Leveraging
Environment Dynamics in Web Navigation》（<a
href="https://arxiv.org/abs/2410.13232">https://arxiv.org/abs/2410.13232</a>）的论文，由延世大学的研究团队撰写，并计划在
ICLR 2025
会议上发表。它直面了当前大型语言模型（LLM）在构建自主网页代理（Web
Agents）时遇到的一个核心瓶颈：<strong>缺乏对环境动态的理解，即没有“世界模型”（World
Model）</strong>。</p>]]></description>
</item>
</channel>
</rss>
