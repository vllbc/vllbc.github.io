<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reading - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/reading/</link>
        <description>Reading - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/reading/" rel="self" type="application/rss+xml" /><item>
    <title>Can Language Models Serve as Text-Based World Simulators?</title>
    <link>https://blog.vllbc.top/can-language-models-serve-as-text-based-world-simulators/</link>
    <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/can-language-models-serve-as-text-based-world-simulators/</guid>
    <description><![CDATA[<p>这篇论文发表于2024年6月，来自亚利桑那大学、微软研究院、艾伦人工智能研究所等顶尖机构，是一项关于大型语言模型（LLM）能力边界探索的严谨、扎实的量化研究。它并没有提出一个全新的、性能超群的模型，而是像一位严谨的实验物理学家，设计了一套精巧的实验装置，来精确测量并回答一个基础且重要的问题：<strong>当前最先进的语言模型，在多大程度上可以取代传统的手工编码，直接作为一个动态世界的“模拟器”？</strong></p>]]></description>
</item>
<item>
    <title>Group Sequence Policy Optimization</title>
    <link>https://blog.vllbc.top/group-sequence-policy-optimization/</link>
    <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/group-sequence-policy-optimization/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献是提出了一种名为 <strong>组序列策略优化 (Group
Sequence Policy Optimization, GSPO)</strong>
的新型强化学习（RL）算法，旨在解决在训练大型语言模型（特别是<strong>混合专家模型,
Mixture-of-Experts,
MoE</strong>）时普遍存在的训练不稳定甚至模型崩溃的痛点。这不仅仅是一次微小的算法改进，而是一次对现有主流RL优化范式的根本性反思与重构，其核心思想是
<strong>“将优化的基本单元与奖励的基本单元对齐”</strong>。</p>]]></description>
</item>
<item>
    <title>Towards Effective Code-Integrated Reasoning</title>
    <link>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇名为《Towards
Effective Code-Integrated Reasoning》的论文
(arXiv:2505.24480v1)。这篇论文系统性地探讨了一个在当前大模型研究中至关重要的前沿方向：如何让模型更稳定、更有效地利用外部工具（特别是代码解释器）来完成复杂的推理任务。</p>]]></description>
</item>
<item>
    <title>Routine：A Structural Planning Framework for LLM Agent System in Enterprise</title>
    <link>https://blog.vllbc.top/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/</link>
    <pubDate>Fri, 25 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/</guid>
    <description><![CDATA[<h3
id="论文深度解读从混沌到有序routine框架如何驯服企业级ai智能体"><strong>论文深度解读：从“混沌”到“有序”，Routine框架如何驯服企业级AI智能体</strong></h3>
<p>当前，以大型语言模型（LLM）为核心的自主智能体（Autonomous
Agents）正以前所未有的速度发展，展现出在数据分析、人机交互等领域的巨大潜力。然而，当我们将这些通用智能体置于规则严密、流程复杂的企业环境中时，往往会遭遇“水土不服”的窘境。论文开篇就指出了这一核心挑战：</p>]]></description>
</item>
<item>
    <title>Search and Refine During Think：Autonomous Retrieval - Augmented Reasoning of LLMs</title>
    <link>https://blog.vllbc.top/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/</link>
    <pubDate>Sun, 20 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/</guid>
    <description><![CDATA[<h3
id="论文深度解读从思考时搜索到思考时搜索并提炼的范式革命">论文深度解读：从“思考时搜索”到“思考时搜索并提炼”的范式革命</h3>
<p>这篇论文的核心贡献在于，它挑战了传统检索增强生成（RAG）系统中一个根深蒂固的、看似理所当然的流程，并提出了一种更为精细、鲁棒和智能的替代范式。传统的
RAG
模型通常遵循一种“<strong>思考时搜索</strong>”（search-during-think）的模式：当模型在生成答案的过程中意识到知识不足时，它会触发一次或多次搜索，获取外部文档，然后直接基于这些（可能混杂着大量噪声的）文档来生成最终答案。这种方法的致命弱点在于，它假设模型能够自行从混杂、冗长甚至可能错误的信息中精准地“淘金”，而现实是，这种“信息投喂”常常导致“<strong>垃圾进，垃圾出</strong>”（garbage
in, garbage out）的困境。</p>]]></description>
</item>
<item>
    <title>Peri-LN：Revisiting Normalization Layer in the Transformer Architecture</title>
    <link>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</link>
    <pubDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇关于
Transformer 架构中归一化策略的优秀论文——《Peri-LN: Revisiting
Normalization Layer in the Transformer Architecture》。</p>]]></description>
</item>
<item>
    <title>ZEROSEARCH：Incentivize the Search Capability of LLMs without Searching</title>
    <link>https://blog.vllbc.top/zerosearchincentivize-the-search-capability-of-llms-without-searching/</link>
    <pubDate>Fri, 18 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/zerosearchincentivize-the-search-capability-of-llms-without-searching/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇富有启发性的论文《ZEROSEARCH:
Incentivize the Search Capability of LLMs without Searching》。</p>
<p>这篇论文的核心思想极其巧妙，它直击了当前训练“搜索智能体（Search
Agent）”LLM 时最头疼的两个问题：高昂的 API
调用成本和不可控的搜索结果质量。传统的做法是让 LLM
在训练时与真实的搜索引擎（如谷歌）进行交互，通过强化学习（RL）来学习何时搜索、搜索什么以及如何利用搜索结果。但这就像让一个新手司机直接在高峰期的纽约街头学开车，不仅成本高昂（每次“练习”都要花钱），而且路况复杂多变（搜索结果时好时坏），很容易让模型“学坏”或者干脆放弃学习。</p>]]></description>
</item>
<item>
    <title>First Return, Entropy-Eliciting Explore</title>
    <link>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/first-return-entropy-eliciting-explore/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，非常荣幸能与您一同深入探讨这篇富有启发性的论文——《First
Return, Entropy-Eliciting Explore》。这篇由字节跳动、M-A-P
及曼彻斯特大学的研究者们共同完成的工作，直面了当前大模型在复杂推理任务中通过强化学习进行优化时的一个核心痛点。</p>]]></description>
</item>
<item>
    <title>Reinforcing General Reasoning without Verifiers</title>
    <link>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</guid>
    <description><![CDATA[<h3
id="一论文的研究目标与意义"><strong>一、论文的研究目标与意义</strong></h3>
<h4 id="研究目标与待解决问题"><strong>研究目标与待解决问题</strong></h4>
<p>论文的核心研究目标是：<strong>将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。</strong></p>]]></description>
</item>
<item>
    <title>GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS</title>
    <link>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇具有开创性意义的论文《Generalist
Reward Models: Found Inside Large Language
Models》。这篇论文提出了一种颠覆性的思想：我们或许不再需要耗费巨资去训练一个独立的奖励模型（Reward
Model），因为一个强大的通用奖励模型，早已“内生”于任何一个通过标准方式（next-token
prediction）训练的语言模型之中。</p>]]></description>
</item>
</channel>
</rss>
