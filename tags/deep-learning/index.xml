<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - 标签 - vllbc02</title>
        <link>https://vllbc.top/tags/deep-learning/</link>
        <description>Deep Learning - 标签 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 29 Nov 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Auto-Encoder</title>
    <link>https://vllbc.top/ae/</link>
    <pubDate>Tue, 29 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/ae/</guid>
    <description><![CDATA[自编码器是一种神经网络模型，可以应用到许多任务中，可以做到降维、生成、特征提取等作用。自编码器大体就分为两个部分，第一个部分为编码器，将原输]]></description>
</item>
<item>
    <title>经典GAN</title>
    <link>https://vllbc.top/gan/</link>
    <pubDate>Tue, 22 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/gan/</guid>
    <description><![CDATA[简介 生成对抗网络（Generative Adversarial Network，简称GAN）是无监督学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。 大白话]]></description>
</item>
<item>
    <title>CNN</title>
    <link>https://vllbc.top/cnn/</link>
    <pubDate>Tue, 08 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/cnn/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>Smooth L1 Loss</title>
    <link>https://vllbc.top/smooth-l1-loss/</link>
    <pubDate>Mon, 07 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/smooth-l1-loss/</guid>
    <description><![CDATA[L1 Loss 也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。 优点： 对离群]]></description>
</item>
<item>
    <title>反向传播算法</title>
    <link>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</link>
    <pubDate>Mon, 07 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[反向传播算法遵循两个法则：梯度下降法则和链式求导法则。 梯度下降法则不用多说，记住一切的目的就是为了减小损失，即朝着局部最小值点移动。链式求导]]></description>
</item>
<item>
    <title>GRU</title>
    <link>https://vllbc.top/gru/</link>
    <pubDate>Thu, 03 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/gru/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>LSTM</title>
    <link>https://vllbc.top/lstm/</link>
    <pubDate>Thu, 03 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/lstm/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>单隐层多分类网络</title>
    <link>https://vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/</link>
    <pubDate>Thu, 27 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/</guid>
    <description><![CDATA[单隐层多分类神经网络（numpy实现） 使用Numpy实现，并且使用命令行的形式设定参数。 是一个作业里面的，实现的时候踩了一些坑，主要是训练里]]></description>
</item>
<item>
    <title>Dropout正则化</title>
    <link>https://vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</link>
    <pubDate>Tue, 30 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</guid>
    <description><![CDATA[Dropout 在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值h以保留概率概率p由随机]]></description>
</item>
<item>
    <title>Adam算法</title>
    <link>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</link>
    <pubDate>Sun, 31 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[Adam算法 背景 作为机器学习的初学者必然会接触梯度下降算法以及SGD，基本上形式如下： $$ \theta_t = \theta_{t-1} - \alpha ;g(\theta) $$ 其中$\alpha$为学习率，$g(\]]></description>
</item>
</channel>
</rss>
