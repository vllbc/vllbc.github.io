<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/deep-learning/</link>
        <description>Deep Learning - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Batch Norm</title>
    <link>https://blog.vllbc.top/batch-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/batch-norm/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>L1 L2正则化</title>
    <link>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid>
    <description><![CDATA[<h1 id="l1正则化">L1正则化</h1>
<h1 id="l2正则化">L2正则化</h1>
<h1 id="权重衰减">权重衰减</h1>
<h1 id="l2正则化和权重衰减的区别">L2正则化和权重衰减的区别</h1>
<p>L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。 </p>]]></description>
</item>
<item>
    <title>Layer Norm</title>
    <link>https://blog.vllbc.top/layer-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/layer-norm/</guid>
    <description><![CDATA[<h1 id="pre-norm">pre-norm</h1>
<p>Pre-norm:<span
class="math inline">\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\)</span></p>
<p><span class="math inline">\(先来看Pre-norm^{+},递归展开：\)</span>
<span class="math display">\[X_{t+1}=X_t+F_t(Norm(X_t))\]</span> <span
class="math inline">\(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\)</span>
其中，展开<span class="math inline">\(^{+}\)</span>后的每一项( <span
class="math inline">\(F_{1}( Norm( X_{1}) ) , \ldots\)</span>, <span
class="math inline">\(F_{t- 1}( Norm( X_{t- 1}) )\)</span>, <span
class="math inline">\(F_{t}( Norm( X_{t})
)\)</span>)之间都是同一量级的， 所以<span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\)</span>和 <span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))\)</span>之间的区别就像t和t-1的区别一样，我们可以将
其记为<span class="math inline">\(X_t+ 1= \mathscr{O} ( t+ 1)\)</span> .
这种特性就导致当t足够大的时候，<span
class="math inline">\(X_{t+1}\)</span>和<span
class="math inline">\(X_t\)</span>之间区别可以忽略不计（直觉上），那么就有：</p>]]></description>
</item>
<item>
    <title>hinge loss</title>
    <link>https://blog.vllbc.top/hinge-loss/</link>
    <pubDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/hinge-loss/</guid>
    <description><![CDATA[<p>在机器学习中，<strong>hinge
loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p>
<p></p>
<p>其中 <span
class="math inline">\(\hat{y}\)</span> 表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。）， <span
class="math inline">\(y\)</span> 表示正确的类别。 - 如果 <span
class="math inline">\(\hat{y}y&lt;1\)</span> ，则损失为： <span
class="math inline">\(1-\hat{y}y\)</span> - 如果<span
class="math inline">\(\hat{y}y&gt;1\)</span> ，则损失为：0</p>]]></description>
</item>
<item>
    <title>early-stopping</title>
    <link>https://blog.vllbc.top/early-stopping/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/early-stopping/</guid>
    <description><![CDATA[<h2 id="介绍">介绍</h2>
<p>早停止（Early
Stopping）是 <strong>当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段</strong>。</p>
<p>如何判断已经收敛？主要看以下几点： -
验证集上的Loss在模型多次迭代后，没有下降 - 验证集上的Loss开始上升。
这时就可以认为模型没有必要训练了，可以停止了，因为训练下去可能就会发生过拟合，所以早停法是一种防止模型过拟合的方法。</p>]]></description>
</item>
<item>
    <title>focal loss</title>
    <link>https://blog.vllbc.top/focal-loss/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/focal-loss/</guid>
    <description><![CDATA[<h1 id="focal-loss">Focal Loss</h1>
<p>Focal Loss主要是为了解决类别不平衡的问题，Focal
Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例：</p>
<h3 id="原始loss">原始Loss</h3>
<p>原始的二分类： </p>
<p>其中 </p>]]></description>
</item>
<item>
    <title>warmup</title>
    <link>https://blog.vllbc.top/warmup/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/warmup/</guid>
    <description><![CDATA[<p>在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning
rate，然后随着训练的批次增加，逐渐增大学习率，直到达到原本想要设置的学习率。</p>]]></description>
</item>
<item>
    <title>标签平滑</title>
    <link>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</guid>
    <description><![CDATA[<p>神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为onehot本身就是一个稀疏的向量，如果所有无关类别都为0的话，就可能会疏忽某些类别之间的联系。
具体的缺点有： -
真是标签与其它标签之间的关系被忽略了，很多有用的知识学不到了。 -
倾向于让模型更加武断，导致泛化性能差 -
面对有噪声的数据更容易收到影响。</p>]]></description>
</item>
<item>
    <title>调参技巧</title>
    <link>https://blog.vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</guid>
    <description><![CDATA[<ul>
<li>基本原则：快速试错。</li>
<li>小步试错，快速迭代</li>
<li>可以试试无脑的配置</li>
<li>实时打印一些结果</li>
<li>自动调参：网格搜索、random search、贝叶斯优化、</li>
<li>参数初始化</li>
<li>学习率warmup，慢慢增加，然后学习率衰减。</li>
</ul>
<h1 id="batch_size和lr">batch_size和lr</h1>
<p><strong>大的batchsize收敛到<a
href="https://zhida.zhihu.com/search?q=sharp+minimum&amp;zhida_source=entity&amp;is_preview=1">sharp
minimum</a>，而小的batchsize收敛到<a
href="https://zhida.zhihu.com/search?q=flat+minimum&amp;zhida_source=entity&amp;is_preview=1">flat
minimum</a>，后者具有更好的泛化能力。</strong>两者的区别就在于变化的趋势，一个快一个慢，如下图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp
minimum。</p>]]></description>
</item>
<item>
    <title>数据不平衡</title>
    <link>https://blog.vllbc.top/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/</guid>
    <description><![CDATA[<h2 id="数据不均衡">数据不均衡</h2>
<p>所谓的不平衡指的是不同类别的样本量差异非常大，或者少数样本代表了业务的关键数据（少量样本更重要），需要对少量样本的模式有很好的学习。样本类别分布不平衡主要出现在分类相关的建模问题上。样本类别分布不平衡从数据规模上可以分为大数据分布不平衡和小数据分布不平衡两种。</p>]]></description>
</item>
</channel>
</rss>
