<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/deep-learning/</link>
        <description>Deep Learning - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Batch Norm</title>
    <link>https://blog.vllbc.top/batch-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/batch-norm/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>L1 L2正则化</title>
    <link>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid>
    <description><![CDATA[<h1 id="l1正则化">L1正则化</h1>
<h1 id="l2正则化">L2正则化</h1>
<h1 id="权重衰减">权重衰减</h1>
<h1 id="l2正则化和权重衰减的区别">L2正则化和权重衰减的区别</h1>
<p>L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。 </p>]]></description>
</item>
<item>
    <title>Layer Norm</title>
    <link>https://blog.vllbc.top/layer-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/layer-norm/</guid>
    <description><![CDATA[<h1 id="pre-norm">pre-norm</h1>
<p>Pre-norm:<span
class="math inline">\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\)</span></p>
<p><span class="math inline">\(先来看Pre-norm^{+},递归展开：\)</span>
<span class="math display">\[X_{t+1}=X_t+F_t(Norm(X_t))\]</span> <span
class="math inline">\(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\)</span>
其中，展开<span class="math inline">\(^{+}\)</span>后的每一项( <span
class="math inline">\(F_{1}( Norm( X_{1}) ) , \ldots\)</span>, <span
class="math inline">\(F_{t- 1}( Norm( X_{t- 1}) )\)</span>, <span
class="math inline">\(F_{t}( Norm( X_{t})
)\)</span>)之间都是同一量级的， 所以<span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\)</span>和 <span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))\)</span>之间的区别就像t和t-1的区别一样，我们可以将
其记为<span class="math inline">\(X_t+ 1= \mathscr{O} ( t+ 1)\)</span> .
这种特性就导致当t足够大的时候，<span
class="math inline">\(X_{t+1}\)</span>和<span
class="math inline">\(X_t\)</span>之间区别可以忽略不计（直觉上），那么就有：</p>]]></description>
</item>
<item>
    <title>hinge loss</title>
    <link>https://blog.vllbc.top/hinge-loss/</link>
    <pubDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/hinge-loss/</guid>
    <description><![CDATA[<p>在机器学习中，<strong>hinge
loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p>
<p></p>
<p>其中 <span
class="math inline">\(\hat{y}\)</span> 表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。）， <span
class="math inline">\(y\)</span> 表示正确的类别。 - 如果 <span
class="math inline">\(\hat{y}y&lt;1\)</span> ，则损失为： <span
class="math inline">\(1-\hat{y}y\)</span> - 如果<span
class="math inline">\(\hat{y}y&gt;1\)</span> ，则损失为：0</p>]]></description>
</item>
<item>
    <title>UDA</title>
    <link>https://blog.vllbc.top/uda/</link>
    <pubDate>Wed, 08 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/uda/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>early-stopping</title>
    <link>https://blog.vllbc.top/early-stopping/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/early-stopping/</guid>
    <description><![CDATA[<h2 id="介绍">介绍</h2>
<p>早停止（Early
Stopping）是 <strong>当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段</strong>。</p>
<p>如何判断已经收敛？主要看以下几点： -
验证集上的Loss在模型多次迭代后，没有下降 - 验证集上的Loss开始上升。
这时就可以认为模型没有必要训练了，可以停止了，因为训练下去可能就会发生过拟合，所以早停法是一种防止模型过拟合的方法。</p>]]></description>
</item>
<item>
    <title>focal loss</title>
    <link>https://blog.vllbc.top/focal-loss/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/focal-loss/</guid>
    <description><![CDATA[<h1 id="focal-loss">Focal Loss</h1>
<p>Focal Loss主要是为了解决类别不平衡的问题，Focal
Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例：</p>
<h3 id="原始loss">原始Loss</h3>
<p>原始的二分类： </p>
<p>其中 </p>]]></description>
</item>
<item>
    <title>EDA</title>
    <link>https://blog.vllbc.top/eda/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/eda/</guid>
    <description><![CDATA[<h1 id="nlp中的eda">NLP中的EDA</h1>
<p><strong>同义词替换，回译，近音字替换，随机插入</strong>，<strong>随机交换</strong>，<strong>随机删除</strong></p>
<h2 id="同义词替换">同义词替换</h2>
<p>做法可以是维护一个同义词表，如哈工大的发布的同义词词典。在每次训练的时候，样本有一定的概率对里面的词语进行替换。如”自动驾驶、医疗、能源……一季度融资最多的人工智能公司”
-&gt;
“自动驾车、医术、能源……一季度融资最多的人工智能公司”。根据经验，<strong>有条件的话最好用项目领域的同义词词典</strong>，如做医疗的文本，就用医疗的同义词词典，做金融领域的就用金融的同义词词典，而不是用一个通用的字典。</p>]]></description>
</item>
<item>
    <title>warmup</title>
    <link>https://blog.vllbc.top/warmup/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/warmup/</guid>
    <description><![CDATA[<p>在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning
rate，然后随着训练的批次增加，逐渐增大学习率，直到达到原本想要设置的学习率。</p>]]></description>
</item>
<item>
    <title>标签平滑</title>
    <link>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</guid>
    <description><![CDATA[<p>神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为onehot本身就是一个稀疏的向量，如果所有无关类别都为0的话，就可能会疏忽某些类别之间的联系。
具体的缺点有： -
真是标签与其它标签之间的关系被忽略了，很多有用的知识学不到了。 -
倾向于让模型更加武断，导致泛化性能差 -
面对有噪声的数据更容易收到影响。</p>]]></description>
</item>
</channel>
</rss>
