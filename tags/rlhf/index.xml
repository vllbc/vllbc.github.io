<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>RLHF - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/rlhf/</link>
        <description>RLHF - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 27 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/rlhf/" rel="self" type="application/rss+xml" /><item>
    <title>vapo</title>
    <link>https://blog.vllbc.top/vapo/</link>
    <pubDate>Sun, 27 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/vapo/</guid>
    <description><![CDATA[<p>这篇由字节跳动Seed团队在2025年4月发表的论文，直面了当前大模型领域中一个核心且棘手的难题：如何通过强化学习（Reinforcement
Learning,
RL）高效、稳定地提升模型在复杂推理任务上的能力。当前，以<strong>链式思考（Chain-of-Thought,
CoT）</strong>为代表的推理技术是实现通用人工智能（AGI）的关键路径，而如何让模型学会更长、更可靠的推理链，是业界公认的瓶颈。</p>]]></description>
</item>
<item>
    <title>dapo</title>
    <link>https://blog.vllbc.top/dapo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dapo/</guid>
    <description><![CDATA[<p>DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling
Policy
Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2
个是主要亮点，是命名的来源）</p>]]></description>
</item>
<item>
    <title>grpo</title>
    <link>https://blog.vllbc.top/grpo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/grpo/</guid>
    <description><![CDATA[<h1 id="grpo-trl-库">GRPO (trl 库)</h1>
<h2 id="重要参数">重要参数</h2>
<ul>
<li>Num_generations: <strong>Number of generations to sample. The
effective batch size (num_processes * per_device_batch_size *
gradient_accumulation_steps) must be evenly divisible by this
value.</strong></li>
<li>generation_batch_size: <strong>Batch size to use for generation. If
<code>None</code>, it defaults to the effective training batch size:
<code>per_device_train_batch_size * num_processes * steps_per_generation</code>.</strong></li>
<li>steps_per_generation: Number of optimization steps per generation.
If <code>None</code>, it defaults to gradient_accumulation_steps.</li>
<li>Num_iterations: Number of iterations per batch (denoted as μ in the
algorithm).</li>
<li>Per_device_train_batch_size</li>
<li>Num_processes (world_size)</li>
</ul>
<p>trl 库的重要参数比较少。其中根据官方文档，generation_batch_size =
`per_device_train_batch_size * num_processes * steps_per_generation
Gradient_accumulation_steps 一般就是 steps_per_generation (对应 verl
中的 mini_batch_size / n_gpus /
ppo_micro_batch_size_per_gpu)，可以理解为 per_device_train_bs (对应 verl
中的 ppo_micro_batch_size_per_gpu) 是使用梯度累计后的 bs，乘 gpu
数，再乘梯度累计的 steps 就是总的 batch_size（对应 verl 中的
train_batch_size * rollout. N）。所以注意，总的 batch_size
(generation_batch_size) 是已经 rollout 采样后的 bs，除以 num_generations
才是针对 prompts 的 bs（verl 中的 train_batch_size）。
下面是_get_train_sampler 方法的注释，对每一个 prompt 重复
num_generations 是该方法实现的。</p>]]></description>
</item>
<item>
    <title>ppo</title>
    <link>https://blog.vllbc.top/ppo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ppo/</guid>
    <description><![CDATA[<h2 id="ppo-openrlhf-库">PPO (openrlhf 库)</h2>
<p>重点记录一下 experience 的采集过程。训练其实很简单。Actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
<item>
    <title>Reinforcing General Reasoning without Verifiers</title>
    <link>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</guid>
    <description><![CDATA[<h3
id="一论文的研究目标与意义"><strong>一、论文的研究目标与意义</strong></h3>
<h4 id="研究目标与待解决问题"><strong>研究目标与待解决问题</strong></h4>
<p>论文的核心研究目标是：<strong>将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。</strong></p>]]></description>
</item>
<item>
    <title>REINFORECE&#43;&#43;</title>
    <link>https://blog.vllbc.top/reinforece/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforece/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>ReMAX（REINFORCE argmax）</title>
    <link>https://blog.vllbc.top/remaxreinforce-argmax/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/remaxreinforce-argmax/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>RLOO</title>
    <link>https://blog.vllbc.top/rloo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rloo/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS</title>
    <link>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇具有开创性意义的论文《Generalist
Reward Models: Found Inside Large Language
Models》。这篇论文提出了一种颠覆性的思想：我们或许不再需要耗费巨资去训练一个独立的奖励模型（Reward
Model），因为一个强大的通用奖励模型，早已“内生”于任何一个通过标准方式（next-token
prediction）训练的语言模型之中。</p>]]></description>
</item>
<item>
    <title>RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS</title>
    <link>https://blog.vllbc.top/rlprextrapolating-rlvr-to-general-domains-without-verifiers/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rlprextrapolating-rlvr-to-general-domains-without-verifiers/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇具有重要价值的论文《RLPR:
Extrapolating RLVR to General Domains without Verifiers》。</p>
<p>这篇论文的核心贡献在于，它巧妙地绕开了现有强化学习方法在提升大模型通用推理能力时遇到的一个核心瓶颈——<strong>验证器（Verifier）</strong>，从而为更广泛、更低成本地提升大模型能力开辟了一条新路径。</p>]]></description>
</item>
</channel>
</rss>
