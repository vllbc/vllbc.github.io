<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>RLHF - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/rlhf/</link>
        <description>RLHF - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 18 May 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/rlhf/" rel="self" type="application/rss+xml" /><item>
    <title>PPO</title>
    <link>https://blog.vllbc.top/rlhf/</link>
    <pubDate>Sun, 18 May 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rlhf/</guid>
    <description><![CDATA[<p>基础部分看猛猿大佬的<a
href="https://zhuanlan.zhihu.com/p/7461863937">人人都能看懂的RL-PPO理论知识</a>即可，通俗易懂，我写不出来比这个更好的了。本文是各RL算法笔记。</p>
<h1 id="ppo-openrlhf库">PPO (openrlhf库)</h1>
<p>重点记录一下experience的采集过程。训练其实很简单。actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
</channel>
</rss>
