<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine Learning - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/machine-learning/</link>
        <description>Machine Learning - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 10 Jun 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>FP-Growth</title>
    <link>https://blog.vllbc.top/fp-growth/</link>
    <pubDate>Sat, 10 Jun 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/fp-growth/</guid>
    <description><![CDATA[<p></p>]]></description>
</item>
<item>
    <title>特征选择</title>
    <link>https://blog.vllbc.top/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link>
    <pubDate>Wed, 08 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid>
    <description><![CDATA[<p><code>特征选择</code>是<code>特征工程</code>里的一个重要问题，其目标是<strong>寻找最优特征子集</strong>。特征选择能剔除不相关(irrelevant)或冗余(redundant
)的特征，从而达到减少特征个数，<strong>提高模型精确度，减少运行时间的目的</strong>。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。</p>]]></description>
</item>
<item>
    <title>最大熵模型</title>
    <link>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>xgboost</title>
    <link>https://blog.vllbc.top/xgboost/</link>
    <pubDate>Wed, 01 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/xgboost/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>LDA</title>
    <link>https://blog.vllbc.top/lda/</link>
    <pubDate>Wed, 21 Dec 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/lda/</guid>
    <description><![CDATA[<h1
id="线性判别分析ldalinear-discriminant-analysis">线性判别分析LDA(Linear
Discriminant Analysis)</h1>
<p>线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别，一起学习一下吧。</p>]]></description>
</item>
<item>
    <title>DBSCAN</title>
    <link>https://blog.vllbc.top/dbscan/</link>
    <pubDate>Thu, 03 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dbscan/</guid>
    <description><![CDATA[<p>DBSCAN属于密度聚类的一种。通常情形下，密度聚类算法从样
本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇
以获得最终的聚类结果。</p>]]></description>
</item>
<item>
    <title>关联规则概念</title>
    <link>https://blog.vllbc.top/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/</link>
    <pubDate>Wed, 02 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/</guid>
    <description><![CDATA[<p>参考：<a
href="https://www.cnblogs.com/bill-h/p/14863262.html">https://www.cnblogs.com/bill-h/p/14863262.html</a></p>
<p>大家可能听说过用于宣传数据挖掘的一个案例:啤酒和尿布；据说是沃尔玛超市在分析顾客的购买记录时，发现许多客户购买啤酒的同时也会购买婴儿尿布，于是超市调整了啤酒和尿布的货架摆放，让这两个品类摆放在一起；结果这两个品类的销量都有明显的增长；分析原因是很多刚生小孩的男士在购买的啤酒时，会顺手带一些婴幼儿用品。</p>]]></description>
</item>
<item>
    <title>高斯混合聚类</title>
    <link>https://blog.vllbc.top/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/</link>
    <pubDate>Tue, 25 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/</guid>
    <description><![CDATA[<p>基础就是高斯混合模型，假设我们熟知的高斯分布的概率密度函数为<span
class="math inline">\(p(x\mid \mu,
\Sigma)\)</span>。则高斯混合分布为：</p>
<p><span class="math display">\[
p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^k \alpha_i \cdot
p\left(\boldsymbol{x} \mid \boldsymbol{\mu}_i,
\boldsymbol{\Sigma}_i\right)
\]</span></p>
<p>分布共由 <span class="math inline">\(k\)</span> 个混合成分组成,
每个混合成分对应一个高斯分布. 其中 <span
class="math inline">\(\mu_i\)</span> 与 <span
class="math inline">\(\Sigma_i\)</span> 是第 <span
class="math inline">\(i\)</span> 个高斯混合成分的参数, 而 <span
class="math inline">\(\alpha_i&gt;0\)</span> 为相应的 “混合系数”
(mixture coefficient), <span class="math inline">\(\sum_{i=1}^k
\alpha_i=1\)</span>。 假设样本的生成过程由高斯混合分布给出: 首先, 根据
<span class="math inline">\(\alpha_1, \alpha_2, \ldots,
\alpha_k\)</span> 定义 的先验分布选择高斯混合成分, 其中 <span
class="math inline">\(\alpha_i\)</span> 为选择第 <span
class="math inline">\(i\)</span> 个混合成分的概率; 然后, 根
据被选择的混合成分的概率密度函数进行采样, 从而生成相应的样本。</p>]]></description>
</item>
<item>
    <title>EM算法</title>
    <link>https://blog.vllbc.top/em%E7%AE%97%E6%B3%95/</link>
    <pubDate>Mon, 03 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/em%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[<h1 id="em算法">EM算法</h1>
<h2 id="引入">引入</h2>
<p>我们经常会从样本观察数据中，找出样本的模型参数。
最常用的方法就是极大化模型分布的对数似然函数。（最大似然估计：利用已知的样本结果，反推最有可能导致这样结果的一组参数）但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。用EM算法可以解决。</p>]]></description>
</item>
<item>
    <title>精确率和召回率</title>
    <link>https://blog.vllbc.top/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/</link>
    <pubDate>Sun, 21 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/</guid>
    <description><![CDATA[<h2 id="精确率和召回率">精确率和召回率</h2>
<h3 id="混淆矩阵">混淆矩阵</h3>
<ul>
<li>True Positive(真正, TP)：将正类预测为正类数.</li>
<li>True Negative(真负 , TN)：将负类预测为负类数.</li>
<li>False Positive(假正, FP)：将负类预测为正类数</li>
<li>False Negative(假负 , FN)：将正类预测为负类数</li>
</ul>
<h3 id="精确率">精确率</h3>
<p><span class="math display">\[
P = \frac{TP}{TP+FP}
\]</span></p>]]></description>
</item>
</channel>
</rss>
