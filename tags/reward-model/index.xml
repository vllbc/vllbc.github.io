<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reward-Model - 标签 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/tags/reward-model/</link>
        <description>Reward-Model - 标签 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/tags/reward-model/" rel="self" type="application/rss+xml" /><item>
    <title>GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS</title>
    <link>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generalist-reward-modelsfound-inside-large-language-models/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇具有开创性意义的论文《Generalist
Reward Models: Found Inside Large Language
Models》。这篇论文提出了一种颠覆性的思想：我们或许不再需要耗费巨资去训练一个独立的奖励模型（Reward
Model），因为一个强大的通用奖励模型，早已“内生”于任何一个通过标准方式（next-token
prediction）训练的语言模型之中。</p>]]></description>
</item>
</channel>
</rss>
