<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>优化算法 - 标签 - vllbc02</title>
        <link>https://vllbc.top/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</link>
        <description>优化算法 - 标签 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" rel="self" type="application/rss+xml" /><item>
    <title>Adam算法</title>
    <link>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</link>
    <pubDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[Adam算法 背景 作为机器学习的初学者必然会接触梯度下降算法以及SGD，基本上形式如下： \[ \theta_t = \theta_{t-1} - \alpha \;g(\theta) \] 其中\(\alpha\)为学习率，\(]]></description>
</item>
<item>
    <title>反向传播算法</title>
    <link>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</link>
    <pubDate>Sat, 09 Apr 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[反向传播算法遵循两个法则：梯度下降法则和链式求导法则。 梯度下降法则不用多说，记住一切的目的就是为了减小损失，即朝着局部最小值点移动。链式求导]]></description>
</item>
<item>
    <title>梯度下降法</title>
    <link>https://vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
    <pubDate>Tue, 04 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
    <description><![CDATA[梯度下降法 简介 批度梯度下降 其实就是一次将整个数据集进行梯度下降的迭代 ## 随机梯度下降 就是对样本进行循环，每循环一个样本就更新一次参数，但是不容]]></description>
</item>
</channel>
</rss>
