<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>单隐层多分类网络 - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="单隐层多分类网络" />
<meta property="og:description" content="单隐层多分类神经网络（numpy实现） 使用Numpy实现，并且使用命令行的形式设定参数。 是一个作业里面的，实现的时候踩了一些坑，主要是训练里" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" /><meta property="og:image" content="https://blog.vllbc.top/logo.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-24T00:00:00+00:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://blog.vllbc.top/logo.png" /><meta name="twitter:title" content="单隐层多分类网络"/>
<meta name="twitter:description" content="单隐层多分类神经网络（numpy实现） 使用Numpy实现，并且使用命令行的形式设定参数。 是一个作业里面的，实现的时候踩了一些坑，主要是训练里"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" /><link rel="prev" href="https://blog.vllbc.top/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" /><link rel="next" href="https://blog.vllbc.top/flip/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "单隐层多分类网络",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "Deep Learning, 项目练习, 单隐层多分类网络","wordcount":  2078 ,
        "url": "https:\/\/blog.vllbc.top\/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C\/","datePublished": "2021-12-19T00:00:00+00:00","dateModified": "2023-03-24T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">单隐层多分类网络</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://blog.vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/deep-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Learning</a>&nbsp;<a href="/categories/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>项目练习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2021-12-19">2021-12-19</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 2078 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 5 分钟&nbsp;<span id="/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" class="leancloud_visitors" data-flag-title="单隐层多分类网络">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1
id="单隐层多分类神经网络numpy实现">单隐层多分类神经网络（numpy实现）</h1>
<p>使用Numpy实现，并且使用命令行的形式设定参数。</p>
<p>是一个作业里面的，实现的时候踩了一些坑，主要是训练里面的，最主要的就是没有实现batch，所以对于batch数据就要加上一层循环了。还有就是shuffle函数，返回值不要和参数一样，意思就是要对一个固定的数据进行打乱，而不是对上一epoch的数据进行打乱，这个坑很难察觉到。</p>
<p>下面是代码：</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> argparse</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># This takes care of command line argument parsing for you!</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># To access a specific argument, simply access args.&lt;argument name&gt;.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>parser <span class="op">=</span> argparse.ArgumentParser()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;train_input&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;path to training input .csv file&#39;</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;validation_input&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;path to validation input .csv file&#39;</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;train_out&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;path to store prediction on training data&#39;</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;validation_out&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;path to store prediction on validation data&#39;</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;metrics_out&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">str</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;path to store training and testing metrics&#39;</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;num_epoch&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>,</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;number of training epochs&#39;</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;hidden_units&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>,</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;number of hidden units&#39;</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;init_flag&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">int</span>, choices<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;weight initialization functions, 1: random&#39;</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;learning_rate&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">float</span>,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;learning rate&#39;</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>parser.add_argument(<span class="st">&#39;--debug&#39;</span>, <span class="bu">type</span><span class="op">=</span><span class="bu">bool</span>, default<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">help</span><span class="op">=</span><span class="st">&#39;set to True to show logging&#39;</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> args2data(args) :</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="st">    No need to modify this function!</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="st">    Parse command line arguments, create train/test data and labels.</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="st">    :return:</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="st">    X_tr: train data *without label column and without bias folded in* (numpy array)</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="st">    y_tr: train label (numpy array)</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="st">    X_te: test data *without label column and without bias folded in* (numpy array)</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="st">    y_te: test label (numpy array)</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="st">    out_tr: predicted output for train data (file)</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="st">    out_te: predicted output for test data (file)</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="st">    out_metrics: output for train and test error (file)</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="st">    n_epochs: number of train epochs</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="st">    n_hid: number of hidden units</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="st">    init_flag: weight initialize flag -- 1 means random, 2 means zero</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="st">    lr: learning rate</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data from arguments</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    out_tr <span class="op">=</span> args.train_out</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    out_te <span class="op">=</span> args.validation_out</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>    out_metrics <span class="op">=</span> args.metrics_out</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>    n_epochs <span class="op">=</span> args.num_epoch</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>    n_hid <span class="op">=</span> args.hidden_units</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>    init_flag <span class="op">=</span> args.init_flag</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> args.learning_rate</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>    X_tr <span class="op">=</span> np.loadtxt(args.train_input, delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>    y_tr <span class="op">=</span> X_tr[:, <span class="dv">0</span>].astype(<span class="bu">int</span>)</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>    X_tr <span class="op">=</span> X_tr[:, <span class="dv">1</span>:] <span class="co"># cut off label column</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>    X_te <span class="op">=</span> np.loadtxt(args.validation_input, delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>    y_te <span class="op">=</span> X_te[:, <span class="dv">0</span>].astype(<span class="bu">int</span>)</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>    X_te <span class="op">=</span> X_te[:, <span class="dv">1</span>:] <span class="co"># cut off label column</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (X_tr, y_tr, X_te, y_te, out_tr, out_te, out_metrics,</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>            n_epochs, n_hid, init_flag, lr)</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shuffle(X, y, epoch):</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="st">    DO NOT modify this function.</span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="st">    Permute the training data for SGD.</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="st">    :param X: The original input data in the order of the file.</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y: The original labels in the order of the file.</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="st">    :param epoch: The epoch number (0-indexed).</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: Permuted X and y training data for the epoch.</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>    np.random.seed(epoch)</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>    ordering <span class="op">=</span> np.random.permutation(N)</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[ordering], y[ordering]</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_init(shape):</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="st">    Randomly initialize a numpy array of the specified shape</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="st">    :param shape: list or tuple of shapes</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: initialized weights</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>    M, D <span class="op">=</span> shape</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>    np.random.seed(M<span class="op">*</span>D) <span class="co"># Don&#39;t change this line!</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: create the random matrix here!</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hint: numpy might have some useful function for this</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>, size<span class="op">=</span>(M, D))</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> zero_init(shape):</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="st">    Do not modify this function.</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a><span class="st">    Initialize a numpy array of the specified shape with zero</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="st">    :param shape: list or tuple of shapes</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: initialized weights</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.zeros(shape <span class="op">=</span> shape)</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(z: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="st">    Implement softmax function.</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="st">    :param z: input logits of shape (num_classes,)</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: softmax output of shape (num_classes,)</span></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>    shiftz <span class="op">=</span> z <span class="op">-</span> np.<span class="bu">max</span>(z)</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>    expz <span class="op">=</span> np.exp(shiftz)</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> expz <span class="op">/</span> np.<span class="bu">sum</span>(expz)</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy(y: np.ndarray, y_hat: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a><span class="st">    Compute cross entropy loss.</span></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y: label (a number or an array containing a single element)</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y_hat: prediction with shape (num_classes,)</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: cross entropy loss</span></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(y, np.ndarray):</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y[<span class="dv">0</span>]</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(y_hat[y])</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> d_softmax_cross_entropy(y: np.ndarray, y_hat: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a><span class="st">    Compute gradient of loss w.r.t. ** softmax input **.</span></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a><span class="st">    Note that here instead of calculating the gradient w.r.t. the softmax</span></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="st">    probabilities, we are directly computing gradient w.r.t. the softmax input.</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="st">    Try deriving the gradient yourself (see Question 1.2(b) on the written),</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a><span class="st">    and you&#39;ll see why we want to calculate this in a single step.</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y: label (a number or an array containing a single element)</span></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y_hat: predicted softmax probability with shape (num_classes,)</span></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a><span class="st">    :return: gradient with shape (num_classes,)</span></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(y, np.ndarray):</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y[<span class="dv">0</span>]</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>    y_hat[y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_hat</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sigmoid(<span class="bu">object</span>):</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="st">        Initialize state for sigmoid activation layer</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create cache to hold values for backward pass</span></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cache: <span class="bu">dict</span>[<span class="bu">str</span>, np.ndarray] <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="st">        Implement sigmoid activation function.</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="st">        :param x: input of shape (num_classes,)</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="st">        :return: sigmoid output of shape (num_classes,)</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>        sigmoid <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cache[<span class="st">&#39;sigmoid&#39;</span>] <span class="op">=</span> sigmoid</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dz: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a><span class="st">        :param dz: partial derivative of loss with respect to output of sigmoid activation</span></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a><span class="st">        :return: partial derivative of loss with respect to input of sigmoid activation</span></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>        sigmoid <span class="op">=</span> <span class="va">self</span>.cache[<span class="st">&#39;sigmoid&#39;</span>]</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dz <span class="op">*</span> sigmoid <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sigmoid)</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="co"># This refers to a function type that takes in a tuple of 2 integers (row, col)</span></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a><span class="co"># and returns a numpy array (which should have the specified dimensions).</span></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(<span class="bu">object</span>):</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size: <span class="bu">int</span>, output_size: <span class="bu">int</span>,</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>                 weight_init_fn, learning_rate: <span class="bu">float</span>):</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a><span class="st">        :param input_size: number of units in the input of the layer</span></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="st">                           *not including* the folded bias</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a><span class="st">        :param output_size: number of units in the output of the layer</span></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="st">        :param weight_init_fn: function that creates and initializes weight</span></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a><span class="st">                               matrices for layer. This function takes in a</span></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a><span class="st">                               tuple (row, col) and returns a matrix with</span></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a><span class="st">                               shape row x col.</span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a><span class="st">        :param learning_rate: learning rate for SGD training updates</span></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Initialize weight matrix for this layer - since we are</span></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>        <span class="co"># folding the bias into the weight matrix, be careful about the</span></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shape you pass in.</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> weight_init_fn((output_size, input_size <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Initialize matrix to store gradient with respect to weights</span></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dw <span class="op">=</span> np.zeros_like(<span class="va">self</span>.w, dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Initialize learning rate for SGD</span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> learning_rate</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create cache to hold certain values for backward pass</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cache: <span class="bu">dict</span>[<span class="bu">str</span>, np.ndarray] <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="st">        :param x: Input to linear layer with shape (input_size,)</span></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a><span class="st">                  where input_size *does not include* the folded bias.</span></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="st">                  In other words, the input does not contain the bias column</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a><span class="st">                  and you will need to add it in yourself in this method.</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a><span class="st">        :return: output z of linear layer with shape (output_size,)</span></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="st">        HINT: You may want to cache some of the values you compute in this</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="st">        function. Inspect your expressions for backprop to see which values</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a><span class="st">        should be cached.</span></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.append(<span class="dv">1</span>, x)</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.append(np.ones((x.shape[<span class="dv">0</span>], <span class="dv">1</span>)), x, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(x, <span class="va">self</span>.w.T)</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cache[<span class="st">&#39;x&#39;</span>] <span class="op">=</span> x</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z        </span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dz: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a><span class="st">        :param dz: partial derivative of loss with respect to output z of linear</span></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a><span class="st">        :return: dx, partial derivative of loss with respect to input x of linear</span></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="st">        Note that this function should set self.dw (gradient of weights with respect to loss)</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a><span class="st">        but not directly modify self.w; NN.step() is responsible for updating the weights.</span></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="st">        HINT: You may want to use some of the values you previously cached in</span></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a><span class="st">        your forward() method.</span></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: implement this based on your written answers!</span></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: when calculating dx, be careful to use the right &quot;version&quot; of</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the weight matrix!</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>        dx <span class="op">=</span> np.dot(<span class="va">self</span>.w.T, dz)[<span class="dv">1</span>:]</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dw <span class="op">=</span> np.dot(dz[:, np.newaxis], <span class="va">self</span>.cache[<span class="st">&#39;x&#39;</span>][:, np.newaxis].T)</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dx</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="st">        Apply SGD update to weights using self.dw, which should have been</span></span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="st">        set in NN.backward().</span></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: implement this!</span></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.dw</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dw <span class="op">=</span> np.zeros_like(<span class="va">self</span>.w, dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NN(<span class="bu">object</span>):</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size: <span class="bu">int</span>, hidden_size: <span class="bu">int</span>, output_size: <span class="bu">int</span>,</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>                 weight_init_fn, learning_rate: <span class="bu">float</span>):</span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a><span class="st">        Initalize neural network (NN) class. Note that this class is composed</span></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a><span class="st">        of the layer objects (Linear, Sigmoid) defined above.</span></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a><span class="st">        :param input_size: number of units in input to network</span></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a><span class="st">        :param hidden_size: number of units in the hidden layer of the network</span></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a><span class="st">        :param output_size: number of units in output of the network - this</span></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a><span class="st">                            should be equal to the number of classes</span></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a><span class="st">        :param weight_init_fn: function that creates and initializes weight</span></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a><span class="st">                               matrices for layer. This function takes in a</span></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a><span class="st">                               tuple (row, col) and returns a matrix with</span></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a><span class="st">                               shape row x col.</span></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a><span class="st">        :param learning_rate: learning rate for SGD training updates</span></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_init_fn <span class="op">=</span> weight_init_fn</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_size <span class="op">=</span> input_size</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> output_size</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: initialize modules (see section 9.1.2 of the writeup)</span></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: use the classes you&#39;ve implemented above!</span></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> Linear(input_size, hidden_size, weight_init_fn, learning_rate)</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> Sigmoid()</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> Linear(hidden_size, output_size, weight_init_fn, learning_rate)</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a><span class="st">        Neural network forward computation.</span></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="st">        Follow the pseudocode!</span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a><span class="st">        :param X: input data point *without the bias folded in*</span></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a><span class="st">        :param nn: neural network class</span></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a><span class="st">        :return: output prediction with shape (num_classes,). This should be</span></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a><span class="st">                 a valid probability distribution over the classes.</span></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>        z1 <span class="op">=</span> <span class="va">self</span>.linear1.forward(x)</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>        a1 <span class="op">=</span> <span class="va">self</span>.activation.forward(z1)</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a>        z2 <span class="op">=</span> <span class="va">self</span>.linear2.forward(a1)</span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> softmax(z2)</span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y: np.ndarray, y_hat: np.ndarray) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a><span class="st">        Neural network backward computation.</span></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a><span class="st">        Follow the pseudocode!</span></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a><span class="st">        :param y: label (a number or an array containing a single element)</span></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a><span class="st">        :param y_hat: prediction with shape (num_classes,)</span></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a><span class="st">        :param nn: neural network class</span></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>        dz2 <span class="op">=</span> d_softmax_cross_entropy(y, y_hat)</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>        da1 <span class="op">=</span> <span class="va">self</span>.linear2.backward(dz2)</span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>        dz1 <span class="op">=</span> <span class="va">self</span>.activation.backward(da1)</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>        dx <span class="op">=</span> <span class="va">self</span>.linear1.backward(dz1)</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a><span class="st">        Apply SGD update to weights.</span></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1.step()</span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2.step()</span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> print_weights(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a><span class="st">        An example of how to use logging to print out debugging infos.</span></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a><span class="st">        Note that we use the debug logging le    vel -- if we use a higher logging</span></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a><span class="st">        level, we will log things with the default logging configuration,</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a><span class="st">        causing potential slowdowns.</span></span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a><span class="st">        Note that we log NumPy matrices on separate lines -- if we do not do this,</span></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a><span class="st">        the arrays will be turned into strings even when our logging is set to</span></span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a><span class="st">        ignore debug, causing potential massive slowdowns.</span></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a><span class="st">        &#39;&#39;&#39;</span></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>        logging.debug(<span class="ss">f&quot;shape of w1: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>linear1<span class="sc">.</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>        logging.debug(<span class="va">self</span>.linear1.w)</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>        logging.debug(<span class="ss">f&quot;shape of w2: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>linear2<span class="sc">.</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>        logging.debug(<span class="va">self</span>.linear2.w)</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(X: np.ndarray, y: np.ndarray, nn: NN):</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a><span class="st">    Compute the label and error rate.</span></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="st">    :param X: input data</span></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y: label</span></span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a><span class="st">    :param nn: neural network class</span></span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a><span class="st">    :return:</span></span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a><span class="st">    labels: predicted labels</span></span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a><span class="st">    error_rate: prediction error rate</span></span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> nn.forward(x)</span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>        labels.append(np.argmax(y_hat))</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.array(labels)</span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a>    error_rate <span class="op">=</span> np.mean(labels <span class="op">!=</span> y)</span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels, error_rate</span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X_tr: np.ndarray, y_tr: np.ndarray,</span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a>          X_test: np.ndarray, y_test: np.ndarray,</span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a>          nn: NN, n_epochs: <span class="bu">int</span>):</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;&#39;&#39;</span></span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a><span class="st">    Train the network using SGD for some epochs.</span></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a><span class="st">    :param X_tr: train data</span></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y_tr: train label</span></span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a><span class="st">    :param X_te: train data</span></span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a><span class="st">    :param y_te: train label</span></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a><span class="st">    :param nn: neural network class</span></span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a><span class="st">    :param n_epochs: number of epochs to train for</span></span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a><span class="st">    :return:</span></span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a><span class="st">    train_losses: Training losses *after* each training epoch</span></span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a><span class="st">    test_losses: Test losses *after* each training epoch</span></span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a><span class="st">    &#39;&#39;&#39;</span></span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: implement this!</span></span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hint: Be sure to shuffle the train data at the start of each epoch</span></span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using *our provided* shuffle() function.</span></span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="op">=</span> []</span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>    test_losses <span class="op">=</span> []</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a>        X_train, y_train <span class="op">=</span> shuffle(X_tr, y_tr, epoch)</span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(X_train, y_train):</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> nn.forward(x)</span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a>            nn.backward(y, y_hat)</span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>            nn.step()</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;alpha:&#39;</span>, nn.linear1.w)</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;beta:&#39;</span>, nn.linear2.w)</span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a>        train_temp <span class="op">=</span> []</span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a>        test_temp <span class="op">=</span> []</span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(X_train, y_train):</span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> nn.forward(x)</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a>            train_temp.append(cross_entropy(y, y_hat))</span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(X_test, y_test):</span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> nn.forward(x)</span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a>            test_temp.append(cross_entropy(y, y_hat))</span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a>        train_losses.append(np.mean(train_temp))</span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a>        test_losses.append(np.mean(test_temp))</span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses, test_losses</span></code></pre></div>
<p>记录下来主要是学习这种思想，神经网络就是不同的组件一点一点拼接起来的，这点在复现transformer的时候深有感受，因此以后复现论文的话要按照这样的思路来，比如linear、sigmoid，都实现了forward和backword，这样的思想就不会显得特别复杂。特此记录。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-03-24</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" data-title="单隐层多分类网络" data-hashtags="Deep Learning,项目练习,单隐层多分类网络"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" data-hashtag="Deep Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" data-title="单隐层多分类网络"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" data-title="单隐层多分类网络"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/" data-title="单隐层多分类网络"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/deep-learning/">Deep Learning</a>,&nbsp;<a href="/tags/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/">项目练习</a>,&nbsp;<a href="/tags/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/">单隐层多分类网络</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/" class="prev" rel="prev" title="序列标注"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>序列标注</a>
            <a href="/flip/" class="next" rel="next" title="flip">flip<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://blog.vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
