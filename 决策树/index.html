<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>决策树 - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="决策树" />
<meta property="og:description" content="参考：https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html 《机器学习》周" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" /><meta property="og:image" content="https://vllbc.top/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-11-12T20:00:26+08:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://vllbc.top/logo.png"/>

<meta name="twitter:title" content="决策树"/>
<meta name="twitter:description" content="参考：https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html 《机器学习》周"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" /><link rel="prev" href="https://vllbc.top/logistic%E5%9B%9E%E5%BD%92/" /><link rel="next" href="https://vllbc.top/selectfrommodel/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "决策树",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/vllbc.top\/%E5%86%B3%E7%AD%96%E6%A0%91\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "Machine Learning, 分类算法","wordcount":  5890 ,
        "url": "https:\/\/vllbc.top\/%E5%86%B3%E7%AD%96%E6%A0%91\/","datePublished": "2022-03-19T00:00:00+00:00","dateModified": "2022-11-12T20:00:26+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/vllbc.top\/images\/avatar.png",
                    "width":  1080 ,
                    "height":  1080 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">决策树</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Machine Learning</a>&nbsp;<a href="/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>分类算法</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-03-19">2022-03-19</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 5890 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 12 分钟&nbsp;<span id="/%E5%86%B3%E7%AD%96%E6%A0%91/" class="leancloud_visitors" data-flag-title="决策树">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#步骤">步骤</a></li>
    <li><a href="#总结">总结</a></li>
    <li><a href="#代码">代码</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>参考：<a href="https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html" target="_blank" rel="noopener noreffer ">https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html</a></p>
<p>《机器学习》周志华</p>
<h1 id="决策树">决策树</h1>
<p>决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。这样，结点说法就与模块说法对应上了，理解就好。</p>
<h2 id="步骤">步骤</h2>
<p>1.特征选择</p>
<p>特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_2.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_2.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_2.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_2.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_2.jpg"
        title="机器学习实战教程（二）：决策树基础篇之让我们从相亲说起" /></p>
<p>希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p>
<p>特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。</p>
<p>(1).香农熵</p>
<p>在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)，这个名字来源于信息论之父克劳德·香农。</p>
<p>如果看不明白什么是信息增益和熵，请不要着急，因为他们自诞生的那一天起，就注定会令世人十分费解。克劳德·香农写完信息论之后，约翰·冯·诺依曼建议使用&quot;熵&quot;这个术语，因为大家都不知道它是什么意思。</p>
<p>熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为 ：</p>
<p>$$
l(x_i) = -log_{2}p(x_i)
$$
其中p(xi)是选择该分类的概率。有人可能会问，信息为啥这样定义啊？答曰：前辈得出的结论。这就跟1+1等于2一样，记住并且会用即可。上述式中的对数以2为底，也可以e为底(自然对数)。</p>
<p>通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到：</p>
<p>$$
H = -\sum_{i=1}^np(x_i)log_2p(x_i)
$$
期中n是分类的数目。熵越大，随机变量的不确定性就越大。</p>
<p>当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, = 1,2,3,..	.,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为 ：</p>
<p>$$
H(D) = -\sum_{k=1}^K \frac{|c_k|}{|D|}log_2\frac{|c_k|}{|D|}
$$
根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为：</p>
<p>$$
H(D) = -\frac{9}{15}log_2\frac{9}{15} - \frac{6}{15}log_2\frac{6}{15} = 0.971
$$
(2)信息增益</p>
<p>在上面，我们已经说过，如何选择特征，需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。</p>
<p>在讲解信息增益定义之前，我们还需要明确一个概念，条件熵。</p>
<p>熵我们知道是什么，条件熵又是个什么鬼？条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：</p>
<p>$$
H(Y|X) = \sum_{i=1}^np_iH(Y|X = x_i), \\
p_i = P(X = x_i)
$$
明确了条件熵和经验条件熵的概念。接下来，让我们说说信息增益。前面也提到了，信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：</p>
<p>$$
g(D,A) = H(D) - H(D|A)
$$
一般地，熵H(D)与条件熵H(D|A)之差称为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<p>设特征A有n个不同的取值{a1,a2,···,an}，根据特征A的取值将D划分为n个子集{D1,D2，···,Dn}，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik = Di ∩ Ck，|Dik|为Dik的样本个数。于是经验条件熵的公式可以些为</p>
<p>$$
H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}
$$</p>
<p>说了这么多概念性的东西，没有听懂也没有关系，举几个例子，再回来看一下概念，就懂了。</p>
<p>以贷款申请样本数据表为例进行说明。看下年龄这一列的数据，也就是特征A1，一共有三个类别，分别是：青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集出现的概率是十五分之五，也就是三分之一。同理，年龄是中年和老年的数据在训练数据集出现的概率也都是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在五个数据中，只有两个数据显示拿到了最终的贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别为五分之三、五分之四。所以计算年龄的信息增益，过程如下：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_13.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_13.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_13.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_13.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_13.jpg"
        title="机器学习实战教程（二）：决策树基础篇之让我们从相亲说起" /></p>
<p>同理，计算其余特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_14_m.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_14_m.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_14_m.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_14_m.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_14_m.jpg"
        title="机器学习实战教程（二）：决策树基础篇之让我们从相亲说起" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_15.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_15.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_15.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/m_2_15.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/m_2_15.jpg"
        title="机器学习实战教程（二）：决策树基础篇之让我们从相亲说起" /></p>
<p>最后，比较特征的信息增益，由于特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征。</p>
<p>由于特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。它将训练集D划分为两个子集D1(A3取值为&quot;是&quot;)和D2(A3取值为&quot;否&quot;)。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。</p>
<p>对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_2-b.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_2-b.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_2-b.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_2-b.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_2-b.jpg"
        title="机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜" /></p>
<p>根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。由于A2有两个可能取值，从这一结点引出两个子结点：一个对应&quot;是&quot;(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为&quot;是&quot;；另一个是对应&quot;否&quot;(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为&quot;否&quot;。</p>
<p>这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_3.jpg"
        data-srcset="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_3.jpg, https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_3.jpg 1.5x, https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_3.jpg 2x"
        data-sizes="auto"
        alt="https://cuijiahua.com/wp-content/uploads/2017/11/ml_3_3.jpg"
        title="机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜" /></p>
<h2 id="总结">总结</h2>
<p>我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。</p>
<p>构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。</p>
<p>决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。</p>
<p><strong>决策树的一些优点：</strong></p>
<ul>
<li>易于理解和解释。决策树可以可视化。</li>
<li>几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。</li>
<li>使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。</li>
<li>可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。</li>
<li>可以处理多值输出变量问题。</li>
<li>使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。</li>
<li>即使对真实模型来说，假设无效的情况下，也可以较好的适用。</li>
</ul>
<p><strong>决策树的一些缺点：</strong></p>
<ul>
<li>决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。</li>
<li>决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。</li>
<li>概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems。</li>
<li>如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。</li>
</ul>
<h2 id="代码">代码</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">equalNums</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        计算标记集中某个标记的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">        label - 某个标记
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        num - 某个标记的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_list</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calcShannonEnt</span><span class="p">(</span><span class="n">label_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        计算信息熵
</span></span></span><span class="line"><span class="cl"><span class="s2">        对应公式 Ent(D) = -∑ Pk*log2(Pk) k=1..len(label_set)
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        shannonEnt - 当前标记集的信息熵
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">len_label_list</span> <span class="o">=</span> <span class="n">label_list</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl">    <span class="n">shannonEnt</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_set</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">prob</span> <span class="o">=</span> <span class="n">equalNums</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span><span class="o">/</span><span class="n">len_label_list</span>
</span></span><span class="line"><span class="cl">        <span class="n">shannonEnt</span> <span class="o">-=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">shannonEnt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">conditionnalEntropy</span><span class="p">(</span><span class="n">feature_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        计算条件信息熵，对应信息增益公式中的被减项
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        feature_list - sample_list中某一列，表示当前属性的所有值
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        entropy - 条件信息熵
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">feature_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">feature_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">entropy</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feature_set</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">pro</span> <span class="o">=</span> <span class="n">equalNums</span><span class="p">(</span><span class="n">feature_list</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span><span class="o">/</span><span class="n">feature_list</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl">        <span class="n">entropy</span> <span class="o">+=</span> <span class="n">pro</span> <span class="o">*</span> <span class="n">calcShannonEnt</span><span class="p">(</span><span class="n">label_list</span><span class="p">[</span><span class="n">feature_list</span> <span class="o">==</span> <span class="n">feat</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">entropy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calcInfoGain</span><span class="p">(</span><span class="n">feature_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        计算信息增益
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        feature_list - sample_list中某一列，表示当前属性的所有值
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        当前属性的信息增益
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">calcShannonEnt</span><span class="p">(</span><span class="n">label_list</span><span class="p">)</span> <span class="o">-</span> <span class="n">conditionnalEntropy</span><span class="p">(</span><span class="n">feature_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">splitDataSet</span><span class="p">(</span><span class="n">sample_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        决策树在选好当前最优划分属性之后划分样本集
</span></span></span><span class="line"><span class="cl"><span class="s2">        依据value选择对应样例，并去除第axis维属性
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        feature_list - sample_list中某一列，表示当前属性的所有值
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        return_sample_list, return_label_list
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># sample_list[sample_list[...,axis] == value] 利用了numpy数组的布尔索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">filtered_sample_list</span> <span class="o">=</span> <span class="n">sample_list</span><span class="p">[</span><span class="n">sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">axis</span><span class="p">]</span> <span class="o">==</span> <span class="n">value</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">return_label_list</span> <span class="o">=</span> <span class="n">label_list</span><span class="p">[</span><span class="n">sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">axis</span><span class="p">]</span> <span class="o">==</span> <span class="n">value</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># np.hstack 将数组横向拼接，也就是去除第axis维属性</span>
</span></span><span class="line"><span class="cl">    <span class="n">return_sample_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">filtered_sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,:</span><span class="n">axis</span><span class="p">],</span> <span class="n">filtered_sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">return_sample_list</span><span class="p">,</span> <span class="n">return_label_list</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">chooseBestFeatureToSplit</span><span class="p">(</span><span class="n">sample_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        选取最优划分属性
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        sample_list - 样本集
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        bestFeat_index - 最优划分属性的索引值
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">numFeatures</span> <span class="o">=</span> <span class="n">sample_list</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">bestInfoGain</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">bestFeat_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numFeatures</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">infoGain</span> <span class="o">=</span> <span class="n">calcInfoGain</span><span class="p">(</span><span class="n">sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">infoGain</span> <span class="o">&gt;</span> <span class="n">bestInfoGain</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">bestInfoGain</span> <span class="o">=</span> <span class="n">infoGain</span>
</span></span><span class="line"><span class="cl">            <span class="n">bestFeat_index</span> <span class="o">=</span> <span class="n">i</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bestFeat_index</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">createTree</span><span class="p">(</span><span class="n">sample_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">attr_list_copy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        生成决策树
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        sample_list - 样本集
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list - 标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">        attr_list_copy - 属性集（之所以加copy是为了删属性的时候是在副本上，防止递归出错）
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        myTree - 最终的决策树
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># attr_list 有del操作，不用副本的话递归会出错</span>
</span></span><span class="line"><span class="cl">    <span class="n">attr_list</span> <span class="o">=</span> <span class="n">attr_list_copy</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">label_list</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>       <span class="c1"># 如果只有一种标记，直接返回标记</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">label_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">sample_list</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>         <span class="c1"># 如果所有属性都被遍历，返回最多的标记</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">voteLabel</span><span class="p">(</span><span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># bestFeat_index 最优划分属性的索引值</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># bestAttr 最优划分属性对应的名字</span>
</span></span><span class="line"><span class="cl">    <span class="n">bestFeat_index</span> <span class="o">=</span> <span class="n">chooseBestFeatureToSplit</span><span class="p">(</span><span class="n">sample_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bestAttr</span> <span class="o">=</span> <span class="n">attr_list</span><span class="p">[</span><span class="n">bestFeat_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">myTree</span> <span class="o">=</span> <span class="p">{</span><span class="n">bestAttr</span><span class="p">:</span> <span class="p">{}}</span>
</span></span><span class="line"><span class="cl">    <span class="k">del</span><span class="p">(</span><span class="n">attr_list</span><span class="p">[</span><span class="n">bestFeat_index</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">feat_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sample_list</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">bestFeat_index</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 依据最优划分属性进行划分，并向下递归</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feat_set</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_sample_list</span><span class="p">,</span> <span class="n">return_label_list</span> <span class="o">=</span> <span class="n">splitDataSet</span><span class="p">(</span><span class="n">sample_list</span><span class="p">,</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">bestFeat_index</span><span class="p">,</span> <span class="n">feat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">myTree</span><span class="p">[</span><span class="n">bestAttr</span><span class="p">][</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">createTree</span><span class="p">(</span><span class="n">return_sample_list</span><span class="p">,</span> <span class="n">return_label_list</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">myTree</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">voteLabel</span><span class="p">(</span><span class="n">label_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        这个函数是用在遍历完所有特征时，返回最多的类别
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        label_list: 标记列表
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        数量最多的标记
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># unique_label_list 是label_list中标记种类列表</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># label_num 是unique_label_list对应的数量列表</span>
</span></span><span class="line"><span class="cl">    <span class="n">unique_label_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">label_list</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_num_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">unique_label_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">label_num_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">equalNums</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># label_num.index(max(label_num))是label_num数组中最大值的下标</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">unique_label_list</span><span class="p">[</span><span class="n">label_num_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">label_num_list</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">decisionTree</span><span class="p">,</span> <span class="n">testVec</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        对tesVec进行分类
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        decisionTree - 决策树
</span></span></span><span class="line"><span class="cl"><span class="s2">        attr_list - 属性名列表
</span></span></span><span class="line"><span class="cl"><span class="s2">        testVec - 测试向量
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        label - 预测的标记
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">feature</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">decisionTree</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>          <span class="c1"># feature为决策树的根节点</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_dict</span> <span class="o">=</span> <span class="n">decisionTree</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>            <span class="c1"># feature_dict为根节点下的子树</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_index</span> <span class="o">=</span> <span class="n">attr_list</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>        <span class="c1"># feature_index为feature对应的属性名索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_value</span> <span class="o">=</span> <span class="n">testVec</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span>          <span class="c1"># feature_value为测试集中对应属性的值</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">feature_value</span> <span class="ow">in</span> <span class="n">feature_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果没有结果就继续向下找</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">[</span><span class="n">feature_value</span><span class="p">])</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">[</span><span class="n">feature_value</span><span class="p">],</span> <span class="n">testVec</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span> <span class="o">=</span> <span class="n">feature_dict</span><span class="p">[</span><span class="n">feature_value</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">label</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">testAccuracy</span><span class="p">(</span><span class="n">decisionTree</span><span class="p">,</span> <span class="n">test_sample_list</span><span class="p">,</span> <span class="n">test_label_list</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    函数说明：
</span></span></span><span class="line"><span class="cl"><span class="s2">        测试十次得到正确率均值
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters：
</span></span></span><span class="line"><span class="cl"><span class="s2">        decisionTree - 决策树
</span></span></span><span class="line"><span class="cl"><span class="s2">        test_sample_list - 测试样本集
</span></span></span><span class="line"><span class="cl"><span class="s2">        test_label_list - 测试标记集
</span></span></span><span class="line"><span class="cl"><span class="s2">        attr_list - 属性集
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns：
</span></span></span><span class="line"><span class="cl"><span class="s2">        
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">oneTime</span><span class="p">(</span><span class="n">decisionTree</span><span class="p">,</span> <span class="n">test_sample_list</span><span class="p">,</span> <span class="n">test_label_list</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">rightNum</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">predict_label_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sample_list</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="n">predict_label</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">decisionTree</span><span class="p">,</span> <span class="n">test_sample_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">attr_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">predict_label_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">predict_label</span> <span class="o">==</span> <span class="n">test_label_list</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="n">rightNum</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">rightNum</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sample_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">accuracy</span>
</span></span><span class="line"><span class="cl">    <span class="nb">sum</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">sum</span> <span class="o">+=</span> <span class="n">oneTime</span><span class="p">(</span><span class="n">decisionTree</span><span class="p">,</span> <span class="n">test_sample_list</span><span class="p">,</span> <span class="n">test_label_list</span><span class="p">,</span> <span class="n">attr_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">av_accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="o">/</span><span class="mi">10</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">av_accuracy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataSet</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span>            
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;年龄&#39;</span><span class="p">,</span> <span class="s1">&#39;有工作&#39;</span><span class="p">,</span> <span class="s1">&#39;有自己的房子&#39;</span><span class="p">,</span> <span class="s1">&#39;信贷情况&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">createTree</span><span class="p">(</span><span class="n">dataSet</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dataSet</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">{&#39;有自己的房子&#39;: {&#39;0&#39;: {&#39;有工作&#39;: {&#39;0&#39;: &#39;no&#39;, &#39;1&#39;: &#39;yes&#39;}}, &#39;1&#39;: &#39;yes&#39;}}
</span></span></code></pre></td></tr></table>
</div>
</div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-11-12&nbsp;<a class="git-hash" href="https://github.com/dillonzq/LoveIt/commit/34df4c4cdc240b2cf86b61483b4316e8db7b193c" target="_blank" title="commit by vllbc(1683070754@qq.com) 34df4c4cdc240b2cf86b61483b4316e8db7b193c: 同上">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>34df4c4</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E5%86%B3%E7%AD%96%E6%A0%91/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="决策树" data-hashtags="Machine Learning,分类算法"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="决策树"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="决策树"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="决策树"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">分类算法</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/logistic%E5%9B%9E%E5%BD%92/" class="prev" rel="prev" title="Logistic回归"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Logistic回归</a>
            <a href="/selectfrommodel/" class="next" rel="next" title="SelectFromModel">SelectFromModel<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.zh-cn","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
