<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>面经2025 - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="面经2025">
  <meta property="og:description" content="常见问题总结 多轮对话sft样本怎么构造？ [大模型微调样本构造 trick]（https://zhuanlan.zhihu.com/p/641562439)
多轮对话的传统组织方式：将多轮对话拆分为多条独立的训练样本，如 Q1A1/Q2A2/Q3A3 可拆分为 Q1—&gt;A1， Q1A1Q2-&gt;A2， Q1A1Q2A2Q3-&gt;A3 三条样本。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-02T00:00:00+00:00">
    <meta property="article:tag" content="面经">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="面经2025">
  <meta name="twitter:description" content="常见问题总结 多轮对话sft样本怎么构造？ [大模型微调样本构造 trick]（https://zhuanlan.zhihu.com/p/641562439)
多轮对话的传统组织方式：将多轮对话拆分为多条独立的训练样本，如 Q1A1/Q2A2/Q3A3 可拆分为 Q1—&gt;A1， Q1A1Q2-&gt;A2， Q1A1Q2A2Q3-&gt;A3 三条样本。">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" /><link rel="prev" href="https://blog.vllbc.top/world_model/" /><link rel="next" href="https://blog.vllbc.top/webevolverenhancing-web-agent-self-improvement-with-coevolving-world-model/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "面经2025",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/%E9%9D%A2%E7%BB%8F2025\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "面经","wordcount":  6419 ,
        "url": "https:\/\/blog.vllbc.top\/%E9%9D%A2%E7%BB%8F2025\/","datePublished": "2025-07-02T00:00:00+00:00","dateModified": "2025-07-02T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animate__animated animate__flipInX">面经2025</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E9%9D%A2%E7%BB%8F/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>面经</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-07-02">2025-07-02</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 6419 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 13 分钟&nbsp;<span id="/%E9%9D%A2%E7%BB%8F2025/" class="leancloud_visitors" data-flag-title="面经2025">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="content" id="content"><h1 id="常见问题总结">常见问题总结</h1>
<h2 id="多轮对话sft样本怎么构造">多轮对话sft样本怎么构造？</h2>
<ul>
<li><p>[大模型微调样本构造
trick]（https://zhuanlan.zhihu.com/p/641562439)</p>
<ul>
<li><p>多轮对话的传统组织方式：将多轮对话拆分为多条独立的训练样本，如
Q1A1/Q2A2/Q3A3 可拆分为 Q1—&gt;A1， Q1A1Q2-&gt;A2， Q1A1Q2A2Q3-&gt;A3
三条样本。</p></li>
<li><p>将整个 session 的对话内容拼接成一个长文本序列，例如：Q1 A1 Q2 A2
Q3 A3。这样，整个 session
被表示为一个连续的文本序列，而不是多条独立的样本。</p></li>
<li><p>构造时计算损失有一些坑，详见
https://zhuanlan.zhihu.com/p/721652210</p></li>
</ul></li>
</ul>
<h2 id="sft-是否可以注入知识">SFT 是否可以注入知识？</h2>
<ul>
<li><p>continue pretrain 注入知识。</p></li>
<li><p>sft 对齐输出格式。（sft
在一些特定的场景下确实是可以注入知识的）</p></li>
</ul>
<h2 id="如何解决灾难性遗忘">如何解决灾难性遗忘？</h2>
<ol type="1">
<li><p>保留通用数据：在进行领域数据训练时，仍然需要保留一部分通用数据用于模型训练。这样可以确保模型仍然能够学习到通用的语言和知识，从而保持一定的通用能力。</p></li>
<li><p>增量学习：使用增量学习（Incremental
Learning）的方法，将领域数据与通用数据逐步交替进行训练。这样可以在学习新领域的同时，保持对通用知识的记忆。</p></li>
<li><p><a
href="https://zhida.zhihu.com/search?content_id=234154468&amp;content_type=Article&amp;match_order=1&amp;q=%E6%95%B0%E6%8D%AE%E9%87%8D%E9%87%87%E6%A0%B7&amp;zhida_source=entity">数据重采样</a>：在进行领域数据训练时，可以使用数据重采样的方法，使得模型在训练过程中能够更多地接触到通用数据，从而缓解遗忘通用能力的问题。</p></li>
<li><p>强化学习：使用强化学习的方法，通过给模型设置奖励机制，鼓励模型在领域任务上表现好，同时保持一定的通用能力。</p></li>
<li><p><a
href="https://zhida.zhihu.com/search?content_id=234154468&amp;content_type=Article&amp;match_order=1&amp;q=%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94%E6%8A%80%E6%9C%AF&amp;zhida_source=entity">领域适应技术</a>：使用领域适应技术，如<a
href="https://zhida.zhihu.com/search?content_id=234154468&amp;content_type=Article&amp;match_order=1&amp;q=%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94&amp;zhida_source=entity">领域自适应</a>（Domain
Adaptation）和领域对抗训练（Domain Adversarial
Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。</p></li>
<li><p><a
href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.13669">SDFT</a>：微调前，让大模型将任务数据集重写一遍。这样的话，重写后的任务<a
href="https://zhida.zhihu.com/search?content_id=234154468&amp;content_type=Article&amp;match_order=2&amp;q=%E6%95%B0%E6%8D%AE%E9%9B%86&amp;zhida_source=entity">数据集</a>的分布和大模型的差异就小了很多。在这样的数据集上微调对大模型分布上的改变会小很多，对大模型通用能力的损害也会降低。</p></li>
<li><p>Llama-Pro:在原始模型中每个Transformer块或者某几个Transformer块后增加一个Transformer块，但为了保持扩展后的模型输出保持不变，需要增加的块为恒等块（输入输出相同）</p></li>
</ol>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MjQwZGU5ZWMyMGI2YmI0MDNiMGRiZDY1NTZiYjBmYTNfT093bXI0bmVSU1FER2ZHMzZzTUo5VUV3WnF3VHhyemNfVG9rZW46RlJ3TGJoTWtFb05WSm54eXB1RmNIdzJRbnRnXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h2
id="为什么大模型都是-decoder-only-架构httpswww.zhihu.comquestion588325646answer3357252612">为什么大模型都是
Decoder-Only
架构？（https://www.zhihu.com/question/588325646/answer/3357252612）</h2>
<ul>
<li><p>泛化性能更好：<a
href="https://zhida.zhihu.com/search?content_id=640486327&amp;content_type=Answer&amp;match_order=1&amp;q=ICML+22&amp;zhida_source=entity">ICML
22</a>的<a
href="https://link.zhihu.com/?target=https%3A//proceedings.mlr.press/v162/wang22u/wang22u.pdf">What
language model architecture and pretraining objective works best for
zero-shot generalization?.</a> 在最大5B参数量、170B
token数据量的规模下做了一些列实验，发现用next token
prediction预训练的decoder-only模型在各种下游任务上zero-shot泛化性能最好</p></li>
<li><p>苏神强调的注意力满秩的问题，双向attention的注意力矩阵容易退化为低秩状态，而
causal
attention的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强；</p></li>
<li><p><span class="citation"
data-cites="yili大佬强调的预训练任务难度问题">@yili大佬强调的预训练任务难度问题</span>，纯粹的decoder-only架构+next
token
predicition预训练，每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高；</p></li>
<li><p><span class="citation" data-cites="mimimumu">@mimimumu</span>
大佬强调，上下文学习为decoder-only架构带来的更好的few-shot性能：prompt
和demonstration的信息可以视为对模型参数的隐式微调，decoder-only的架构相比encoder-decoder在in-context
learning上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，微调的信号更强；</p></li>
<li><p>多位大佬强调了一个很容易被忽视的属性，causal
attention（就是decoder-only的单向
attention）具有隐式的位置编码功能，打破了transformer的位置不变性，而带有双向
attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。</p></li>
<li><p>decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到。</p></li>
</ul>
<h2 id="transfomer-attention计算为什么除以根号-d">Transfomer
attention计算为什么除以根号 d？</h2>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NWU0MTA2YzUyZjJhZDhhOTdmM2JkZDAwN2Y4NWE4NDBfOFlSRjM5eVdEOVh5MmVTS2dXSktlSllUN1hUSHZFNVBfVG9rZW46TE5vcmJ4MWZSb0ljM2N4ak82UmM5Y1dBbk1nXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h2
id="为什么不在rollout阶段保存logp而是再计算一遍logp">为什么不在rollout阶段保存logp而是再计算一遍logp？</h2>
<p>因为forward的时候和generate的时候logprob由于推理引擎（vllm）和训练引擎（fsdp）的优化目标不一样，会造成两者对不上，因此需要做两次。</p>
<p>batch 算子的细微差异，都会造成这两个 log_prob
不完全一致。推理引擎要的是快速出 token
id，训练引擎需要保证一定的log_prob 精度。</p>
<h2 id="为什么需要-server-来完成-rollout">为什么需要 Server 来完成
rollout？</h2>
<p>为了配合 agentic LLM 的训练，在现有的PPO/GRPO 算法的基础上，从 single
turn rollout 改动为 environment interactive multi-turn rollout
的需求非常强烈。</p>
<p>这一过程中，policy 与 environment 的交互存在绝对不可忽视的延迟，turn
之间的等待时间很长。一直用 Engine 做 rollout 的话（
engine.generate），可能连 continuous batching 都组不起来。所以，改用
server 来通过 https 做 rollout的需求就呼之欲出了。实际上，这也是
最自然的工作方式。除此之外，environment 的交互往往也是通过 https
请求来完成的。譬如，众多 coding sandbox 都是 environment 自己启动一个
server 暴露一个 port，然后往里面发请求来实现交互的。</p>
<p>总之，为了在 training engine,rollout 和 environment
三个子进程中保持良好的通讯和交互，选择 server 势在必行。</p>
<h2
id="mcp和fuction-call的区别httpszhuanlan.zhihu.comp1898326676087223572">MCP和fuction
call的区别？（https://zhuanlan.zhihu.com/p/1898326676087223572）</h2>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NDlkNjY4ZDUyNjNkOTY1ZjVkNTc5ZWE3Y2ExMGFmYTNfUHMwcVcyNlZiTE5PaFhGSXEzV1dydHNXc0ZHVkdLSk9fVG9rZW46QUxMNGJabktQb0x2bEt4bmlna2N3d042bm9iXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<ul>
<li><p>函数调用是一种机制，它允许 LLM
根据用户的输入识别它需要什么工具以及何时调用它。</p></li>
<li><p>MCP（即模型上下文协议,<a
href="https://zhida.zhihu.com/search?content_id=256840389&amp;content_type=Article&amp;match_order=1&amp;q=Model+Context+Protocol&amp;zhida_source=entity">Model
Context Protocol</a>）试图标准化此过程。 MCP (Model Context
Protocol):是一个开放协议和标准，旨在标准化AI 应用（MCP
客户端）如何发现、连接和与外部工具/数据源（实现为 MCP
服务器）进行交互。它关注的是系统间的通信和集成，解决 Function Calling
指令生成后，如何高效、安全、可扩展地执行这些调用。</p></li>
</ul>
<h2 id="为什么r1不使用模型reward">为什么R1不使用模型reward？</h2>
<ul>
<li><p>reward hacking</p></li>
<li><p>训练资源问题，成本过高。</p></li>
</ul>
<h2
id="为什么prmmcts这条路走不通httpszhuanlan.zhihu.comp19623772462">为什么PRM+MCTS这条路走不通？（https://zhuanlan.zhihu.com/p/19623772462）</h2>
<ul>
<li><p>在reasoning任务中如何显式定义step，比如以<code>\n</code>
还是以推理逻辑来划分step？</p></li>
<li><p>如何定义step正确性，将影响step labeler来高效标注</p></li>
<li><p>PRM容易reward hacking</p></li>
<li><p>LLM比象棋搜索空间大太多</p></li>
<li><p>MCTS价值影响模型生成质量（不如纯CoT采样）</p></li>
</ul>
<h2 id="grpo中可以去掉kl项吗">GRPO中可以去掉KL项吗？</h2>
<p>可以。</p>
<ol type="1">
<li><p>去除KL项, 意味着不需要ref-model,
减少一个模型的显存，减少一次前向ref_policy的计算。</p></li>
<li><p>没有KL的约束，那么可以将过大的梯度进行裁剪(max_grad_norm)，避免优化的不稳定性(这也是另一种层面的clip)。</p></li>
<li><p>没有KL的约束，参数的优化更加自由，更容易探索到好的回答</p></li>
</ol>
<h2
id="grpo-的损失为什么会为负参考-httpszhuanlan.zhihu.comp28326620566">GRPO
的损失为什么会为负（参考 https://zhuanlan.zhihu.com/p/28326620566）</h2>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YmNkMDNkNTM1ZWM0ZGU3NWY1ZjhjMmY1YWRlZGVlZGZfb3FxVTFqSUVxdWYyOGhJeTdjUHZVTTBEWDZYZGwxM3ZfVG9rZW46RHdqT2J1UVlDb05hbkJ4WVdqMWNxMmlsblZlXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p>总结：组奖励变化对 loss 上升影响不直观。优化策略远离 ref，KL
变化大，导致 Loss 上升明显。</p>
<h2
id="qwen模型为什么随机奖励也能workhttpsrethink-rlvr.notion.sitespurious-rewards-rethinking-training-signals-in-rlvr-1f4df34dac1880948858f95aeb88872f">Qwen模型为什么随机奖励也能work？（https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f）</h2>
<ul>
<li><p>模型依赖性：研究发现，RLVR的有效性更多地依赖于模型的预训练能力，而不是监督信号的质量。Qwen模型在预训练期间学会了特定的推理策略，这些策略可以通过RLVR轻易地被激发出来，而其他模型则不具备这些策略。</p></li>
<li><p>代码推理策略：Qwen-Math模型在预训练阶段就频繁地使用Python代码来解决数学问题，即使在没有代码执行器的情况下，也能生成正确的代码输出和答案。RLVR训练（无论奖励质量如何）进一步增加了这种代码推理的频率，从而提高了性能。</p></li>
<li><p>奖励信号的作用：不可靠奖励通过放大模型在预训练期间学到的有用推理表示来发挥作用。这些奖励信号并没有教会模型任务质量，而是触发了一种集中效应，使模型专注于其现有的推理模式分布。</p></li>
</ul>
<h2
id="为什么dpo里chosen和rejected概率会同时下降httpszhuanlan.zhihu.comp6327313416">为什么DPO里Chosen和Rejected概率会同时下降?（https://zhuanlan.zhihu.com/p/6327313416）</h2>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZWNkNTA5N2U4NTM0YjExNTM1MjE1YTFhMjU2YWIwMzZfdUNYSDNmbnA2bTNzbmxqMDJIQ3YzckYyaFFlQUFPRFBfVG9rZW46UHpEbmJFRTFvb3pNUUJ4aUV4bWNsQjBzbmpjXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmVhMDM1OTZiMGZiN2U5ZGU3ODRiYWE0NWNhN2Q2ZWFfVFBEMFdlNHdESWtSaWtFelVFajY4TlhMTndCQkxvcWNfVG9rZW46VWZ5QWJzOGowb0d4M0Z4VHY3ZWN6VEIwbk5jXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h2 id="reward-hacking如何解决">reward hacking如何解决？</h2>
<h2
id="梯度累积两次跟-batch-size-增大-2-倍在多数情况下效果一样吗loss-的-3-次平均"><a
href="https://www.zhihu.com/question/583011902/answer/7205474551">梯度累积两次，跟
batch size 增大 2 倍，在多数情况下，效果一样吗？</a>（loss 的 3
次平均）</h2>
<p>理论上，<a
href="https://zhida.zhihu.com/search?content_id=694556710&amp;content_type=Answer&amp;match_order=1&amp;q=%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1&amp;zhida_source=entity">梯度累计</a>在数学上应该等同于<a
href="https://zhida.zhihu.com/search?content_id=694556710&amp;content_type=Answer&amp;match_order=1&amp;q=%E5%85%A8%E6%89%B9%E9%87%8F%E8%AE%AD%E7%BB%83&amp;zhida_source=entity">全批量训练</a>，但实际发现
loss 并不匹配。( <a
href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/trl/issues/2175">Gradient
accumulation yields worse results than the equivalent batch size · Issue
#2175 · huggingface/trl</a>)</p>
<p>一般情况下，loss 计算会经历三次平均</p>
<ol type="1">
<li><p>micro batch 维度，分母是这个 micro batch 中的所有 label 不是 -100
的 token 数<strong>（不同 token 之间 loss 的平均）</strong></p></li>
<li><p>DP 维度，分母是 DP size <strong>（和 GPU 数量相关，不同机器之间
loss 的平均）</strong></p></li>
<li><p>梯度累加维度，分母是梯度累加数。<strong>（不同 batch 之间的 loss
的平均）</strong></p></li>
</ol>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NDkxZDY5NzFlYTM1ZjdiODJmNzM4NTE0NjBiNDgzNTFfNkRicjNyRWVZNFpxQXVmcG1lamw2SzR0ZGxoNGRzRG9fVG9rZW46SWY4UGJMRzlkb2l5c1h4NEFHZGNLRkxjbjVmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h2 id="为什么不用大模型做-embedding">为什么不用大模型做
embedding？</h2>
<ul>
<li><p>大模型主要训练的预测 next token 的能力，而非判断整个句子
embedding 的好坏。因此使用 LLM 做嵌入效果不理想。</p></li>
<li><p>部署成本高。</p></li>
</ul>
<h1 id="基础知识">基础知识</h1>
<h2
id="post-training总结httpsmp.weixin.qq.comsvlwu3ynja1szrcyszqc4hw">Post-Training总结（https://mp.weixin.qq.com/s/VLWU3YnJa1SZRCySZQc4Hw）</h2>
<h2 id="flash-attention">Flash Attention</h2>
<h2 id="moe">MoE</h2>
<h2 id="kv-cache">KV cache</h2>
<h2 id="从-mha-到-mla">从 MHA 到 MLA</h2>
<h2 id="transformer-相关">Transformer 相关</h2>
<h3 id="手撕各模块代码">手撕各模块代码</h3>
<h3
id="free-running-mode-和-teacher-forcing-modehttpszhuanlan.zhihu.comp630356292">free-running
mode 和 teacher forcing
mode（https://zhuanlan.zhihu.com/p/630356292)</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDdhODlkYTVmMTI5NmYyODNiNGNhYjI1ZDg4NDNkMWRfc0RiVUJDOVVsWFFnWTR6SnpQeXAyT0xYYVlGZmlxMDhfVG9rZW46T25xbGJ3bnF3b2x0UmF4MXhnWmNqV244bkhmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="transformer-参数量计算">Transformer 参数量计算</h3>
<h2 id="overthinking-怎么解决">overthinking 怎么解决</h2>
<h3 id="相关文献">相关文献</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=OTBhZjk0NDk5YTc5YTkyN2NmNDBiMzAyNTYzOWNjNDdfeWpKMzVEUThkNjNoMTFTZGI5elpGUzQ5Y2hicDNGRVlfVG9rZW46VGhvYWJLMVVRb3puSVZ4NWxJY2NkRGJKbjRjXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="方法">方法</h3>
<ul>
<li><p>推理链压缩：在保持推理质量的同时，减少生成的 token
数量。</p></li>
<li><p>长度预算控制：在推理过程中动态调整生成内容的长度，以提高效率。</p></li>
<li><p>系统切换与模型切换：根据任务需求灵活选择不同的模型或推理路径。</p></li>
<li><p>并行搜索：利用并行计算加速推理过程。</p></li>
</ul>
<h2 id="deepspeedzero-显存优化">DeepSpeed（zero 显存优化）</h2>
<h3
id="模型显存占用bf16fp16httpszhuanlan.zhihu.comp665172400">模型显存占用（bf16/fp16）（https://zhuanlan.zhihu.com/p/665172400）</h3>
<h4 id="训练时">训练时</h4>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MTZkZTU5ZTM4NmZiNmMwMWU5NDRjZDNjZDNhZmFjNmVfckJHRVZabkVzZGhab1BIejhvdlhNRFVTVTl6SWRHanFfVG9rZW46U0FYNmJlRGMyb2xvdzh4dFVBQmNZeW1JbmphXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZWU0YTEzZjIwNDNiNWIzNjM1NTRkYWJmNzVjZDg3YTVfOGdpTWJXeG5qajZLaFJJdlRqR25BVDk1QmZlZDlRT2ZfVG9rZW46RFdTMGJwaVgybzZoSWh4bkZjTGM5Rjk5bm1iXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p>可见激活值占大头。</p>
<p>优化方法如下：</p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTMzMGY2ZjNmYTkwNzRhYjk0OWE1ZjE0MTk0YzhlNGRfSE0xekJPVEFQZHhmNTRnOXNWQTd0ZlZ2YllXaWdTTTFfVG9rZW46TkxHTGJ0QXdJb2dqbWN4MVc4WmNNNUZ6bjZmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h4 id="推理时">推理时</h4>
<p>一个经验法则是：推理时的峰值显存大致是模型参数显存的 1.5 - 2.5
倍（尤其在处理长序列或大批次时）。更精确的估计需要结合具体模型和输入</p>
<ul>
<li><p>输入/输出的 Token 存储：需要显存存储输入的 Token
嵌入（embedding）和生成的输出 Token。</p></li>
<li><p>中间激活值（Intermediate
Activations）：前向传播过程中每一层的输出（如 Attention 的 Key/Value
缓存、FFN 的中间结果等）。</p></li>
<li><p>Key-Value 缓存（KV Cache）：自回归生成时，为避免重复计算历史
Token 的
Key/Value，需缓存这些中间结果（显存占用与输入+输出长度成正比）</p></li>
</ul>
<h3 id="zero3">zero3</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MDhiYWZiNDg4ZWM2OTE0ZjMwZGNiZjNkNGMyMzZlM2ZfakI3MnlCd3h5RkRyUGRlV1NRZjhrMVB0ZkNVZ3lPMFdfVG9rZW46VldsRmJnSnJ1b1VxNW54M0pvYmN2MG5rbmJmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NWI2Njk5YjliYmNlZWEyMmM1MmIxYWI1NTEzNWZlMjJfT1BtRnZ2VWxQZ3dMNFExNXZpR0dYMlJZMFNQMnJjRDdfVG9rZW46QUdGRGIyOGdzb2ZET1d4eU1JaGM5T0dtbjNjXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p>这是inter-layer + broadcast +
reduce-scatter的实现方法，是很久之前官方的实现，现在官方的真正做法是：<strong>intra-layer
+ all-gather+ reduce-scatter（与fsdp一致）</strong></p>
<h2 id="位置编码rope">位置编码（RoPE）</h2>
<h2 id="lora手撕">LoRA（手撕）</h2>
<h2 id="r1-相关">R1 相关</h2>
<h3 id="训练整体流程">训练整体流程</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YjgxMjE5YzI4ZGMwNDQ5NTM5N2IxY2I3MjUyODRkOTJfcjBTbXhwQUxZZVZoQktZOXJtMU1SaG9nMFlmUm5KQjVfVG9rZW46R3R6UWJSN0xxb0RTMkV4N2Fyd2NvZjg4bk5lXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="aha-monment">aha monment</h3>
<p>在训练过程中，研究团队使用数学问题来训练和评估模型的逻辑推理能力。在观察模型输出时，研究人员正是在下面这个数学方程的解题过程中捕捉到了一个引人注目的”顿悟时刻”，充分展现了模型通过强化学习自然获得的自主反思能力：</p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQ2MjVkZWZlZmM4YmQ5NWY2YmY5ZGE1NWUxMzhjOGRfaVlWZnQ0cjUzMGJpUXlnTlMyU0hoZmxueHJpbEdSbmZfVG9rZW46T0txVWJWWUZEbzc1Z3B4SG9aZmNWYmgybm5iXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="k1-k2-k3">k1 k2 k3</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MDBmMjNjYmU0YjhkOWYzMWFkNzI0ZDg0YTlkNDc3ZDVfeVIxUHVLUjBmM2hSVWlES3BtenNuUHlqMHFvNmxaZG1fVG9rZW46Q0p6RWJsWkk5b2h6ZVJ4MFFBRGN2d1FhbnFIXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="r1-zero">R1-zero</h3>
<ul>
<li>qwen2.5-0.5B 模型，num_generations 为 2，gsm8k 数据集准确率
0.45489006823351025。num_generations 为 4，准确率为
0.47763457164518575</li>
</ul>
<h3 id="rule-based-reward">Rule-based reward</h3>
<ul>
<li>准确性奖励（Accuracy Rewards）：答案是否正确</li>
</ul>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=M2Y2NGU1NDFlMzU2NmRmOTViNWNhMWZjYmY3N2Y3OGNfeWlOVzVVOUo4THA0WTNZdUdGMzd3dGx5VDdCbXIzcndfVG9rZW46Vk11TGJpVnU3b1RZekx4RFNqVWNvREl4bmYxXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<ul>
<li><p>格式奖励（Format Rewards）：是否有思考标签<think></p></li>
<li><p>语言混合问题：对语言一致性进行打分</p></li>
</ul>
<h2 id="梯度累计">梯度累计</h2>
<h3
id="代码httpszhuanlan.zhihu.comp423359955">代码（https://zhuanlan.zhihu.com/p/423359955)</h3>
<p>暂时无法在飞书文档外展示此内容</p>
<p>gradient_accumulation_steps 是梯度累积次数，累积几次，原本的 loss
就要除以几，这是为了对多个批次的数据的梯度做累积。</p>
<p>有人说，应该有这一行代码才算累加</p>
<p>暂时无法在飞书文档外展示此内容</p>
<p>这样理解是错误的。</p>
<p>要明白最重要的一点是，梯度累加，累加的并不是损失，而是根据损失得到的梯度。</p>
<p>梯度累计如何节省显存</p>
<ul>
<li><p>减少瞬时激活值显存：每次仅处理小批量的数据，激活值显存占用降低为原来的
<code>1/k</code>（例如 <code>k=4</code> 时，显存占用降至
25%）。</p></li>
<li><p>复用显存：每次小批量计算完成后，释放当前激活值显存，供下一次计算使用（显存占用峰值始终为小批量对应的量）。</p></li>
<li><p>梯度显存不变：模型参数和梯度的显存占用与批量大小无关，因此不受影响（但需额外存储累积梯度的变量，这部分开销极小）。</p></li>
</ul>
<h2 id="adam-和-adamw">Adam 和 AdamW</h2>
<h2 id="rlhf-相关pporlooreinforceremaxgrposac">RLHF
相关（PPO、RLOO、REINFORCE++、ReMax、GRPO、SAC）</h2>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MGQ5NzAzZjc3MWY4OGJmYzMyNWFhNTY3Yjc1MDVjY2JfdWRyU1dQNXlLTndMZ2Z0bzVWbUJMSkIxb0VxWGRMYjdfVG9rZW46QUVvVmJUQzRub2Y4WEp4MkRaT2NPNTFzblFnXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3
id="on-policy-和-off-policy参考-httpszhuanlan.zhihu.comp346433931">on-policy
和 off-policy（参考 https://zhuanlan.zhihu.com/p/346433931）</h3>
<ul>
<li><p>Off-policy: the learning is from the data off the target
policy（引自《Reinforcement Learning An Introduction》）。也就是说 RL
算法中，数据来源于一个单独的用于探索的策略（不是最终要求的策略）。（Off-policy
方法中不一定非要采用重要性采样，要根据实际情况采用（比如，需要精确估计值函数时需要采用重要性采样；若是用于使值函数靠近最优值函数则不一定））</p></li>
<li><p>On-policy: the target and the behavior polices are the same.
也就是说 on-policy
里面只有一种策略，它既为目标策略又为行为策略。</p></li>
</ul>
<p>总结：PPO 算法，虽然有 2 个 policy，用<span
class="math inline">\(\pi_{old}\)</span>采样去更新 pi，但是由于 pi_old
的参数是从 pi 复制的，本质上还是属于同一个策略。所以 PPO
是一个看起来很像 off-policy 的 <strong>on-policy</strong> 算法。</p>
<h3 id="online-和-offline">online 和 offline</h3>
<p>更新的数据是否是最新 agent 的模型生成的数据</p>
<ul>
<li><p>Online:
更新的数据为最新模型采样得到的（采样的数据一次性用完）</p></li>
<li><p>Offline: 更新的数据为 x
次更新前模型采样得到的（采样的数据更新多次）</p></li>
</ul>
<h3
id="onlineofflineon-policyoff-policy">online、offline、on-policy、off-policy</h3>
<p>online 学习中可以是 on/off policy 的。而 offline
学习中除了第一次更新模型的学习可能是 on policy
的，之后的所有学习只有仍是 offline 的则一定是 off policy。</p>
<h3
id="critic-和-reward-区别参考-httpswww.zhihu.comquestion1900547615495545054answer1901411039406457541">critic
和 reward 区别（参考
https://www.zhihu.com/question/1900547615495545054/answer/1901411039406457541）</h3>
<ul>
<li><p>reward model 评估整个 response
质量，给出整体奖励信号，无法直接映射到每个 token 的贡献。</p></li>
<li><p>critic model
估计价值函数，预测未来可能获得的累积奖励，为策略更新提供稳定的 advantage
信号</p></li>
</ul>
<p>reward 扮演的是<strong>环境</strong>的角色，而 critic 属于 llm
这个智能体的一部分，就好比在考试中，你自己检查卷子和老师给你打分的区别。</p>
<h3 id="clip-细节">clip 细节</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTBmMDI0ZDIxMjA3MjUwZWJiMjdmYzhlOTVjYWQ5MmZfc3F5MXNUOERrVzZ2dThKZFhaWlZPc1lndmNkR09HWmJfVG9rZW46UnBncGJQOTdqb2hYTFJ4b3QzaGNjZWFobm1iXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTc5M2M3Y2RhYjNkMmExMWZhZGZkZmVkNWQ2MzgyN2NfbGdLc2JTUDZVTzAwUUJQNXAza2JKNGh6MzlpbHRScmNfVG9rZW46TDdDTWJaMG5Qb0RiaFV4RnZEeGNQU1hZbmxnXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="ppo-和-grpo-的区别">PPO 和 GRPO 的区别</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YWY0Mzc3MmI3ZWY0NTVhZTYwZjJhOGY5OTZmNDhlMTdfd2FSTnNwaWxTYUdNYW5SRmF2Y0lLRTdrMjJBeUJibG5fVG9rZW46V0dzR2JYbG9qb3pCYmN4YTRLa2N6TG9JbjBmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h4 id="优势值区别">优势值区别</h4>
<h4 id="kl-散度区别">kl 散度区别</h4>
<ul>
<li><p>PPO 的 KL 计算是在每个 token 的生成过程中发生的，不断计算当前
token 和 ref_model 的 KL 散度。</p></li>
<li><p>GRPO 的 KL 计算是在一个回答生成结束后发生的，一次性对句子中的每个
token 计算 KL 散度，并参与最终 loss 的计算；</p></li>
<li><p>PPO 的 KL 塞到 reward 里（reward shaping）</p></li>
<li><p>GRPO 的 KL 是独立的损失项。</p></li>
<li><p>advantage 角度来看，PPO 的 KL 惩罚是 token-level 的</p></li>
<li><p>GRPO 的 KL 惩罚是 sentence-level（但是也是逐个 token 算 kl 再取
mean）的</p></li>
</ul>
<h3 id="reinforece">REINFORECE++</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YTY3NmIzYzM1NTM5MGZkMTdmNjlkMjUzN2Y3Y2Q1MTFfazM3Nm1zZFJjNjY2amxMQkZFS05WOVJlV3hBY2l4UFZfVG9rZW46SDMwdGJzajNCb2lCc2t4NFp6d2NvRnZqblFoXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="rlooreinforce-leave-one-out">RLOO（REINFORCE
leave-one-out）</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YTE3MjllMTI2Nzk3YjZmZGM5OGJkOGY0M2I0NDdmYmVfTUdya09tWFBRMDFvTktLM09ZNGtIbGlJdHNxQjN0VEhfVG9rZW46R0FIUGJXTTBVb09mNzB4VFdaUGNJd0k2bjFnXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="remaxreinforce-argmax">ReMAX（REINFORCE argmax）</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTQ2YjUxOGZjMzIyZDg2MTdkZWRkNzg4Mjk0ZTIxMjFfb3hMdXBOMjZra3hWQmNlWWt3WG11RHduVDdrUThlbTZfVG9rZW46RWtLVGJjREFEb0ZQc0l4UGVoR2NyMjgzbmdmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="dpo">DPO</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=YjdlMWUyYzdiM2ZiMjdkYTg2NjQ2MjZmYjNhMmViZjhfQXJIWmZtMzhIV0NiSVd5NU91VTRqN0JtVllJTDFEcHlfVG9rZW46Unpsa2JKWXRxb2s4OWd4bFdqaWNJend5blZmXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NTg0YWJkZjYyNDI1MmU3YjcwNjA2NjA3YTA4ZDI5ZmVfdzdlR2k1ZXM2SmJOZ3E0U2xBNENxdGlLZWx4RGQ2aW1fVG9rZW46TEttMWJLVjlTb3RBMEt4MXNEMWNlcm5QbnJjXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<p>a变大，b变小，理想情况，chosen response概率提升，rejected
response概率下降</p>
<h2 id="针对grpo的改进">针对GRPO的改进</h2>
<h3
id="dapo参考-httpswww.zhihu.comquestion1895273986537014226answer1899582779408245950">DAPO（参考
https://www.zhihu.com/question/1895273986537014226/answer/1899582779408245950）</h3>
<p>DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling
Policy
Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2
个是主要亮点，是命名的来源）：</p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=MmM5MWQwMjBkMzU1MzEwOWYzZTY2NTAwNjk0M2Y5ZDVfZTFJa3RNVGlJMTVJa2o5ZHhGSUNMclVsRFQ3MkxuMHZfVG9rZW46Uk41RWJhWWgzbzMxUHh4dnVRZmNONnY2bjhkXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h4 id="更高裁剪">更高裁剪</h4>
<h4 id="动态采样dynamic-sampling">动态采样（Dynamic Sampling）</h4>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=N2Q1ZTBkNTEyNmIxNWE0Nzc3Nzc1ODg3YTE0ZDRlMzZfUUJOcGhPRVVZSnRUaTZrSFVpTUxoUlpwNFdDVDVPS0lfVG9rZW46RUx6MGJFTVMwb3pxV2V4MFVNbWNJankxbjVlXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h4 id="token-级策略梯度损失token-level-policy-gradient-loss">Token
级策略梯度损失（Token-Level Policy Gradient Loss）</h4>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjZiNTE4NzI5ZTk4NDYxMTY5MDBmNThiOWJlMjVlZjJfeERiMmlTc0JUbDAxWnhFYklMQnV5NXd0OUJwR2xrNlZfVG9rZW46Q1o4S2JYUkU0b1VaZHR4NVdqUGMwbGtoblRkXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h4 id="超长奖励塑造overlong-reward-shaping">超长奖励塑造（Overlong
Reward Shaping）</h4>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=NGU2N2U4MGFhOTBlOGE4M2Y4ZTM2ZDFkNTFmMTIyYTNfYXBIa1g5N3JrVFRVNjR0Z0xLSm5nMW84QlV0NklrR0tfVG9rZW46U2lYV2JVamJDb045TEp4bk8wcGM1RG9IbkVjXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="dr.grpo">DR.GRPO</h3>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDNjY2FhYmMyODQ3MTQyYmE0ODMwZWQ0NDQwYzhhYTFfVlUwSVByTElRNG91MllRbjVwYVg1SnUxQncyUExXMEVfVG9rZW46WmlhOWJPYjJRb2t3cEl4RDRlWmNNekUxbnhlXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h3 id="gpg">GPG</h3>
<p>GPG彻底使用policy-based的方法，去除了其他的PPO小trick</p>
<p><img
src="https://q0r4cw0t2o.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDFhODQxMDlhYjM2YzMwZDNiNDQ3OTdkYjMxZGYxNDVfamY0MjFpWTFkNmhzSW9Ia1VudG85aHpMZzRWbTE3T3BfVG9rZW46Q2FNZ2JYU2pLb2RMTEF4ZDFoWmNZVXR0blpnXzE3NTE0NjEyMDQ6MTc1MTQ2NDgwNF9WNA" /></p>
<h2 id="fp16-和-bf16-的区别">FP16 和 BF16 的区别</h2>
<h2 id="tokenize-相关">Tokenize 相关</h2>
<h2 id="面经">面经</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/690824731?utm_psn=1808294053495853057">大模型微调经验</a></p>
<p><a
href="https://github.com/WeThinkIn/AIGC-Interview-Book/tree/main?tab=readme-ov-file">ai
八股</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2025-07-02</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E9%9D%A2%E7%BB%8F2025/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" data-title="面经2025" data-hashtags="面经"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" data-hashtag="面经"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" data-title="面经2025"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" data-title="面经2025"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/%E9%9D%A2%E7%BB%8F2025/" data-title="面经2025"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E9%9D%A2%E7%BB%8F/">面经</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/world_model/" class="prev" rel="prev" title="world_model"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>world_model</a>
            <a href="/webevolverenhancing-web-agent-self-improvement-with-coevolving-world-model/" class="next" rel="next" title="WebEvolver：Enhancing Web Agent Self-Improvement with Coevolving World Model">WebEvolver：Enhancing Web Agent Self-Improvement with Coevolving World Model<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>

    </div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
