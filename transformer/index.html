<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Transformer - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/transformer/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Transformer">
  <meta property="og:description" content="Transformer \[ -\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_i^{&#43;}\right) / \tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{&#43;}\right) / \tau})&#43;\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right))} \]
背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。
而ELMo就解决了这个问题，它使用了双向的LSTM，具体的可以看ELMo,总之使用RNN作为特征提取器，解决了多义词的问题，但现在来看，RNN的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了RNN和CNN的地位，成为了最流行的特征提取器，大火的GPT和BERT都与Transformer离不开关系。拿bank为例，RNN在读取整个句子之前不会理解bank的含义，也就是RNN的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer 的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-attention），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6 个）中。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-06-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-20T21:02:19+08:00">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Transformer">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Transformer">
  <meta name="twitter:description" content="Transformer \[ -\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_i^{&#43;}\right) / \tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{&#43;}\right) / \tau})&#43;\exp({\operatorname{sim}\left(\mathbf{h}_i, \mathbf{h}_j^{-}\right) / \tau}\right))} \]
背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。
而ELMo就解决了这个问题，它使用了双向的LSTM，具体的可以看ELMo,总之使用RNN作为特征提取器，解决了多义词的问题，但现在来看，RNN的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了RNN和CNN的地位，成为了最流行的特征提取器，大火的GPT和BERT都与Transformer离不开关系。拿bank为例，RNN在读取整个句子之前不会理解bank的含义，也就是RNN的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer 的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-attention），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6 个）中。">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/transformer/" /><link rel="prev" href="https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/" /><link rel="next" href="https://blog.vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Transformer",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/transformer\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "NLP, Transformer","wordcount":  9605 ,
        "url": "https:\/\/blog.vllbc.top\/transformer\/","datePublished": "2022-06-08T00:00:00+00:00","dateModified": "2025-04-20T21:02:19+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Transformer</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-06-08">2022-06-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 9605 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 20 分钟&nbsp;<span id="/transformer/" class="leancloud_visitors" data-flag-title="Transformer">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1 id="transformer">Transformer</h1>
<p><span class="math display">\[
-\log \frac{\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_i^{+}\right) /
\tau})}{\sum_{j=1}^N\left(\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{+}\right) /
\tau})+\exp({\operatorname{sim}\left(\mathbf{h}_i,
\mathbf{h}_j^{-}\right) / \tau}\right))}
\]</span></p>
<h2 id="背景">背景</h2>
<p>先从word2vec开始说起，word2vec可以看作是一个<a
href="预训练模型.md">预训练模型</a>，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用<a
href="语言模型.md">语言模型</a>训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的<a
href="Word%20Embedding.md">word embedding</a>空间里去。</p>
<p>而<a href="ELMo.md">ELMo</a>就解决了这个问题，它使用了双向的<a
href="../Deep%20Learning/循环神经网络系列/LSTM.md">LSTM</a>，具体的可以看<a
href="ELMo.md">ELMo</a>,总之使用<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>作为特征提取器，解决了多义词的问题，但现在来看，<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>和<a
href="../Deep%20Learning/卷积神经网络系列/CNN.md">CNN</a>的地位，成为了最流行的特征提取器，大火的<a
href="GPT.md">GPT</a>和<a
href="BERT.md">BERT</a>都与Transformer离不开关系。拿bank为例，<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>在读取整个句子之前不会理解bank的含义，也就是<a
href="../Deep%20Learning/循环神经网络系列/RNN.md">RNN</a>的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer
的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-<a
href="Attention.md">attention</a>），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6
个）中。</p>
<p>在每个解码器层中，前缀标记也通过自注意力机制相互交互。</p>
<p>下面就详细介绍一下。</p>
<h2 id="self-attention">self-attention</h2>
<p>首先介绍一下最主要的self-attention，可以说是self-attention实现了上述的token之间交互的功能。</p>
<p>自注意力是模型的关键组成部分之一。注意 和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803001837.png" /></p>
<p>形式上，这种直觉是通过查询键值注意来实现的。self-attention
中的每个输入标记都会收到三种表示，对应于它可以扮演的角色：</p>
<ul>
<li>query</li>
<li>key</li>
<li>value</li>
</ul>
<p>进入正题：</p>
<p>作为我们想要翻译的输入语句“The animal didn’t cross the street because
it was too tired”。句子中”it”指的是什么呢？“it”指的是”street”
还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。<br />
当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。当模型处理每个位置的词时，self-attention允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。如果你对RNN熟悉，就能想到RNN的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer使用self-attention来将相关词的理解编码到当前词中。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003041.png" /></p>
<p>下面看一下self-attention是如何计算的：</p>
<h3 id="向量计算">向量计算</h3>
<p><strong>第一步</strong>，根据编码器的输入向量，生成三个向量，比如，对每个词向量，生成query-vec,
key-vec,
value-vec，生成方法为分别乘以三个矩阵，这些矩阵在训练过程中需要学习。【注意：不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；<strong>权重矩阵是基于输入位置的转换矩阵</strong>；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】<br />
注意到这些新向量的维度比输入词向量的维度要小（512–&gt;64），并不是必须要小的，是为了让多头attention的计算更稳定。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003136.png" /></p>
<p><strong>第二步</strong>，计算attention就是计算一个分值。对“Thinking
Matchines”这句话，对“Thinking”（pos#1）计算attention
分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。<br />
这个分，通过“Thing”对应query-vector与所有词的key-vec依次做点积得到。所以当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积。这也就是所谓的注意力得分.
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003219.png" /></p>
<p><strong>第三步和第四步</strong>，除以8(<span
class="math inline">\(=\sqrt{dim_{key}}\)</span>)，这样梯度会更稳定。然后加上softmax操作，归一化分值使得全为正数且加和为1。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003300.png" /></p>
<p>softmax分值决定着在这个位置，每个词的表达程度（关注度）。很明显，这个位置的词应该有最高的归一化分数，但大部分时候总是有助于关注该词的相关的词。</p>
<p><strong>第五步</strong>，将softmax分值与value-vec按位相乘。保留关注词的value值，削弱非相关词的value值。</p>
<p><strong>第六步</strong>，将所有加权向量加和，产生该位置的self-attention的输出结果。
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003336.png" /></p>
<p>上述就是self-attention的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。</p>
<h3 id="矩阵计算">矩阵计算</h3>
<p><strong>第一步</strong>，计算query/key/value
matrix，将所有输入词向量合并成输入矩阵<span
class="math inline">\(X\)</span>，并且将其分别乘以权重矩阵<span
class="math inline">\(W^q, W^k,W^v\)</span></p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003540.png" />
<strong>最后</strong>，鉴于我们使用矩阵处理，将步骤2~6合并成一个计算self-attention层输出的公式。
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003549.png" /></p>
<h3 id="多头注意力机制">多头注意力机制</h3>
<p>论文进一步增加了multi-headed的机制到self-attention上，在如下两个方面提高了attention层的效果：</p>
<ol type="1">
<li>多头机制扩展了模型集中于不同位置的能力。在上面的例子中，z1只包含了其他词的很少信息，仅由实际自己词决定。在其他情况下，比如翻译
“The animal didn’t cross the street because it was too
tired”时，我们想知道单词”it”指的是什么。</li>
<li>多头机制赋予attention多种子表达方式。像下面的例子所示，在多头下有多组query/key/value-matrix，而非仅仅一组（论文中使用8-heads）。每一组都是随机初始化，经过训练之后，输入向量可以被映射到不同的子表达空间中。</li>
</ol>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003844.png" />
如果我们计算multi-headed self-attention的，分别有八组不同的Q/K/V
matrix，我们得到八个不同的矩阵。 <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003859.png" /></p>
<p>这会带来点麻烦，前向网络并不能接收八个矩阵，而是希望输入是一个矩阵，所以要有种方式处理下八个矩阵合并成一个矩阵。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003914.png" /></p>
<p>上述就是多头自注意机制的内容，我认为还仅是一部分矩阵，下面尝试着将它们放到一个图上可视化如下。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220803003932.png" />
#### 代码 下面实现一下多头注意力机制，在原论文中，实现的方法如下： <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221114122851.png" />
也就是对每个W进行多头的设置，即为原维度/head，然后拼接后，再经过<span
class="math inline">\(hd_v\times
d_{model}\)</span>的转换又得到原来的维度，代码的实现不太一样，代码是W还是<span
class="math inline">\(d_{model}\times
d_{model}\)</span>的矩阵然后得到q,k,v之后再进行截断，实现如下。</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h为head，这里为8，d_model为embedding的维度，这里为512</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h <span class="co"># 64</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.V_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.Q_Linear(query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k) <span class="co"># (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64)</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> query.transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64)</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.K_Linear(key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.V_Linear(value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        x, <span class="va">self</span>.attn <span class="op">=</span> attention(query, key, value, mask<span class="op">=</span>mask, dropout<span class="op">=</span><span class="va">self</span>.dropout) <span class="co"># x为(batch_size, h, seq_len, d_k)</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn为(batch_size, h, seq_len1, seq_len2)</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (batch_size, h, seq_len, d_k) -&gt; (batch_size, seq_len, h, d_k) -&gt; (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.res_Linear(x)</span></code></pre></div>
<h3 id="masked-self-attention">Masked self-attention</h3>
<p>在训练的时候，主要是消除后面的信息对预测的影响，因为decoder输入的是整个句子，也就是我们所谓的参考答案，而实际预测的时候就是预测后面的token，用不到后面的token，如果不mask掉，当前的token将看到“未来”，这不是我们想要的，因此必须要mask掉。</p>
<p>其实decoder里的sequence mask与encoder里的padding
mask异曲同工，padding
mask其实很简单，就是为了使句子长度一致进行了padding，而为了避免关注padding的位置，进行了mask，具体的做法就是将这些位置的值变成负无穷，这样softmax之后就接近于0了。</p>
<p>而sequence mask思想也差不多：</p>
<p>假设现在解码器的输入”&lt; s &gt; who am i &lt; e
&gt;“在分别乘上一个矩阵进行线性变换后得到了Q、K、V，且Q与K作用后得到了注意力权重矩阵（此时还未进行softmax操作），如图17所示。
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924003204.png" /></p>
<p>此时已经计算得到了注意力权重矩阵。由第1行的权重向量可知，在解码第1个时刻时应该将20%（严格来说应该是经过softmax后的值）的注意力放到’&lt;
s
&gt;’上，30%的注意力放到’who’上等等。不过此时有一个问题就是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer中的Decoder通过加入注意力掩码机制来解决了这一问题。
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924003355.png" />
当然还要进行softmax等计算。</p>
<p>在网上查了很多资料，说法都很不一样，不过我更倾向于这样的看法。而在预测的时候是用前面的输出结果作为输入的。</p>
<p>几张图帮助理解： <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924001536.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924001544.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924001920.png" />
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220924001925.png" /></p>
<p>后面还有padding mask，所有的self
attention都要用这个，因为pad的位置没有任何意义。 实践一下加深理解：
首先我们来定义模型：</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 词典数为10， 词向量维度为8</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(<span class="dv">10</span>, <span class="dv">8</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义Transformer，注意一定要改成eval模型，否则每次输出结果不一样</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>transformer <span class="op">=</span> nn.Transformer(d_model<span class="op">=</span><span class="dv">8</span>, batch_first<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span></code></pre></div>
<p>接下来定义我们的src和tgt：</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encoder的输入</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>src <span class="op">=</span> torch.LongTensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Decoder的输入</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> torch.LongTensor([[<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>]])</span></code></pre></div>
<p>然后我们将<code>[4]</code>送给Transformer进行预测，模拟推理时的第一步：</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>transformer(embedding(src), embedding(tgt[:, :<span class="dv">1</span>]),</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 这个就是用来生成阶梯式的mask的</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>            tgt_mask<span class="op">=</span>nn.Transformer.generate_square_subsequent_mask(<span class="dv">1</span>))</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>tensor([[[ <span class="fl">1.4053</span>, <span class="op">-</span><span class="fl">0.4680</span>,  <span class="fl">0.8110</span>,  <span class="fl">0.1218</span>,  <span class="fl">0.9668</span>, <span class="op">-</span><span class="fl">1.4539</span>, <span class="op">-</span><span class="fl">1.4427</span>,</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.0598</span>]]], grad_fn<span class="op">=&lt;</span>NativeLayerNormBackward0<span class="op">&gt;</span>)</span></code></pre></div>
<p>然后我们将<code>[4, 3]</code>送给Transformer，模拟推理时的第二步：</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>transformer(embedding(src), embedding(tgt[:, :<span class="dv">2</span>]), tgt_mask<span class="op">=</span>nn.Transformer.generate_square_subsequent_mask(<span class="dv">2</span>))</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tensor([[[ <span class="fl">1.4053</span>, <span class="op">-</span><span class="fl">0.4680</span>,  <span class="fl">0.8110</span>,  <span class="fl">0.1218</span>,  <span class="fl">0.9668</span>, <span class="op">-</span><span class="fl">1.4539</span>, <span class="op">-</span><span class="fl">1.4427</span>,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.0598</span>],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>         [ <span class="fl">1.2726</span>, <span class="op">-</span><span class="fl">0.3516</span>,  <span class="fl">0.6584</span>,  <span class="fl">0.3297</span>,  <span class="fl">1.1161</span>, <span class="op">-</span><span class="fl">1.4204</span>, <span class="op">-</span><span class="fl">1.5652</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>          <span class="op">-</span><span class="fl">0.0396</span>]]], grad_fn<span class="op">=&lt;</span>NativeLayerNormBackward0<span class="op">&gt;</span>)</span></code></pre></div>
<p>出的第一个向量和上面那个一模一样。</p>
<p>最后我们再将tgt一次性送给transformer，模拟训练过程：</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>transformer(embedding(src), embedding(tgt), tgt_mask<span class="op">=</span>nn.Transformer.generate_square_subsequent_mask(<span class="dv">5</span>))</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tensor([[[ <span class="fl">1.4053</span>, <span class="op">-</span><span class="fl">0.4680</span>,  <span class="fl">0.8110</span>,  <span class="fl">0.1218</span>,  <span class="fl">0.9668</span>, <span class="op">-</span><span class="fl">1.4539</span>, <span class="op">-</span><span class="fl">1.4427</span>,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.0598</span>],</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>         [ <span class="fl">1.2726</span>, <span class="op">-</span><span class="fl">0.3516</span>,  <span class="fl">0.6584</span>,  <span class="fl">0.3297</span>,  <span class="fl">1.1161</span>, <span class="op">-</span><span class="fl">1.4204</span>, <span class="op">-</span><span class="fl">1.5652</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>          <span class="op">-</span><span class="fl">0.0396</span>],</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>         [ <span class="fl">1.4799</span>, <span class="op">-</span><span class="fl">0.3575</span>,  <span class="fl">0.8310</span>,  <span class="fl">0.1642</span>,  <span class="fl">0.8811</span>, <span class="op">-</span><span class="fl">1.3140</span>, <span class="op">-</span><span class="fl">1.5643</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>          <span class="op">-</span><span class="fl">0.1204</span>],</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>         [ <span class="fl">1.4359</span>, <span class="op">-</span><span class="fl">0.6524</span>,  <span class="fl">0.8377</span>,  <span class="fl">0.1742</span>,  <span class="fl">1.0521</span>, <span class="op">-</span><span class="fl">1.3222</span>, <span class="op">-</span><span class="fl">1.3799</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>          <span class="op">-</span><span class="fl">0.1454</span>],</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>         [ <span class="fl">1.3465</span>, <span class="op">-</span><span class="fl">0.3771</span>,  <span class="fl">0.9107</span>,  <span class="fl">0.1636</span>,  <span class="fl">0.8627</span>, <span class="op">-</span><span class="fl">1.5061</span>, <span class="op">-</span><span class="fl">1.4732</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.0729</span>]]], grad_fn<span class="op">=&lt;</span>NativeLayerNormBackward0<span class="op">&gt;</span>)</span></code></pre></div>
<p>可以看到使用mask后就可以保证前面的结果都是不变的，不然如果没有mask则计算attention时因为计算注意力变化所以结果都会变化，这就是Mask
self-attention的意义。 到这里self-attention就介绍完了</p>
<h3 id="代码">代码</h3>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(query, key, value, mask<span class="op">=</span><span class="va">None</span>, dropout<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> query.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.matmul(query, key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k) <span class="co"># 最后两个维度相乘，即为scores，再scale一下。</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">1e9</span>) <span class="co"># 将mask的位置的scores置为-1e9</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 实际上pad mask的时候，pad也会作为key与其它token对应的k,v计算score，pad mask只是消除pad作为k,v时候的影响。但在最后softmax的时候，将pad的损失值全部置为0</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    p_attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># 将scores进行softmax，得到p_attn，这里是在最后一个维度上softmax，因为对每个query的所有key进行softmax</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dropout:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        p_attn <span class="op">=</span> dropout(p_attn)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.matmul(p_attn, value), p_attn</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadedAttention(nn.Module):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, h, d_model, dropout<span class="op">=</span><span class="fl">0.1</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h为head，这里为8，d_model为embedding的维度，这里为512</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_model <span class="op">%</span> h <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> h <span class="co"># 64</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.V_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_Linear <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> mask.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> query.size(<span class="dv">0</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.Q_Linear(query).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k) <span class="co"># (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64)</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> query.transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64)</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.K_Linear(key).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.V_Linear(value).view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h, <span class="va">self</span>.d_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>        x, <span class="va">self</span>.attn <span class="op">=</span> attention(query, key, value, mask<span class="op">=</span>mask, dropout<span class="op">=</span><span class="va">self</span>.dropout) <span class="co"># x为(batch_size, h, seq_len, d_k)</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn为(batch_size, h, seq_len1, seq_len2)</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.h <span class="op">*</span> <span class="va">self</span>.d_k)</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (batch_size, h, seq_len, d_k) -&gt; (batch_size, seq_len, h, d_k) -&gt; (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512)</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.res_Linear(x)</span></code></pre></div>
<h2 id="模型架构">模型架构</h2>
<p>下面是原始论文中的架构： <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180842.png" />
self-attention上面已经讲的比较详细了，下面说一下其余的部分。</p>
<h3 id="ffn前馈网络">FFN(前馈网络)</h3>
<p>除了注意力以外，每一层都有一个前馈网络：两个线性层之间具有ReLU非线性：</p>
<p><span class="math display">\[
FFN(x) = max(0, xW_1+b_1)W_2+b_2
\]</span> <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810224612.png" /></p>
<p>在通过注意力机制查看其他令牌之后，模型使用 FFN
块来处理这些新信息。</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionwiseFeedForward(nn.Module):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, d_ff, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_1 <span class="op">=</span> nn.Linear(d_model, d_ff)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_2 <span class="op">=</span> nn.Linear(d_ff, d_model)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.w_2(<span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.w_1(x))))</span></code></pre></div>
<h3 id="残差连接">残差连接</h3>
<p>残差连接非常简单（将块的输入添加到其输出），但同时也非常有用：它们缓解了通过网络的梯度流并允许堆叠很多层。解决了网络退化的问题。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810225238.png" /></p>
<p>在 Transformer 中，在每个注意力和 FFN
块之后使用残差连接。在上图中，残差显示为围绕一个块到黄色 “Add &amp;
Norm”层的箭头。在“Add &amp; Norm”部分， “Add”部分代表残差连接。</p>
<h3 id="layer-norm">Layer Norm</h3>
<p>“Add &amp; Norm”层中的“Norm”部分 表示 <a
href="https://arxiv.org/pdf/1607.06450.pdf">Layer
Normalization</a>。它批量独立地标准化每个示例的向量表示 -
这样做是为了控制“流”到下一层。层归一化提高了收敛稳定性，有时甚至提高了质量。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810225344.png" />
这里的scale和bias都是可以训练的参数。</p>
<p>注意<a href="../Deep%20Learning/网络正则化/Layer%20Norm.md">Layer
Norm</a>与<a href="../Deep%20Learning/网络正则化/Batch%20Norm.md">Batch
Norm</a>是不同的，这里引用一下沐神的视频：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810225706.png" /></p>
<p>这是Batch Norm的切法，即对每个特征进行norm。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810225801.png" />
这是Layer norm的切法，即对每个样本进行norm。</p>
<p>为什么用layer norm而不用Batch norm呢？</p>
<p>当你的样本长度变化比较大的时候，使用batch
norm计算的均值和方差波动比较大，而且batch
norm需要记录全局的均值和方差，当遇到新的测试样本的时候，由于长度的原因，之前的均值方差可能就效果不太好了。</p>
<p>但是如果使用layer norm
的话就没有那么多的问题，因为它是每个样本自己计算均值方差，不需要存在一个全局的均值方差，所以会稳定一点。</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, features, eps<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a_2 <span class="op">=</span> nn.Parameter(torch.ones(features))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_2 <span class="op">=</span> nn.Parameter(torch.zeros(features))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> x.std(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a_2 <span class="op">*</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps) <span class="op">+</span> <span class="va">self</span>.b_2</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SublayerConnection(nn.Module):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, dropout) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(size)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, sublayer):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.dropout(sublayer(<span class="va">self</span>.norm(x))) <span class="co"># 这里和论文不同，先norm再扔给sublayer（比如多头注意力、ffd）,理论上是self.norm(x+self.dropout(sublayer(x)))</span></span></code></pre></div>
<h3 id="位置编码position-encoding">位置编码(position encoding)</h3>
<p>(Position Embedding是学习式，而Position Encoding为固定式)
请注意，由于 Transformer
不包含递归或卷积，它不知道输入标记(token)的顺序。因此，我们必须让模型明确地知道标记的位置。为此，我们有两组嵌入：用于标记（我们总是这样做）和用于位置（该模型所需的新嵌入）。那么令牌的输入表示是两个嵌入的总和：令牌和位置。</p>
<p>位置嵌入是可以学习的，但作者发现固定的嵌入不会影响质量。Transformer
中使用的固定位置编码是：</p>
<p><span class="math display">\[
PE_{pos,2i} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\]</span></p>
<p><span class="math display">\[
PE_{pos,2i+1} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\]</span> <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220810230806.png" />
可以看到，每个词的维度都是512维，假设句子长度为10，则位置编码的计算如上图所示。</p>
<p>得到位置编码后，将位置编码与词嵌入简单相加即可。 #### 代码</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, dropout, max_len<span class="op">=</span><span class="dv">5000</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        position_embedding <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        position_embedding[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        position_embedding[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        position_embedding <span class="op">=</span> position_embedding.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 增加一维预留batch size的位置，所以后面forward要在第二维上选取序列长度</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">&#39;PositionalEncoding&#39;</span>, position_embedding)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dropout(x <span class="op">+</span> Variable(<span class="va">self</span>.PositionalEncoding[:, :x.size(<span class="dv">1</span>)], requires_grad<span class="op">=</span><span class="va">False</span>))</span></code></pre></div>
<p>这里为了计算做了转换。 <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220902235745.png" />
### Padding Mask</p>
<p>对于输入序列一般我们都要进行padding补齐，也就是说设定一个统一长度N，在较短的序列后面填充0到长度为N。对于那些补零的数据来说，我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样经过softmax后，这些位置的权重就会接近0。Transformer的padding
mask实际上是一个张量，每个值都是一个Boolean，值为false的地方就是要进行处理的地方。</p>
<h3 id="label-smoothing标签平滑">label smoothing(<a
href="../Deep%20Learning/训练trick/标签平滑.md">标签平滑</a>)</h3>
<p>神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。</p>
<p>label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft
one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</p>
<p>增加label smoothing后真实的概率分布有如下改变：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220905214056.png" />
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220905214100.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220905214114.png" /></p>
<h4 id="代码-1">代码</h4>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LabelSmoothing(nn.Module): <span class="co"># [标签平滑](../Deep%20Learning/训练trick/标签平滑.md)损失函数</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, padding_idx, smoothing<span class="op">=</span><span class="fl">0.0</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> nn.KLDivLoss(size_average<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.padding_idx <span class="op">=</span> padding_idx</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.confidence <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> smoothing</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.smoothing <span class="op">=</span> smoothing</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.true_dist <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, target):</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  x的shape为(batch.size * seq.len, target.vocab.size)</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y的shape是(batch.size * seq.len)</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x=logits，(seq.len, target.vocab.size)</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 每一行，代表一个位置的词</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 类似于：假设seq.len=3, target.vocab.size=5</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x中保存的是log(prob)</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># target 类似于：</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># target = tensor([2, 1, 0])，torch.size=(3)</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> x.size(<span class="dv">1</span>) <span class="op">==</span> <span class="va">self</span>.size</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        true_dist <span class="op">=</span> x.data.clone()</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># true_dist = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>        true_dist.fill_(<span class="va">self</span>.smoothing <span class="op">/</span> (<span class="va">self</span>.size <span class="op">-</span> <span class="dv">2</span>))</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 注意，这里分母target.vocab.size-2是因为</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (1) 最优值 0.6要占一个位置；</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (2) 填充词 &lt;blank&gt; 要被排除在外</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 所以被激活的目标语言词表大小就是self.size-2</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>        true_dist.scatter_(<span class="dv">1</span>, target.data.unsqueeze(<span class="dv">1</span>), <span class="va">self</span>.confidence)</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>          <span class="co"># target.data.unsqueeze(1) -&gt;</span></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tensor([[2],</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[1],</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0]]); shape=torch.Size([3, 1])  </span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.confidence = 0.6</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 根据target.data的指示，按照列优先(1)的原则，把0.6这个值</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 填入true_dist: 因为target.data是2,1,0的内容，</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 所以，0.6填入第0行的第2列（列号，行号都是0开始）</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 0.6填入第1行的第1列</span></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 0.6填入第2行的第0列：</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],</span></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.1333, 0.6000, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])</span></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>        true_dist[:, <span class="va">self</span>.padding_idx] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],</span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>        <span class="co">#[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])</span></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 设置true_dist这个tensor的第一列的值全为0</span></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因为这个是填充词&#39;&lt;blank&gt;&#39;所在的id位置，不应该计入</span></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>        <span class="co">#中的一个词的id</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> torch.nonzero(target.data <span class="op">==</span> <span class="va">self</span>.padding_idx)</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>         <span class="co"># mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0，</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 因为是&#39;&lt;blank&gt;&#39;的id，所以通过上面的一步，把他们找出来</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 如果不加上nonzero，那么mask的shape就是torch.Size([3])</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask.dim() <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>            true_dist.index_fill_(<span class="dv">0</span>, mask.squeeze(), <span class="fl">0.0</span>)</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 当target reference序列中有0这个&#39;&lt;blank&gt;&#39;的时候，则需要把</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 这一行的值都清空。</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pad &#39;&lt;blank&gt;&#39;来填充，所以会出现类似于(2,1,0)这样的情况</span></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a>            <span class="co"># true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>            <span class="co"># [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a>            <span class="co"># [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])</span></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.true_dist <span class="op">=</span> true_dist</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.criterion(x, Variable(true_dist, requires_grad<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 这一步就是调用KL loss来计算</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a>          <span class="co"># x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>          <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],</span></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a>          <span class="co">#[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>          <span class="co"># true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>          <span class="co"># [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],</span></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>          <span class="co"># [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])</span></span></code></pre></div>
<h2 id="预测">预测</h2>
<p><strong>预测过程与一般seq2seq不同的是，t时刻是将1到t-1时刻所有的预测结果作为序列进行预测，而<a
href="seq2seq.md">seq2seq</a>只是使用前一时刻的输出作为当前时刻的输入，这里困扰了我很久，实现了transformer代码后对比李沐老师的代码才理解。</strong></p>
<p>其中seq2seq: <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220926004931.png" />
transformer: <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220926005017.png" />
注意ys最后与之前的ys使用cat函数合并在一起。 ## 总结
Transformer还有很多的模型细节，以后遇到了再记录一下，在面试中很容易问到这些细节，因此可以参考面经边学习边记录，可以查缺补漏也可以学到新的东西。接下来把代码复现一下可以加深理解，并且提高自己的代码水平和实践能力。</p>
<h2 id="一些问题">一些问题</h2>
<h3
id="transformer在哪里做了权重共享为什么可以做权重共享">Transformer在哪里做了权重共享，为什么可以做权重共享？</h3>
<p>Transformer在两个地方进行了权重共享：</p>
<p>（1）Encoder和Decoder间的Embedding层权重共享；</p>
<p>（2）Decoder中Embedding层和FC层权重共享。</p>
<p><strong>对于（1）</strong>，《Attention is all you
need》中Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，<strong>嵌入时都只有对应语言的embedding会被激活</strong>，因此是可以共用一张词表做权重共享的。</p>
<p>论文中，Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。</p>
<p>但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。</p>
<p><strong>对于（2）</strong>，Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为
x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。</p>
<p>那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x
相同的那一行对应的点积和softmax概率会是最大的。</p>
<p>因此，Embedding层和FC层权重共享，Embedding层中和向量 x
最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的<strong>Embedding层和FC层有点像互为逆过程</strong>。</p>
<p>通过这样的权重共享可以减少参数的数量，加快收敛。</p>
<h3 id="为什么除以根号d">为什么除以根号d</h3>
<p>论文中的解释是：向量的<a
href="https://zhida.zhihu.com/search?q=%E7%82%B9%E7%A7%AF&amp;zhida_source=entity&amp;is_preview=1">点积</a>结果会很大，将
softmax 函数 push 到梯度很小的区域，scaled 会缓解这种现象。</p>
<p><span
class="math display">\[\frac{\partial\mathbf{y}}{\partial\mathbf{x}}=\mathrm{diag}(\mathbf{y})-\mathbf{y}\mathbf{y}^T\]</span>
当<span class="math inline">\(\mathbf{y} =\)</span>softmax<span
class="math inline">\(( \mathbf{x} )\)</span>时，<span
class="math inline">\(\mathbf{y}\)</span>对<span
class="math inline">\(\mathbf{x}\)</span>的梯度为：
这是一个jacobi矩阵<span
class="math inline">\(^{+}\)</span>,表示y的每一个元素对x每一个元素的导数是什么。
展开： <span
class="math display">\[\frac{\partial\mathbf{y}}{\partial\mathbf{x}}=\begin{bmatrix}y_1&amp;0&amp;\cdots&amp;0\\0&amp;y_2&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;y_d\end{bmatrix}-\begin{bmatrix}y_1^2&amp;y_1y_2&amp;\cdots&amp;y_1y_d\\y_2y_1&amp;y_2^2&amp;\cdots&amp;y_2y_d\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\y_dy_1&amp;y_dy_2&amp;\cdots&amp;y_d^2\end{bmatrix}\]</span>
根据前面的讨论，当输入 <span class="math inline">\(\mathbf{x}\)</span>
的某一个元素较大时，softmax 会把大部分概率分布<span
class="math inline">\(^{+}\)</span>分配给最大的元
素，假设我们的输入数量级很大，那么就将产生一个接近 one-hot 的向量 <span
class="math display">\[\mathbf{y}\approx[1,0,\cdots,0]^\top \]</span>
此时上面的矩阵变为如下形式 <span
class="math display">\[\frac{\partial\mathbf{y}}{\partial\mathbf{x}}\approx\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\\0&amp;0&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;0\end{bmatrix}-\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\\0&amp;0&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;0\end{bmatrix}=\mathbf{0}\]</span>
也就是所有的梯度都接近0 除以<span class="math inline">\(\sqrt{ d
}\)</span>后就使x的分布更加平缓，从而防止梯度消失。</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> softmax</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_gradient(dim, time_steps<span class="op">=</span><span class="dv">50</span>, scale<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assume components of the query and keys are drawn from N(0, 1) independently</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> np.random.randn(dim)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    ks <span class="op">=</span> np.random.randn(time_steps, dim)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.<span class="bu">sum</span>(q <span class="op">*</span> ks, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> scale  <span class="co"># x.shape = (time_steps,)</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> softmax(x)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> np.diag(y) <span class="op">-</span> np.outer(y, y)<span class="co"># softmax gradient(dy/dx)</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(grad))  <span class="co"># the maximum component of gradients</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>NUMBER_OF_EXPERIMENTS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="co"># results of 5 random runs without scaling</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([test_gradient(<span class="dv">100</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NUMBER_OF_EXPERIMENTS)])</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([test_gradient(<span class="dv">1000</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NUMBER_OF_EXPERIMENTS)])</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co"># results of 5 random runs with scaling</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([test_gradient(<span class="dv">100</span>, scale<span class="op">=</span>np.sqrt(<span class="dv">100</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NUMBER_OF_EXPERIMENTS)])</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([test_gradient(<span class="dv">1000</span>, scale<span class="op">=</span>np.sqrt(<span class="dv">1000</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NUMBER_OF_EXPERIMENTS)])</span></code></pre></div>
<p>输出可看到下面的梯度比上面的梯度更大。</p>
<p>这时又有一个问题，为什么多分类的softmax+交叉熵不需要除以东西呢？这是因为交叉熵中有一个log，log_softmax的梯度和刚才算出来的不同，就算输入的某一个x过大也不会梯度消失。所以就又可以推断出softmax+MSE会导致梯度消失，因为MSE中没有Log，这是为什么分类任务不使用MSE损失函数的原因之一。
### 为什么 Transformer 需要进行 Multi-head Attention
实验证明多头是必要的，8/16个头都可以取得更好的效果，但是超过16个反而效果不好。每个头关注的信息不同，但是头之间的差异随着层数增加而减少。并且不是所有头都有用，有工作尝试剪枝，可以得到更好的表现。</p>
<p>论文中提到模型分为多个头，形成多个子空间，每个头关注不同方面的信息。</p>
<p>那为什么每个头的维度要降呢? <a
href="https://zhida.zhihu.com/search?q=%E4%B8%80%E8%A8%80%E8%94%BD%E4%B9%8B&amp;zhida_source=entity&amp;is_preview=1">一言蔽之</a>的话，大概是：在<strong>不增加时间复杂度</strong>的情况下，同时，借鉴<a
href="https://zhida.zhihu.com/search?q=CNN&amp;zhida_source=entity&amp;is_preview=1">CNN</a>多核的思想，在更低的维度，在<strong>多个独立的<a
href="https://zhida.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&amp;zhida_source=entity&amp;is_preview=1">特征空间</a></strong>，<strong>更容易</strong>学习到更丰富的特征信息。
### 为什么 Transformer 的 Embedding 最后要乘dmodel
具体的原因是，如果使用 Xavier 初始化，Embedding 的方差为
1/d_model，当d_model非常大时，矩阵中的每一个值都会减小。通过乘一个 dmodel 可以将方差恢复到1。</p>
<p>因为Position Encoding是通过三角函数算出来的，值域为[-1,
1]。所以当加上 Position Encoding 时，需要放大 <a
href="https://zhida.zhihu.com/search?q=embedding&amp;zhida_source=entity&amp;is_preview=1">embedding</a> 的数值，否则规模不一致相加后会丢失信息。</p>
<p>因为 Bert 使用的是学习式的Embedding，所以 Bert 这里就不需要放大。 #
参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/559495068">Bert/Transformer
被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com)</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2025-04-20&nbsp;<a class="git-hash" href="https://github.com/vllbc/vllbc.github.io/commit/8a82d1cd17936b335daa66e9b3b94f6a3ed1505a" target="_blank" title="commit by vllbc(1683070754@qq.com) 8a82d1cd17936b335daa66e9b3b94f6a3ed1505a: fist commit">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>8a82d1c</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/transformer/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/transformer/" data-title="Transformer" data-hashtags="NLP,Transformer"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/transformer/" data-hashtag="NLP"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/transformer/" data-title="Transformer"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/transformer/" data-title="Transformer"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/transformer/" data-title="Transformer"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/transformer/">Transformer</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/" class="prev" rel="prev" title="最大子序和"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>最大子序和</a>
            <a href="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" class="next" rel="next" title="主题模型">主题模型<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>

    <link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="/lib/aplayer/dark.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.js"></script><script type="text/javascript" src="/lib/mapbox-gl/mapbox-gl-language.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"data":{"id-1":"这一个带有基于 \u003ca href=\"https://typeitjs.com/\"\u003eTypeIt\u003c/a\u003e 的 \u003cstrong\u003e打字动画\u003c/strong\u003e 的 \u003cem\u003e段落\u003c/em\u003e…","id-2":["\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;hello world\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e"],"id-3":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":31.233,"lightStyle":"mapbox://styles/mapbox/light-v10?optimize=true","lng":121.485,"marked":true,"navigation":true,"scale":true,"zoom":12},"id-4":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":37.453,"lightStyle":"mapbox://styles/mapbox/streets-zh-v1","lng":-122.252,"marked":false,"navigation":true,"scale":true,"zoom":10}},"mapbox":{"RTLTextPlugin":"https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-rtl-text/v0.2.0/mapbox-gl-rtl-text.js","accessToken":"pk.eyJ1IjoiZGlsbG9uenEiLCJhIjoiY2s2czd2M2x3MDA0NjNmcGxmcjVrZmc2cyJ9.aSjv2BNuZUfARvxRYjSVZQ"},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
