<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Data Engineering for Scaling Language Models to 128K Context - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Data Engineering for Scaling Language Models to 128K Context">
  <meta property="og:description" content="Data Engineering for Scaling Language Models to 128K Context ğŸ’¡ Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 æœŸåˆŠæ ‡ç­¾ DOI 10.48550/arXiv.2402.10171 é™„ä»¶ Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf ğŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®çš„ è®ºæ–‡ä¸»è¦ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æ•°æ®å·¥ç¨‹çš„æ–¹æ³•ï¼Œå°†è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°128Kä¸ªtokenã€‚è¿™é¡¹ç ”ç©¶çš„é‡ç‚¹åœ¨äºæ•°æ®å·¥ç¨‹ï¼Œä½œè€…ä»¬æå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼šé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ä»»æ„è¾“å…¥ä½ç½®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¸»è¦æ˜¯é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—çš„ï¼Œå¹¶ä¸”è¿™ç§èƒ½åŠ›å¯ä»¥é€šè¿‡è½»é‡çº§çš„æŒç»­é¢„è®­ç»ƒåœ¨é€‚å½“çš„æ•°æ®æ··åˆä¸Šæ‰©å±•åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ›´é•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œä»4Kæ‰©å±•åˆ°128Kï¼‰ã€‚">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-20T21:02:19+08:00">
    <meta property="article:tag" content="æ–‡çŒ®">
    <meta property="article:tag" content="LLM">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Data Engineering for Scaling Language Models to 128K Context">
  <meta name="twitter:description" content="Data Engineering for Scaling Language Models to 128K Context ğŸ’¡ Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 æœŸåˆŠæ ‡ç­¾ DOI 10.48550/arXiv.2402.10171 é™„ä»¶ Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf ğŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®çš„ è®ºæ–‡ä¸»è¦ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æ•°æ®å·¥ç¨‹çš„æ–¹æ³•ï¼Œå°†è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°128Kä¸ªtokenã€‚è¿™é¡¹ç ”ç©¶çš„é‡ç‚¹åœ¨äºæ•°æ®å·¥ç¨‹ï¼Œä½œè€…ä»¬æå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼šé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ä»»æ„è¾“å…¥ä½ç½®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¸»è¦æ˜¯é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—çš„ï¼Œå¹¶ä¸”è¿™ç§èƒ½åŠ›å¯ä»¥é€šè¿‡è½»é‡çº§çš„æŒç»­é¢„è®­ç»ƒåœ¨é€‚å½“çš„æ•°æ®æ··åˆä¸Šæ‰©å±•åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ›´é•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œä»4Kæ‰©å±•åˆ°128Kï¼‰ã€‚">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" /><link rel="prev" href="https://blog.vllbc.top/kv-cache/" /><link rel="next" href="https://blog.vllbc.top/rope/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Data Engineering for Scaling Language Models to 128K Context",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/data-engineering-for-scaling-language-models-to-128k-context\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "æ–‡çŒ®, LLM","wordcount":  2822 ,
        "url": "https:\/\/blog.vllbc.top\/data-engineering-for-scaling-language-models-to-128k-context\/","datePublished": "2024-08-08T00:00:00+00:00","dateModified": "2025-04-20T21:02:19+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> æ‰€æœ‰æ–‡ç«  </a><a class="menu-item" href="/tags/"> æ ‡ç­¾ </a><a class="menu-item" href="/categories/"> åˆ†ç±» </a><a class="menu-item" href="/about/"> å…³äº </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="æœç´¢æ–‡ç« æ ‡é¢˜æˆ–å†…å®¹..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="æœç´¢">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="æ¸…ç©º">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="åˆ‡æ¢ä¸»é¢˜">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="æœç´¢æ–‡ç« æ ‡é¢˜æˆ–å†…å®¹..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="æœç´¢">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="æ¸…ç©º">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        å–æ¶ˆ
                    </a>
                </div><a class="menu-item" href="/posts/" title="">æ‰€æœ‰æ–‡ç« </a><a class="menu-item" href="/tags/" title="">æ ‡ç­¾</a><a class="menu-item" href="/categories/" title="">åˆ†ç±»</a><a class="menu-item" href="/about/" title="">å…³äº</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="åˆ‡æ¢ä¸»é¢˜">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">ç›®å½•</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Data Engineering for Scaling Language Models to 128K Context</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">æ”¶å½•äº <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>æ–‡çŒ®å’Œæºç é˜…è¯»</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-08">2024-08-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;çº¦ 2822 å­—&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;é¢„è®¡é˜…è¯» 6 åˆ†é’Ÿ&nbsp;<span id="/data-engineering-for-scaling-language-models-to-128k-context/" class="leancloud_visitors" data-flag-title="Data Engineering for Scaling Language Models to 128K Context">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;æ¬¡é˜…è¯»
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>ç›®å½•</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1
id="data-engineering-for-scaling-language-models-to-128k-context">Data
Engineering for Scaling Language Models to 128K Context</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">ğŸ’¡ Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Data Engineering for Scaling
Language Models to 128K Context</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Yao Fu; Rameswar Panda;
Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao
Peng</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2024-02-15</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">æœŸåˆŠæ ‡ç­¾</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2402.10171" rel="noopener noreferrer nofollow">10.48550/arXiv.2402.10171</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">é™„ä»¶</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_Z5AQISDH" rel="noopener noreferrer nofollow">Fu
et al_2024_Data Engineering for Scaling Language Models to 128K
Context.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="ç ”ç©¶èƒŒæ™¯-åŸºç¡€-ç›®çš„"><span style="color: #E65100"><span
style="background-color: #fff8e1">ğŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp;
ç›®çš„</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">è®ºæ–‡ä¸»è¦ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æ•°æ®å·¥ç¨‹çš„æ–¹æ³•ï¼Œå°†è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°128Kä¸ªtokenã€‚è¿™é¡¹ç ”ç©¶çš„é‡ç‚¹åœ¨äºæ•°æ®å·¥ç¨‹ï¼Œä½œè€…ä»¬æå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼šé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ä»»æ„è¾“å…¥ä½ç½®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¸»è¦æ˜¯é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—çš„ï¼Œå¹¶ä¸”è¿™ç§èƒ½åŠ›å¯ä»¥é€šè¿‡è½»é‡çº§çš„æŒç»­é¢„è®­ç»ƒåœ¨é€‚å½“çš„æ•°æ®æ··åˆä¸Šæ‰©å±•åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ›´é•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œä»4Kæ‰©å±•åˆ°128Kï¼‰ã€‚</span></span></p>
<h2 id="ç ”ç©¶å†…å®¹"><span style="color: #2E7D32"><span
style="background-color: #f1f8e9">ğŸ“Š ç ”ç©¶å†…å®¹</span></span></h2>
<hr />
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22CJM6DHQP%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B218.221%2C393.004%2C271.165%2C401.911%5D%2C%5B75.366%2C381.049%2C271.165%2C389.956%5D%2C%5B75.366%2C369.094%2C269.518%2C378.001%5D%2C%5B75.366%2C357.138%2C270.341%2C366.045%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=CJM6DHQP">â€œ(1)
for quantity, we show that 500 million to 5 billion tokens are enough to
enable the model to retrieve information anywhere within the 128K
context;â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 1</a></span>)</span> æ•°æ®é‡è¾ƒå°‘</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22T4THUP8D%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B75.037%2C345.183%2C271.169%2C354.09%5D%2C%5B75.366%2C333.228%2C270.764%2C342.135%5D%2C%5B75.007%2C321.273%2C269.512%2C330.23%5D%2C%5B75.366%2C309.318%2C269.518%2C318.225%5D%2C%5B75.366%2C297.363%2C270.761%2C306.27%5D%2C%5B75.366%2C285.407%2C271.165%2C294.314%5D%2C%5B75.366%2C273.452%2C93.149%2C282.359%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=T4THUP8D">â€œ(2)
for quality, our results equally emphasize domain balance and length
upsampling. Concretely, we find that na ÌˆÄ±vely upsampling longer data on
certain domains like books, a common practice of existing work, gives
suboptimal performance, and that a balanced domain mixture is
important.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 1</a></span>)</span>
å¯¹äºè´¨é‡æ¥è¯´ä½¿ç”¨ä¸Šé‡‡æ ·å¯ä»¥å¤§å¹…æé«˜èƒ½åŠ›</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22AHKRN8DL%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B307.082%2C529.114%2C541.445%2C538.021%5D%2C%5B307.44%2C517.158%2C541.437%2C526.065%5D%2C%5B307.44%2C505.203%2C541.438%2C514.11%5D%2C%5B306.115%2C493.248%2C355.629%2C502.155%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=AHKRN8DL">â€œwhich
asks the model to precisely recite the information in a given sentence
where the sentence (the â€œneedleâ€) is placed in an arbitrary location of
a 128K long document (the â€œhaystackâ€).â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 1</a></span>)</span> å¹²è‰å †æµ‹è¯•çš„å®šä¹‰</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22H3DCD7N4%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B325.054%2C331.853%2C463.996%2C340.76%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=H3DCD7N4">â€œattention
has quadratic complexityâ€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 1</a></span>)</span>
transformeråŸå§‹æ³¨æ„åŠ›å°±æ˜¯ä¸€ä¸ªå¹³æ–¹å¤æ‚åº¦çš„æ³¨æ„åŠ›</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22NFME4Y23%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B306.972%2C194.369%2C541.436%2C203.276%5D%2C%5B307.44%2C182.414%2C542.108%2C191.321%5D%2C%5B307.44%2C170.458%2C543.093%2C179.365%5D%2C%5B307.44%2C158.503%2C484.139%2C167.41%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=NFME4Y23">â€œWe
hypothesize that the capability to utilize information at arbitrary
locations within long context length is (mostly) already acquired during
pretraining, even for models pretrained on substantially shorter 4K
contexts.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 1</a></span>)</span>
ä½œè€…è®¤ä¸ºæ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹å°±å­¦ä¹ åˆ°äº†åˆ©ç”¨ä½ç½®ä¿¡æ¯çš„èƒ½åŠ›</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22X5VSAXJP%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B55.44%2C295.099%2C291.093%2C304.006%5D%2C%5B55.44%2C283.144%2C181.377%2C292.051%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%222%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=2&#x26;annotation=X5VSAXJP">â€œbecause,
as we observe, this results in perplexiy degradations in other domains
(Table 5).â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 2</a></span>)</span>
åªæ˜¯å•ç‹¬å¯¹ä¸€ä¸ªé‚»åŸŸçš„æ•°æ®ä¸Šé‡‡æ ·ä¼šä½¿å¾—å…¶å®ƒé‚»åŸŸçš„æ€§èƒ½ä¸‹é™ã€‚</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%225QEXM69Z%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B450.837%2C695.966%2C541.439%2C703.982%5D%2C%5B55.162%2C685.007%2C261.661%2C693.023%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=5QEXM69Z">â€œwe
use 80K compared to Togetherâ€™s 32K, which does not generalizes beyond
32K;â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span> ä½¿ç”¨80kçš„ä¸Šä¸‹æ–‡è¿›è¡Œè®­ç»ƒ</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22XW9MQ3AJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B276.666%2C685.007%2C541.444%2C693.023%5D%2C%5B55.44%2C674.048%2C196.774%2C682.064%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=XW9MQ3AJ">â€œdata
mixture: we use SlimPajama which has balanced domains compared to YaRN,
which uses book-only PG19;â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span> ä½¿ç”¨é‚»åŸŸæ•°æ®æ›´å¹³è¡¡çš„æ··åˆæ•°æ®é›†</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22KCAPRVRH%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B211.846%2C674.048%2C543.006%2C682.064%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=KCAPRVRH">â€œlength
upsampling: we upsample long sequences compared to LongLoRA, which does
not.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span> é‚»åŸŸæ•°æ®è¿›è¡Œå¹³è¡¡æ€§çš„é•¿æ–‡æœ¬ä¸Šé‡‡æ ·</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22Y3JBRINU%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B55.082%2C223.737%2C289.8%2C232.644%5D%2C%5B55.44%2C211.781%2C289.437%2C220.688%5D%2C%5B55.082%2C199.826%2C289.444%2C208.733%5D%2C%5B55.44%2C187.871%2C291.185%2C196.778%5D%2C%5B55.131%2C175.916%2C291.094%2C184.823%5D%2C%5B55.44%2C163.961%2C291.215%2C172.868%5D%2C%5B55.44%2C152.006%2C289.437%2C160.913%5D%2C%5B55.44%2C140.05%2C289.788%2C148.957%5D%2C%5B55.44%2C128.095%2C291.091%2C137.002%5D%2C%5B55.44%2C116.14%2C289.438%2C125.047%5D%2C%5B55.44%2C104.185%2C289.436%2C113.092%5D%2C%5B55.44%2C92.23%2C119.091%2C101.137%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=Y3JBRINU">â€œAnother
important related work is the previous LLaMA Long (Xiong et al., 2023)
work and the concurrent XVERSE (XVerse, 2024) work, which continue
pretraining the model on 32K sequences for about 500 billion tokens.
These works are implicitly motivated by the view that longcontext
modeling is a new capability that must be â€œinjectedâ€ through large-scale
training. We instead hypothesize that the base model has mostly already
acquired this capability through large-scale pretraining, and thus a
lightweight continual pretraining on relatively small data (e.g., 5B
tokens) is enough to extend these capabilities to much longer context
lengths (Fig. 3).â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span>
ä¸å¦ä¸€ç§è§‚ç‚¹å¯¹æ¯”ï¼Œå¦ä¸€ç§è§‚ç‚¹æ˜¯å¤§æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ˜¯é€šè¿‡å¤§è§„æ¨¡çš„ç»§ç»­é¢„è®­ç»ƒæ³¨å…¥çš„ã€‚</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22LZLDCTVD%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B306.972%2C527.153%2C541.613%2C536.06%5D%2C%5B307.44%2C515.198%2C543.093%2C524.105%5D%2C%5B307.44%2C503.243%2C543.096%2C512.15%5D%2C%5B307.44%2C491.288%2C541.437%2C500.195%5D%2C%5B307.44%2C479.332%2C542.687%2C488.239%5D%2C%5B307.092%2C467.377%2C543.096%2C476.284%5D%2C%5B307.44%2C455.422%2C350.049%2C464.329%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=LZLDCTVD">â€œWe
use the SlimPajama (Soboleva et al., 2023) dataset for continual
pretraining. This dataset is an open-source reproduction of the LLaMA
(Touvron et al., 2023a) pretraining data mixture, consisting of 82% web
data (67% from CommonCrawl and 15% from C4), 4.5% code (Github), 4.5%
Wikipedia, 4.5% books, 2.5% Arxiv, and 2.0% StackExchange.â€</a></span>
<span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span> æœ¬æ–‡ä½¿ç”¨çš„SlimPajamaæ•°æ®é›†çš„æ„æˆ</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22YRFJETBI%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B354.611%2C455.422%2C541.437%2C464.329%5D%2C%5B307.44%2C443.467%2C543.092%2C452.374%5D%2C%5B307.44%2C431.512%2C541.438%2C440.419%5D%2C%5B307.44%2C419.556%2C524.027%2C428.463%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=YRFJETBI">â€œSince
this dataset closely mirrors that used to pretrain the LLaMA models,
there is less concern of distribution shift during continual
pretraining; it is therefore used by many recent works like Fuzhao Xue
&amp; You (2023).â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span>
å³æœ¬æ–‡ç»§ç»­é¢„è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†ä¸llamaé¢„è®­ç»ƒä½¿ç”¨çš„æ•°æ®é›†ç›¸æ¯”åˆ†å¸ƒæ¯”è¾ƒæ¥è¿‘ï¼Œåç§»è¾ƒå°‘ï¼Œå¯¹é¢„è®­ç»ƒæƒé‡å½±å“ä¸å¤§ã€‚</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22N65J3N2C%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B422.195%2C341.848%2C541.445%2C350.755%5D%2C%5B307.44%2C329.893%2C541.437%2C338.8%5D%2C%5B307.44%2C317.938%2C541.437%2C326.845%5D%2C%5B307.44%2C305.982%2C543.093%2C314.889%5D%2C%5B307.44%2C294.027%2C462.53%2C302.934%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=N65J3N2C">â€œDirectly
upsampling long data changes the domain mixture, e.g., upsampling
sequences longer than 100K will increase the portion of the books
domain. Likewise, changes in the domain mixture will result in shifts of
the length distribution.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span>
ä¸èƒ½ç›´æ¥ä¸Šé‡‡æ ·ï¼Œä¼šæ”¹å˜ä¸åŒé‚»åŸŸæ•°æ®çš„æ··åˆæ¯”ä¾‹<br />
ğŸ”¤ç›´æ¥ä¸Šé‡‡æ ·é•¿æ•°æ®ä¼šæ”¹å˜åŸŸæ··åˆï¼Œä¾‹å¦‚ï¼Œä¸Šé‡‡æ ·åºåˆ—å¤§äº100Kä¼šå¢åŠ å›¾ä¹¦åŸŸçš„æ¯”ä¾‹ã€‚åŒæ ·ï¼ŒåŸŸæ··åˆçš„å˜åŒ–ä¹Ÿä¼šå¯¼è‡´é•¿åº¦åˆ†å¸ƒçš„å˜åŒ–ã€‚ğŸ”¤</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22D7NT48L5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B307.44%2C108.722%2C543.089%2C117.748%5D%2C%5B307.44%2C96.767%2C543.187%2C105.674%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=D7NT48L5">â€œPer-source
Upsampling: This retains the domain mixture, then upsamples long
documents within each domain.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 3</a></span>)</span> æ ¸å¿ƒåšæ³•</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22WGK374Y9%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B307.44%2C254.531%2C541.437%2C263.438%5D%2C%5B307.44%2C242.576%2C541.437%2C251.483%5D%2C%5B307.44%2C230.621%2C541.437%2C239.528%5D%2C%5B307.44%2C218.666%2C542.687%2C227.573%5D%2C%5B307.44%2C206.711%2C541.437%2C215.618%5D%2C%5B307.44%2C194.756%2C541.442%2C203.663%5D%2C%5B307.44%2C182.8%2C541.437%2C191.707%5D%2C%5B307.44%2C170.845%2C541.437%2C179.752%5D%2C%5B307.44%2C158.89%2C541.437%2C167.797%5D%2C%5B307.44%2C146.935%2C541.442%2C155.842%5D%2C%5B307.44%2C134.98%2C434.543%2C143.887%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%224%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=4&#x26;annotation=WGK374Y9">â€œFor
training, we use a constant learning rate 2e-5. We modify the base of
RoPE positional encoding to adjust it to longer context, as in Xiong et
al.Â (2023). We pack all data to 80K chunks regardless of the document
boundary, following common practice (Raffel et al., 2020; Touvron et
al., 2023a). We set the batch size to be 4M tokens. Note that this batch
size is the same as training on 4K context length, as we increase the
length of a chunk but decrease the number of chunks in a batch. We train
the model on 5B tokens, which translates to 5B (size of data) / 4M
(batch size) = 2000 optimization steps.â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 4</a></span>)</span> è®­ç»ƒçš„ä¸€äº›è¶…å‚æ•°</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22WMY68AJ5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B307.44%2C146.518%2C541.437%2C155.425%5D%2C%5B307.44%2C134.563%2C542.684%2C143.47%5D%2C%5B307.44%2C122.608%2C540.228%2C131.515%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%225%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=5&#x26;annotation=WMY68AJ5">â€œIn
Table 3 we show that our method not only improves precise retrieval, but
maintains short context performance, evidenced by strong MMLU (Hendrycks
et al., 2020) scoreâ€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 5</a></span>)</span> MMLUæ˜¯ä¸€ç§çŸ­æ–‡æœ¬æµ‹è¯„æ–¹æ³•</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22M26JU5GR%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B95.518%2C367.941%2C289.437%2C376.848%5D%2C%5B55.44%2C355.986%2C289.437%2C364.893%5D%2C%5B55.44%2C344.031%2C289.437%2C352.938%5D%2C%5B55.44%2C332.075%2C289.436%2C340.982%5D%2C%5B55.44%2C320.12%2C291.093%2C329.027%5D%2C%5B55.44%2C308.165%2C289.437%2C317.072%5D%2C%5B55.44%2C296.21%2C289.44%2C305.117%5D%2C%5B55.082%2C284.255%2C289.445%2C293.162%5D%2C%5B55.44%2C272.3%2C289.444%2C281.207%5D%2C%5B55.082%2C260.344%2C289.445%2C269.251%5D%2C%5B55.44%2C248.389%2C251.076%2C257.296%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=M26JU5GR">â€œOur
method outperforms LongLoRA and Yarn Mistral (even though Mistral 7B is
a stronger base model than LLaMA 2 7B we use). Our 13B model performance
closes the gap to GPT-4 128K, and we anticipate that future scaling and
instruction tuning will further improve performance. While there are
other long-context benchmarks in InfiniBench (Zhang et al., 2023), in
our initial experiments we found that models often had trouble
understanding the instruction (because they are not instruction tuned).
Hence we focus on the BookQA benchmark where base LLMs performed
reasonably without instruction tuning.â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 6</a></span>)</span>
å…¶å®ƒçš„é•¿ä¸Šä¸‹åˆåŸºå‡†ç¨‹åºéƒ½æ˜¯åŸºäºæŒ‡ä»¤çš„ï¼Œä½†æœ¬æ–‡å¹¶æ²¡æœ‰å¯¹æ¨¡å‹è¿›è¡Œinstruct
tune å› æ­¤æ¨¡å‹éš¾ä»¥ç†è§£æŒ‡ä»¤ã€‚</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22ANHTQP8Z%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B172.563%2C180.693%2C289.438%2C189.6%5D%2C%5B55.44%2C168.738%2C291.098%2C177.645%5D%2C%5B55.44%2C156.783%2C289.438%2C165.69%5D%2C%5B55.44%2C144.828%2C291.185%2C153.735%5D%2C%5B55.44%2C132.873%2C291.093%2C141.78%5D%2C%5B55.44%2C120.918%2C289.439%2C129.825%5D%2C%5B55.44%2C108.962%2C289.437%2C117.869%5D%2C%5B55.082%2C97.007%2C289.436%2C105.914%5D%2C%5B55.44%2C85.052%2C226.428%2C93.959%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=ANHTQP8Z">â€œOur
hypothesis is that precise retreival over long-range context is an
intrinsic capability obtained by large-scale pretraining, even when the
pretraining context length is substantially shorter (4K in many cases).
If this hypothesis is true, then lightweight continual pretraining
should be enough to extend this capability to much longer context
lengths than see in training. That is, we would not need data-intensive
continual pretraining as used by Xiong et al.Â (2023) and XVerse
(2024).â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 6</a></span>)</span> æœ¬æ–‡çš„è§‚ç‚¹</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22YD6ZRCBU%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B467.894%2C236.182%2C543.09%2C245.089%5D%2C%5B307.44%2C224.227%2C541.438%2C233.134%5D%2C%5B307.44%2C212.272%2C543.09%2C221.179%5D%2C%5B307.44%2C200.317%2C543.093%2C209.224%5D%2C%5B307.44%2C188.361%2C541.442%2C197.268%5D%2C%5B307.44%2C176.406%2C353.443%2C185.313%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=YD6ZRCBU">â€œAt
500M to 1B tokens, the model achieves relatively good performance within
its continually pretrained 80K context, but does not generalize to
80K-128K range. After 5B tokens, the model performs well on 0-80K, and
can generalize to unseen lengths 80K-128K.â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 6</a></span>)</span>
æ•°æ®é›†è¶Šå¤§åˆ™æ¨¡å‹çš„å¤„ç†é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ä¹Ÿè¶Šå¼ºï¼Œä¸è¿‡æœ€ç»ˆä¼šæ”¶æ•›</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%227UAFKUN3%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B178.185%2C355.636%2C289.61%2C364.543%5D%2C%5B55.44%2C343.681%2C291.092%2C352.588%5D%2C%5B55.44%2C331.725%2C289.792%2C340.632%5D%2C%5B55.44%2C319.77%2C289.445%2C328.677%5D%2C%5B55.44%2C307.815%2C289.441%2C316.722%5D%2C%5B55.44%2C295.86%2C289.44%2C304.767%5D%2C%5B55.44%2C283.905%2C291.095%2C292.812%5D%2C%5B55.44%2C271.95%2C289.438%2C280.857%5D%2C%5B55.44%2C259.994%2C290.68%2C268.901%5D%2C%5B55.082%2C248.039%2C289.61%2C256.946%5D%2C%5B55.44%2C236.084%2C289.438%2C244.991%5D%2C%5B55.44%2C224.129%2C259.883%2C233.036%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=7UAFKUN3">â€œOur
results suggest that for supervised finetuning, since training on
long-context is substantially cheaper than previously thought, future
work may dive deeper on the solutions for 100K length finetuning and
reasoning, which so far has almost no open-source work to our knowledge.
For pretraining research, currently there is no definite answer as to
whether long-context continual pretraining should be combined with other
capabilities, such as math (Azerbayev et al., 2023) and code (Chen et
al., 2021), which typically require hundreds of billions of tokens. Our
results suggest that long-context continual pretraining could be a
separate stage after code and math pretraining.â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 7</a></span>)</span>
å¯¹æœªæ¥çš„å±•æœ›ï¼Œè¡¨ç¤ºæœ¬æ–‡çš„æ–¹æ³•å¯åŠ å…¥ä½œä¸ºé¢„è®­ç»ƒçš„ä¸€ç¯</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22HVNPP79N%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B229.489%2C168.388%2C289.444%2C177.295%5D%2C%5B55.44%2C156.433%2C289.438%2C165.34%5D%2C%5B55.44%2C144.478%2C290.27%2C153.385%5D%2C%5B55.44%2C132.523%2C289.692%2C141.43%5D%2C%5B55.44%2C120.567%2C289.442%2C129.474%5D%2C%5B55.44%2C108.612%2C289.442%2C117.519%5D%2C%5B55.44%2C96.657%2C161.276%2C105.564%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=HVNPP79N">â€œRecall
that this strategy keeps the mixture ratio of the data sources the same
as the original data, i.e., 67% CommonCrawl (CC), 15% C4, 4.5% Github,
4.5% Wikipedia, 4.5% books, 2.5% Arxiv and 2.0% StackExchange for
SlimPajama. Then in each of the domains, we upsample sequences longer
than 4K from about 30% to about 70%.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 7</a></span>)</span> æ³¨æ„æ˜¯æ¯ä¸€ä¸ªé‚»åŸŸä¸­éƒ½åˆ†åˆ«è¿›è¡Œä¸Šé‡‡æ ·</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22ELC3SDEK%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B462.352%2C367.591%2C541.792%2C376.498%5D%2C%5B307.44%2C355.636%2C543.093%2C364.543%5D%2C%5B307.44%2C343.681%2C541.438%2C352.588%5D%2C%5B307.44%2C331.725%2C541.437%2C340.632%5D%2C%5B307.44%2C319.77%2C383.076%2C328.677%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=ELC3SDEK">â€œIn
contrast, globally upsampling long sequences (without considering their
domain), or intentionally upsampling code/ book/ Arxiv (since they are
long) changes both the domain mixture and the length
distribution.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 7</a></span>)</span>
per-sourceä¸Šé‡‡æ ·åªæ”¹å˜äº†è®­ç»ƒæ•°æ®é›†çš„é•¿åº¦åˆ†å¸ƒï¼Œè€Œå…¶ä½™çš„è¿™äº›æ–¹æ³•ä¸ä»…æ”¹å˜äº†é•¿åº¦åˆ†å¸ƒï¼Œä¹Ÿæ”¹å˜äº†é‚»åŸŸçš„æ··åˆæ¯”ä¾‹</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22LSKDMQV5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B307.131%2C212.174%2C541.44%2C221.081%5D%2C%5B307.44%2C200.219%2C543.093%2C209.126%5D%2C%5B307.44%2C188.263%2C541.438%2C197.17%5D%2C%5B307.44%2C176.308%2C543.093%2C185.215%5D%2C%5B307.44%2C164.353%2C542.687%2C173.26%5D%2C%5B307.44%2C152.398%2C430.703%2C161.305%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=LSKDMQV5">â€œTable
5 compares the per-domain loss differences of all the data mixture
against the baseline original mixture. We report the differences of the
validation loss, where a more than 0.01 loss change is considered
significant, following common pretraining practice (Kaplan et al., 2020;
Peng et al., 2023; Hoffmann et al., 2022).â€</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 7</a></span>)</span>
é€šè¿‡å¯¹æ¯”å®ç°æ¥è¯´æ˜per-sourceæ˜¯æœ€å¹³è¡¡çš„ä¸€ç§æ–¹æ³•ï¼Œä¸ä¼šæé«˜å¤ªå¤šçŸ­æ–‡æœ¬çš„æŸå¤±</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%229NI36D2A%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B87.159%2C218.249%2C289.44%2C227.156%5D%2C%5B55.44%2C206.294%2C289.61%2C215.201%5D%2C%5B55.44%2C194.339%2C289.437%2C203.246%5D%2C%5B55.44%2C182.384%2C291.096%2C191.291%5D%2C%5B55.44%2C170.429%2C289.793%2C179.336%5D%2C%5B55.44%2C158.473%2C289.437%2C167.38%5D%2C%5B55.44%2C146.518%2C291.18%2C155.425%5D%2C%5B55.131%2C134.563%2C289.788%2C143.47%5D%2C%5B55.44%2C122.608%2C289.438%2C131.515%5D%2C%5B55.44%2C110.653%2C289.437%2C119.56%5D%2C%5B55.44%2C98.698%2C289.44%2C107.605%5D%2C%5B55.44%2C86.742%2C130.983%2C95.649%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=8&#x26;annotation=9NI36D2A">â€œNote
that LongLoRA (Chen et al., 2023b) uses the original data mixture
without length upsampling, so our results also explains why we achieve
better performance than LongLoRA (Fig. 1). We see that the original data
mixture without length upsampling, despite achieving a very close loss,
underperforms on precise retrieval. Per-source length upsampling
significantly improves precise retrieval. This observation also serves
as strong evidence why only using test loss, the evaluation used in most
prior work (Chen et al., 2023a; Peng et al., 2023; Chen et al., 2023b;
Xiao et al., 2023; Anthropic, 2023), may conceal the underlying model
differences.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 8</a></span>)</span>
æ²¡æœ‰ä¸Šé‡‡æ ·è™½ç„¶æŸå¤±ç›¸è¿‘ï¼Œä½†æ˜¯åœ¨å¹²è‰å †ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚å› æ­¤å¦‚æœåªæ˜¯ä½¿ç”¨æŸå¤±æ¥è¿›è¡Œæ¨¡å‹çš„è¯„ä¼°æ˜¾ç„¶æ˜¯ç‰‡é¢çš„ï¼Œè¿™ä¼šæ©ç›–æ¨¡å‹æ½œåœ¨çš„å·®å¼‚ã€‚</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22U4JVRYZG%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B307.44%2C346.752%2C541.437%2C355.659%5D%2C%5B307.44%2C334.797%2C541.437%2C343.704%5D%2C%5B307.44%2C322.841%2C541.438%2C331.748%5D%2C%5B307.44%2C310.886%2C543.098%2C319.793%5D%2C%5B307.44%2C298.931%2C541.438%2C307.838%5D%2C%5B307.44%2C286.976%2C543.093%2C295.883%5D%2C%5B307.44%2C275.021%2C541.442%2C283.928%5D%2C%5B307.082%2C263.066%2C528.939%2C271.973%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=8&#x26;annotation=U4JVRYZG">â€œLong-context
language model research at the 100K-level is still a developing research
area. This work only studies continual pretraining, and research on
instruction finetuning language models on tasks of 100K context length
(e.g., repolevel code understanding) is still limited. So far there
seems to no open-source instruction-finetuned 100K context language
models. We hope our work serve as a basis for future work on 100K-level
long context superivsed finetuning.â€</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
ç­‰, 2024, p.Â 8</a></span>)</span> 100kçº§åˆ«ä¸Šä¸‹æ–‡çš„å¾®è°ƒä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜</p>
<h2 id="ç ”ç©¶ç»“è®º"><span style="color: #4A148C"><span
style="background-color: #f5f5f5">ğŸš© ç ”ç©¶ç»“è®º</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">è®ºæ–‡æ€»ç»“äº†ç ”ç©¶æˆæœï¼ŒæŒ‡å‡ºé€šè¿‡æŒç»­é¢„è®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°æ‰©å±•è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„é•¿ä¸Šä¸‹æ–‡æŒ‡ä»¤å¾®è°ƒç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</span></span></p>
<h2 id="æ„Ÿæƒ³-ç–‘é—®"><span style="color: #006064"><span
style="background-color: #e0f7fa">ğŸ“Œ æ„Ÿæƒ³ &amp; ç–‘é—®</span></span></h2>
<hr />
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ•°æ®å·¥ç¨‹çš„æ–¹æ³•æ¥è¿›è¡Œé•¿ä¸‹æ–‡å»ºæ¨¡ï¼Œä¸»è¦æ˜¯é€šè¿‡åŠ é•¿ç»§ç»­é¢„è®­ç»ƒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶ä¸”é€‰ç”¨æ–°çš„æ··åˆé‚»åŸŸæ•°æ®é›†ï¼Œå¯¹æ¯ä¸€ä¸ªé‚»åŸŸéƒ½è¿›è¡Œé•¿æ–‡æœ¬çš„ä¸Šé‡‡æ ·ï¼Œä»è€Œæé«˜äº†æ•°æ®è´¨é‡ï¼Œå®éªŒè¯æ˜é€šè¿‡è¿™ç§æ–¹æ³•å¾—åˆ°çš„æ¨¡å‹åœ¨å¹²è‰å †å®éªŒä¸Šçš„æ•ˆæœä¸chatgptæ¥è¿‘ï¼Œå¹¶ä¸”ä¸ä¼šæŸå¤±å¤ªå¤šåœ¨çŸ­æ–‡æœ¬ä¸Šçš„æ€§èƒ½ã€‚</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>æ›´æ–°äº 2025-04-20&nbsp;<a class="git-hash" href="https://github.com/vllbc/vllbc.github.io/commit/8a82d1cd17936b335daa66e9b3b94f6a3ed1505a" target="_blank" title="commit by vllbc(1683070754@qq.com) 8a82d1cd17936b335daa66e9b3b94f6a3ed1505a: fist commit">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>8a82d1c</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/data-engineering-for-scaling-language-models-to-128k-context/index.md" target="_blank">é˜…è¯»åŸå§‹æ–‡æ¡£</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="åˆ†äº«åˆ° X" data-sharer="x" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context" data-hashtags="æ–‡çŒ®,LLM"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-hashtag="æ–‡çŒ®"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Line" data-sharer="line" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° å¾®åš" data-sharer="weibo" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%96%87%E7%8C%AE/">æ–‡çŒ®</a>,&nbsp;<a href="/tags/llm/">LLM</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">è¿”å›</a></span>&nbsp;|&nbsp;<span><a href="/">ä¸»é¡µ</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/kv-cache/" class="prev" rel="prev" title="KV cache"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>KV cache</a>
            <a href="/rope/" class="next" rel="next" title="rope">rope<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>
 </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="/lib/aplayer/dark.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.js"></script><script type="text/javascript" src="/lib/mapbox-gl/mapbox-gl-language.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"å¤åˆ¶åˆ°å‰ªè´´æ¿","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"å¦‚ä½•è¯„ä»·è¿™ç¯‡åšæ–‡ï¼Ÿ","recordIP":true,"visitor":true}},"data":{"id-1":"è¿™ä¸€ä¸ªå¸¦æœ‰åŸºäº \u003ca href=\"https://typeitjs.com/\"\u003eTypeIt\u003c/a\u003e çš„ \u003cstrong\u003eæ‰“å­—åŠ¨ç”»\u003c/strong\u003e çš„ \u003cem\u003eæ®µè½\u003c/em\u003eâ€¦","id-2":["\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;hello world\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e"],"id-3":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":31.233,"lightStyle":"mapbox://styles/mapbox/light-v10?optimize=true","lng":121.485,"marked":true,"navigation":true,"scale":true,"zoom":12},"id-4":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":37.453,"lightStyle":"mapbox://styles/mapbox/streets-zh-v1","lng":-122.252,"marked":false,"navigation":true,"scale":true,"zoom":10}},"mapbox":{"RTLTextPlugin":"https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-rtl-text/v0.2.0/mapbox-gl-rtl-text.js","accessToken":"pk.eyJ1IjoiZGlsbG9uenEiLCJhIjoiY2s2czd2M2x3MDA0NjNmcGxmcjVrZmc2cyJ9.aSjv2BNuZUfARvxRYjSVZQ"},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"æ²¡æœ‰æ‰¾åˆ°ç»“æœ","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="å›åˆ°é¡¶éƒ¨">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="æŸ¥çœ‹è¯„è®º">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"å¦‚ä½•è¯„ä»·è¿™ç¯‡åšæ–‡ï¼Ÿ","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"æ²¡æœ‰æ‰¾åˆ°ç»“æœ","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
