<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Data Engineering for Scaling Language Models to 128K Context - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Data Engineering for Scaling Language Models to 128K Context">
  <meta property="og:description" content="Data Engineering for Scaling Language Models to 128K Context 💡 Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 期刊标签 DOI 10.48550/arXiv.2402.10171 附件 Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf 📜 研究背景 &amp; 基础 &amp; 目的 论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-20T21:02:19+08:00">
    <meta property="article:tag" content="文献">
    <meta property="article:tag" content="LLM">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Data Engineering for Scaling Language Models to 128K Context">
  <meta name="twitter:description" content="Data Engineering for Scaling Language Models to 128K Context 💡 Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 期刊标签 DOI 10.48550/arXiv.2402.10171 附件 Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf 📜 研究背景 &amp; 基础 &amp; 目的 论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" /><link rel="prev" href="https://blog.vllbc.top/kv-cache/" /><link rel="next" href="https://blog.vllbc.top/rope/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Data Engineering for Scaling Language Models to 128K Context",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/data-engineering-for-scaling-language-models-to-128k-context\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "文献, LLM","wordcount":  2822 ,
        "url": "https:\/\/blog.vllbc.top\/data-engineering-for-scaling-language-models-to-128k-context\/","datePublished": "2024-08-08T00:00:00+00:00","dateModified": "2025-04-20T21:02:19+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Data Engineering for Scaling Language Models to 128K Context</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>文献和源码阅读</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-08">2024-08-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 2822 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 6 分钟&nbsp;<span id="/data-engineering-for-scaling-language-models-to-128k-context/" class="leancloud_visitors" data-flag-title="Data Engineering for Scaling Language Models to 128K Context">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1
id="data-engineering-for-scaling-language-models-to-128k-context">Data
Engineering for Scaling Language Models to 128K Context</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Data Engineering for Scaling
Language Models to 128K Context</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Yao Fu; Rameswar Panda;
Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao
Peng</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2024-02-15</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2402.10171" rel="noopener noreferrer nofollow">10.48550/arXiv.2402.10171</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_Z5AQISDH" rel="noopener noreferrer nofollow">Fu
et al_2024_Data Engineering for Scaling Language Models to 128K
Context.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。</span></span></p>
<h2 id="研究内容"><span style="color: #2E7D32"><span
style="background-color: #f1f8e9">📊 研究内容</span></span></h2>
<hr />
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22CJM6DHQP%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B218.221%2C393.004%2C271.165%2C401.911%5D%2C%5B75.366%2C381.049%2C271.165%2C389.956%5D%2C%5B75.366%2C369.094%2C269.518%2C378.001%5D%2C%5B75.366%2C357.138%2C270.341%2C366.045%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=CJM6DHQP">“(1)
for quantity, we show that 500 million to 5 billion tokens are enough to
enable the model to retrieve information anywhere within the 128K
context;”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 1</a></span>)</span> 数据量较少</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22T4THUP8D%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B75.037%2C345.183%2C271.169%2C354.09%5D%2C%5B75.366%2C333.228%2C270.764%2C342.135%5D%2C%5B75.007%2C321.273%2C269.512%2C330.23%5D%2C%5B75.366%2C309.318%2C269.518%2C318.225%5D%2C%5B75.366%2C297.363%2C270.761%2C306.27%5D%2C%5B75.366%2C285.407%2C271.165%2C294.314%5D%2C%5B75.366%2C273.452%2C93.149%2C282.359%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=T4THUP8D">“(2)
for quality, our results equally emphasize domain balance and length
upsampling. Concretely, we find that na ̈ıvely upsampling longer data on
certain domains like books, a common practice of existing work, gives
suboptimal performance, and that a balanced domain mixture is
important.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 1</a></span>)</span>
对于质量来说使用上采样可以大幅提高能力</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22AHKRN8DL%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B307.082%2C529.114%2C541.445%2C538.021%5D%2C%5B307.44%2C517.158%2C541.437%2C526.065%5D%2C%5B307.44%2C505.203%2C541.438%2C514.11%5D%2C%5B306.115%2C493.248%2C355.629%2C502.155%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=AHKRN8DL">“which
asks the model to precisely recite the information in a given sentence
where the sentence (the “needle”) is placed in an arbitrary location of
a 128K long document (the “haystack”).”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 1</a></span>)</span> 干草堆测试的定义</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22H3DCD7N4%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B325.054%2C331.853%2C463.996%2C340.76%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=H3DCD7N4">“attention
has quadratic complexity”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 1</a></span>)</span>
transformer原始注意力就是一个平方复杂度的注意力</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22NFME4Y23%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B306.972%2C194.369%2C541.436%2C203.276%5D%2C%5B307.44%2C182.414%2C542.108%2C191.321%5D%2C%5B307.44%2C170.458%2C543.093%2C179.365%5D%2C%5B307.44%2C158.503%2C484.139%2C167.41%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=1&#x26;annotation=NFME4Y23">“We
hypothesize that the capability to utilize information at arbitrary
locations within long context length is (mostly) already acquired during
pretraining, even for models pretrained on substantially shorter 4K
contexts.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 1</a></span>)</span>
作者认为模型在预训练过程就学习到了利用位置信息的能力</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22X5VSAXJP%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B55.44%2C295.099%2C291.093%2C304.006%5D%2C%5B55.44%2C283.144%2C181.377%2C292.051%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%222%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=2&#x26;annotation=X5VSAXJP">“because,
as we observe, this results in perplexiy degradations in other domains
(Table 5).”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 2</a></span>)</span>
只是单独对一个邻域的数据上采样会使得其它邻域的性能下降。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%225QEXM69Z%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B450.837%2C695.966%2C541.439%2C703.982%5D%2C%5B55.162%2C685.007%2C261.661%2C693.023%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=5QEXM69Z">“we
use 80K compared to Together’s 32K, which does not generalizes beyond
32K;”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span> 使用80k的上下文进行训练</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22XW9MQ3AJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B276.666%2C685.007%2C541.444%2C693.023%5D%2C%5B55.44%2C674.048%2C196.774%2C682.064%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=XW9MQ3AJ">“data
mixture: we use SlimPajama which has balanced domains compared to YaRN,
which uses book-only PG19;”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span> 使用邻域数据更平衡的混合数据集</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22KCAPRVRH%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B211.846%2C674.048%2C543.006%2C682.064%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=KCAPRVRH">“length
upsampling: we upsample long sequences compared to LongLoRA, which does
not.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span> 邻域数据进行平衡性的长文本上采样</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22Y3JBRINU%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B55.082%2C223.737%2C289.8%2C232.644%5D%2C%5B55.44%2C211.781%2C289.437%2C220.688%5D%2C%5B55.082%2C199.826%2C289.444%2C208.733%5D%2C%5B55.44%2C187.871%2C291.185%2C196.778%5D%2C%5B55.131%2C175.916%2C291.094%2C184.823%5D%2C%5B55.44%2C163.961%2C291.215%2C172.868%5D%2C%5B55.44%2C152.006%2C289.437%2C160.913%5D%2C%5B55.44%2C140.05%2C289.788%2C148.957%5D%2C%5B55.44%2C128.095%2C291.091%2C137.002%5D%2C%5B55.44%2C116.14%2C289.438%2C125.047%5D%2C%5B55.44%2C104.185%2C289.436%2C113.092%5D%2C%5B55.44%2C92.23%2C119.091%2C101.137%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=Y3JBRINU">“Another
important related work is the previous LLaMA Long (Xiong et al., 2023)
work and the concurrent XVERSE (XVerse, 2024) work, which continue
pretraining the model on 32K sequences for about 500 billion tokens.
These works are implicitly motivated by the view that longcontext
modeling is a new capability that must be “injected” through large-scale
training. We instead hypothesize that the base model has mostly already
acquired this capability through large-scale pretraining, and thus a
lightweight continual pretraining on relatively small data (e.g., 5B
tokens) is enough to extend these capabilities to much longer context
lengths (Fig. 3).”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span>
与另一种观点对比，另一种观点是大模型的长上下文能力是通过大规模的继续预训练注入的。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22LZLDCTVD%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B306.972%2C527.153%2C541.613%2C536.06%5D%2C%5B307.44%2C515.198%2C543.093%2C524.105%5D%2C%5B307.44%2C503.243%2C543.096%2C512.15%5D%2C%5B307.44%2C491.288%2C541.437%2C500.195%5D%2C%5B307.44%2C479.332%2C542.687%2C488.239%5D%2C%5B307.092%2C467.377%2C543.096%2C476.284%5D%2C%5B307.44%2C455.422%2C350.049%2C464.329%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=LZLDCTVD">“We
use the SlimPajama (Soboleva et al., 2023) dataset for continual
pretraining. This dataset is an open-source reproduction of the LLaMA
(Touvron et al., 2023a) pretraining data mixture, consisting of 82% web
data (67% from CommonCrawl and 15% from C4), 4.5% code (Github), 4.5%
Wikipedia, 4.5% books, 2.5% Arxiv, and 2.0% StackExchange.”</a></span>
<span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span> 本文使用的SlimPajama数据集的构成</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22YRFJETBI%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B354.611%2C455.422%2C541.437%2C464.329%5D%2C%5B307.44%2C443.467%2C543.092%2C452.374%5D%2C%5B307.44%2C431.512%2C541.438%2C440.419%5D%2C%5B307.44%2C419.556%2C524.027%2C428.463%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=YRFJETBI">“Since
this dataset closely mirrors that used to pretrain the LLaMA models,
there is less concern of distribution shift during continual
pretraining; it is therefore used by many recent works like Fuzhao Xue
&amp; You (2023).”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span>
即本文继续预训练使用的数据集与llama预训练使用的数据集相比分布比较接近，偏移较少，对预训练权重影响不大。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22N65J3N2C%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B422.195%2C341.848%2C541.445%2C350.755%5D%2C%5B307.44%2C329.893%2C541.437%2C338.8%5D%2C%5B307.44%2C317.938%2C541.437%2C326.845%5D%2C%5B307.44%2C305.982%2C543.093%2C314.889%5D%2C%5B307.44%2C294.027%2C462.53%2C302.934%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=N65J3N2C">“Directly
upsampling long data changes the domain mixture, e.g., upsampling
sequences longer than 100K will increase the portion of the books
domain. Likewise, changes in the domain mixture will result in shifts of
the length distribution.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span>
不能直接上采样，会改变不同邻域数据的混合比例<br />
🔤直接上采样长数据会改变域混合，例如，上采样序列大于100K会增加图书域的比例。同样，域混合的变化也会导致长度分布的变化。🔤</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22D7NT48L5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B307.44%2C108.722%2C543.089%2C117.748%5D%2C%5B307.44%2C96.767%2C543.187%2C105.674%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=3&#x26;annotation=D7NT48L5">“Per-source
Upsampling: This retains the domain mixture, then upsamples long
documents within each domain.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 3</a></span>)</span> 核心做法</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22WGK374Y9%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B307.44%2C254.531%2C541.437%2C263.438%5D%2C%5B307.44%2C242.576%2C541.437%2C251.483%5D%2C%5B307.44%2C230.621%2C541.437%2C239.528%5D%2C%5B307.44%2C218.666%2C542.687%2C227.573%5D%2C%5B307.44%2C206.711%2C541.437%2C215.618%5D%2C%5B307.44%2C194.756%2C541.442%2C203.663%5D%2C%5B307.44%2C182.8%2C541.437%2C191.707%5D%2C%5B307.44%2C170.845%2C541.437%2C179.752%5D%2C%5B307.44%2C158.89%2C541.437%2C167.797%5D%2C%5B307.44%2C146.935%2C541.442%2C155.842%5D%2C%5B307.44%2C134.98%2C434.543%2C143.887%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%224%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=4&#x26;annotation=WGK374Y9">“For
training, we use a constant learning rate 2e-5. We modify the base of
RoPE positional encoding to adjust it to longer context, as in Xiong et
al. (2023). We pack all data to 80K chunks regardless of the document
boundary, following common practice (Raffel et al., 2020; Touvron et
al., 2023a). We set the batch size to be 4M tokens. Note that this batch
size is the same as training on 4K context length, as we increase the
length of a chunk but decrease the number of chunks in a batch. We train
the model on 5B tokens, which translates to 5B (size of data) / 4M
(batch size) = 2000 optimization steps.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 4</a></span>)</span> 训练的一些超参数</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22WMY68AJ5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B307.44%2C146.518%2C541.437%2C155.425%5D%2C%5B307.44%2C134.563%2C542.684%2C143.47%5D%2C%5B307.44%2C122.608%2C540.228%2C131.515%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%225%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=5&#x26;annotation=WMY68AJ5">“In
Table 3 we show that our method not only improves precise retrieval, but
maintains short context performance, evidenced by strong MMLU (Hendrycks
et al., 2020) score”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 5</a></span>)</span> MMLU是一种短文本测评方法</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22M26JU5GR%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B95.518%2C367.941%2C289.437%2C376.848%5D%2C%5B55.44%2C355.986%2C289.437%2C364.893%5D%2C%5B55.44%2C344.031%2C289.437%2C352.938%5D%2C%5B55.44%2C332.075%2C289.436%2C340.982%5D%2C%5B55.44%2C320.12%2C291.093%2C329.027%5D%2C%5B55.44%2C308.165%2C289.437%2C317.072%5D%2C%5B55.44%2C296.21%2C289.44%2C305.117%5D%2C%5B55.082%2C284.255%2C289.445%2C293.162%5D%2C%5B55.44%2C272.3%2C289.444%2C281.207%5D%2C%5B55.082%2C260.344%2C289.445%2C269.251%5D%2C%5B55.44%2C248.389%2C251.076%2C257.296%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=M26JU5GR">“Our
method outperforms LongLoRA and Yarn Mistral (even though Mistral 7B is
a stronger base model than LLaMA 2 7B we use). Our 13B model performance
closes the gap to GPT-4 128K, and we anticipate that future scaling and
instruction tuning will further improve performance. While there are
other long-context benchmarks in InfiniBench (Zhang et al., 2023), in
our initial experiments we found that models often had trouble
understanding the instruction (because they are not instruction tuned).
Hence we focus on the BookQA benchmark where base LLMs performed
reasonably without instruction tuning.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 6</a></span>)</span>
其它的长上下午基准程序都是基于指令的，但本文并没有对模型进行instruct
tune 因此模型难以理解指令。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22ANHTQP8Z%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B172.563%2C180.693%2C289.438%2C189.6%5D%2C%5B55.44%2C168.738%2C291.098%2C177.645%5D%2C%5B55.44%2C156.783%2C289.438%2C165.69%5D%2C%5B55.44%2C144.828%2C291.185%2C153.735%5D%2C%5B55.44%2C132.873%2C291.093%2C141.78%5D%2C%5B55.44%2C120.918%2C289.439%2C129.825%5D%2C%5B55.44%2C108.962%2C289.437%2C117.869%5D%2C%5B55.082%2C97.007%2C289.436%2C105.914%5D%2C%5B55.44%2C85.052%2C226.428%2C93.959%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=ANHTQP8Z">“Our
hypothesis is that precise retreival over long-range context is an
intrinsic capability obtained by large-scale pretraining, even when the
pretraining context length is substantially shorter (4K in many cases).
If this hypothesis is true, then lightweight continual pretraining
should be enough to extend this capability to much longer context
lengths than see in training. That is, we would not need data-intensive
continual pretraining as used by Xiong et al. (2023) and XVerse
(2024).”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 6</a></span>)</span> 本文的观点</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22YD6ZRCBU%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B467.894%2C236.182%2C543.09%2C245.089%5D%2C%5B307.44%2C224.227%2C541.438%2C233.134%5D%2C%5B307.44%2C212.272%2C543.09%2C221.179%5D%2C%5B307.44%2C200.317%2C543.093%2C209.224%5D%2C%5B307.44%2C188.361%2C541.442%2C197.268%5D%2C%5B307.44%2C176.406%2C353.443%2C185.313%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=6&#x26;annotation=YD6ZRCBU">“At
500M to 1B tokens, the model achieves relatively good performance within
its continually pretrained 80K context, but does not generalize to
80K-128K range. After 5B tokens, the model performs well on 0-80K, and
can generalize to unseen lengths 80K-128K.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 6</a></span>)</span>
数据集越大则模型的处理长上下文能力也越强，不过最终会收敛</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%227UAFKUN3%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B178.185%2C355.636%2C289.61%2C364.543%5D%2C%5B55.44%2C343.681%2C291.092%2C352.588%5D%2C%5B55.44%2C331.725%2C289.792%2C340.632%5D%2C%5B55.44%2C319.77%2C289.445%2C328.677%5D%2C%5B55.44%2C307.815%2C289.441%2C316.722%5D%2C%5B55.44%2C295.86%2C289.44%2C304.767%5D%2C%5B55.44%2C283.905%2C291.095%2C292.812%5D%2C%5B55.44%2C271.95%2C289.438%2C280.857%5D%2C%5B55.44%2C259.994%2C290.68%2C268.901%5D%2C%5B55.082%2C248.039%2C289.61%2C256.946%5D%2C%5B55.44%2C236.084%2C289.438%2C244.991%5D%2C%5B55.44%2C224.129%2C259.883%2C233.036%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=7UAFKUN3">“Our
results suggest that for supervised finetuning, since training on
long-context is substantially cheaper than previously thought, future
work may dive deeper on the solutions for 100K length finetuning and
reasoning, which so far has almost no open-source work to our knowledge.
For pretraining research, currently there is no definite answer as to
whether long-context continual pretraining should be combined with other
capabilities, such as math (Azerbayev et al., 2023) and code (Chen et
al., 2021), which typically require hundreds of billions of tokens. Our
results suggest that long-context continual pretraining could be a
separate stage after code and math pretraining.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 7</a></span>)</span>
对未来的展望，表示本文的方法可加入作为预训练的一环</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22HVNPP79N%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B229.489%2C168.388%2C289.444%2C177.295%5D%2C%5B55.44%2C156.433%2C289.438%2C165.34%5D%2C%5B55.44%2C144.478%2C290.27%2C153.385%5D%2C%5B55.44%2C132.523%2C289.692%2C141.43%5D%2C%5B55.44%2C120.567%2C289.442%2C129.474%5D%2C%5B55.44%2C108.612%2C289.442%2C117.519%5D%2C%5B55.44%2C96.657%2C161.276%2C105.564%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=HVNPP79N">“Recall
that this strategy keeps the mixture ratio of the data sources the same
as the original data, i.e., 67% CommonCrawl (CC), 15% C4, 4.5% Github,
4.5% Wikipedia, 4.5% books, 2.5% Arxiv and 2.0% StackExchange for
SlimPajama. Then in each of the domains, we upsample sequences longer
than 4K from about 30% to about 70%.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 7</a></span>)</span> 注意是每一个邻域中都分别进行上采样</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22ELC3SDEK%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B462.352%2C367.591%2C541.792%2C376.498%5D%2C%5B307.44%2C355.636%2C543.093%2C364.543%5D%2C%5B307.44%2C343.681%2C541.438%2C352.588%5D%2C%5B307.44%2C331.725%2C541.437%2C340.632%5D%2C%5B307.44%2C319.77%2C383.076%2C328.677%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=ELC3SDEK">“In
contrast, globally upsampling long sequences (without considering their
domain), or intentionally upsampling code/ book/ Arxiv (since they are
long) changes both the domain mixture and the length
distribution.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 7</a></span>)</span>
per-source上采样只改变了训练数据集的长度分布，而其余的这些方法不仅改变了长度分布，也改变了邻域的混合比例</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22LSKDMQV5%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B307.131%2C212.174%2C541.44%2C221.081%5D%2C%5B307.44%2C200.219%2C543.093%2C209.126%5D%2C%5B307.44%2C188.263%2C541.438%2C197.17%5D%2C%5B307.44%2C176.308%2C543.093%2C185.215%5D%2C%5B307.44%2C164.353%2C542.687%2C173.26%5D%2C%5B307.44%2C152.398%2C430.703%2C161.305%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=7&#x26;annotation=LSKDMQV5">“Table
5 compares the per-domain loss differences of all the data mixture
against the baseline original mixture. We report the differences of the
validation loss, where a more than 0.01 loss change is considered
significant, following common pretraining practice (Kaplan et al., 2020;
Peng et al., 2023; Hoffmann et al., 2022).”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 7</a></span>)</span>
通过对比实现来说明per-source是最平衡的一种方法，不会提高太多短文本的损失</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%229NI36D2A%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B87.159%2C218.249%2C289.44%2C227.156%5D%2C%5B55.44%2C206.294%2C289.61%2C215.201%5D%2C%5B55.44%2C194.339%2C289.437%2C203.246%5D%2C%5B55.44%2C182.384%2C291.096%2C191.291%5D%2C%5B55.44%2C170.429%2C289.793%2C179.336%5D%2C%5B55.44%2C158.473%2C289.437%2C167.38%5D%2C%5B55.44%2C146.518%2C291.18%2C155.425%5D%2C%5B55.131%2C134.563%2C289.788%2C143.47%5D%2C%5B55.44%2C122.608%2C289.438%2C131.515%5D%2C%5B55.44%2C110.653%2C289.437%2C119.56%5D%2C%5B55.44%2C98.698%2C289.44%2C107.605%5D%2C%5B55.44%2C86.742%2C130.983%2C95.649%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=8&#x26;annotation=9NI36D2A">“Note
that LongLoRA (Chen et al., 2023b) uses the original data mixture
without length upsampling, so our results also explains why we achieve
better performance than LongLoRA (Fig. 1). We see that the original data
mixture without length upsampling, despite achieving a very close loss,
underperforms on precise retrieval. Per-source length upsampling
significantly improves precise retrieval. This observation also serves
as strong evidence why only using test loss, the evaluation used in most
prior work (Chen et al., 2023a; Peng et al., 2023; Chen et al., 2023b;
Xiao et al., 2023; Anthropic, 2023), may conceal the underlying model
differences.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 8</a></span>)</span>
没有上采样虽然损失相近，但是在干草堆任务上表现不佳。因此如果只是使用损失来进行模型的评估显然是片面的，这会掩盖模型潜在的差异。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FZ5AQISDH%22%2C%22annotationKey%22%3A%22U4JVRYZG%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B307.44%2C346.752%2C541.437%2C355.659%5D%2C%5B307.44%2C334.797%2C541.437%2C343.704%5D%2C%5B307.44%2C322.841%2C541.438%2C331.748%5D%2C%5B307.44%2C310.886%2C543.098%2C319.793%5D%2C%5B307.44%2C298.931%2C541.438%2C307.838%5D%2C%5B307.44%2C286.976%2C543.093%2C295.883%5D%2C%5B307.44%2C275.021%2C541.442%2C283.928%5D%2C%5B307.082%2C263.066%2C528.939%2C271.973%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/Z5AQISDH?page=8&#x26;annotation=U4JVRYZG">“Long-context
language model research at the 100K-level is still a developing research
area. This work only studies continual pretraining, and research on
instruction finetuning language models on tasks of 100K context length
(e.g., repolevel code understanding) is still limited. So far there
seems to no open-source instruction-finetuned 100K context language
models. We hope our work serve as a basis for future work on 100K-level
long context superivsed finetuning.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FP3J78PXW%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/P3J78PXW">Fu
等, 2024, p. 8</a></span>)</span> 100k级别上下文的微调仍然是一个问题</p>
<h2 id="研究结论"><span style="color: #4A148C"><span
style="background-color: #f5f5f5">🚩 研究结论</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">论文总结了研究成果，指出通过持续预训练可以有效地扩展语言模型的上下文长度，并为未来的长上下文指令微调研究奠定了基础。</span></span></p>
<h2 id="感想-疑问"><span style="color: #006064"><span
style="background-color: #e0f7fa">📌 感想 &amp; 疑问</span></span></h2>
<hr />
<p>这篇论文提出了一种通过数据工程的方法来进行长下文建模，主要是通过加长继续预训练的上下文长度，并且选用新的混合邻域数据集，对每一个邻域都进行长文本的上采样，从而提高了数据质量，实验证明通过这种方法得到的模型在干草堆实验上的效果与chatgpt接近，并且不会损失太多在短文本上的性能。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2025-04-20&nbsp;<a class="git-hash" href="https://github.com/vllbc/vllbc.github.io/commit/8a82d1cd17936b335daa66e9b3b94f6a3ed1505a" target="_blank" title="commit by vllbc(1683070754@qq.com) 8a82d1cd17936b335daa66e9b3b94f6a3ed1505a: fist commit">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>8a82d1c</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/data-engineering-for-scaling-language-models-to-128k-context/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context" data-hashtags="文献,LLM"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-hashtag="文献"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/" data-title="Data Engineering for Scaling Language Models to 128K Context"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%96%87%E7%8C%AE/">文献</a>,&nbsp;<a href="/tags/llm/">LLM</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/kv-cache/" class="prev" rel="prev" title="KV cache"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>KV cache</a>
            <a href="/rope/" class="next" rel="next" title="rope">rope<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>
 </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><link rel="stylesheet" href="/lib/aplayer/dark.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.6.0/dist/index.umd.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mapbox-gl@2.9.1/dist/mapbox-gl.min.js"></script><script type="text/javascript" src="/lib/mapbox-gl/mapbox-gl-language.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"data":{"id-1":"这一个带有基于 \u003ca href=\"https://typeitjs.com/\"\u003eTypeIt\u003c/a\u003e 的 \u003cstrong\u003e打字动画\u003c/strong\u003e 的 \u003cem\u003e段落\u003c/em\u003e…","id-2":["\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;hello world\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e","\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e"],"id-3":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":31.233,"lightStyle":"mapbox://styles/mapbox/light-v10?optimize=true","lng":121.485,"marked":true,"navigation":true,"scale":true,"zoom":12},"id-4":{"darkStyle":"mapbox://styles/mapbox/dark-v10?optimize=true","fullscreen":true,"geolocate":true,"lat":37.453,"lightStyle":"mapbox://styles/mapbox/streets-zh-v1","lng":-122.252,"marked":false,"navigation":true,"scale":true,"zoom":10}},"mapbox":{"RTLTextPlugin":"https://api.mapbox.com/mapbox-gl-js/plugins/mapbox-gl-rtl-text/v0.2.0/mapbox-gl-rtl-text.js","accessToken":"pk.eyJ1IjoiZGlsbG9uenEiLCJhIjoiY2s2czd2M2x3MDA0NjNmcGxmcjVrZmc2cyJ9.aSjv2BNuZUfARvxRYjSVZQ"},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
