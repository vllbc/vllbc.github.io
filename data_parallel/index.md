# data_parallel

å¦‚æœæƒ³å°†æ¨¡å‹è®­ç»ƒæ‰©å±•åˆ°å¤§çš„æ‰¹æ¬¡ï¼Œåˆ™å¾ˆå¿«å°±ä¼šè¾¾åˆ°åœ¨å•ä¸ª GPU ä¸Šå¯ä»¥åšçš„æé™ã€‚å…·ä½“æ¥è¯´ï¼Œä¼šå‘ç”Ÿ `RuntimeError: CUDA out of memory`ã€‚
[æ¢¯åº¦ç´¯è®¡](æ¢¯åº¦ç´¯è®¡.md)ã€[Activation checkpointing](Activation%20checkpointing.md) å’Œ [CPU offloading](CPU%20offloading.md) éƒ½å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘æ˜¾å­˜çš„å ç”¨ï¼Œä¸ºäº†_æœ‰æ•ˆåœ°_æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹å¤§å°å’Œä¸æ–­å¢é•¿çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä»ç„¶åœ¨åˆç†çš„æ—¶é—´å†…è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†è®¡ç®—**åˆ†å¸ƒåœ¨**ä¸€ç»„æœºå™¨ä¸Šã€‚

3 D å¹¶è¡Œå³ï¼šæ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œ
åä¸¤è€…å¯ä»¥ç»Ÿä¸€åˆ’åˆ†åˆ°æ¨¡å‹å¹¶è¡Œï¼ŒåŒºåˆ«æ˜¯ä¸€ä¸ªæ˜¯å±‚å†…å¹¶è¡Œï¼Œä¸€ä¸ªæ˜¯å±‚é—´å¹¶è¡Œã€‚

è¿™é‡Œä»‹ç»æ•°æ®å¹¶è¡Œã€‚

## Naive data parallel

ä¸€ä¸ªå¾ˆç›´è§‰çš„åšæ³•å°±æ˜¯åœ¨ batch ç»´åº¦ä¸Šè¿›è¡Œåˆ’åˆ†ï¼Œå„ä¸ªå¡ä¸Šåˆå§‹åŒ–å®Œæ•´çš„æ¨¡å‹ï¼Œç„¶åå°†å°†åˆ’åˆ†çš„ä¸åŒçš„ batch å‘é€åˆ°å„ä¸ªå¡ä¸Šè¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå†ç”±ä¸€ä¸ªå¡æ•´åˆæ¢¯åº¦å†ä¸‹å‘ç»™å„ GPUï¼Œç„¶åå„ GPU æ›´æ–°è‡ªå·±ç»´æŠ¤çš„æ¨¡å‹å‚æ•°ã€‚

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20250723214626.png)


ä½†è¿™ç§åšæ³•æ˜¾ç„¶æœ‰å¾ˆå¤šé—®é¢˜ï¼Œéœ€è¦æœ‰ä¸€ä¸ª gpu æ‹…ä»»æ¢¯åº¦èšåˆå’Œä¸‹å‘çš„è§’è‰²ï¼Œå¦‚æœè¿™ä¸ª gpu å‡ºé—®é¢˜äº†æ€ä¹ˆåŠï¼Ÿæ¯ä¸€ä¸ª gpu éƒ½éœ€è¦ç»´æŠ¤å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨ï¼Œè¿™éƒ¨åˆ†çš„æ˜¾å­˜æ²¡æœ‰å¾—åˆ°å‡å°‘ï¼›æ­¤å¤–è¿™ç§æ–¹å¼é€šè®¯é‡å¾ˆå¤§ï¼Œè¯¦è§[æ˜¾å­˜å ç”¨è®¡ç®—](../infra/æ˜¾å­˜å ç”¨è®¡ç®—.md)

## DDP

DDP è§£å†³çš„é—®é¢˜å°±æ˜¯å°† Server ä¸Šçš„é€šè®¯å‹åŠ›å‡è¡¡è½¬ç§»åˆ°å„ä¸ª worker ä¸Šï¼ˆServer å³æ‹…ä»»æ¢¯åº¦èšåˆå’Œä¸‹å‘çš„è§’è‰²ï¼Œè€Œ worker å°±æ˜¯å„ä¸ª gpuï¼‰ï¼Œå› æ­¤å¼•å…¥äº† [ring-all-reduce](ring-all-reduce.md) ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æŠŠåå‘ä¼ æ’­åçš„æ¢¯åº¦åˆ‡åˆ†æˆ Nï¼ˆworld_sizeï¼‰ä»½æ¥è¿›è¡Œ ring-all-reduce ç®—æ³•ã€‚

## zero

[zero](zero.md)
## fsdp

[fsdp](fsdp.md)
## å‚è€ƒ

- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&response-content-type=application%2Fpdf&Expires=1751735939&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&Key-Pair-Id=K24J24Z295AEI9)
- [ğŸ’¥ Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups \| by Thomas Wolf \| HuggingFace \| Medium](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)
- [Training extremely large neural networks across thousands of GPUs.](https://www.jeremyjordan.me/distributed-training/)
- [# å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šæ•°æ®å¹¶è¡Œä¸Šç¯‡(DP, DDPä¸ZeRO)](https://zhuanlan.zhihu.com/p/617133971)
