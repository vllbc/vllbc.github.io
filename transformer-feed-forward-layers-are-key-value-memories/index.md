# Transformer Feed-Forward Layers Are Key-Value Memories


# Transformer Feed-Forward Layers Are Key-Value Memories

***

## <span style="color: #1B5E20"><span style="background-color: #f1f8e9">ğŸ’¡ Meta Data</span></span>

| <span style="background-color: #dbeedd">Title</span>     | <span style="background-color: #dbeedd">Transformer Feed-Forward Layers Are Key-Value Memories</span>                                                                                                   |
| -------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <span style="background-color: #f3faf4">Journal</span>   |                                                                                                                                                                                                         |
| <span style="background-color: #dbeedd">Authors</span>   | <span style="background-color: #dbeedd">Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy</span>                                                                                                      |
| <span style="background-color: #f3faf4">Pub. date</span> | <span style="background-color: #f3faf4">2021-09-05</span>                                                                                                                                               |
| <span style="background-color: #dbeedd">æœŸåˆŠæ ‡ç­¾</span>      |                                                                                                                                                                                                         |
| <span style="background-color: #f3faf4">DOI</span>       | <span style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2012.14913" rel="noopener noreferrer nofollow">10.48550/arXiv.2012.14913</a></span>                                     |
| <span style="background-color: #dbeedd">é™„ä»¶</span>        | <span style="background-color: #dbeedd"><a href="zotero://open-pdf/0_NUWXXUEK" rel="noopener noreferrer nofollow">Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf</a></span> |

## <span style="color: #E65100"><span style="background-color: #fff8e1">ğŸ“œ ç ”ç©¶èƒŒæ™¯ &#x26; åŸºç¡€ &#x26; ç›®çš„</span></span>

***

<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">å‰é¦ˆå±‚å æ®äº† Transformer æ¨¡å‹å‚æ•°çš„ä¸‰åˆ†ä¹‹äºŒï¼Œä½†å…¶åœ¨ç½‘ç»œä¸­çš„ä½œç”¨å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚ä½œè€…å‘ç° Transformer è¯­è¨€æ¨¡å‹ä¸­çš„å‰é¦ˆå±‚å¯ä»¥ä½œä¸ºé”®å€¼è®°å¿†ï¼ˆkey-value memoriesï¼‰æ¥æ“ä½œã€‚æ¯ä¸ªé”®ï¼ˆkeyï¼‰ä¸è®­ç»ƒç¤ºä¾‹ä¸­çš„æ–‡æœ¬æ¨¡å¼ç›¸å…³è”ï¼Œæ¯ä¸ªå€¼ï¼ˆvalueï¼‰åˆ™è¯±å¯¼è¾“å‡ºè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚ä½œè€…å‘ç° Transformer è¯­è¨€æ¨¡å‹ä¸­çš„å‰é¦ˆå±‚å¯ä»¥ä½œä¸ºé”®å€¼è®°å¿†ï¼ˆkey-value memoriesï¼‰æ¥æ“ä½œã€‚æ¯ä¸ªé”®ï¼ˆkeyï¼‰ä¸è®­ç»ƒç¤ºä¾‹ä¸­çš„æ–‡æœ¬æ¨¡å¼ç›¸å…³è”ï¼Œæ¯ä¸ªå€¼ï¼ˆvalueï¼‰åˆ™è¯±å¯¼è¾“å‡ºè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚å‰é¦ˆå±‚çš„è¾“å‡ºæ˜¯å…¶è®°å¿†çš„ç»„åˆï¼Œå¹¶é€šè¿‡æ¨¡å‹å±‚çš„æ®‹å·®è¿æ¥é€æ­¥ç»†åŒ–ï¼Œä»¥äº§ç”Ÿæœ€ç»ˆçš„è¾“å‡ºåˆ†å¸ƒã€‚</span></span>

## <span style="color: #2E7D32"><span style="background-color: #f1f8e9">ğŸ“Š ç ”ç©¶å†…å®¹</span></span>

***

<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">å‰é¦ˆå±‚ä¸é”®å€¼ç¥ç»è®°å¿†éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ç¥ç»è®°å¿†ä½¿ç”¨ softmax ä½œä¸ºéçº¿æ€§å‡½æ•°ï¼Œè€Œ Transformer ä¸­çš„å‰é¦ˆå±‚ä¸ä½¿ç”¨å½’ä¸€åŒ–å‡½æ•°ã€‚</span></span>

å…¶ä¸­

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B305.629%2C176.904%2C526.218%2C186.831%5D%2C%5B306.142%2C163.355%2C525.776%2C173.107%5D%2C%5B305.749%2C148.979%2C524.412%2C159.732%5D%2C%5B306.142%2C135.43%2C526.319%2C146.183%5D%2C%5B305.804%2C122.707%2C524.412%2C132.459%5D%2C%5B306.142%2C109.158%2C526.224%2C118.91%5D%2C%5B306.142%2C95.609%2C525.145%2C105.361%5D%2C%5B306.142%2C82.059%2C524.411%2C91.811%5D%2C%5B306.142%2C68.51%2C506.743%2C78.262%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2">â€œWe posit that the key vectors K in feed-forward layers act as pattern detectors over the input sequence, where each individual key vector ki corresponds to a specific pattern over the input prefix x1, . . . , xj. To test our claim, we analyze the keys of a trained language modelâ€™s feed-forward layers. We first retrieve the training examples (prefixes of a sentence) most associated with a given key, that is, the input texts where the memory coefficient is highest.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 2</a></span>)</span>

è¡¨æ˜ç¥ç»å…ƒä¸­çš„$k_i$ä»£è¡¨äº†ä¸€ç§æ¨¡å¼ï¼Œè€Œå¯¹åº”çš„å‚æ•°çŸ©é˜µKï¼ˆæŸä¸€åˆ—å‘é‡ï¼‰å……å½“è¿™ä¸ªæ¨¡å¼çš„æ¨¡å¼æ£€æµ‹å™¨ã€‚å½“æ£€æµ‹åˆ°æœ‰å¯¹åº”æ¨¡å¼æ—¶ï¼Œåˆ™è¡¨ç°ä¸ºå¯¹åº”çš„$k_i$å€¼è¾ƒé«˜ï¼Œç›¸å½“äºæ³¨æ„åŠ›ä¸­çš„å¾—åˆ†ï¼Œè€Œå…¶å¯¹åº”çš„ç¬¬äºŒå±‚å‚æ•°çŸ©é˜µVï¼ˆæŸä¸€åˆ—å‘é‡ï¼‰ä»£è¡¨äº†è¿™ç§æ¨¡å¼å¯¹åº”çš„tokençš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä¹˜åµŒå…¥çŸ©é˜µè¿›è¡Œè½¬æ¢ï¼‰ï¼Œè€Œå°†å¾—åˆ†ä¸Vå‘é‡æ¦‚ç‡åˆ†å¸ƒç›¸ä¹˜åå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒå³æ˜¯æœ€ç»ˆè¦å¾—åˆ°tokençš„æ¦‚ç‡åˆ†å¸ƒã€‚å³è¿™æ˜¯ä¸€ç§æ··åˆå“åº”è¾“å‡ºã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22GKR4GYSJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B306.142%2C576.822%2C526.217%2C586.574%5D%2C%5B306.142%2C563.273%2C524.415%2C573.025%5D%2C%5B306.142%2C549.724%2C526.217%2C559.476%5D%2C%5B306.142%2C536.175%2C525.773%2C546.713%5D%2C%5B305.749%2C522.626%2C524.411%2C532.378%5D%2C%5B306.142%2C509.076%2C526.318%2C518.828%5D%2C%5B305.804%2C494.701%2C526.216%2C505.454%5D%2C%5B306.142%2C481.978%2C524.415%2C491.73%5D%2C%5B306.142%2C468.429%2C526.22%2C480.722%5D%2C%5B306.142%2C454.88%2C524.414%2C464.632%5D%2C%5B306.142%2C441.33%2C526.219%2C451.082%5D%2C%5B306.142%2C426.955%2C524.41%2C437.708%5D%2C%5B306.142%2C414.232%2C363.709%2C423.984%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2&#x26;annotation=GKR4GYSJ">â€œComparing equations 1 and 2 shows that feedforward layers are almost identical to key-value neural memories; the only difference is that neural memory uses softmax as the non-linearity f (Â·), while the canonical transformer does not use a normalizing function in the feed-forward layer. The hidden dimension dm is essentially the number of memories in the layer, and the activation m = f (x Â· K>), commonly referred to as the hidden layer, is a vector containing an unnormalized non-negative coefficient for each memory. We refer to each mi as the memory coefficient of the ith memory cell.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 2</a></span>)</span> ffnä¸­çš„kvè§£é‡Šä¸self attentionä¸­kvçš„åŒºåˆ«

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22ZNFYPFXW%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B215.467%2C351.295%2C289.13%2C361.178%5D%2C%5B70.866%2C337.746%2C290.95%2C347.498%5D%2C%5B70.866%2C324.196%2C289.132%2C333.948%5D%2C%5B70.866%2C308.402%2C290.945%2C322.493%5D%2C%5B70.866%2C297.098%2C289.13%2C307.025%5D%2C%5B70.866%2C281.303%2C289.41%2C295.395%5D%2C%5B70.866%2C269.173%2C290.947%2C279.927%5D%2C%5B70.048%2C256.45%2C151.491%2C268.169%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=ZNFYPFXW">â€œWe assume that patterns stored in memory cells originate from examples the model was trained on. Therefore, given a key ki` that corresponds to the i-th hidden dimension of the `-th feed-forward layer, we compute the memory coefficient ReLU(xj` Â· ki`) for every prefix x1, . . . , xj of every sentence from the WikiText103â€™s training set.3â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 3</a></span>)</span> ä¸keyå‘é‡ç›¸ä¹˜åå¾—åˆ°çš„æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œå³memory coefficientï¼Œç”¨å‰ç¼€ä¸­æœ€åä¸€ä¸ªtokençš„è¾“å…¥å‘é‡ä¹˜ä»¥ç¥ç»å…ƒå¯¹åº”çš„æ¨¡å¼æ£€æµ‹å™¨kiï¼Œå³å¾—åˆ°è¿™ä¸ªå‰ç¼€ç›¸å¯¹äºè¿™ä¸ªè¾“å…¥æ¨¡å¼çš„åŒ¹é…ç¨‹åº¦ã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22FWYKNDNQ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B124.695%2C215.803%2C290.941%2C225.73%5D%2C%5B70.866%2C202.254%2C289.133%2C212.181%5D%2C%5B70.866%2C186.459%2C291.043%2C200.55%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=FWYKNDNQ">â€œThen, we retrieve the top-t trigger examples, that is, the t prefixes whose representation at layer ` yielded the highest inner product with ki`.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 3</a></span>)</span> å¯¹äºæ¯ä¸€å±‚çš„æ¯ä¸€ä¸ªkiéƒ½æ‰¾åˆ°å‰tä¸ªmemory coefficientæœ€å¤§çš„å¥å­ï¼ˆå‰ç¼€ï¼‰

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CDV6NRYC%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B347.541%2C206.265%2C524.412%2C216.192%5D%2C%5B306.142%2C192.716%2C524.412%2C202.643%5D%2C%5B305.816%2C179.407%2C316.07%2C191.013%5D%2C%5B312.437%2C176.921%2C476.741%2C191.013%5D%2C%5B472.988%2C176.921%2C526.218%2C188.919%5D%2C%5B305.324%2C163.372%2C524.415%2C177.464%5D%2C%5B305.749%2C149.823%2C526.32%2C163.914%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=4&#x26;annotation=CDV6NRYC">â€œFor every layer ` and memory dimension i, we compare the top-ranked token according to v` i , (argmax(pi`)) to the next token w` i in the top1 trigger example according to ki` (the example whose memory coefficient for ki` is the highest).â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 4</a></span>)</span> å³å¯¹åº”viå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„å¯¹åº”tokenä¸å‰é¢kiä¸­å¾—åˆ°çš„æœ€é«˜memory coeffcientçš„å¥å­çš„ä¸‹ä¸€ä¸ªtokenè¿›è¡Œå¯¹æ¯”ã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22MV5HS87H%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B81.775%2C393.848%2C290.947%2C407.939%5D%2C%5B70.866%2C382.544%2C144.764%2C394.39%5D%2C%5B141.011%2C380.299%2C289.138%2C392.296%5D%2C%5B70.593%2C366.749%2C289.136%2C380.841%5D%2C%5B70.866%2C355.446%2C290.943%2C365.198%5D%2C%5B70.866%2C341.896%2C262.553%2C353.742%5D%2C%5B258.8%2C339.651%2C289.135%2C351.648%5D%2C%5B70.866%2C328.347%2C262.964%2C338.099%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=5&#x26;annotation=MV5HS87H">â€œNext, we take the next token of ki`â€™s top-1 trigger example (w` i ), and find where it ranks in the value vectorâ€™s distribution pi`. Figure 5 shows that the rank of the next token of a trigger example increases through the layers, meaning that w` i tends to get higher probability in the upper layers.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 5</a></span>)</span> å¯¹äºkiå¾—åˆ†æœ€é«˜çš„å¥å­ï¼Œå…¶ä¸‹ä¸€ä¸ªtokenå³è¦é¢„æµ‹çš„tokenåœ¨kiå¯¹åº”çš„viçš„æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä½ç½®ï¼ˆrankï¼‰ã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22HV4A9MZX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B70.866%2C391.569%2C289.139%2C401.321%5D%2C%5B70.866%2C378.02%2C289.132%2C387.772%5D%2C%5B70.866%2C364.47%2C290.949%2C374.222%5D%2C%5B70.866%2C350.921%2C291.078%2C360.673%5D%2C%5B70.866%2C337.372%2C138.742%2C347.124%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=HV4A9MZX">â€œHere, the validation set is used (rather than the training set used to find trigger examples) since we are trying to characterize the modelâ€™s behavior at inference time, not find the examples it â€œmemorizesâ€ during training.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 6</a></span>)</span> ğŸ”¤ä¸ºä»€ä¹ˆç”¨éªŒè¯é›†çš„åŸå› ğŸ”¤

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22LQX2ZAT7%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B81.775%2C188.331%2C289.521%2C198.083%5D%2C%5B70.866%2C174.782%2C289.517%2C184.534%5D%2C%5B70.866%2C161.232%2C289.134%2C170.984%5D%2C%5B70.866%2C147.683%2C289.132%2C157.435%5D%2C%5B70.866%2C134.134%2C290.944%2C143.886%5D%2C%5B70.866%2C120.585%2C138.742%2C130.337%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=LQX2ZAT7">â€œWhile there are cases where a single memory cell dominates the output of a layer, the majority of outputs are clearly compositional. We count the number of instances where the feed-forward layerâ€™s top prediction is different from all of the memoriesâ€™ top predictions.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 6</a></span>)</span> æ„æ€ä¸ºåŸå§‹viå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„tokenä¸å’ŒKeyè®¡ç®—åŠ æƒåçš„yiå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„tokenæ˜¯å¦ä¸€è‡´ã€‚è¿™é‡Œçš„tokenéƒ½æ˜¯é¢„æµ‹çš„å‰ç¼€çš„ä¸‹ä¸€ä¸ªtokenã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CTTUKNHX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B112.878%2C594.634%2C290.79%2C603.541%5D%2C%5B70.866%2C582.679%2C289.137%2C591.586%5D%2C%5B70.866%2C570.723%2C274.541%2C579.63%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=CTTUKNHX">â€œThe fraction of examples in a random sample of 4,000 examples where the layerâ€™s prediction is different from the prediction of all of its memories.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 7</a></span>)</span> å›¾8è¡¨æ˜äº†vçš„æ··åˆç›¸åº”è¾“å‡ºä¸åŸå§‹vå®Œå…¨ä¸åŒï¼Œè€Œä¸”ç›¸åŒçš„ä¾‹å­éƒ½æ˜¯ä¸€äº›åœç”¨è¯ã€‚å› æ­¤æ··åˆç›¸åº”è¾“å‡ºéå¸¸æœ‰æ„ä¹‰ã€‚

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22H6JLHYGL%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B70.866%2C185.633%2C289.135%2C195.385%5D%2C%5B70.866%2C172.084%2C291.045%2C181.836%5D%2C%5B70.528%2C158.535%2C290.493%2C168.287%5D%2C%5B70.866%2C144.986%2C290.941%2C154.738%5D%2C%5B70.866%2C131.437%2C180.23%2C141.189%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=H6JLHYGL">â€œFigure 9 shows that roughly a third of the modelâ€™s predictions are determined in the bottom few layers. This number grows rapidly from layer 10 onwards, implying that the majority of â€œhardâ€ decisions occur before the final layer.â€</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva ç­‰, 2021, p. 7</a></span>)</span> å³æŸä¸€å±‚å¾—åˆ°çš„æœ€ç»ˆè¾“å‡ºç­‰äºç»è¿‡æ•´ä¸ªæ¨¡å‹å¾—åˆ°çš„æœ€ç»ˆè¾“å‡ºã€‚

## <span style="color: #4A148C"><span style="background-color: #f5f5f5">ğŸš© ç ”ç©¶ç»“è®º</span></span>

***

*   **<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">å‰é¦ˆå±‚çš„ä½œç”¨</span></span>**

ä½œè€…æå‡ºå‰é¦ˆå±‚æ¨¡æ‹Ÿé”®å€¼è®°å¿†ï¼Œå¹¶å±•ç¤ºäº†å®éªŒç»“æœï¼Œè¡¨æ˜é”®ä¸å¯è§£é‡Šçš„è¾“å…¥æ¨¡å¼ç›¸å…³è”ï¼Œå€¼åœ¨æ¨¡å‹ä¸Šå±‚è¯±å¯¼ä¸ä¸‹ä¸€ä¸ªæ ‡è®°åˆ†å¸ƒç›¸å…³çš„è¾“å‡ºè¯æ±‡è¡¨åˆ†å¸ƒã€‚


*   **<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">ç ”ç©¶æ„ä¹‰</span></span>**

è¿™äº›å‘ç°ä¸ºç†è§£ Transformer è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºç°ä»£ NLP æ¨¡å‹çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚



## <span style="color: #006064"><span style="background-color: #e0f7fa">ğŸ“Œ æ„Ÿæƒ³ &#x26; ç–‘é—®</span></span>

***

è¯¥æ–‡ä»kvè§’åº¦è§£è¯»äº†transformerä¸­å‰é¦ˆå±‚çš„ä½œç”¨ï¼Œå¾ˆå…·æœ‰å¯å‘æ€§ï¼Œå¹¶å¾—å‡ºäº†æ·±å±‚å­¦ä¹ å¥å­çš„é«˜çº§ç‰¹å¾ï¼Œæµ…å±‚å­¦ä¹ å¥å­çš„è¡¨é¢ç‰¹å¾(å³å¥å­ä»¥æŸä¸ªwordä¸ºç»“å°¾)çš„ç»“è®ºã€‚

