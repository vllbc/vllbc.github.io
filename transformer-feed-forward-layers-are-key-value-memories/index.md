# Transformer Feed-Forward Layers Are Key-Value Memories


# Transformer Feed-Forward Layers Are Key-Value Memories

***

## <span style="color: #1B5E20"><span style="background-color: #f1f8e9">💡 Meta Data</span></span>

| <span style="background-color: #dbeedd">Title</span>     | <span style="background-color: #dbeedd">Transformer Feed-Forward Layers Are Key-Value Memories</span>                                                                                                   |
| -------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <span style="background-color: #f3faf4">Journal</span>   |                                                                                                                                                                                                         |
| <span style="background-color: #dbeedd">Authors</span>   | <span style="background-color: #dbeedd">Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy</span>                                                                                                      |
| <span style="background-color: #f3faf4">Pub. date</span> | <span style="background-color: #f3faf4">2021-09-05</span>                                                                                                                                               |
| <span style="background-color: #dbeedd">期刊标签</span>      |                                                                                                                                                                                                         |
| <span style="background-color: #f3faf4">DOI</span>       | <span style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2012.14913" rel="noopener noreferrer nofollow">10.48550/arXiv.2012.14913</a></span>                                     |
| <span style="background-color: #dbeedd">附件</span>        | <span style="background-color: #dbeedd"><a href="zotero://open-pdf/0_NUWXXUEK" rel="noopener noreferrer nofollow">Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf</a></span> |

## <span style="color: #E65100"><span style="background-color: #fff8e1">📜 研究背景 &#x26; 基础 &#x26; 目的</span></span>

***

<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">前馈层占据了 Transformer 模型参数的三分之二，但其在网络中的作用尚未被充分探索。作者发现 Transformer 语言模型中的前馈层可以作为键值记忆（key-value memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。作者发现 Transformer 语言模型中的前馈层可以作为键值记忆（key-value memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。前馈层的输出是其记忆的组合，并通过模型层的残差连接逐步细化，以产生最终的输出分布。</span></span>

## <span style="color: #2E7D32"><span style="background-color: #f1f8e9">📊 研究内容</span></span>

***

<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">前馈层与键值神经记忆非常相似，唯一的区别是神经记忆使用 softmax 作为非线性函数，而 Transformer 中的前馈层不使用归一化函数。</span></span>

其中

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B305.629%2C176.904%2C526.218%2C186.831%5D%2C%5B306.142%2C163.355%2C525.776%2C173.107%5D%2C%5B305.749%2C148.979%2C524.412%2C159.732%5D%2C%5B306.142%2C135.43%2C526.319%2C146.183%5D%2C%5B305.804%2C122.707%2C524.412%2C132.459%5D%2C%5B306.142%2C109.158%2C526.224%2C118.91%5D%2C%5B306.142%2C95.609%2C525.145%2C105.361%5D%2C%5B306.142%2C82.059%2C524.411%2C91.811%5D%2C%5B306.142%2C68.51%2C506.743%2C78.262%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2">“We posit that the key vectors K in feed-forward layers act as pattern detectors over the input sequence, where each individual key vector ki corresponds to a specific pattern over the input prefix x1, . . . , xj. To test our claim, we analyze the keys of a trained language model’s feed-forward layers. We first retrieve the training examples (prefixes of a sentence) most associated with a given key, that is, the input texts where the memory coefficient is highest.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 2</a></span>)</span>

表明神经元中的$k_i$代表了一种模式，而对应的参数矩阵K（某一列向量）充当这个模式的模式检测器。当检测到有对应模式时，则表现为对应的$k_i$值较高，相当于注意力中的得分，而其对应的第二层参数矩阵V（某一列向量）代表了这种模式对应的token的概率分布（乘嵌入矩阵进行转换），而将得分与V向量概率分布相乘后得到的概率分布即是最终要得到token的概率分布。即这是一种混合响应输出。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22GKR4GYSJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B306.142%2C576.822%2C526.217%2C586.574%5D%2C%5B306.142%2C563.273%2C524.415%2C573.025%5D%2C%5B306.142%2C549.724%2C526.217%2C559.476%5D%2C%5B306.142%2C536.175%2C525.773%2C546.713%5D%2C%5B305.749%2C522.626%2C524.411%2C532.378%5D%2C%5B306.142%2C509.076%2C526.318%2C518.828%5D%2C%5B305.804%2C494.701%2C526.216%2C505.454%5D%2C%5B306.142%2C481.978%2C524.415%2C491.73%5D%2C%5B306.142%2C468.429%2C526.22%2C480.722%5D%2C%5B306.142%2C454.88%2C524.414%2C464.632%5D%2C%5B306.142%2C441.33%2C526.219%2C451.082%5D%2C%5B306.142%2C426.955%2C524.41%2C437.708%5D%2C%5B306.142%2C414.232%2C363.709%2C423.984%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2&#x26;annotation=GKR4GYSJ">“Comparing equations 1 and 2 shows that feedforward layers are almost identical to key-value neural memories; the only difference is that neural memory uses softmax as the non-linearity f (·), while the canonical transformer does not use a normalizing function in the feed-forward layer. The hidden dimension dm is essentially the number of memories in the layer, and the activation m = f (x · K>), commonly referred to as the hidden layer, is a vector containing an unnormalized non-negative coefficient for each memory. We refer to each mi as the memory coefficient of the ith memory cell.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 2</a></span>)</span> ffn中的kv解释与self attention中kv的区别

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22ZNFYPFXW%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B215.467%2C351.295%2C289.13%2C361.178%5D%2C%5B70.866%2C337.746%2C290.95%2C347.498%5D%2C%5B70.866%2C324.196%2C289.132%2C333.948%5D%2C%5B70.866%2C308.402%2C290.945%2C322.493%5D%2C%5B70.866%2C297.098%2C289.13%2C307.025%5D%2C%5B70.866%2C281.303%2C289.41%2C295.395%5D%2C%5B70.866%2C269.173%2C290.947%2C279.927%5D%2C%5B70.048%2C256.45%2C151.491%2C268.169%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=ZNFYPFXW">“We assume that patterns stored in memory cells originate from examples the model was trained on. Therefore, given a key ki` that corresponds to the i-th hidden dimension of the `-th feed-forward layer, we compute the memory coefficient ReLU(xj` · ki`) for every prefix x1, . . . , xj of every sentence from the WikiText103’s training set.3”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 3</a></span>)</span> 与key向量相乘后得到的是一个数值，即memory coefficient，用前缀中最后一个token的输入向量乘以神经元对应的模式检测器ki，即得到这个前缀相对于这个输入模式的匹配程度。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22FWYKNDNQ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B124.695%2C215.803%2C290.941%2C225.73%5D%2C%5B70.866%2C202.254%2C289.133%2C212.181%5D%2C%5B70.866%2C186.459%2C291.043%2C200.55%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=FWYKNDNQ">“Then, we retrieve the top-t trigger examples, that is, the t prefixes whose representation at layer ` yielded the highest inner product with ki`.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 3</a></span>)</span> 对于每一层的每一个ki都找到前t个memory coefficient最大的句子（前缀）

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CDV6NRYC%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B347.541%2C206.265%2C524.412%2C216.192%5D%2C%5B306.142%2C192.716%2C524.412%2C202.643%5D%2C%5B305.816%2C179.407%2C316.07%2C191.013%5D%2C%5B312.437%2C176.921%2C476.741%2C191.013%5D%2C%5B472.988%2C176.921%2C526.218%2C188.919%5D%2C%5B305.324%2C163.372%2C524.415%2C177.464%5D%2C%5B305.749%2C149.823%2C526.32%2C163.914%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=4&#x26;annotation=CDV6NRYC">“For every layer ` and memory dimension i, we compare the top-ranked token according to v` i , (argmax(pi`)) to the next token w` i in the top1 trigger example according to ki` (the example whose memory coefficient for ki` is the highest).”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 4</a></span>)</span> 即对应vi得到的概率分布的对应token与前面ki中得到的最高memory coeffcient的句子的下一个token进行对比。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22MV5HS87H%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B81.775%2C393.848%2C290.947%2C407.939%5D%2C%5B70.866%2C382.544%2C144.764%2C394.39%5D%2C%5B141.011%2C380.299%2C289.138%2C392.296%5D%2C%5B70.593%2C366.749%2C289.136%2C380.841%5D%2C%5B70.866%2C355.446%2C290.943%2C365.198%5D%2C%5B70.866%2C341.896%2C262.553%2C353.742%5D%2C%5B258.8%2C339.651%2C289.135%2C351.648%5D%2C%5B70.866%2C328.347%2C262.964%2C338.099%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=5&#x26;annotation=MV5HS87H">“Next, we take the next token of ki`’s top-1 trigger example (w` i ), and find where it ranks in the value vector’s distribution pi`. Figure 5 shows that the rank of the next token of a trigger example increases through the layers, meaning that w` i tends to get higher probability in the upper layers.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 5</a></span>)</span> 对于ki得分最高的句子，其下一个token即要预测的token在ki对应的vi的概率分布中的位置（rank）。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22HV4A9MZX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B70.866%2C391.569%2C289.139%2C401.321%5D%2C%5B70.866%2C378.02%2C289.132%2C387.772%5D%2C%5B70.866%2C364.47%2C290.949%2C374.222%5D%2C%5B70.866%2C350.921%2C291.078%2C360.673%5D%2C%5B70.866%2C337.372%2C138.742%2C347.124%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=HV4A9MZX">“Here, the validation set is used (rather than the training set used to find trigger examples) since we are trying to characterize the model’s behavior at inference time, not find the examples it “memorizes” during training.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 6</a></span>)</span> 🔤为什么用验证集的原因🔤

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22LQX2ZAT7%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B81.775%2C188.331%2C289.521%2C198.083%5D%2C%5B70.866%2C174.782%2C289.517%2C184.534%5D%2C%5B70.866%2C161.232%2C289.134%2C170.984%5D%2C%5B70.866%2C147.683%2C289.132%2C157.435%5D%2C%5B70.866%2C134.134%2C290.944%2C143.886%5D%2C%5B70.866%2C120.585%2C138.742%2C130.337%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=LQX2ZAT7">“While there are cases where a single memory cell dominates the output of a layer, the majority of outputs are clearly compositional. We count the number of instances where the feed-forward layer’s top prediction is different from all of the memories’ top predictions.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 6</a></span>)</span> 意思为原始vi得到的概率分布对应的token与和Key计算加权后的yi得到的概率分布对应的token是否一致。这里的token都是预测的前缀的下一个token。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CTTUKNHX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B112.878%2C594.634%2C290.79%2C603.541%5D%2C%5B70.866%2C582.679%2C289.137%2C591.586%5D%2C%5B70.866%2C570.723%2C274.541%2C579.63%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=CTTUKNHX">“The fraction of examples in a random sample of 4,000 examples where the layer’s prediction is different from the prediction of all of its memories.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 7</a></span>)</span> 图8表明了v的混合相应输出与原始v完全不同，而且相同的例子都是一些停用词。因此混合相应输出非常有意义。

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22H6JLHYGL%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B70.866%2C185.633%2C289.135%2C195.385%5D%2C%5B70.866%2C172.084%2C291.045%2C181.836%5D%2C%5B70.528%2C158.535%2C290.493%2C168.287%5D%2C%5B70.866%2C144.986%2C290.941%2C154.738%5D%2C%5B70.866%2C131.437%2C180.23%2C141.189%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D" ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=H6JLHYGL">“Figure 9 shows that roughly a third of the model’s predictions are determined in the bottom few layers. This number grows rapidly from layer 10 onwards, implying that the majority of “hard” decisions occur before the final layer.”</a></span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva 等, 2021, p. 7</a></span>)</span> 即某一层得到的最终输出等于经过整个模型得到的最终输出。

## <span style="color: #4A148C"><span style="background-color: #f5f5f5">🚩 研究结论</span></span>

***

*   **<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">前馈层的作用</span></span>**

作者提出前馈层模拟键值记忆，并展示了实验结果，表明键与可解释的输入模式相关联，值在模型上层诱导与下一个标记分布相关的输出词汇表分布。


*   **<span style="color: rgb(6, 6, 7)"><span style="background-color: rgb(255, 255, 255)">研究意义</span></span>**

这些发现为理解 Transformer 语言模型的工作原理提供了新的视角，并为现代 NLP 模型的研究开辟了新的研究方向。



## <span style="color: #006064"><span style="background-color: #e0f7fa">📌 感想 &#x26; 疑问</span></span>

***

该文从kv角度解读了transformer中前馈层的作用，很具有启发性，并得出了深层学习句子的高级特征，浅层学习句子的表面特征(即句子以某个word为结尾)的结论。

