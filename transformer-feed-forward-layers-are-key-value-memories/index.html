<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Transformer Feed-Forward Layers Are Key-Value Memories - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="Transformer Feed-Forward Layers Are Key-Value Memories" />
<meta property="og:description" content="Transformer Feed-Forward Layers Are Key-Value Memories 💡 Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 期刊标签 DOI 10.48550/arXiv.2012.14913 附件 Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf 📜 研究背景 &amp; 基础 &amp; 目的 前馈层占据了 Transformer 模型参数的三分" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" /><meta property="og:image" content="https://blog.vllbc.top/logo.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-08-07T00:00:00+00:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://blog.vllbc.top/logo.png" /><meta name="twitter:title" content="Transformer Feed-Forward Layers Are Key-Value Memories"/>
<meta name="twitter:description" content="Transformer Feed-Forward Layers Are Key-Value Memories 💡 Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 期刊标签 DOI 10.48550/arXiv.2012.14913 附件 Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf 📜 研究背景 &amp; 基础 &amp; 目的 前馈层占据了 Transformer 模型参数的三分"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" /><link rel="prev" href="https://blog.vllbc.top/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" /><link rel="next" href="https://blog.vllbc.top/moe/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Transformer Feed-Forward Layers Are Key-Value Memories",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/transformer-feed-forward-layers-are-key-value-memories\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "文献, Transformer","wordcount":  1998 ,
        "url": "https:\/\/blog.vllbc.top\/transformer-feed-forward-layers-are-key-value-memories\/","datePublished": "2024-08-07T00:00:00+00:00","dateModified": "2024-08-07T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Transformer Feed-Forward Layers Are Key-Value Memories</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://blog.vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Categories</a>&nbsp;<a href="/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>文献和源码阅读</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-07">2024-08-07</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 1998 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 4 分钟&nbsp;<span id="/transformer-feed-forward-layers-are-key-value-memories/" class="leancloud_visitors" data-flag-title="Transformer Feed-Forward Layers Are Key-Value Memories">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1
id="transformer-feed-forward-layers-are-key-value-memories">Transformer
Feed-Forward Layers Are Key-Value Memories</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Transformer Feed-Forward
Layers Are Key-Value Memories</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Mor Geva; Roei Schuster;
Jonathan Berant; Omer Levy</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2021-09-05</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2012.14913" rel="noopener noreferrer nofollow">10.48550/arXiv.2012.14913</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_NUWXXUEK" rel="noopener noreferrer nofollow">Geva
et al_2021_Transformer Feed-Forward Layers Are Key-Value
Memories.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">前馈层占据了 Transformer
模型参数的三分之二，但其在网络中的作用尚未被充分探索。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。前馈层的输出是其记忆的组合，并通过模型层的残差连接逐步细化，以产生最终的输出分布。</span></span></p>
<h2 id="研究内容"><span style="color: #2E7D32"><span
style="background-color: #f1f8e9">📊 研究内容</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">前馈层与键值神经记忆非常相似，唯一的区别是神经记忆使用
softmax 作为非线性函数，而 Transformer
中的前馈层不使用归一化函数。</span></span></p>
<p>其中</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B305.629%2C176.904%2C526.218%2C186.831%5D%2C%5B306.142%2C163.355%2C525.776%2C173.107%5D%2C%5B305.749%2C148.979%2C524.412%2C159.732%5D%2C%5B306.142%2C135.43%2C526.319%2C146.183%5D%2C%5B305.804%2C122.707%2C524.412%2C132.459%5D%2C%5B306.142%2C109.158%2C526.224%2C118.91%5D%2C%5B306.142%2C95.609%2C525.145%2C105.361%5D%2C%5B306.142%2C82.059%2C524.411%2C91.811%5D%2C%5B306.142%2C68.51%2C506.743%2C78.262%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2">“We
posit that the key vectors K in feed-forward layers act as pattern
detectors over the input sequence, where each individual key vector ki
corresponds to a specific pattern over the input prefix x1, . . . , xj.
To test our claim, we analyze the keys of a trained language model’s
feed-forward layers. We first retrieve the training examples (prefixes
of a sentence) most associated with a given key, that is, the input
texts where the memory coefficient is highest.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 2</a></span>)</span></p>
<p>表明神经元中的<span
class="math inline">\(k_i\)</span>代表了一种模式，而对应的参数矩阵K（某一列向量）充当这个模式的模式检测器。当检测到有对应模式时，则表现为对应的<span
class="math inline">\(k_i\)</span>值较高，相当于注意力中的得分，而其对应的第二层参数矩阵V（某一列向量）代表了这种模式对应的token的概率分布（乘嵌入矩阵进行转换），而将得分与V向量概率分布相乘后得到的概率分布即是最终要得到token的概率分布。即这是一种混合响应输出。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22GKR4GYSJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B306.142%2C576.822%2C526.217%2C586.574%5D%2C%5B306.142%2C563.273%2C524.415%2C573.025%5D%2C%5B306.142%2C549.724%2C526.217%2C559.476%5D%2C%5B306.142%2C536.175%2C525.773%2C546.713%5D%2C%5B305.749%2C522.626%2C524.411%2C532.378%5D%2C%5B306.142%2C509.076%2C526.318%2C518.828%5D%2C%5B305.804%2C494.701%2C526.216%2C505.454%5D%2C%5B306.142%2C481.978%2C524.415%2C491.73%5D%2C%5B306.142%2C468.429%2C526.22%2C480.722%5D%2C%5B306.142%2C454.88%2C524.414%2C464.632%5D%2C%5B306.142%2C441.33%2C526.219%2C451.082%5D%2C%5B306.142%2C426.955%2C524.41%2C437.708%5D%2C%5B306.142%2C414.232%2C363.709%2C423.984%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=2&#x26;annotation=GKR4GYSJ">“Comparing
equations 1 and 2 shows that feedforward layers are almost identical to
key-value neural memories; the only difference is that neural memory
uses softmax as the non-linearity f (·), while the canonical transformer
does not use a normalizing function in the feed-forward layer. The
hidden dimension dm is essentially the number of memories in the layer,
and the activation m = f (x · K&gt;), commonly referred to as the hidden
layer, is a vector containing an unnormalized non-negative coefficient
for each memory. We refer to each mi as the memory coefficient of the
ith memory cell.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 2</a></span>)</span> ffn中的kv解释与self
attention中kv的区别</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22ZNFYPFXW%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B215.467%2C351.295%2C289.13%2C361.178%5D%2C%5B70.866%2C337.746%2C290.95%2C347.498%5D%2C%5B70.866%2C324.196%2C289.132%2C333.948%5D%2C%5B70.866%2C308.402%2C290.945%2C322.493%5D%2C%5B70.866%2C297.098%2C289.13%2C307.025%5D%2C%5B70.866%2C281.303%2C289.41%2C295.395%5D%2C%5B70.866%2C269.173%2C290.947%2C279.927%5D%2C%5B70.048%2C256.45%2C151.491%2C268.169%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=ZNFYPFXW">“We
assume that patterns stored in memory cells originate from examples the
model was trained on. Therefore, given a key
ki<code>that corresponds to the i-th hidden dimension of the</code>-th
feed-forward layer, we compute the memory coefficient
ReLU(xj<code>· ki</code>) for every prefix x1, . . . , xj of every
sentence from the WikiText103’s training set.3”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 3</a></span>)</span>
与key向量相乘后得到的是一个数值，即memory
coefficient，用前缀中最后一个token的输入向量乘以神经元对应的模式检测器ki，即得到这个前缀相对于这个输入模式的匹配程度。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22FWYKNDNQ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B124.695%2C215.803%2C290.941%2C225.73%5D%2C%5B70.866%2C202.254%2C289.133%2C212.181%5D%2C%5B70.866%2C186.459%2C291.043%2C200.55%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=3&#x26;annotation=FWYKNDNQ">“Then,
we retrieve the top-t trigger examples, that is, the t prefixes whose
representation at layer
<code>yielded the highest inner product with ki</code>.”</a></span>
<span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 3</a></span>)</span> 对于每一层的每一个ki都找到前t个memory
coefficient最大的句子（前缀）</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CDV6NRYC%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B347.541%2C206.265%2C524.412%2C216.192%5D%2C%5B306.142%2C192.716%2C524.412%2C202.643%5D%2C%5B305.816%2C179.407%2C316.07%2C191.013%5D%2C%5B312.437%2C176.921%2C476.741%2C191.013%5D%2C%5B472.988%2C176.921%2C526.218%2C188.919%5D%2C%5B305.324%2C163.372%2C524.415%2C177.464%5D%2C%5B305.749%2C149.823%2C526.32%2C163.914%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=4&#x26;annotation=CDV6NRYC">“For
every layer
<code>and memory dimension i, we compare the top-ranked token according to v</code>
i , (argmax(pi<code>)) to the next token w</code> i in the top1 trigger
example according to
ki<code>(the example whose memory coefficient for ki</code> is the
highest).”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 4</a></span>)</span>
即对应vi得到的概率分布的对应token与前面ki中得到的最高memory
coeffcient的句子的下一个token进行对比。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22MV5HS87H%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B81.775%2C393.848%2C290.947%2C407.939%5D%2C%5B70.866%2C382.544%2C144.764%2C394.39%5D%2C%5B141.011%2C380.299%2C289.138%2C392.296%5D%2C%5B70.593%2C366.749%2C289.136%2C380.841%5D%2C%5B70.866%2C355.446%2C290.943%2C365.198%5D%2C%5B70.866%2C341.896%2C262.553%2C353.742%5D%2C%5B258.8%2C339.651%2C289.135%2C351.648%5D%2C%5B70.866%2C328.347%2C262.964%2C338.099%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=5&#x26;annotation=MV5HS87H">“Next,
we take the next token of ki<code>’s top-1 trigger example (w</code> i
), and find where it ranks in the value vector’s distribution
pi<code>. Figure 5 shows that the rank of the next token of a trigger example increases through the layers, meaning that w</code>
i tends to get higher probability in the upper layers.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%225%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 5</a></span>)</span>
对于ki得分最高的句子，其下一个token即要预测的token在ki对应的vi的概率分布中的位置（rank）。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22HV4A9MZX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B70.866%2C391.569%2C289.139%2C401.321%5D%2C%5B70.866%2C378.02%2C289.132%2C387.772%5D%2C%5B70.866%2C364.47%2C290.949%2C374.222%5D%2C%5B70.866%2C350.921%2C291.078%2C360.673%5D%2C%5B70.866%2C337.372%2C138.742%2C347.124%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=HV4A9MZX">“Here,
the validation set is used (rather than the training set used to find
trigger examples) since we are trying to characterize the model’s
behavior at inference time, not find the examples it “memorizes” during
training.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 6</a></span>)</span> 🔤为什么用验证集的原因🔤</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22LQX2ZAT7%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B81.775%2C188.331%2C289.521%2C198.083%5D%2C%5B70.866%2C174.782%2C289.517%2C184.534%5D%2C%5B70.866%2C161.232%2C289.134%2C170.984%5D%2C%5B70.866%2C147.683%2C289.132%2C157.435%5D%2C%5B70.866%2C134.134%2C290.944%2C143.886%5D%2C%5B70.866%2C120.585%2C138.742%2C130.337%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=6&#x26;annotation=LQX2ZAT7">“While
there are cases where a single memory cell dominates the output of a
layer, the majority of outputs are clearly compositional. We count the
number of instances where the feed-forward layer’s top prediction is
different from all of the memories’ top predictions.”</a></span> <span
class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 6</a></span>)</span>
意思为原始vi得到的概率分布对应的token与和Key计算加权后的yi得到的概率分布对应的token是否一致。这里的token都是预测的前缀的下一个token。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22CTTUKNHX%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B112.878%2C594.634%2C290.79%2C603.541%5D%2C%5B70.866%2C582.679%2C289.137%2C591.586%5D%2C%5B70.866%2C570.723%2C274.541%2C579.63%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=CTTUKNHX">“The
fraction of examples in a random sample of 4,000 examples where the
layer’s prediction is different from the prediction of all of its
memories.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 7</a></span>)</span>
图8表明了v的混合相应输出与原始v完全不同，而且相同的例子都是一些停用词。因此混合相应输出非常有意义。</p>
<p><span class="highlight"
data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2FNUWXXUEK%22%2C%22annotationKey%22%3A%22H6JLHYGL%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B70.866%2C185.633%2C289.135%2C195.385%5D%2C%5B70.866%2C172.084%2C291.045%2C181.836%5D%2C%5B70.528%2C158.535%2C290.493%2C168.287%5D%2C%5B70.866%2C144.986%2C290.941%2C154.738%5D%2C%5B70.866%2C131.437%2C180.23%2C141.189%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%7D"
data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/NUWXXUEK?page=7&#x26;annotation=H6JLHYGL">“Figure
9 shows that roughly a third of the model’s predictions are determined
in the bottom few layers. This number grows rapidly from layer 10
onwards, implying that the majority of “hard” decisions occur before the
final layer.”</a></span> <span class="citation"
data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10533898%2Fitems%2F3NJ5N446%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"
data-ztype="zcitation">(<span
class="citation-item"><a href="zotero://select/library/items/3NJ5N446">Geva
等, 2021, p. 7</a></span>)</span>
即某一层得到的最终输出等于经过整个模型得到的最终输出。</p>
<h2 id="研究结论"><span style="color: #4A148C"><span
style="background-color: #f5f5f5">🚩 研究结论</span></span></h2>
<hr />
<ul>
<li><strong><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">前馈层的作用</span></span></strong></li>
</ul>
<p>作者提出前馈层模拟键值记忆，并展示了实验结果，表明键与可解释的输入模式相关联，值在模型上层诱导与下一个标记分布相关的输出词汇表分布。</p>
<ul>
<li><strong><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">研究意义</span></span></strong></li>
</ul>
<p>这些发现为理解 Transformer 语言模型的工作原理提供了新的视角，并为现代
NLP 模型的研究开辟了新的研究方向。</p>
<h2 id="感想-疑问"><span style="color: #006064"><span
style="background-color: #e0f7fa">📌 感想 &amp; 疑问</span></span></h2>
<hr />
<p>该文从kv角度解读了transformer中前馈层的作用，很具有启发性，并得出了深层学习句子的高级特征，浅层学习句子的表面特征(即句子以某个word为结尾)的结论。</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2024-08-07</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/transformer-feed-forward-layers-are-key-value-memories/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories" data-hashtags="文献,Transformer"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-hashtag="文献"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%96%87%E7%8C%AE/">文献</a>,&nbsp;<a href="/tags/transformer/">Transformer</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" class="prev" rel="prev" title="课程表（拓扑排序）"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>课程表（拓扑排序）</a>
            <a href="/moe/" class="next" rel="next" title="Moe">Moe<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://blog.vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
