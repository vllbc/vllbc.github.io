<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Transformer Feed-Forward Layers Are Key-Value Memories - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Transformer Feed-Forward Layers Are Key-Value Memories">
  <meta property="og:description" content="这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward Network, FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：FFN层在功能上等同于一个键值记忆（Key-Value Memory）系统。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-08-07T00:00:00+00:00">
    <meta property="article:tag" content="文献">
    <meta property="article:tag" content="Transformer">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Transformer Feed-Forward Layers Are Key-Value Memories">
  <meta name="twitter:description" content="这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward Network, FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：FFN层在功能上等同于一个键值记忆（Key-Value Memory）系统。">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" /><link rel="prev" href="https://blog.vllbc.top/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" /><link rel="next" href="https://blog.vllbc.top/moe/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Transformer Feed-Forward Layers Are Key-Value Memories",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/transformer-feed-forward-layers-are-key-value-memories\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "文献, Transformer","wordcount":  6840 ,
        "url": "https:\/\/blog.vllbc.top\/transformer-feed-forward-layers-are-key-value-memories\/","datePublished": "2024-08-07T00:00:00+00:00","dateModified": "2024-08-07T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Transformer Feed-Forward Layers Are Key-Value Memories</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/reading/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Reading</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-07">2024-08-07</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 6840 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 14 分钟&nbsp;<span id="/transformer-feed-forward-layers-are-key-value-memories/" class="leancloud_visitors" data-flag-title="Transformer Feed-Forward Layers Are Key-Value Memories">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="content" id="content"><p>这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward
Network,
FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：<strong>FFN层在功能上等同于一个键值记忆（Key-Value
Memory）系统</strong>。</p>
<p>具体来说，论文揭示了FFN内部的运作机制。FFN层包含两个线性变换矩阵，论文将它们分别类比为“键（Keys）”和“值（Values）”。当一个输入向量（代表某个词元或文本片段的含义）进入FFN层时，它会首先与所有的“键”进行匹配计算。这个匹配度，论文称之为<strong>记忆系数（memory
coefficient）</strong>，决定了每个“键”所对应的“值”被激活的强度。每个“值”本身并不直接是一个词，而是一个向量，它蕴含了一个关于“下一个可能出现的词”的概率分布。最终，FFN层的输出是所有被激活的“值”向量的加权和。这个加权和的输出，实际上是对模型下一步应该生成什么内容的一次“提议”或“修正”。</p>
<p>论文的一个惊人发现是，这些“记忆”是具有层次性和可解释性的。</p>
<blockquote>
<p>Our experiments show that the learned patterns are
human-interpretable, and that lower layers tend to capture shallow
patterns, while upper layers learn more semantic ones.
我们的实验表明，学习到的模式是人类可解释的，并且底层网络倾向于捕获浅层模式，而上层网络则学习更具语义的模式。</p>
</blockquote>
<p>例如，在模型的<strong>底层</strong>（如1-9层），FFN的“键”主要响应一些<strong>浅层模式（shallow
patterns）</strong>，比如特定的n-gram短语（例如，以“substitutes”结尾的句子）。而到了模型的<strong>高层</strong>（如10-16层），“键”则演变为响应更抽象的<strong>语义模式（semantic
patterns）</strong>，比如某个特定的话题（例如，关于“电视剧”的讨论）。这一点在论文的
<strong>图2</strong>
中得到了清晰的展示，图中显示了随着层数的增加，语义模式（绿色部分）的占比显著提升，而浅层模式（蓝色部分）则相应减少。</p>
<p><img src="httpsa://i.imgur.com/uG9Xo0T.png" alt="Figure 2" />
<em>图2：论文中的关键图表，展示了不同层级的FFN记忆中，浅层模式与语义模式的比例变化。</em></p>
<p>更进一步，论文发现高层FFN中的“值”向量与对应的“键”模式高度相关。当一个高层的“键”被某个输入模式激活时，其对应的“值”所编码的词汇分布，确实能很大概率地预测出该模式之后最可能出现的词。例如，如果一个“键”专门识别“关于二战轰炸机基地”的语境，那么它对应的“值”向量，在被激活后，就会倾向于生成“任务（missions）”、“攻击（attacked）”等相关词汇。论文在
<strong>图4</strong> 中通过“一致率（agreement
rate）”这个指标量化了这一发现，在高层（11-16层），这种一致率达到了约3.5%，虽然看似不高，但论文强调这比随机猜测（约0.0004%）高出几个数量级，证明了其非凡的预测能力。</p>
<p>最后，论文还探讨了这些记忆是如何被<strong>聚合（Aggregating）</strong>的。在一个FFN层内部，最终的输出通常不是由单个最强的记忆决定的，而是由数百个被激活的记忆<strong>组合（composition）</strong>而成，这在
<strong>图8</strong>
中有明确数据支撑，超过68%的情况下，层的输出与任何单个记忆的预测都不同。在层与层之间，FFN的输出通过<strong>残差连接（residual
connections）</strong>对主干信息流进行精细的<strong>修正（refinement）</strong>。模型并非在最后一层才做出最终决定，而是在逐层传递的过程中，不断地利用FFN的“记忆”来调整和优化对下一个词的预测。</p>
<p>总而言之，这篇论文将FFN从一个难以理解的计算模块，变成了一个结构清晰、功能明确的分布式记忆系统。它不仅为我们理解Transformer的内部工作原理打开了一扇窗，也为后续的模型编辑、可信AI和模型压缩等研究方向奠定了坚实的基础。</p>
<h3
id="论文的研究目标是什么-想要解决什么实际问题这个问题对于行业发展有什么重要意义">1.
论文的研究目标是什么？
想要解决什么实际问题？这个问题对于行业发展有什么重要意义？</h3>
<ul>
<li><p><strong>研究目标</strong>：论文的核心研究目标是揭开Transformer模型中前馈神经网络（FFN）层的神秘面纱，并为其功能提供一个清晰、可验证的解释。具体来说，它试图回答一个核心问题：占模型三分之二参数的FFN层，究竟在做什么？</p></li>
<li><p><strong>解决的实际问题</strong>：</p>
<ol type="1">
<li><strong>模型可解释性缺失</strong>：在当时，自注意力机制的研究已经比较深入，但FFN的功能却是一个巨大的知识盲区。这使得我们对大模型的决策过程理解不完整，难以信任其输出，也难以在出错时进行调试。</li>
<li><strong>模型行为的不可预测性</strong>：我们不知道模型为何会产生“幻觉”、输出有害内容或泄露隐私数据。如果能理解FFN的功能，就可能从机理上找到这些问题的原因。</li>
</ol></li>
<li><p><strong>对行业发展的重要意义</strong>：</p>
<ul>
<li><strong>推动可信AI（Trustworthy
AI）</strong>：理解是信任的前提。通过将FFN解释为键值记忆，这篇论文极大地增强了我们对大模型内部工作原理的理解，是迈向构建更安全、更可控、更值得信赖的AI系统的关键一步。</li>
<li><strong>催生新的技术方向</strong>：该研究直接启发了<strong>模型编辑（Model
Editing）</strong>领域。如果我们知道某个事实性错误（比如“埃菲尔铁塔在罗马”）存储在哪个或哪些FFN“记忆”中，我们就有可能直接修改这些“值”向量来纠正错误，而无需重新训练整个庞大的模型。</li>
<li><strong>提升模型效率和安全性</strong>：通过识别FFN中存储的“记忆”，我们可以分析哪些是冗余的，从而进行模型压缩；也可以识别哪些“记忆”可能存储了训练数据中的敏感信息，为隐私保护和数据安全审计提供新的工具。</li>
</ul></li>
</ul>
<h3
id="论文提出了哪些新的思路方法或模型跟之前的方法相比有什么特点和优势请尽可能参考论文中的细节进行分析">2.
论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。</h3>
<ul>
<li><p><strong>新的思路/模型</strong>：论文并未提出一个全新的模型架构，而是提出了一个关于现有Transformer
FFN层的<strong>功能性类比（functional
analogy）</strong>——即<strong>FFN作为非归一化的键值记忆</strong>。</p></li>
<li><p><strong>核心方法论</strong>：</p>
<ol type="1">
<li><strong>概念重构</strong>：将FFN的数学公式
<code>FFN(x) = ReLU(x * K^T) * V</code> (简化后) 与神经记忆网络
<code>MN(x) = softmax(x * K^T) * V</code> 进行对比。 &gt; Comparing
equations 1 and 2 shows that feed-forward layers are almost identical to
key-value neural memories; the only difference is that neural memory
uses softmax as the non-linearity f(·), while the canonical transformer
does not use a normalizing function in the feed-forward layer. &gt;
对比公式1和2可以发现，前馈层与键值神经记忆几乎完全相同；唯一的区别在于，神经记忆使用softmax作为非线性函数f(·)，而标准的Transformer在前馈层中不使用归一化函数。</li>
<li><strong>关键区别与优势</strong>：论文敏锐地指出了<code>ReLU</code>与<code>softmax</code>的区别。<code>softmax</code>会进行归一化，使得所有激活值的和为1，这意味着它倾向于只选择一个或少数几个最匹配的记忆。而<code>ReLU</code>是非归一化的，它允许<strong>多个记忆单元被同时、独立地激活</strong>。这恰好解释了为何FFN的输出是“组合式”的——它是多个被触发的“想法”（记忆）的叠加，而不是非此即彼的选择。这个洞察是论文的核心优势，更贴近模型在实践中的复杂行为。</li>
</ol></li>
<li><p><strong>分析方法</strong>：论文采用了一种“假设-验证”的实验探究方法，而非纯理论推导。</p>
<ul>
<li><strong>寻找触发样本（Trigger
Examples）</strong>：为了探究“键”<code>k_i</code>的功能，研究者在庞大的训练集中搜索，找到那些能够让该键的记忆系数<code>ReLU(x * k_i)</code>取得最大值的输入文本<code>x</code>。这个方法非常巧妙，它让我们能直接“拷问”每个神经元：“什么样的信息最能让你兴奋？”</li>
<li><strong>模式标注与分类</strong>：通过人类专家（NLP研究生）对这些触发样本进行归纳，识别出其中的共性模式，并将其分为“浅层”和“语义”两类。这是一种将复杂的神经活动与人类可理解的概念联系起来的有效手段。</li>
<li><strong>值向量分析</strong>：将“值”向量<code>v_i</code>通过乘以输出嵌入矩阵<code>E</code>并应用<code>softmax</code>，将其转换为一个完整的词汇表概率分布<code>p_i</code>。这使得研究者可以直接评估每个“记忆”的预测倾向。</li>
</ul></li>
</ul>
<p>与之前笼统地将FFN视为“特征提取器”或简单地增加模型容量的方法相比，这篇论文提供了一个<strong>结构化、模块化且可操作的视角</strong>，其优势在于将抽象的数学运算赋予了具体的认知功能——记忆与联想。</p>
<h3
id="论文通过什么实验来验证所提出方法的有效性实验是如何设计的实验数据和结果如何请引用关键数据加以说明">3.
论文通过什么实验来验证所提出方法的有效性？实验是如何设计的？实验数据和结果如何？请引用关键数据加以说明。</h3>
<p>论文设计了一系列环环相扣的实验来验证其核心假设。</p>
<ul>
<li><strong>实验一：验证“键（Keys）”捕获可解释的输入模式</strong>
<ul>
<li><strong>设计</strong>：研究者从一个16层的Transformer语言模型中，每层随机抽取10个“键”（共160个）。对每个键，他们检索出训练集中最能激活它的前25个文本前缀。然后，由人类专家分析这25个前缀，归纳出它们共同的模式。</li>
<li><strong>数据与结果</strong>：
<ul>
<li><strong>高覆盖率</strong>：对于几乎每一个被分析的键，专家都能找到至少一种可识别的模式。平均每个键能识别出<strong>3.6个</strong>不同的模式。</li>
<li><strong>模式分层</strong>：如前所述，<strong>图2</strong>清晰地展示了从低层到高层，模式由浅层（如固定短语）向语义（如主题）的转变。在第1层，超过80%的模式是浅层的；而在第16层，约70%的模式是语义的。</li>
<li><strong>具体案例</strong>：<strong>表1</strong>
提供了一些生动的例子。例如，第4层的<code>k_449</code>专门响应以“substitutes”结尾的句子；而第16层的<code>k_1635</code>则响应关于“电视剧”的上下文。</li>
</ul></li>
</ul></li>
<li><strong>实验二：验证“值（Values）”存储与键相关的输出分布</strong>
<ul>
<li><strong>设计</strong>：对于每个键<code>k_i</code>，研究者找到最能激活它的那个文本前缀（top-1
trigger
example），并记录其真实的下一个词<code>w_t</code>。同时，他们将对应的“值”向量<code>v_i</code>转换为词汇表分布<code>p_i</code>，并查看其预测概率最高的词<code>argmax(p_i)</code>。他们计算这两个词匹配的频率，即“一致率”。</li>
<li><strong>数据与结果</strong>：
<ul>
<li><strong>高层一致性</strong>：<strong>图4</strong>
显示，在1-10层，一致率几乎为零。但从第11层开始，一致率迅速攀升，到第16层时达到<strong>约3.5%</strong>。这证明了高层“值”向量确实存储了有意义的、与“键”模式匹配的预测信息。</li>
<li><strong>排名提升</strong>：<strong>图5</strong>
从另一个角度验证了这一点。它展示了真实下一个词<code>w_t</code>在“值”向量<code>v_i</code>的预测排名分布。随着层数加深，这个排名显著变好（箱形图整体下移），意味着<code>w_t</code>被赋予了更高的概率。</li>
</ul></li>
</ul></li>
<li><strong>实验三：验证记忆的聚合机制（Intra-layer &amp;
Inter-layer）</strong>
<ul>
<li><strong>设计</strong>：
<ol type="1">
<li><strong>层内聚合</strong>：研究者随机抽取4000个验证集样本，观察每一层FFN的输出。他们统计有多少记忆被激活，并比较最终的层输出预测是否与任何单个记忆的预测相同。</li>
<li><strong>层间聚合</strong>：他们比较了每一层的残差流（FFN的输入）的预测与经过FFN修正后的输出预测，以及模型的最终预测。</li>
</ol></li>
<li><strong>数据与结果</strong>：
<ul>
<li><strong>组合性是常态</strong>：<strong>图8</strong>
的结果非常关键，它显示在所有层中，有<strong>超过68%</strong>的情况下，FFN层的最终输出预测与该层内<strong>任何一个</strong>单独的记忆单元的预测都<strong>不相同</strong>。这强有力地证明了FFN的输出是组合（compositional）而非选择性的。</li>
<li><strong>逐层精炼</strong>：<strong>图9</strong>
显示，模型的最终预测结果在很深的层就已经被残差流（residual）的顶层预测所确定。例如，在第10层，超过60%的情况下，残差流的预测就是模型的最终预测。这说明FFN层扮演的角色更像是对一个已经不错的“草稿”进行微调和修正。</li>
</ul></li>
</ul></li>
</ul>
<h3
id="结合大模型领域的当前学术理解未来在该研究方向上还有哪些值得进一步探索的问题和挑战这可能催生出什么新的技术和投资机会">4.
结合大模型领域的当前学术理解，未来在该研究方向上还有哪些值得进一步探索的问题和挑战？这可能催生出什么新的技术和投资机会?</h3>
<p>这篇论文开辟了一个充满机遇的研究领域，至今仍在不断发展。</p>
<ul>
<li><strong>值得探索的问题和挑战</strong>：
<ol type="1">
<li><strong>与自注意力的协同机制</strong>：论文主要孤立地研究FFN，但FFN的输入恰好是自注意力层的输出。未来的一个核心挑战是理解自注意力层是如何“预处理”和“路由”信息，以便激活正确的FFN记忆的。它们之间存在怎样的“电路（circuits）”？</li>
<li><strong>跨模型和任务的泛化性</strong>：该论文的结论主要基于一个中等规模的GPT风格模型。这些发现在今天的千亿、万亿参数模型（如GPT-4、Llama系列）中是否仍然成立？在不同的模型架构（如Encoder-Decoder、MoE）和不同任务（如代码生成、多模态理解）中，FFN的记忆机制有何异同？</li>
<li><strong>记忆的形成与学习</strong>：FFN中的这些“记忆”是在训练过程中如何形成的？它们与训练数据中的具体样本有何对应关系？理解这一点对于解决数据隐私和版权问题至关重要。</li>
<li><strong>组合性的语法</strong>：我们知道FFN的输出是组合的，但这种组合遵循什么样的“语法”或规则？当多个记忆被激活时，模型如何权衡和整合它们的信息？这背后是否存在更深层次的代数结构？</li>
</ol></li>
<li><strong>可能催生的新技术和投资机会</strong>：
<ol type="1">
<li><strong>AI安全与对齐（AI Safety and
Alignment）</strong>：通过深入理解FFN的记忆机制，可以开发出更先进的工具来检测和抑制模型的有害行为（如偏见、谎言）。这是一个巨大的投资赛道，专注于“可控AI”的公司将有很大潜力。</li>
<li><strong>精准模型编辑与维护</strong>：基于对FFN记忆的理解，可以发展出成熟的“模型手术”技术，对线上部署的大模型进行实时、低成本的知识更新和错误修复。这将催生“大模型运维（LLMOps）”市场中的新兴服务。</li>
<li><strong>自动化可解释性工具</strong>：将论文中需要人工标注的模式发现过程自动化，开发出能够自动分析并报告任意大模型内部“记忆”功能的商业软件。这将成为AI开发和审计流程中的标准工具。</li>
<li><strong>新一代AI芯片设计</strong>：如果FFN的核心功能是稀疏激活的键值查找，那么未来的AI硬件或许可以为此进行专门优化，设计出能效更高、更适合大模型推理的芯片架构。</li>
</ol></li>
</ul>
<h3
id="退一步从批判的视角看这篇论文还存在哪些不足及缺失又有哪些需要进一步验证和存疑的">5.
退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？又有哪些需要进一步验证和存疑的？</h3>
<p>尽管这篇论文极其出色，但从今天的视角看，我们仍可以提出一些批判性的思考。</p>
<ul>
<li><strong>主要不足与缺失</strong>：
<ol type="1">
<li><strong>对“模式”的定义依赖于人类主观性</strong>：论文的核心证据之一来自于人类专家对触发样本的标注。这个过程是主观的，且可能过度简化了神经元实际响应的复杂模式。一个神经元可能响应的是比人类语言标签更抽象、更高维的特征组合。</li>
<li><strong>对“值”向量功能的简化</strong>：“值”向量<code>v_i</code>被分析为对下一个词的直接预测，但它的实际作用是向残差流中添加一个方向向量，这个向量会影响<strong>后续所有层</strong>的计算。论文的分析方法捕捉到了其主要功能，但可能忽略了它对模型深层计算流的更长远、更复杂的影响。</li>
<li><strong>实验范围的局限性</strong>：如前所述，实验仅限于一个特定的模型（Baevski
&amp; Auli,
2019）和一个特定的数据集（WikiText-103）。其结论在更大、更多样化的现代模型上的普适性有待验证。</li>
</ol></li>
<li><strong>需要进一步验证和存疑之处</strong>：
<ol type="1">
<li><strong>浅层与语义的截然划分</strong>：论文将模式清晰地划分为浅层和语义，并观察到在第9-10层左右的转变。这个转变是平滑过渡还是突变？在更大的模型中，这个分界点是否存在，或者说是否存在更复杂的、多阶段的模式演化？</li>
<li><strong>记忆的稀疏性假设</strong>：论文提到，大部分记忆单元在单次推理中是不活跃的。这一稀疏性是FFN高效工作的关键吗？这直接启发了后来的混合专家（MoE）模型。但FFN层是否真的可以被看作是一个大型的、静态的MoE层，这一点仍需更深入的验证。</li>
<li><strong>“组合”的本质</strong>：当FFN的输出是一个与所有单个记忆都不同的“妥协”或“组合”时，这个新的预测是如何产生的？论文称之为“composition”，但其背后的计算原理仍不清晰。它仅仅是向量的线性叠加，还是存在更复杂的非线性相互作用？</li>
</ol></li>
</ul>
<h3
id="我希望从这篇论文中找一些拿来即用的创新想法我应该从这篇论文中重点学什么有哪些启发你认为我还需要补充了解哪些背景知识">6.
我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？有哪些启发？你认为我还需要补充了解哪些背景知识?</h3>
<p>当然，这篇论文是创新思想的宝库。</p>
<ul>
<li><strong>重点学习与启发</strong>：
<ol type="1">
<li><strong>功能性抽象的思维方式</strong>：最重要的启发是，<strong>不要将模型的组件视为纯粹的数学黑箱，而要尝试为其赋予功能性的、可类比的解释</strong>。将FFN类比为“键值记忆”就是一个绝佳的例子。当您遇到一个复杂的系统时，尝试提出一个简单的、可检验的功能假设，然后设计实验去验证它。</li>
<li><strong>“触发样本”的调试方法</strong>：这是一个非常实用的调试和分析技术。当您的模型做出奇怪或错误的预测时，可以借鉴论文的方法，去检查是哪些内部单元（如FFN神经元）被异常激活了。然后，去查找这些单元通常在什么情况下被激活（即它们的“触发样本”），这往往能帮助您定位问题的根源。</li>
<li><strong>分层和组合的视角</strong>：理解模型的决策是一个<strong>逐层构建和精炼</strong>的过程，而非一步到位。同时，要认识到最终的输出往往是多个“内部意见”组合的结果。这启发我们在设计和分析模型时，不仅要看最终输出，更要关注信息在网络中流动的中间状态。</li>
</ol></li>
<li><strong>需要补充的背景知识</strong>：
<ol type="1">
<li><strong>Transformer基础架构</strong>：您需要非常熟悉原始论文《Attention
Is All You
Need》中的所有细节，包括自注意力、多头注意力、位置编码、FFN和残差连接的具体计算过程。</li>
<li><strong>机械可解释性（Mechanistic
Interpretability）</strong>：这是本篇论文所属的领域。建议了解该领域的目标，即彻底理解模型内部的每一个计算步骤和参数是如何协同工作以实现其功能的，可以关注一下“电路（circuits）”这个概念。</li>
<li><strong>向量语义与嵌入（Vector Semantics &amp;
Embeddings）</strong>：深入理解词嵌入（如Word2Vec,
GloVe）和句子嵌入的思想，即如何将文本表示为高维空间中的向量，以及这些向量在数学上如何捕捉语义关系。这是理解FFN中“键”与输入匹配、“值”编码输出分布的基础。</li>
<li><strong>后续研究进展</strong>：阅读一些跟进这项工作的论文，比如关于<strong>模型编辑</strong>的
ROME 和 MEMIT
算法，或者关于<strong>定位模型知识</strong>的研究，这将帮助您了解这个领域最新的发展。</li>
</ol></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2024-08-07</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/transformer-feed-forward-layers-are-key-value-memories/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories" data-hashtags="文献,Transformer"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-hashtag="文献"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/" data-title="Transformer Feed-Forward Layers Are Key-Value Memories"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%96%87%E7%8C%AE/">文献</a>,&nbsp;<a href="/tags/transformer/">Transformer</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" class="prev" rel="prev" title="课程表（拓扑排序）"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>课程表（拓扑排序）</a>
            <a href="/moe/" class="next" rel="next" title="MoE">MoE<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>

    </div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
