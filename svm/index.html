<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>SVM - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="SVM" />
<meta property="og:description" content="SVM kernel 介绍 其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。 一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/svm/" /><meta property="og:image" content="https://example.com/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-11-11T17:56:12+08:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://example.com/logo.png"/>

<meta name="twitter:title" content="SVM"/>
<meta name="twitter:description" content="SVM kernel 介绍 其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。 一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://example.com/svm/" /><link rel="prev" href="https://example.com/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" /><link rel="next" href="https://example.com/gpt/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "SVM",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/example.com\/svm\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/example.com\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "Machine Learning, 分类算法","wordcount":  4876 ,
        "url": "https:\/\/example.com\/svm\/","datePublished": "2022-07-29T00:00:00+00:00","dateModified": "2022-11-11T17:56:12+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/example.com\/images\/avatar.png",
                    "width":  1080 ,
                    "height":  1080 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">SVM</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Machine Learning</a>&nbsp;<a href="/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>分类算法</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-07-29">2022-07-29</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 4876 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 10 分钟&nbsp;<span id="/svm/" class="leancloud_visitors" data-flag-title="SVM">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#kernel">kernel</a>
      <ul>
        <li><a href="#介绍">介绍</a></li>
        <li><a href="#例子">例子</a></li>
        <li><a href="#常用核函数理解">常用核函数理解</a></li>
        <li><a href="#核函数类别">核函数类别</a></li>
        <li><a href="#参考">参考</a></li>
      </ul>
    </li>
    <li><a href="#线性可分支持向量机">线性可分支持向量机</a>
      <ul>
        <li><a href="#线性可分">线性可分</a></li>
        <li><a href="#最大间隔超平面">最大间隔超平面</a></li>
        <li><a href="#支持向量">支持向量</a></li>
        <li><a href="#最优化问题">最优化问题</a></li>
        <li><a href="#对偶问题">对偶问题</a>
          <ul>
            <li><a href="#拉格朗日乘数法拉格朗日对偶和kkt条件">拉格朗日乘数法、拉格朗日对偶和KKT条件</a></li>
            <li><a href="#应用">应用</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#线性支持向量机与软间隔">线性支持向量机与软间隔</a>
      <ul>
        <li><a href="#软间隔">软间隔</a></li>
        <li><a href="#优化目标与求解">优化目标与求解</a>
          <ul>
            <li><a href="#软间隔kkt条件">软间隔KKT条件</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#非线性支持向量机">非线性支持向量机</a>
      <ul>
        <li><a href="#核函数">核函数</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a>
      <ul>
        <li><a href="#优点">优点</a></li>
        <li><a href="#缺点">缺点</a></li>
      </ul>
    </li>
    <li><a href="#参考-1">参考</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="svm">SVM</h1>
<h2 id="kernel">kernel</h2>
<h3 id="介绍">介绍</h3>
<p>其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。</p>
<p>一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超平面。</p>
<p>围绕这个，那么我们所做的就是要在Feature Space套用原本在线性可分情况下的Input Space中使用过的优化方法，来找到那个Maximaizing Margin的超平面。原理机制一模一样，是二次规划，唯一不同是代入数据的不同，将原来的$x_i$替换成了高维空间中的$\phi(x_i)$，这就是映射函数，映射到高维空间。</p>
<p>具体的技巧(trick)，就是简化计算二次规划中间的一步内积计算。也即中间步骤有一步必须求得$\phi(x_i) \phi(x_j)$，我们可以定义核函数$K(x_i,x_j) = \phi(x_i)\phi(x_j)$，这样我们不需要显式计算每一个$\phi(x_i)$，甚至不需要知道它的形式，就可以直接计算结果出来。</p>
<p>也就是说，核函数、内积、相似度这三个词是等价的。因为inner product其实就是一种similarity的度量。核函数和映射是无关的。</p>
<h3 id="例子">例子</h3>
<p>举一个例子：</p>
<p>考虑一个带有特征映射的二维输入空间 $\chi \subseteq \mathbb{R}^{2}$ :
特征映射二维到三维: $\quad \Phi: x=\left(x_{1}, x_{2}\right) \rightarrow \Phi(x)=\left(x_{1}^{2}, x_{2}^{2}, \sqrt{2} x_{1} x_{2}\right) \in F=\mathbb{R}^{3}$
特征空间中的内积：</p>
<p>$$
\begin{aligned}
\langle\Phi(x), \Phi(z)\rangle &amp;=\left\langle\left(x_{1}^{2}, x_{2}^{2}, \sqrt{2} x_{1} x_{2}\right),\left(z_{1}^{2}, z_{2}^{2}, \sqrt{2} z_{1} z_{2}\right)\right\rangle \\
&amp;=x_{1}^{2} z_{1}^{2}+x_{2}^{2} z_{2}^{2}+2 x_{1} x_{2} z_{1} z_{2} \\
&amp;=\left\langle x_{1} z_{1}+x_{2} z_{2}\right\rangle^{2} \\
&amp;=\langle x, z\rangle^{2}
\end{aligned}
$$</p>
<p>根据上面可得，核函数$k(x,z) = \langle x,z \rangle^2=\phi(x)^T \phi(z)$</p>
<p>而这里为什么映射函数是这样的形式呢，其实可以是反推出来的，我也不知道，反正凑巧通过这种映射函数可以得到这个核函数。</p>
<h3 id="常用核函数理解">常用核函数理解</h3>
<p>以高斯核函数为例，
$$
\kappa\left(x_{1}, x_{2}\right)=\exp \left(-\frac{\left|x_{1}-x_{2}\right|^{2}}{2 \sigma^{2}}\right)
$$
我们假设 $\sigma=1$ ，则</p>
<p>$$
\begin{aligned}
\kappa\left(x_{1}, x_{2}\right) &amp;=\exp \left(-\frac{\left|x_{1}-x_{2}\right|^{2}}{2 \sigma^{2}}\right) \\
&amp;=\exp \left(-\left(x_{1}-x_{2}\right)^{2}\right) \\
&amp;=\exp \left(-x_{1}^{2}\right) \exp \left(-x_{2}^{2}\right) \exp \left(2 x_{1} x_{2}\right) \\
&amp; \text { Taylor } \\
&amp;=\exp \left(-x_{1}^{2}\right) \exp \left(-x_{2}^{2}\right)\left(\sum_{i=0}^{\infty} \frac{\left(2 x_{1} x_{2}\right)^{i}}{i !}\right) \\
&amp;=\sum_{i=0}^{\infty}\left(\exp \left(-x_{1}^{2}\right) \exp \left(-x_{2}^{2}\right) \sqrt{\left.\frac{2^{i}}{i !} \sqrt{\frac{2^{i}}{i !}} x_{1}^{i} x_{2}^{i}\right)}\right.\\
&amp;=\sum_{i=0}^{\infty}\left(\left[\exp \left(-x_{1}^{2}\right) \sqrt{\frac{2^{i}}{i !}} x_{1}^{i}\right]\left[\exp \left(-x_{2}^{2}\right) \sqrt{\frac{2^{i}}{i !}} x_{2}^{i}\right]\right) \\
&amp;=\phi\left(x_{1}\right)^{T} \phi\left(x_{2}\right)
\end{aligned}
$$</p>
<p>w这不，已经有了定义的那种形式，对于 $\phi(x)$ ，由于</p>
<p>$$
\phi(x)=\exp \left(-x^{2}\right) \cdot\left(1, \sqrt{\frac{2^{1}}{1 !}} x, \sqrt{\frac{2^{2}}{2 !}} x^{2}, \cdots\right)
$$</p>
<p>所以，可以映射到任何一个维度上。</p>
<h3 id="核函数类别">核函数类别</h3>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220802000231.png"
        data-srcset="/svm/image/Pasted%20image%2020220802000231.png, /svm/image/Pasted%20image%2020220802000231.png 1.5x, /svm/image/Pasted%20image%2020220802000231.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220802000231.png"
        title="/svm/image/Pasted%20image%2020220802000231.png" width="1198" height="575" />其实常用的就那几个，高斯核函数最为常用。</p>
<h3 id="参考">参考</h3>
<blockquote>
<p><a href="https://www.cnblogs.com/damin1909/p/12955240.html" target="_blank" rel="noopener noreffer ">https://www.cnblogs.com/damin1909/p/12955240.html</a>
<a href="https://blog.csdn.net/mengjizhiyou/article/details/103437423" target="_blank" rel="noopener noreffer ">https://blog.csdn.net/mengjizhiyou/article/details/103437423</a></p>
</blockquote>
<h2 id="线性可分支持向量机">线性可分支持向量机</h2>
<h3 id="线性可分">线性可分</h3>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827233624.png"
        data-srcset="/svm/image/Pasted%20image%2020220827233624.png, /svm/image/Pasted%20image%2020220827233624.png 1.5x, /svm/image/Pasted%20image%2020220827233624.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827233624.png"
        title="/svm/image/Pasted%20image%2020220827233624.png" width="1026" height="750" /></p>
<p>在二维空间上，两类点被一条直线完全分开叫做线性可分。</p>
<h3 id="最大间隔超平面">最大间隔超平面</h3>
<p>以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。</p>
<h3 id="支持向量">支持向量</h3>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827233700.png"
        data-srcset="/svm/image/Pasted%20image%2020220827233700.png, /svm/image/Pasted%20image%2020220827233700.png 1.5x, /svm/image/Pasted%20image%2020220827233700.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827233700.png"
        title="/svm/image/Pasted%20image%2020220827233700.png" width="720" height="330" />
样本中距离超平面最近的一些点，这些点叫做支持向量。</p>
<h3 id="最优化问题">最优化问题</h3>
<p>SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。任意超平面可以用下面这个线性方程来描述：</p>
<p>$$
w^Tx+b=0
$$</p>
<p>二维空间点(x,y)到直线$Ax+By+C=0$的距离公式为：</p>
<p>$$
\frac{|Ax+By+C|}{\sqrt{A^2+B^2}}
$$</p>
<p>扩展到n维空间中，$x=(x_1, x_2,\dots, x_n)$到直线$w^Tx+b=0$的距离为：</p>
<p>$$
\frac{|w^Tx+b|}{||w||}
$$
如图所示，根据支持向量的定义我们知道，支持向量到超平面的距离为 d，其他点到超平面的距离大于 d。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827234112.png"
        data-srcset="/svm/image/Pasted%20image%2020220827234112.png, /svm/image/Pasted%20image%2020220827234112.png 1.5x, /svm/image/Pasted%20image%2020220827234112.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827234112.png"
        title="/svm/image/Pasted%20image%2020220827234112.png" width="720" height="509" /></p>
<p>于是我们有这样的一个公式：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827234141.png"
        data-srcset="/svm/image/Pasted%20image%2020220827234141.png, /svm/image/Pasted%20image%2020220827234141.png 1.5x, /svm/image/Pasted%20image%2020220827234141.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827234141.png"
        title="/svm/image/Pasted%20image%2020220827234141.png" width="563" height="200" /></p>
<p>之后得到:</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827234154.png"
        data-srcset="/svm/image/Pasted%20image%2020220827234154.png, /svm/image/Pasted%20image%2020220827234154.png 1.5x, /svm/image/Pasted%20image%2020220827234154.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827234154.png"
        title="/svm/image/Pasted%20image%2020220827234154.png" width="595" height="237" /></p>
<p>分母都是正数，因此可以令它为1。
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235615.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235615.png, /svm/image/Pasted%20image%2020220827235615.png 1.5x, /svm/image/Pasted%20image%2020220827235615.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235615.png"
        title="/svm/image/Pasted%20image%2020220827235615.png" width="535" height="149" /></p>
<p>合并得：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235627.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235627.png, /svm/image/Pasted%20image%2020220827235627.png 1.5x, /svm/image/Pasted%20image%2020220827235627.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235627.png"
        title="/svm/image/Pasted%20image%2020220827235627.png" width="448" height="119" /></p>
<p>至此我们就可以得到最大间隔超平面的上下两个超平面：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235656.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235656.png, /svm/image/Pasted%20image%2020220827235656.png 1.5x, /svm/image/Pasted%20image%2020220827235656.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235656.png"
        title="/svm/image/Pasted%20image%2020220827235656.png" width="720" height="509" /></p>
<p>每个支持向量到超平面的距离可以写为：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235715.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235715.png, /svm/image/Pasted%20image%2020220827235715.png 1.5x, /svm/image/Pasted%20image%2020220827235715.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235715.png"
        title="/svm/image/Pasted%20image%2020220827235715.png" width="352" height="160" /></p>
<p>所以我们得到：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235827.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235827.png, /svm/image/Pasted%20image%2020220827235827.png 1.5x, /svm/image/Pasted%20image%2020220827235827.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235827.png"
        title="/svm/image/Pasted%20image%2020220827235827.png" width="299" height="101" /></p>
<p>最大化这个距离：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220827235837.png"
        data-srcset="/svm/image/Pasted%20image%2020220827235837.png, /svm/image/Pasted%20image%2020220827235837.png 1.5x, /svm/image/Pasted%20image%2020220827235837.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220827235837.png"
        title="/svm/image/Pasted%20image%2020220827235837.png" width="362" height="134" /></p>
<p>这里乘上 2 倍也是为了后面推导，对目标函数没有影响。刚刚我们得到支持向量$y(w^Tx+b) = 1$，所以我们得到：</p>
<p>$$
\max \frac{2}{||w||}
$$</p>
<p>对目标进行转换：</p>
<p>$$
\min \frac{1}{2}||w||^2
$$</p>
<p>所以得到的最优化问题是：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828000128.png"
        data-srcset="/svm/image/Pasted%20image%2020220828000128.png, /svm/image/Pasted%20image%2020220828000128.png 1.5x, /svm/image/Pasted%20image%2020220828000128.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828000128.png"
        title="/svm/image/Pasted%20image%2020220828000128.png" width="783" height="209" /></p>
<h3 id="对偶问题">对偶问题</h3>
<h4 id="拉格朗日乘数法拉格朗日对偶和kkt条件">拉格朗日乘数法、拉格朗日对偶和KKT条件</h4>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/38163970" target="_blank" rel="noopener noreffer ">https://zhuanlan.zhihu.com/p/38163970</a>
给定约束优化问题：</p>
<p>$$
\begin{aligned}
&amp;\min f(x) \\
&amp; s.t. g(x) = 0
\end{aligned}
$$</p>
<p>为方便分析，假设 f 与 g 是连续可导函数。Lagrange乘数法是等式约束优化问题的典型解法。定义Lagrangian函数</p>
<p>$$
L(x, \lambda) = f(x) + \lambda g(x)
$$
其中 λ 称为Lagrange乘数。Lagrange乘数法将原本的约束优化问题转换成等价的无约束优化问题
计算 L 对 x 与 λ 的偏导数并设为零，可得最优解的必要条件：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828151556.png"
        data-srcset="/svm/image/Pasted%20image%2020220828151556.png, /svm/image/Pasted%20image%2020220828151556.png 1.5x, /svm/image/Pasted%20image%2020220828151556.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828151556.png"
        title="/svm/image/Pasted%20image%2020220828151556.png" width="563" height="203" /></p>
<p>接下来是不等式约束：</p>
<p>$$
\begin{aligned}
&amp; \min f(x) \\
&amp; s.t. g(x) \leq 0
\end{aligned}
$$</p>
<p>据此我们定义可行域(feasible region)$K=x\in R^n | g(x)\leq 0$。设$x^*$为满足条件的最佳解，分情况讨论：</p>
<ol>
<li>$g(x^*) &lt; 0$，最佳解位于K的内部，为内部解，这时的约束是无效的。</li>
<li>$g(x^*) = 0$，最佳解落在K的边界，称为边界解，此时的约束是有效的。
这两种情况的最佳解具有不同的必要条件。</li>
</ol>
<p>具有不同的必要条件：</p>
<ol>
<li>内部解：在约束条件无效的情况下，$g(x)$不起作用，约束优化问题退化为无约束优化问题，因此$x^*$满足$\lambda = 0$</li>
<li>边界解：在约束有效的情况下，约束不等式变为等式$g(x)=0$。此时拉格朗日函数在$x^*$的梯度为0，即$\nabla f = -\lambda \nabla g$，$f(x)$的极小值在边界取到，那么可行域内部的$f(x)$应该都是大于这个极小值，则$\nabla f(x)$的方向是可行域内部。而$\nabla g$的方向为可行域外部，因为约束条件是$g(x) \leq 0$，也就是可行域外部都是$g(x) &gt; 0$，所以梯度方向就是指向函数增加的方向。说明两个函数的梯度方向相反，要想上面的等式成立，必须有$\lambda \geq 0$，这就是对偶可行性。
因此，不论是内部解或边界解， $\lambda g(x)=0$ 恒成立</li>
</ol>
<p>整合上述两种情况，最佳解的必要条件包括Lagrangian函数的定常方程式、原始可行性、对偶可行性，以及互补松弛性：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828153456.png"
        data-srcset="/svm/image/Pasted%20image%2020220828153456.png, /svm/image/Pasted%20image%2020220828153456.png 1.5x, /svm/image/Pasted%20image%2020220828153456.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828153456.png"
        title="/svm/image/Pasted%20image%2020220828153456.png" width="446" height="260" /></p>
<p>这就是KKT条件。</p>
<p>上面结果可推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题(或称非线性规划)：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828153544.png"
        data-srcset="/svm/image/Pasted%20image%2020220828153544.png, /svm/image/Pasted%20image%2020220828153544.png 1.5x, /svm/image/Pasted%20image%2020220828153544.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828153544.png"
        title="/svm/image/Pasted%20image%2020220828153544.png" width="645" height="192" /></p>
<p>定义Lagrangian 函数</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828153556.png"
        data-srcset="/svm/image/Pasted%20image%2020220828153556.png, /svm/image/Pasted%20image%2020220828153556.png 1.5x, /svm/image/Pasted%20image%2020220828153556.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828153556.png"
        title="/svm/image/Pasted%20image%2020220828153556.png" width="880" height="125" /></p>
<p>则KKT条件为
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828153608.png"
        data-srcset="/svm/image/Pasted%20image%2020220828153608.png, /svm/image/Pasted%20image%2020220828153608.png 1.5x, /svm/image/Pasted%20image%2020220828153608.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828153608.png"
        title="/svm/image/Pasted%20image%2020220828153608.png" width="590" height="291" /></p>
<h4 id="应用">应用</h4>
<p>已知svm优化的主要问题：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828000902.png"
        data-srcset="/svm/image/Pasted%20image%2020220828000902.png, /svm/image/Pasted%20image%2020220828000902.png 1.5x, /svm/image/Pasted%20image%2020220828000902.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828000902.png"
        title="/svm/image/Pasted%20image%2020220828000902.png" width="1002" height="178" /></p>
<p>那么求解线性可分的 SVM 的步骤为：</p>
<p><strong>步骤1：</strong></p>
<p>构造拉格朗日函数：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828000939.png"
        data-srcset="/svm/image/Pasted%20image%2020220828000939.png, /svm/image/Pasted%20image%2020220828000939.png 1.5x, /svm/image/Pasted%20image%2020220828000939.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828000939.png"
        title="/svm/image/Pasted%20image%2020220828000939.png" width="1128" height="220" /></p>
<p><strong>步骤2：</strong></p>
<p>利用强对偶性转化：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001013.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001013.png, /svm/image/Pasted%20image%2020220828001013.png 1.5x, /svm/image/Pasted%20image%2020220828001013.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001013.png"
        title="/svm/image/Pasted%20image%2020220828001013.png" width="415" height="114" /></p>
<p>现对参数 w 和 b 求偏导数：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001025.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001025.png, /svm/image/Pasted%20image%2020220828001025.png 1.5x, /svm/image/Pasted%20image%2020220828001025.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001025.png"
        title="/svm/image/Pasted%20image%2020220828001025.png" width="618" height="246" /></p>
<p>具体步骤：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001050.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001050.png, /svm/image/Pasted%20image%2020220828001050.png 1.5x, /svm/image/Pasted%20image%2020220828001050.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001050.png"
        title="/svm/image/Pasted%20image%2020220828001050.png" width="863" height="374" /></p>
<p>在前面的步骤中即为：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001120.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001120.png, /svm/image/Pasted%20image%2020220828001120.png 1.5x, /svm/image/Pasted%20image%2020220828001120.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001120.png"
        title="/svm/image/Pasted%20image%2020220828001120.png" width="410" height="239" /></p>
<p>我们将这个结果带回到函数中可得：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001143.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001143.png, /svm/image/Pasted%20image%2020220828001143.png 1.5x, /svm/image/Pasted%20image%2020220828001143.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001143.png"
        title="/svm/image/Pasted%20image%2020220828001143.png" width="997" height="646" /></p>
<p>也就是说：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001152.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001152.png, /svm/image/Pasted%20image%2020220828001152.png 1.5x, /svm/image/Pasted%20image%2020220828001152.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001152.png"
        title="/svm/image/Pasted%20image%2020220828001152.png" width="993" height="122" /></p>
<p><strong>步骤3：</strong>
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001208.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001208.png, /svm/image/Pasted%20image%2020220828001208.png 1.5x, /svm/image/Pasted%20image%2020220828001208.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001208.png"
        title="/svm/image/Pasted%20image%2020220828001208.png" width="869" height="226" /></p>
<p>由上述过程需要满足KKT条件（$\alpha$就是本文中的$\lambda$）：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828002201.png"
        data-srcset="/svm/image/Pasted%20image%2020220828002201.png, /svm/image/Pasted%20image%2020220828002201.png 1.5x, /svm/image/Pasted%20image%2020220828002201.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828002201.png"
        title="/svm/image/Pasted%20image%2020220828002201.png" width="346" height="143" /></p>
<p>易得，当$\lambda_i$大于0，则必有$y_if(x_i)=1$,所对应的样本点是一个支持向量，即位于最大间隔边界上。</p>
<p>我们可以看出来这是一个二次规划问题，问题规模正比于训练样本数，我们常用 SMO(Sequential Minimal Optimization) 算法求解。</p>
<p>SMO(Sequential Minimal Optimization)，序列最小优化算法，其核心思想非常简单：每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的应用。</p>
<p>我们刚说了 SMO 算法每次只优化一个参数，但我们的优化目标有约束条件，没法一次只变动一个参数。所以我们选择了一次选择两个参数。具体步骤为：</p>
<ol>
<li>选择两个需要更新的参数$\lambda _i$和$\lambda_j$，固定其他参数。于是我们有以下约束：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828001631.png"
        data-srcset="/svm/image/Pasted%20image%2020220828001631.png, /svm/image/Pasted%20image%2020220828001631.png 1.5x, /svm/image/Pasted%20image%2020220828001631.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828001631.png"
        title="/svm/image/Pasted%20image%2020220828001631.png" width="656" height="130" /></li>
</ol>
<p>其中$c = -\sum_{k\neq i, j}\lambda_ky_k$， 因此可以得出$\lambda_j = \frac{c-\lambda_iy_i}{y_j}$，这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是$\lambda_i&gt;0$</p>
<ol start="2">
<li>对于仅有一个约束条件的最优化问题，我们完全可以在$\lambda_i$上对优化目标求偏导，令导数为零，从而求出变量值$\lambda_{inew}$，从而求出$\lambda_{jnew}$</li>
<li>多次迭代直至收敛。
通过 SMO 求得最优解$\lambda^*$</li>
</ol>
<p><strong>步骤4：</strong></p>
<p>我们求偏导数时得到：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828002003.png"
        data-srcset="/svm/image/Pasted%20image%2020220828002003.png, /svm/image/Pasted%20image%2020220828002003.png 1.5x, /svm/image/Pasted%20image%2020220828002003.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828002003.png"
        title="/svm/image/Pasted%20image%2020220828002003.png" width="334" height="167" /></p>
<p>由上式可求得 w。</p>
<p>由于所有$\lambda_i&gt;0$的点都是支持向量，可以随便找一个支持向量代入$y_s(w^Tx_s+b)=1$，求出b即可。</p>
<p>两边同时乘以$y_s$，最后得$b = y_s-wx_s$</p>
<p>为了更具鲁棒性，我们可以求得支持向量的均值：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828002938.png"
        data-srcset="/svm/image/Pasted%20image%2020220828002938.png, /svm/image/Pasted%20image%2020220828002938.png 1.5x, /svm/image/Pasted%20image%2020220828002938.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828002938.png"
        title="/svm/image/Pasted%20image%2020220828002938.png" width="577" height="147" /></p>
<p><strong>步骤5：</strong>
w 和 b 都求出来了，我们就能构造出最大分割超平面：$w^Tx+b=0$</p>
<p>分类决策函数：$f(x) = sign(w^Tx+b)$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828003127.png"
        data-srcset="/svm/image/Pasted%20image%2020220828003127.png, /svm/image/Pasted%20image%2020220828003127.png 1.5x, /svm/image/Pasted%20image%2020220828003127.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828003127.png"
        title="/svm/image/Pasted%20image%2020220828003127.png" width="595" height="216" /></p>
<p>将新样本点导入到决策函数中既可得到样本的分类。</p>
<h2 id="线性支持向量机与软间隔">线性支持向量机与软间隔</h2>
<h3 id="软间隔">软间隔</h3>
<p>在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们应该怎么办？比如下面这个：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828105535.png"
        data-srcset="/svm/image/Pasted%20image%2020220828105535.png, /svm/image/Pasted%20image%2020220828105535.png 1.5x, /svm/image/Pasted%20image%2020220828105535.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828105535.png"
        title="/svm/image/Pasted%20image%2020220828105535.png" width="720" height="619" /></p>
<p>于是我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面，即允许出现分类错误的样本：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828105552.png"
        data-srcset="/svm/image/Pasted%20image%2020220828105552.png, /svm/image/Pasted%20image%2020220828105552.png 1.5x, /svm/image/Pasted%20image%2020220828105552.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828105552.png"
        title="/svm/image/Pasted%20image%2020220828105552.png" width="720" height="645" /></p>
<p>我们允许部分样本点不满足约束条件：</p>
<p>$$
y_i(w^Tx_i+b) \geq 1
$$</p>
<p>则优化目标变成了</p>
<p>$$
\min <em>{w, b} \frac{1}{2}|{w}|^{2}+C \sum</em>{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left({w}^{\mathrm{T}} {x}_{i}+b\right)-1\right),
$$</p>
<p>其中 $C&gt;0$ 是一个常数, $\ell_{0 / 1}$ 是 “ $0 / 1$ 损失函数”</p>
<p>$$
\ell_{0 / 1}(z)= \begin{cases}1, &amp; \text { if } z&lt;0 \\ 0, &amp; \text { otherwise. }\end{cases}
$$</p>
<p>显然, 当 $C$ 为无穷大时, $\xi_i$必然无穷小，如此一来线性svm就又变成了线性可分svm，当$C$为有限值时，才会允许部分样本不遵循约束条件</p>
<p>然而, $\ell_{0 / 1}$ 非凸、非连续, 数学性质不太好, 使得不易直接求解. 于 是, 人们通常用其他一些函数来代替 $\ell_{0 / 1}$, 称为 “替代损失” (surrogate loss). 替代损失函数一般具有较好的数学性质, 如它们通常是凸的连续函数且是 $\ell_{0 / 1}$ 的上界. 给出了三种常用的替代损失函数:</p>
<p>hinge 损失: $\ell_{\text {hinge }}(z)=\max (0,1-z)$;</p>
<p>指数损失(exponential loss): $\ell_{\exp }(z)=\exp (-z)$;</p>
<p>对率损失(logistic loss): $\ell_{\log }(z)=\log (1+\exp (-z))$.</p>
<p>若采用 hinge 损失, 则变成</p>
<p>$$
\min <em>{w, b} \frac{1}{2}|{w}|^{2}+C \sum</em>{i=1}^{m} \max \left(0,1-y_{i}\left({w}^{\mathrm{T}} {x}_{i}+b\right)\right)
$$</p>
<p>为了度量这个间隔软到何种程度，我们为每个样本引入一个松弛变量$\xi_i$，令$\xi_i \geq 0$，且$1-y_i(w^Tx_i+b)-\xi_i\leq 0$，如下图：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828111154.png"
        data-srcset="/svm/image/Pasted%20image%2020220828111154.png, /svm/image/Pasted%20image%2020220828111154.png 1.5x, /svm/image/Pasted%20image%2020220828111154.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828111154.png"
        title="/svm/image/Pasted%20image%2020220828111154.png" width="720" height="613" /></p>
<h3 id="优化目标与求解">优化目标与求解</h3>
<p>优化目标：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112312.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112312.png, /svm/image/Pasted%20image%2020220828112312.png 1.5x, /svm/image/Pasted%20image%2020220828112312.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112312.png"
        title="/svm/image/Pasted%20image%2020220828112312.png" width="920" height="274" /></p>
<p><strong>步骤1：</strong></p>
<p>构造拉格朗日函数：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112251.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112251.png, /svm/image/Pasted%20image%2020220828112251.png 1.5x, /svm/image/Pasted%20image%2020220828112251.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112251.png"
        title="/svm/image/Pasted%20image%2020220828112251.png" width="804" height="298" /></p>
<p><strong>步骤2：</strong>
分别求导，得出以下关系：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112422.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112422.png, /svm/image/Pasted%20image%2020220828112422.png 1.5x, /svm/image/Pasted%20image%2020220828112422.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112422.png"
        title="/svm/image/Pasted%20image%2020220828112422.png" width="372" height="323" /></p>
<p>将这些关系带入拉格朗日函数中，得到：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112445.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112445.png, /svm/image/Pasted%20image%2020220828112445.png 1.5x, /svm/image/Pasted%20image%2020220828112445.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112445.png"
        title="/svm/image/Pasted%20image%2020220828112445.png" width="1065" height="169" /></p>
<p>则：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112459.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112459.png, /svm/image/Pasted%20image%2020220828112459.png 1.5x, /svm/image/Pasted%20image%2020220828112459.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112459.png"
        title="/svm/image/Pasted%20image%2020220828112459.png" width="895" height="256" /></p>
<p>我们可以看到这个和硬间隔的一样，只是多了个约束条件。</p>
<p>然后使用SMO算法求$\lambda^*$</p>
<h4 id="软间隔kkt条件">软间隔KKT条件</h4>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828113029.png"
        data-srcset="/svm/image/Pasted%20image%2020220828113029.png, /svm/image/Pasted%20image%2020220828113029.png 1.5x, /svm/image/Pasted%20image%2020220828113029.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828113029.png"
        title="/svm/image/Pasted%20image%2020220828113029.png" width="317" height="185" />
其中$\alpha$对应本文的$\lambda$，$\mu$对应本文的$\mu$</p>
<p>因此由第三个式子得必有$\lambda_i =0$或者$y_if(x_i) - 1+\xi_i \geq 0$
$\lambda_i=0$，则该样本对其没有任何影响。
$\lambda_i &gt; 0$，则样本为支持向量。
若$\lambda_i &lt;C$,则$\mu_i &gt; 0$，进而有$\xi_i=0$，则样本恰在最大间隔边界上。也是支持向量。
若$\lambda_i=C$,则有$\mu_i=0$，此时若$\xi_i\leq 1$，则样本落在最大间隔内部。若$\xi_i&gt;1$则样本被错误分类。</p>
<p>再看一下下面这图就理解了。
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828111154.png"
        data-srcset="/svm/image/Pasted%20image%2020220828111154.png, /svm/image/Pasted%20image%2020220828111154.png 1.5x, /svm/image/Pasted%20image%2020220828111154.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828111154.png"
        title="/svm/image/Pasted%20image%2020220828111154.png" width="720" height="613" />
<strong>步骤3：</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828112623.png"
        data-srcset="/svm/image/Pasted%20image%2020220828112623.png, /svm/image/Pasted%20image%2020220828112623.png 1.5x, /svm/image/Pasted%20image%2020220828112623.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828112623.png"
        title="/svm/image/Pasted%20image%2020220828112623.png" width="609" height="257" /></p>
<p>然后我们通过上面两个式子求出 w 和 b，最终求得超平面</p>
<p><strong>这边要注意一个问题，在间隔内的那部分样本点是不是支持向量？</strong></p>
<p>我们可以由求参数 w 的那个式子可看出，只要 $\lambda_i &gt; 0$的点都能影响我们的超平面，因此都是支持向量。</p>
<h2 id="非线性支持向量机">非线性支持向量机</h2>
<p>我们刚刚讨论的硬间隔和软间隔都是在说样本的完全线性可分或者大部分样本点的线性可分。</p>
<p>但我们可能会碰到的一种情况是样本点不是线性可分的，比如：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828135755.png"
        data-srcset="/svm/image/Pasted%20image%2020220828135755.png, /svm/image/Pasted%20image%2020220828135755.png 1.5x, /svm/image/Pasted%20image%2020220828135755.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828135755.png"
        title="/svm/image/Pasted%20image%2020220828135755.png" width="720" height="636" /></p>
<p>这种情况的解决方法就是：将二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分，比如：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828135803.png"
        data-srcset="/svm/image/Pasted%20image%2020220828135803.png, /svm/image/Pasted%20image%2020220828135803.png 1.5x, /svm/image/Pasted%20image%2020220828135803.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828135803.png"
        title="/svm/image/Pasted%20image%2020220828135803.png" width="720" height="539" /></p>
<p>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。</p>
<p>我们用 x 表示原来的样本点，用$\phi(x)$表示 x 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为: $f(x) = w\phi(x)+b$</p>
<p>对于非线性 SVM 的对偶问题就变成了：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/svm/image/Pasted%20image%2020220828143014.png"
        data-srcset="/svm/image/Pasted%20image%2020220828143014.png, /svm/image/Pasted%20image%2020220828143014.png 1.5x, /svm/image/Pasted%20image%2020220828143014.png 2x"
        data-sizes="auto"
        alt="/svm/image/Pasted%20image%2020220828143014.png"
        title="/svm/image/Pasted%20image%2020220828143014.png" width="822" height="242" /></p>
<p>区别就在于优化目标中的内积。</p>
<h3 id="核函数">核函数</h3>
<p>我们不禁有个疑问：只是做个内积运算，为什么要有核函数的呢？</p>
<p>这是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘全部计算好，这样的计算量太大了。</p>
<p>但如果我们有这样的一核函数$k(x,y) = (\phi(x), \phi(y))$，x与y在特征空间中的内积，就等于它们在原始空间中通过函数$k(x,y)$计算的结果，我们就不需要知道映射函数和计算高维空间中的内积了。</p>
<p>有关内容看本文一开始对kernel的介绍。</p>
<h2 id="总结">总结</h2>
<p>SVM是深度学习流行之前的首选分类方法，在许多任务上都有很好的效果，稍微修改后可以用于回归任务中。总结一下svm算法的优缺点。</p>
<h3 id="优点">优点</h3>
<ul>
<li>有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题；</li>
<li>能找出对任务至关重要的关键样本（即：支持向量）；</li>
<li>采用核技巧之后，可以处理非线性分类/回归任务；</li>
<li>最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。</li>
</ul>
<h3 id="缺点">缺点</h3>
<ul>
<li>训练时间长。当采用 SMO 算法时，每次都需要挑选一对参数</li>
<li>当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O(N^2)$</li>
<li>模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
</ul>
<h2 id="参考-1">参考</h2>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/77750026" target="_blank" rel="noopener noreffer ">https://zhuanlan.zhihu.com/p/77750026</a></p>
</blockquote>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-11-11&nbsp;<a class="git-hash" href="https://github.com/dillonzq/LoveIt/commit/a367590c3334642e32350bb5b0764d882c8186a1" target="_blank" title="commit by vllbc(1683070754@qq.com) a367590c3334642e32350bb5b0764d882c8186a1: 玩火了 保存一下">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>a367590</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/svm/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://example.com/svm/" data-title="SVM" data-hashtags="Machine Learning,分类算法"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://example.com/svm/" data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://example.com/svm/" data-title="SVM"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://example.com/svm/" data-title="SVM"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://example.com/svm/" data-title="SVM"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">分类算法</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" class="prev" rel="prev" title="线性判别分析"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>线性判别分析</a>
            <a href="/gpt/" class="next" rel="next" title="GPT">GPT<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI","appKey":"WBmoGyJtbqUswvfLh6L8iEBr","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"你的评论 ...","recordIP":true,"serverURLs":"https://leancloud.hugoloveit.com","visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.zh-cn","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
