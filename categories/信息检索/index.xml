<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>信息检索 - 分类 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/categories/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/</link>
        <description>信息检索 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 16 Jan 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/categories/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/" rel="self" type="application/rss+xml" /><item>
    <title>SimCSE</title>
    <link>http://localhost:1313/simcse/</link>
    <pubDate>Tue, 16 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/simcse/</guid>
    <description><![CDATA[<h1 id="无监督">无监督</h1>
<h2 id="info-noise-contrastive-estimation-loss">info Noise Contrastive
Estimation loss</h2>
<h1 id="有监督">有监督</h1>
<h1 id="复现代码">复现代码</h1>
<p>只贴最核心的损失函数代码</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simcse_unsup_loss(y_pred, device, temp<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;&quot;&quot;无监督的损失函数</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">    y_pred (tensor): bert的输出, [batch_size * 2, 768] ,2为句子个数，即一个句子对</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">    &quot;&quot;&quot;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 得到y_pred对应的label, [1, 0, 3, 2, ..., batch_size-1, batch_size-2]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> torch.arange(y_pred.shape[<span class="dv">0</span>], device<span class="op">=</span>device)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> (y_true <span class="op">-</span> y_true <span class="op">%</span> <span class="dv">2</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch内两两计算相似度, 得到相似度矩阵(batch_size*batch_size)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    sim <span class="op">=</span> F.cosine_similarity(y_pred.unsqueeze(<span class="dv">1</span>), y_pred.unsqueeze(<span class="dv">0</span>), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(sim)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(sim.shape)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 将相似度矩阵对角线置为很小的值, 消除自身的影响</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    sim <span class="op">=</span> sim <span class="op">-</span> torch.eye(y_pred.shape[<span class="dv">0</span>], device<span class="op">=</span>device) <span class="op">*</span> <span class="fl">1e12</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 相似度矩阵除以温度系数</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    sim <span class="op">=</span> sim <span class="op">/</span> temp</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算相似度矩阵与y_true的交叉熵损失</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 计算交叉熵，每个case都会计算与其他case的相似度得分，得到一个得分向量，目的是使得该得分向量中正样本的得分最高，负样本的得分最低</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(sim, y_true)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean(loss)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="st">    苏神keras源码</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="st">    def simcse_loss(y_true, y_pred):</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="st">        idxs = K.arange(0, K.shape(y_pred)[0]) #生成batch内句子的编码 [0,1,2,3,4,5]为例子</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="st">        idxs_1 = idxs[None, :] # 给idxs添加一个维度，变成： [[0,1,2,3,4,5]]</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="st">        idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None] # 这个意思就是说，如果一个句子id为奇数，那么和它同义的句子的id就是它的上一句，如果一个句子id为偶数，那么和它同义的句子的id就是它的下一句。 [:, None] 是在列上添加一个维度。初步生成了label。[[1], [0], [3], [2], [5], [4]]</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="st">        y_true = K.equal(idxs_1, idxs_2) # equal会让idxs1和idxs2都映射到6*6,idxs1垂直，idxs2水平</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="st">        y_true = K.cast(y_true, K.floatx()) # 生成label</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="st">        y_pred = K.l2_normalize(y_pred, axis=1) # 对句向量各个维度做了一个L2正则，使其变得各项同性，避免下面计算相似度时，某一个维度影响力过大。</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="st">        similarities = K.dot(y_pred, K.transpose(y_pred)) # 计算batch内每句话和其他句子的内积相似度。</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="st">        similarities = similarities - tf.eye(K.shape(y_pred)[0]) * 1e12 # 将和自身的相似度变为0(后面的softmax之后)。</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="st">        similarities = similarities * 20 # 将所有相似度乘以20，这个目的是想计算softmax概率时，更加有区分度。</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="st">        loss = K.categorical_crossentropy(y_true, similarities, from_logits=True)</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="st">        return K.mean(loss)</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="st">    &quot;&quot;&quot;</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simcse_sup_loss(y_pred, device, lamda<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="st">    有监督损失函数</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="st">    &quot;&quot;&quot;</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> F.cosine_similarity(y_pred.unsqueeze(<span class="dv">0</span>), y_pred.unsqueeze(<span class="dv">1</span>), dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    row <span class="op">=</span> torch.arange(<span class="dv">0</span>, y_pred.shape[<span class="dv">0</span>], <span class="dv">3</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> torch.arange(<span class="dv">0</span>, y_pred.shape[<span class="dv">0</span>])</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> col[col <span class="op">%</span> <span class="dv">3</span> <span class="op">!=</span> <span class="dv">0</span>]</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> similarities[row, :]</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> similarities[:, col]</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> similarities <span class="op">/</span> lamda</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> torch.arange(<span class="dv">0</span>, <span class="bu">len</span>(col), <span class="dv">2</span>, device<span class="op">=</span>device)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(similarities, y_true)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code></pre></div>
<h1 id="参考文献">参考文献</h1>
<ol type="1">
<li><a href="https://zhuanlan.zhihu.com/p/483453992">SIMCSE算法源码分析
- 知乎 (zhihu.com)</a></li>
<li><a
href="https://transformerswsz.github.io/2022/05/01/SimCSE%E8%AE%BA%E6%96%87%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">SimCSE论文及源码解读
| Swift’s Blog (transformerswsz.github.io)</a></li>
</ol>]]></description>
</item>
<item>
    <title>pagerank</title>
    <link>http://localhost:1313/pagerank/</link>
    <pubDate>Mon, 19 Jun 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/pagerank/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>文本匹配概述</title>
    <link>http://localhost:1313/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Fri, 07 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[<p>文本语义匹配是自然语言处理中一个重要的基础问题，NLP
领域的很多任务都可以抽象为文本匹配任务。例如，信息检索可以归结为查询项和文档的匹配，问答系统可以归结为问题和候选答案的匹配(基于文本的问答系统)，对话系统可以归结为对话和回复的匹配。语义匹配在搜索优化、推荐系统、快速检索排序、智能客服上都有广泛的应用。如何提升文本匹配的准确度，是自然语言处理领域的一个重要挑战。</p>]]></description>
</item>
<item>
    <title>检索式问答系统</title>
    <link>http://localhost:1313/%E6%A3%80%E7%B4%A2%E5%BC%8F%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/</link>
    <pubDate>Thu, 25 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E6%A3%80%E7%B4%A2%E5%BC%8F%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>布尔检索</title>
    <link>http://localhost:1313/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/</link>
    <pubDate>Fri, 05 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/</guid>
    <description><![CDATA[<p>信息检索就是从大规模非结构化数据（通常是文本）的集合（通常保存在计算机上）中找出满足用户信息需求的资料（通常是文档）的过程。</p>
<p>布尔索引顾名思义就是利用布尔操作对文档进行索引，对于检索任务来说，要求的就是对于一个query，检索出与其相关的文档，首先肯定的是文档中要出现query中的词项才有可能有关，易得的一个想法就是建立词项-文档关联矩阵，行向量表示词项在各文档中是否出现，列向量表示某文档中各词项是否出现，得到了这个矩阵，对于query，就可以根据布尔检索得到对应的doc。这是一个很容易想到的方法。</p>]]></description>
</item>
<item>
    <title>DSSM</title>
    <link>http://localhost:1313/dssm/</link>
    <pubDate>Thu, 21 Apr 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/dssm/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>NeuralCF</title>
    <link>http://localhost:1313/neuralcf/</link>
    <pubDate>Mon, 05 Jul 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/neuralcf/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>词项词典及倒排记录表</title>
    <link>http://localhost:1313/%E8%AF%8D%E9%A1%B9%E8%AF%8D%E5%85%B8%E5%8F%8A%E5%80%92%E6%8E%92%E8%AE%B0%E5%BD%95%E8%A1%A8/</link>
    <pubDate>Thu, 24 Jun 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E8%AF%8D%E9%A1%B9%E8%AF%8D%E5%85%B8%E5%8F%8A%E5%80%92%E6%8E%92%E8%AE%B0%E5%BD%95%E8%A1%A8/</guid>
    <description><![CDATA[<p></p>]]></description>
</item>
<item>
    <title>协同过滤</title>
    <link>http://localhost:1313/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</link>
    <pubDate>Sat, 06 Mar 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</guid>
    <description><![CDATA[<p>协同过滤是推荐系统里面一个常用的算法，因为信息检索领域和推荐系统领域其实是有点相似的（对我目前的认知来说，对两个领域的了解都不深），所以就把协同过滤的笔记整理到了信息检索的目录下。现在主要的都是深度学习的方法了吧，双塔模型什么的。还不是太了解，继续学习。</p>]]></description>
</item>
</channel>
</rss>
