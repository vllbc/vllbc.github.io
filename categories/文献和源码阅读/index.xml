<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>文献和源码阅读 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link>
        <description>文献和源码阅读 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" rel="self" type="application/rss+xml" /><item>
    <title>Data Engineering for Scaling Language Models to 128K Context</title>
    <link>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</link>
    <pubDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</guid>
    <description><![CDATA[Data Engineering for Scaling Language Models to 128K Context 💡 Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 期刊标签 DOI 10.48550/arXiv.2402.10171 附件 Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf 📜 研究背景 &amp; 基础 &amp; 目]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[Transformer Feed-Forward Layers Are Key-Value Memories 💡 Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 期刊标签 DOI 10.48550/arXiv.2012.14913 附件 Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf 📜 研究背景 &amp; 基础 &amp; 目的 前馈层占据了 Transformer 模型参数的三分]]></description>
</item>
</channel>
</rss>
