<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>文献和源码阅读 - 分类 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link>
        <description>文献和源码阅读 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" rel="self" type="application/rss+xml" /><item>
    <title>Data Engineering for Scaling Language Models to 128K Context</title>
    <link>http://localhost:1313/data-engineering-for-scaling-language-models-to-128k-context/</link>
    <pubDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/data-engineering-for-scaling-language-models-to-128k-context/</guid>
    <description><![CDATA[<h1
id="data-engineering-for-scaling-language-models-to-128k-context">Data
Engineering for Scaling Language Models to 128K Context</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Data Engineering for Scaling
Language Models to 128K Context</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Yao Fu; Rameswar Panda;
Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao
Peng</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2024-02-15</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2402.10171" rel="noopener noreferrer nofollow">10.48550/arXiv.2402.10171</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_Z5AQISDH" rel="noopener noreferrer nofollow">Fu
et al_2024_Data Engineering for Scaling Language Models to 128K
Context.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。</span></span></p>]]></description>
</item>
<item>
    <title>Data Engineering for Scaling Language Models to 128K Context</title>
    <link>http://localhost:1313/data-engineering-for-scaling-language-models-to-128k-context/</link>
    <pubDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/data-engineering-for-scaling-language-models-to-128k-context/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以专家的身份，与您一同深入探讨这篇在长上下文（Long
Context）领域具有重要影响力的论文——《Data Engineering for Scaling
Language Models to 128K Context》。</p>]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>http://localhost:1313/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[<h1
id="transformer-feed-forward-layers-are-key-value-memories">Transformer
Feed-Forward Layers Are Key-Value Memories</h1>
<hr />
<h2 id="meta-data"><span style="color: #1B5E20"><span
style="background-color: #f1f8e9">💡 Meta Data</span></span></h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr>
<th><span style="background-color: #dbeedd">Title</span></th>
<th><span style="background-color: #dbeedd">Transformer Feed-Forward
Layers Are Key-Value Memories</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span style="background-color: #f3faf4">Journal</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">Authors</span></td>
<td><span style="background-color: #dbeedd">Mor Geva; Roei Schuster;
Jonathan Berant; Omer Levy</span></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">Pub. date</span></td>
<td><span style="background-color: #f3faf4">2021-09-05</span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">期刊标签</span></td>
<td></td>
</tr>
<tr>
<td><span style="background-color: #f3faf4">DOI</span></td>
<td><span
style="background-color: #f3faf4"><a href="https://doi.org/10.48550/arXiv.2012.14913" rel="noopener noreferrer nofollow">10.48550/arXiv.2012.14913</a></span></td>
</tr>
<tr>
<td><span style="background-color: #dbeedd">附件</span></td>
<td><span
style="background-color: #dbeedd"><a href="zotero://open-pdf/0_NUWXXUEK" rel="noopener noreferrer nofollow">Geva
et al_2021_Transformer Feed-Forward Layers Are Key-Value
Memories.pdf</a></span></td>
</tr>
</tbody>
</table>
<h2 id="研究背景-基础-目的"><span style="color: #E65100"><span
style="background-color: #fff8e1">📜 研究背景 &amp; 基础 &amp;
目的</span></span></h2>
<hr />
<p><span style="color: rgb(6, 6, 7)"><span
style="background-color: rgb(255, 255, 255)">前馈层占据了 Transformer
模型参数的三分之二，但其在网络中的作用尚未被充分探索。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。作者发现
Transformer 语言模型中的前馈层可以作为键值记忆（key-value
memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。前馈层的输出是其记忆的组合，并通过模型层的残差连接逐步细化，以产生最终的输出分布。</span></span></p>]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>http://localhost:1313/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward
Network,
FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：<strong>FFN层在功能上等同于一个键值记忆（Key-Value
Memory）系统</strong>。</p>]]></description>
</item>
</channel>
</rss>
