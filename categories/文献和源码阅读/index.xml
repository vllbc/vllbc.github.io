<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>æ–‡çŒ®å’Œæºç é˜…è¯» - åˆ†ç±» - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link>
        <description>æ–‡çŒ®å’Œæºç é˜…è¯» - åˆ†ç±» - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E6%96%87%E7%8C%AE%E5%92%8C%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" rel="self" type="application/rss+xml" /><item>
    <title>Data Engineering for Scaling Language Models to 128K Context</title>
    <link>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</link>
    <pubDate>Thu, 08 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data-engineering-for-scaling-language-models-to-128k-context/</guid>
    <description><![CDATA[Data Engineering for Scaling Language Models to 128K Context ğŸ’¡ Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 æœŸåˆŠæ ‡ç­¾ DOI 10.48550/arXiv.2402.10171 é™„ä»¶ Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf ğŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®]]></description>
</item>
<item>
    <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
    <link>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer-feed-forward-layers-are-key-value-memories/</guid>
    <description><![CDATA[Transformer Feed-Forward Layers Are Key-Value Memories ğŸ’¡ Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 æœŸåˆŠæ ‡ç­¾ DOI 10.48550/arXiv.2012.14913 é™„ä»¶ Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf ğŸ“œ ç ”ç©¶èƒŒæ™¯ &amp; åŸºç¡€ &amp; ç›®çš„ å‰é¦ˆå±‚å æ®äº† Transformer æ¨¡å‹å‚æ•°çš„ä¸‰åˆ†]]></description>
</item>
</channel>
</rss>
