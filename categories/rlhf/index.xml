<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>RLHF - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/rlhf/</link>
        <description>RLHF - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/rlhf/" rel="self" type="application/rss+xml" /><item>
    <title>dapo</title>
    <link>https://blog.vllbc.top/dapo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dapo/</guid>
    <description><![CDATA[<p>DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling
Policy
Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2
个是主要亮点，是命名的来源）</p>]]></description>
</item>
<item>
    <title>grpo</title>
    <link>https://blog.vllbc.top/grpo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/grpo/</guid>
    <description><![CDATA[<h1 id="grpo-trl-库">GRPO (trl 库)</h1>
<h2 id="重要参数">重要参数</h2>
<ul>
<li>Num_generations: <strong>Number of generations to sample. The
effective batch size (num_processes * per_device_batch_size *
gradient_accumulation_steps) must be evenly divisible by this
value.</strong></li>
<li>generation_batch_size: <strong>Batch size to use for generation. If
<code>None</code>, it defaults to the effective training batch size:
<code>per_device_train_batch_size * num_processes * steps_per_generation</code>.</strong></li>
<li>steps_per_generation: Number of optimization steps per generation.
If <code>None</code>, it defaults to gradient_accumulation_steps.</li>
<li>Num_iterations: Number of iterations per batch (denoted as μ in the
algorithm).</li>
<li>Per_device_train_batch_size</li>
<li>Num_processes (world_size)</li>
</ul>
<p>trl 库的重要参数比较少。其中根据官方文档，generation_batch_size =
`per_device_train_batch_size * num_processes * steps_per_generation
Gradient_accumulation_steps 一般就是 steps_per_generation (对应 verl
中的 mini_batch_size / n_gpus /
ppo_micro_batch_size_per_gpu)，可以理解为 per_device_train_bs (对应 verl
中的 ppo_micro_batch_size_per_gpu) 是使用梯度累计后的 bs，乘 gpu
数，再乘梯度累计的 steps 就是总的 batch_size（对应 verl 中的
train_batch_size * rollout. N）。所以注意，总的 batch_size
(generation_batch_size) 是已经 rollout 采样后的 bs，除以 num_generations
才是针对 prompts 的 bs（verl 中的 train_batch_size）。
下面是_get_train_sampler 方法的注释，对每一个 prompt 重复
num_generations 是该方法实现的。</p>]]></description>
</item>
<item>
    <title>ppo</title>
    <link>https://blog.vllbc.top/ppo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ppo/</guid>
    <description><![CDATA[<h2 id="ppo-openrlhf-库">PPO (openrlhf 库)</h2>
<p>重点记录一下 experience 的采集过程。训练其实很简单。Actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
<item>
    <title>REINFORECE&#43;&#43;</title>
    <link>https://blog.vllbc.top/reinforece/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforece/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>ReMAX（REINFORCE argmax）</title>
    <link>https://blog.vllbc.top/remaxreinforce-argmax/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/remaxreinforce-argmax/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>RLOO</title>
    <link>https://blog.vllbc.top/rloo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rloo/</guid>
    <description><![CDATA[
]]></description>
</item>
</channel>
</rss>
