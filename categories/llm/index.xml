<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/llm/</link>
        <description>LLM - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>MHA</title>
    <link>https://blog.vllbc.top/mha/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mha/</guid>
    <description><![CDATA[<h2 id="self-attention">Self-attention</h2>
<p>首先介绍一下最主要的 self-attention，可以说是 self-attention
实现了上述的 token 之间交互的功能。</p>
<p>自注意力是模型的关键组成部分之一。注意和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。</p>]]></description>
</item>
<item>
    <title>dapo</title>
    <link>https://blog.vllbc.top/dapo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dapo/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>flash attention</title>
    <link>https://blog.vllbc.top/flash-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/flash-attention/</guid>
    <description><![CDATA[<p>Safe softmax 并没有 1-pass 算法，那么 Attention
会不会有呢？有！这就是 FlashAttention！</p>
<p>在使用 online attention 的情况下，从头开始计算 attention score
的过程如下： <span
class="math inline">\(\operatorname{NOTATIONS}\)</span></p>
<p><span class="math inline">\(Q[k,:]:\)</span> the <span
class="math inline">\(k\)</span> -th row vector of <span
class="math inline">\(Q\)</span> matrix. <span
class="math inline">\(\begin{aligned}O[k,:]:\mathrm{~the~}k\text{-th row
of output }O\mathrm{~matrix.}\\\mathbf{V}[i,i]:\mathrm{~the~}k\text{-th
row of output }O\mathrm{~matrix.}\end{aligned}\)</span> <span
class="math inline">\(V[i,:]{:\text{ the }i\text{-th row of }V\text{
matrix}}.\)</span> <span
class="math inline">\(\{\boldsymbol{o}_i\}{:}\sum_{j=1}^ia_jV[j,:]\)</span>,
a row vector storing partial aggregation result <span
class="math inline">\(A[k,:i]\times V[:i,:]\)</span> BODY</p>]]></description>
</item>
<item>
    <title>GQA</title>
    <link>https://blog.vllbc.top/gqa/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gqa/</guid>
    <description><![CDATA[<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>如上图所示，GQA 就是在 MHA 和 MQA 之间做了一个平衡。对 query heads
进行分组，分成几组就对应多少个 kv heads，然后每一组内的 query Heads
共享相同的 KV head。 GQA 可以在减少计算量和 KV Cache
同时确保模型效果不受到大的影响。</p>]]></description>
</item>
<item>
    <title>grpo</title>
    <link>https://blog.vllbc.top/grpo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/grpo/</guid>
    <description><![CDATA[<h1 id="grpo-trl-库">GRPO (trl 库)</h1>
<h2 id="重要参数">重要参数</h2>
<ul>
<li>Num_generations: <strong>Number of generations to sample. The
effective batch size (num_processes * per_device_batch_size *
gradient_accumulation_steps) must be evenly divisible by this
value.</strong></li>
<li>generation_batch_size: <strong>Batch size to use for generation. If
<code>None</code>, it defaults to the effective training batch size:
<code>per_device_train_batch_size * num_processes * steps_per_generation</code>.</strong></li>
<li>steps_per_generation: Number of optimization steps per generation.
If <code>None</code>, it defaults to gradient_accumulation_steps.</li>
<li>Num_iterations: Number of iterations per batch (denoted as μ in the
algorithm).</li>
<li>Per_device_train_batch_size</li>
<li>Num_processes (world_size)</li>
</ul>
<p>trl 库的重要参数比较少。其中根据官方文档，generation_batch_size =
`per_device_train_batch_size * num_processes * steps_per_generation
Gradient_accumulation_steps 一般就是 steps_per_generation (对应 verl
中的 mini_batch_size / n_gpus /
ppo_micro_batch_size_per_gpu)，可以理解为 per_device_train_bs (对应 verl
中的 ppo_micro_batch_size_per_gpu) 是使用梯度累计后的 bs，乘 gpu
数，再乘梯度累计的 steps 就是总的 batch_size（对应 verl 中的
train_batch_size * rollout. N）。所以注意，总的 batch_size
(generation_batch_size) 是已经 rollout 采样后的 bs，除以 num_generations
才是针对 prompts 的 bs（verl 中的 train_batch_size）。
下面是_get_train_sampler 方法的注释，对每一个 prompt 重复
num_generations 是该方法实现的。</p>]]></description>
</item>
<item>
    <title>online attention</title>
    <link>https://blog.vllbc.top/online-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/online-attention/</guid>
    <description><![CDATA[<h3 id="pass">3-pass</h3>
<p><span class="math inline">\(\mathsf{NO}\)</span> TATIONS</p>
<p><span
class="math inline">\(\{m_i\}{:}\max_{j=1}^i\left\{x_j\right\}\)</span>,
with initial value <span class="math inline">\(m_0=-\infty.\)</span>
<span class="math inline">\(\{d_i\}{:}\sum_{j=1}^ie^{x_j-m_N}\)</span>,
with initial value <span class="math inline">\(d_0=0,d_N\)</span> is the
denominator of safe softmax. <span class="math inline">\(\{a_i\}{:\text{
the final softmax value}}.\)</span></p>
<p>BODY <span class="math inline">\(\textbf{for }i\leftarrow 1,
N\textbf{ do}\)</span> <span
class="math display">\[m_i\leftarrow\max\left(m_{i-1},x_i\right)\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span class="math display">\[d_i\leftarrow
d_{i-1}+e^{x_i-m_N}\]</span> <span
class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span
class="math display">\[a_i\leftarrow\frac{e^{x_i-m_N}}{d_N}\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p>这是 3 step 计算 attention
的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于 sram
中没有足够的存储空间，因此需要多次访存。 ### Online attention <span
class="math display">\[\begin{aligned}
d_i^{\prime}&amp; =\sum_{j=1}^ie^{x_j-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1} e^{x_j-m_i}\right)+e^{x_i-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1}
e^{x_j-m_{i-1}}\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
&amp;= d_{i-1}&#39; e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}\]</span> 找到迭代式之后就可以从 3 step 降到 2 step <span
class="math display">\[\begin{aligned}&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;m_i&amp;&amp;\leftarrow&amp;\max\left(m_{i-1},x_i\right)\\&amp;&amp;&amp;d_i^{\prime}&amp;&amp;\leftarrow&amp;d_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\&amp;\mathbf{end}\\&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;a_i\leftarrow&amp;&amp;\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&amp;\mathbf{end}\end{aligned}\]</span>
好像 FLOPs
计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的 scale</p>]]></description>
</item>
<item>
    <title>paged attention</title>
    <link>https://blog.vllbc.top/paged-attention/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/paged-attention/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>ppo</title>
    <link>https://blog.vllbc.top/ppo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ppo/</guid>
    <description><![CDATA[<h2 id="ppo-openrlhf-库">PPO (openrlhf 库)</h2>
<p>重点记录一下 experience 的采集过程。训练其实很简单。Actor 在 RLHF
会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会
prefill，不会 decode。所以，我们将 actor 的推理特定称为
rollout，而其他模型的推理称为 inference。 </p>]]></description>
</item>
<item>
    <title>Reinforcing General Reasoning without Verifiers</title>
    <link>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforcing-general-reasoning-without-verifiers/</guid>
    <description><![CDATA[<h3
id="一论文的研究目标与意义"><strong>一、论文的研究目标与意义</strong></h3>
<h4 id="研究目标与待解决问题"><strong>研究目标与待解决问题</strong></h4>
<p>论文的核心研究目标是：<strong>将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。</strong></p>]]></description>
</item>
<item>
    <title>REINFORECE&#43;&#43;</title>
    <link>https://blog.vllbc.top/reinforece/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reinforece/</guid>
    <description><![CDATA[
]]></description>
</item>
</channel>
</rss>
