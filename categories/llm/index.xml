<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/llm/</link>
        <description>LLM - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>MCTS和PRM</title>
    <link>https://blog.vllbc.top/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[<h2 id="核心总结">核心总结</h2>
<ul>
<li><strong>PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果</strong>。例如，
<ul>
<li>单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。</li>
<li>单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。</li>
</ul></li>
<li><strong>PRM 和 MCTS
既可以应用于优化训练数据，也可以用来预测用</strong>
<ul>
<li>用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward
model训练数据。</li>
<li>用于推理：很简单，推理用MCTS的方式把 test-scaling
做上来，再结合PRM的方式从众多路径中挑选最佳答案。</li>
</ul></li>
<li><strong>PRM和MCTS的缺点</strong><br />
这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。</li>
<li>Process Reward Model(PRM) 在实际应用中有三大局限：
<ul>
<li>第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。</li>
<li>第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。</li>
<li>第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy
model，但反而更多时间去优化 reward model 去了。</li>
</ul></li>
<li>对MCTS的看法：
<ul>
<li>文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。</li>
<li>MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。
## 参考 https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs
Can Master Math Reasoning with Self-Evolved Deep Thinking</li>
</ul></li>
</ul>]]></description>
</item>
<item>
    <title>generate</title>
    <link>https://blog.vllbc.top/generate/</link>
    <pubDate>Sun, 09 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate/</guid>
    <description><![CDATA[<p>理论部分在这：<a
href="../../NLP/LLM/generate相关.md">generate相关</a> ##
generate参数</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        inputs: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        generation_config: Optional[GenerationConfig] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        logits_processor: Optional[LogitsProcessorList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        stopping_criteria: Optional[StoppingCriteriaList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        prefix_allowed_tokens_fn: Optional[Callable[[<span class="bu">int</span>, torch.Tensor], List[<span class="bu">int</span>]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        synced_gpus: Optional[<span class="bu">bool</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        assistant_model: Optional[<span class="st">&quot;PreTrainedModel&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        streamer: Optional[<span class="st">&quot;BaseStreamer&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        negative_prompt_ids: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        negative_prompt_attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Union[GenerateOutput, torch.LongTensor]:</span></code></pre></div>
<p>在代码中可以看到在函数入口显式的定义了很多参数。他们的具体含义如下</p>]]></description>
</item>
<item>
    <title>llama系列</title>
    <link>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</link>
    <pubDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/llama%E7%B3%BB%E5%88%97/</guid>
    <description><![CDATA[<h1 id="llama介绍">LLaMA介绍</h1>
<p>LLaMA 是目前为止，效果最好的开源 LLM 之一。</p>
<blockquote>
<p><strong>论文的核心思想：相比于GPT，更小的模型+更多的训练数据</strong>**也可以获得可比的效果</p>
</blockquote>
<p>基于更多 tokens
的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为
<code>LLaMA</code>，参数范围从 7B 到 65B 不等，与现有最佳 LLM
相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于
GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM
的使用和研究平民化，因为它可以在单个 GPU
上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如
Chinchilla 或 PaLM-540B）相竞争的能力。</p>]]></description>
</item>
<item>
    <title>frequency_penalty&amp;presence_penalty</title>
    <link>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</link>
    <pubDate>Thu, 05 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</guid>
    <description><![CDATA[<p>LLM解码时采用的自回归采样，其过程如下：</p>
<ol type="1">
<li>小模型使用前缀作为输入，将输出结果处理+归一化成<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83&amp;zhida_source=entity">概率分布</a>后，采样生成下一个token。</li>
<li>将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。</li>
</ol>
<p>将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=top-k&amp;zhida_source=entity">top-k</a>和top-n等
# 贪心搜索
直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复
# 随机采样
按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
# beam search 维护一个大小为 k
的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k
个单词，然后保留总概率最高的 k
个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。
# top-k </p>]]></description>
</item>
<item>
    <title>rwkv</title>
    <link>https://blog.vllbc.top/rwkv/</link>
    <pubDate>Wed, 04 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rwkv/</guid>
    <description><![CDATA[<h1 id="线性transformer">线性Transformer</h1>
<p><span class="math display">\[V_i&#39;=\frac{\sum_{j=1}^N
sim(Q_i,K_j)V_j}{\sum_{j=1}^N sim(Q_i,K_j)}\]</span> 注意下标i。
其中</p>
<p><span
class="math display">\[sim(Q_{i},K_{j})=\phi(Q_{i},K_{j})\]</span></p>
<p>此时有：</p>
<p><span
class="math display">\[V_{i}^{\prime}=\frac{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}V_{j}}{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}}\]</span></p>
<p>注意可以将<span
class="math inline">\(\phi(Q_{i})\)</span>提出来。</p>
<p>原始Transformer的计算复杂度随序列长N呈二次方增长，这是因为attention的计算包含两层for循环，外层是对于每一个Query，我们需要计算它对应token的新表征；内层for循环是为了计算每一个Query对应的新表征，需要让该Query与每一个Key进行计算。
所以外层是 for q in Queries，内层是 for k in
Keys。Queries数量和Keys数量都是N，所以复杂度是 O(N^2) 。而Linear
Transformer，它只有外层for q in
Queries这个循环了。因为求和项的计算与i无关，所以所有的 Qi
可以共享求和项的值。换言之，求和项的值可以只计算一次，然后存在内存中供所有
Qi 去使用。所以Linear Transformer的计算复杂度是O(N) 。</p>]]></description>
</item>
<item>
    <title>rope</title>
    <link>https://blog.vllbc.top/rope/</link>
    <pubDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rope/</guid>
    <description><![CDATA[<h1 id="证明">证明</h1>
<p>核心思想就是找到一个转换，可以通过点积操作将位置信息注入，即： <span
class="math display">\[&lt;f_q\left(x_m,m\right),f_k\left(x_n,n\right)&gt;=g\left(x_m,x_n,m-n\right)\]</span>
而通过复数的一些性质，找到了满足上述操作的转换：</p>
<p><span class="math display">\[\begin{aligned}
&amp;f_{q}\left(\boldsymbol{x}_{m},m\right)=\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)e^{im\theta}
\\
&amp;f_{k}\left(\boldsymbol{x}_{n},n\right)=\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)e^{in\theta}
\\
&amp;g\left(\boldsymbol{x}_{m},\boldsymbol{x}_{n},m-n\right)=\mathrm{Re}\left[\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)^{*}e^{i(m-n)\theta}\right]
\end{aligned}\]</span> 可以发现g函数中存在相对位置信息。 欧拉公式：<span
class="math inline">\(e^{ix}=\cos x+i\sin x\)</span></p>]]></description>
</item>
<item>
    <title>ICL</title>
    <link>https://blog.vllbc.top/icl/</link>
    <pubDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/icl/</guid>
    <description><![CDATA[<p>ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot
learning，允许输入数条示例和一则任务说明； - One-shot
learning，只允许输入一条示例和一则任务说明； - Zero-shot
learning，不允许输入任何示例，只允许输入一则任务说明。 </p>]]></description>
</item>
</channel>
</rss>
