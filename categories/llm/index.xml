<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - åˆ†ç±» - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/llm/</link>
        <description>LLM - åˆ†ç±» - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 08 Aug 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>qwen</title>
    <link>https://blog.vllbc.top/qwen/</link>
    <pubDate>Fri, 08 Aug 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/qwen/</guid>
    <description><![CDATA[<pre class="python3"><code>------------------------------------------------------------------------------------------------------
# Qwen-1
+ Embedding and output projection. (Untied embedding for input embedding and output projection)
+ ROPE
+ QKV bias required
+ Pre-Norm &amp; RMSNorm
+ SwiGLU

------------------------------------------------------------------------------------------------------
# Qwen-2
- Multi-Head Attention

+ MoE
+ Grouped Query Attention
+ Dual Chunk Attention
+ YARN
+ Expert Granularity
+ Expert Routing
+ Expert Initialization
+ Shared Experts

------------------------------------------------------------------------------------------------------
# Qwen-2.5
+ More control tokens. 3 -&gt; 22

------------------------------------------------------------------------------------------------------
# Qwen-3
- QKV bias
- Shared experts

+ QK-Norm
------------------------------------------------------------------------------------------------------</code></pre>
<h2 id="å‚è€ƒ">å‚è€ƒ</h2>
<p><a href="https://zhuanlan.zhihu.com/p/1933558376689804480">Qwen
å„ç‰ˆæœ¬ä¸»è¦ç»“æ„å˜åŒ–</a></p>]]></description>
</item>
<item>
    <title>ä¿¡ç”¨åˆ†é…</title>
    <link>https://blog.vllbc.top/%E4%BF%A1%E7%94%A8%E5%88%86%E9%85%8D/</link>
    <pubDate>Tue, 05 Aug 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E4%BF%A1%E7%94%A8%E5%88%86%E9%85%8D/</guid>
    <description><![CDATA[<p>æœ€è¿‘æ¶Œç°äº†å¾ˆå¤šå…³äºä¿¡ç”¨åˆ†é…çš„è®ºæ–‡ï¼Œå› æ­¤æ•´ç†ä¸€ä¸‹</p>
<h2 id="first-return-entropy-eliciting-explore"><a
href="../../reading/Reasoning/First%20Return,%20Entropy-Eliciting%20Explore.md">First
Return, Entropy-Eliciting Explore</a></h2>
<h2
id="good-learners-think-their-thinkinggenerative-prm-makes-large-reasoning-model-more-efficient-math-learne"><a
href="../../reading/Reasoning/Good%20Learners%20Think%20Their%20Thinkingï¼šGenerative%20PRM%20Makes%20Large%20Reasoning%20Model%20More%20Efficient%20Math%20Learne.md">Good
Learners Think Their Thinkingï¼šGenerative PRM Makes Large Reasoning
Model More Efficient Math Learne</a></h2>
<h2 id="group-sequence-policy-optimization"><a
href="../../reading/LLM-RL/Group%20Sequence%20Policy%20Optimization.md">Group
Sequence Policy Optimization</a></h2>
<h2 id="process-reinforcement-through-implicit-rewards"><a
href="../../reading/Reasoning/PROCESS%20REINFORCEMENT%20THROUGH%20IMPLICIT%20REWARDS.md">PROCESS
REINFORCEMENT THROUGH IMPLICIT REWARDS</a></h2>
<h2
id="rlvmrreinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents"><a
href="../../reading/Planning/RLVMRï¼šReinforcement%20Learning%20with%20Verifiable%20Meta-Reasoning%20Rewards%20for%20Robust%20Long-Horizon%20Agents.md">RLVMRï¼šReinforcement
Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon
Agents</a></h2>
<h2
id="vapoefficient-and-reliable-reinforcement-learning-for-advanced-reasoning-tasks"><a
href="../../reading/LLM-RL/VAPOï¼šEfficient%20and%20Reliable%20Reinforcement%20Learning%20for%20Advanced%20Reasoning%20Tasks.md">VAPOï¼šEfficient
and Reliable Reinforcement Learning for Advanced Reasoning
Tasks</a></h2>
<h2 id="group-in-group-policy-optimization-for-llm-agent-training"><a
href="../../reading/Planning/Group-in-Group%20Policy%20Optimization%20for%20LLM%20Agent%20Training.md">Group-in-Group
Policy Optimization for LLM Agent Training</a></h2>
<h2
id="capotowards-enhancing-llm-reasoning-through-verifiable-generative-credit-assignment"><a
href="../../reading/Reasoning/CAPOï¼šTowards%20Enhancing%20LLM%20Reasoning%20through%20Verifiable%20Generative%20Credit%20Assignment.md">CAPOï¼šTowards
Enhancing LLM Reasoning through Verifiable Generative Credit
Assignment</a></h2>
<h2
id="beyond-policy-optimizationa-data-curation-flywheel-for-sparse-reward-long-horizon-planning"><a
href="../../reading/Planning/Beyond%20Policy%20Optimizationï¼šA%20Data%20Curation%20Flywheel%20for%20Sparse-Reward%20Long-Horizon%20Planning.md">Beyond
Policy Optimizationï¼šA Data Curation Flywheel for Sparse-Reward
Long-Horizon Planning</a></h2>
<h2
id="gtpo-and-grpo-stoken-and-sequence-level-reward-shaping-with-policy-entropy"><a
href="../../reading/Reasoning/GTPO%20and%20GRPO-Sï¼šToken%20and%20Sequence-Level%20Reward%20Shaping%20with%20Policy%20Entropy.md">GTPO
and GRPO-Sï¼šToken and Sequence-Level Reward Shaping with Policy
Entropy</a></h2>
<h2
id="gtpotrajectory-based-policy-optimization-in-large-language-models"><a
href="../../reading/Reasoning/GTPOï¼šTrajectory-Based%20Policy%20Optimization%20in%20Large%20Language%20Models.md">GTPOï¼šTrajectory-Based
Policy Optimization in Large Language Models</a></h2>]]></description>
</item>
<item>
    <title>K2</title>
    <link>https://blog.vllbc.top/k2/</link>
    <pubDate>Wed, 30 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/k2/</guid>
    <description><![CDATA[<h2 id="é¢„è®­ç»ƒ">é¢„è®­ç»ƒ</h2>
<h3 id="muon-clip">Muon-clip</h3>
<p>è¯¦è§<a href="../../Deep%20Learning/ä¼˜åŒ–å™¨/Muon.md">Muon</a></p>
<p>é€šè¿‡è£å‰ªæƒé‡è§£å†³Max-Logité—®é¢˜ï¼Œå®ç°ç¨³å®šè®­ç»ƒã€‚</p>
<h3 id="æ•°æ®å¢å¼º">æ•°æ®å¢å¼º</h3>
<p>é€šè¿‡æ”¹å†™å¥å­æ¥æé«˜tokenæ•ˆç‡ï¼Œé¿å…é‡å¤ä½¿ç”¨tokené€ æˆçš„è¿‡æ‹Ÿåˆï¼Œæ”¹å†™æ–¹æ³•å¦‚ä¸‹ï¼š</p>]]></description>
</item>
<item>
    <title>ALiBi</title>
    <link>https://blog.vllbc.top/alibi/</link>
    <pubDate>Tue, 29 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/alibi/</guid>
    <description><![CDATA[<p>åœ¨softmaxçš„ç»“æœåæ·»åŠ ä¸€ä¸ªé™æ€çš„ä¸å¯å­¦ä¹ çš„åç½®é¡¹ã€‚  q1å’Œk1ä¹‹é—´çš„è·ç¦»æ˜¯0ï¼Œæ‰€ä»¥å¯¹åº”ä½ç½®å°±æ˜¯0<br />
Â q2å’Œk1ä¹‹é—´çš„è·ç¦»æ˜¯ã€Œç›¸å¯¹ä½ç½®åç§»ä¸ºâ€œ<strong>kçš„ç´¢å¼•</strong>â€1ã€ -
ã€Œqçš„ç´¢å¼•2ã€ï¼Œå¾—åˆ°1-2 = -1ï¼Œå°±å¯¹åº”åˆ°äº†ä¸­é—´çŸ©é˜µçš„å–å€¼ä¸º-1äº†<br />
ä»¥æ­¤ç±»æ¨ï¼Œç›¸å¯¹è·ç¦»çŸ©é˜µçš„ä¸­é—´å¯¹è§’çº¿ä¸Šéƒ½æ˜¯0ï¼Œç„¶åå·¦ä¸‹è§’çš„å–å€¼éƒ½æ˜¯å¯¹åº”çš„ã€Œkçš„ç´¢å¼•ã€-ã€Œqçš„ç´¢å¼•ã€äº†</p>]]></description>
</item>
<item>
    <title>Gemma</title>
    <link>https://blog.vllbc.top/gemma/</link>
    <pubDate>Tue, 29 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gemma/</guid>
    <description><![CDATA[<h2 id="gemma-3">Gemma 3</h2>
<h3 id="qk-norm">QK-Norm</h3>
<p>ç®€å•æ¥è¯´å°±æ˜¯åœ¨Qå’ŒKçŸ©é˜µä¸Šè¿›è¡ŒRMSNormï¼Œå³ï¼š</p>
<p><span class="math display">\[
\begin{aligned}
O &amp;= softmax(\bar{Q}\bar{K}^T)V \\
\bar{Q} &amp;=RMSNorm(Q) \\
\bar{K} &amp;=RMSNorm(V)
\end{aligned}\
\]</span></p>
<p>ä½†è¿™ç§æ–¹æ³•çš„é—®é¢˜æ˜¯ä¸é€‚ç”¨MLAçš„æ¨ç†é˜¶æ®µï¼Œå› ä¸ºæ¨ç†é˜¶æ®µçš„MLAå°†Wkå¸å–åˆ°äº†Qä¸­ï¼Œå…·ä½“è§<a
href="../Attention/MLA.md">MLA</a></p>]]></description>
</item>
<item>
    <title>device_mesh</title>
    <link>https://blog.vllbc.top/device_mesh/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/device_mesh/</guid>
    <description><![CDATA[<h2 id="verlä¸­çš„device_mesh">verlä¸­çš„device_mesh</h2>
<p>verlä¸­æœ‰3ä¸ªdevice_meshï¼Œåˆ†åˆ«æ˜¯ï¼š - è®­ç»ƒç”¨çš„FSDP meshï¼ˆé€šå¸¸æ˜¯ä¸€ç»´ï¼‰ -
æ¨ç†ç”¨çš„rollout meshï¼ˆåŒ…å«tpç»´åº¦ï¼‰ - Ulyssesåºåˆ—å¹¶è¡Œçš„meshï¼ˆdpÃ—spï¼‰</p>]]></description>
</item>
<item>
    <title>data_parallel</title>
    <link>https://blog.vllbc.top/data_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data_parallel/</guid>
    <description><![CDATA[<p>å¦‚æœæƒ³å°†æ¨¡å‹è®­ç»ƒæ‰©å±•åˆ°å¤§çš„æ‰¹æ¬¡ï¼Œåˆ™å¾ˆå¿«å°±ä¼šè¾¾åˆ°åœ¨å•ä¸ª GPU
ä¸Šå¯ä»¥åšçš„æé™ã€‚å…·ä½“æ¥è¯´ï¼Œä¼šå‘ç”Ÿ
<code>RuntimeError: CUDA out of memory</code>ã€‚ <a
href="æ¢¯åº¦ç´¯è®¡.md">æ¢¯åº¦ç´¯è®¡</a>ã€<a
href="Activation%20checkpointing.md">Activation checkpointing</a> å’Œ <a
href="CPU%20offloading.md">CPU offloading</a>
éƒ½å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘æ˜¾å­˜çš„å ç”¨ï¼Œä¸ºäº†_æœ‰æ•ˆåœ°_æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹å¤§å°å’Œä¸æ–­å¢é•¿çš„æ•°æ®é›†ï¼ŒåŒæ—¶ä»ç„¶åœ¨åˆç†çš„æ—¶é—´å†…è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†è®¡ç®—<strong>åˆ†å¸ƒåœ¨</strong>ä¸€ç»„æœºå™¨ä¸Šã€‚</p>]]></description>
</item>
<item>
    <title>pipeline parallelism</title>
    <link>https://blog.vllbc.top/pipeline-parallelism/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/pipeline-parallelism/</guid>
    <description><![CDATA[<h2 id="å‚è€ƒ">å‚è€ƒ</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">ğŸ’¥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>tensor_parallel</title>
    <link>https://blog.vllbc.top/tensor_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/tensor_parallel/</guid>
    <description><![CDATA[<h2 id="å‚è€ƒ">å‚è€ƒ</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">ğŸ’¥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>zero</title>
    <link>https://blog.vllbc.top/zero/</link>
    <pubDate>Tue, 22 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/zero/</guid>
    <description><![CDATA[<p>åˆ†ä¸ºzero1ã€zero2ã€zero3ï¼Œè™½ç„¶zero3å¯¹æ¨¡å‹è¿›è¡Œäº†åˆ†å‰²ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šè¿˜æ˜¯å±äºæ•°æ®å¹¶è¡Œï¼Œå› ä¸ºåœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éœ€è¦all-gatheræ¨¡å‹å‚æ•°ï¼Œéœ€è¦å®Œæ•´çš„æ¨¡å‹æƒé‡ã€‚</p>]]></description>
</item>
</channel>
</rss>
