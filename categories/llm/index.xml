<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 分类 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/categories/llm/</link>
        <description>LLM - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>MCTS和PRM</title>
    <link>http://localhost:1313/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[核心总结 PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果。例如， 单独使用PRM：我]]></description>
</item>
<item>
    <title>generate</title>
    <link>http://localhost:1313/generate/</link>
    <pubDate>Sun, 09 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/generate/</guid>
    <description><![CDATA[理论部分在这：generate相关 ## generate参数 def generate( self, inputs: Optional[torch.Tensor] = None, generation_config: Optional[GenerationConfig] = None, logits_processor: Optional[LogitsProcessorList] = None, stopping_criteria: Optional[StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[&quot;PreTrainedModel&quot;] = None, streamer: Optional[&quot;BaseStreamer&quot;] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask:]]></description>
</item>
<item>
    <title>llama系列</title>
    <link>http://localhost:1313/llama%E7%B3%BB%E5%88%97/</link>
    <pubDate>Thu, 26 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/llama%E7%B3%BB%E5%88%97/</guid>
    <description><![CDATA[LLaMA介绍 LLaMA 是目前为止，效果最好的开源 LLM 之一。 论文的核心思想：相比于GPT，更小的模型+更多的训练数据**也可以获得可比的效果 基于更多 tokens]]></description>
</item>
<item>
    <title>frequency_penalty&amp;presence_penalty</title>
    <link>http://localhost:1313/generate%E7%9B%B8%E5%85%B3/</link>
    <pubDate>Thu, 05 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/generate%E7%9B%B8%E5%85%B3/</guid>
    <description><![CDATA[LLM解码时采用的自回归采样，其过程如下： 小模型使用前缀作为输入，将输出结果处理+归一化成概率分布后，采样生成下一个token。 将生成的to]]></description>
</item>
<item>
    <title>rwkv</title>
    <link>http://localhost:1313/rwkv/</link>
    <pubDate>Wed, 04 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/rwkv/</guid>
    <description><![CDATA[线性Transformer \[V_i&#39;=\frac{\sum_{j=1}^N sim(Q_i,K_j)V_j}{\sum_{j=1}^N sim(Q_i,K_j)}\] 注意下标i。 其中 \[sim(Q_{i},K_{j})=\phi(Q_{i},K_{j})\] 此时有： \[V_{i}^{\prime}=\frac{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}V_{j}}{\phi(Q_{i})\sum_{j=1}^{i}\phi(K_{j})^{T}}\] 注意可以将\(\phi(Q_{i})\)提出来。 原始Transformer的计算]]></description>
</item>
<item>
    <title>rope</title>
    <link>http://localhost:1313/rope/</link>
    <pubDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/rope/</guid>
    <description><![CDATA[证明 核心思想就是找到一个转换，可以通过点积操作将位置信息注入，即： \[&lt;f_q\left(x_m,m\right),f_k\left(x_n,n\right)&gt;=g\left(x_m,x_n,m-n\right)\] 而通过复数的一些性质，找到了满足上述操作的转换： \[\begin{aligned} &amp;f_{q}\left(\boldsymbol{x}_{m},m\right)=\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)e^{im\theta} \\ &amp;f_{k}\left(\boldsymbol{x}_{n},n\right)=\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)e^{in\theta} \\ &amp;g\left(\boldsymbol{x}_{m},\boldsymbol{x}_{n},m-n\right)=\mathrm{Re}\left[\left(\boldsymbol{W}_{q}\boldsymbol{x}_{m}\right)\left(\boldsymbol{W}_{k}\boldsymbol{x}_{n}\right)^{*}e^{i(m-n)\theta}\right] \end{aligned}\] 可以发现g]]></description>
</item>
<item>
    <title>ICL</title>
    <link>http://localhost:1313/icl/</link>
    <pubDate>Thu, 14 Mar 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/icl/</guid>
    <description><![CDATA[ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot learning，允许输入数条示例和一则任务说明； - One-shot learnin]]></description>
</item>
</channel>
</rss>
