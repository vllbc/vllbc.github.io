<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/llm/</link>
        <description>LLM - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 27 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>vapo</title>
    <link>https://blog.vllbc.top/vapo/</link>
    <pubDate>Sun, 27 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/vapo/</guid>
    <description><![CDATA[<p>这篇由字节跳动Seed团队在2025年4月发表的论文，直面了当前大模型领域中一个核心且棘手的难题：如何通过强化学习（Reinforcement
Learning,
RL）高效、稳定地提升模型在复杂推理任务上的能力。当前，以<strong>链式思考（Chain-of-Thought,
CoT）</strong>为代表的推理技术是实现通用人工智能（AGI）的关键路径，而如何让模型学会更长、更可靠的推理链，是业界公认的瓶颈。</p>]]></description>
</item>
<item>
    <title>device_mesh</title>
    <link>https://blog.vllbc.top/device_mesh/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/device_mesh/</guid>
    <description><![CDATA[<h2 id="verl中的device_mesh">verl中的device_mesh</h2>
<p>verl中有3个device_mesh，分别是： - 训练用的FSDP mesh（通常是一维） -
推理用的rollout mesh（包含tp维度） - Ulysses序列并行的mesh（dp×sp）</p>]]></description>
</item>
<item>
    <title>data_parallel</title>
    <link>https://blog.vllbc.top/data_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data_parallel/</guid>
    <description><![CDATA[<p>如果想将模型训练扩展到大的批次，则很快就会达到在单个 GPU
上可以做的极限。具体来说，会发生
<code>RuntimeError: CUDA out of memory</code>。 <a
href="梯度累计.md">梯度累计</a>、<a
href="Activation%20checkpointing.md">Activation checkpointing</a> 和 <a
href="CPU%20offloading.md">CPU offloading</a>
都可以一定程度上减少显存的占用，为了_有效地_扩展到更大的模型大小和不断增长的数据集，同时仍然在合理的时间内训练模型，我们需要将计算<strong>分布在</strong>一组机器上。</p>]]></description>
</item>
<item>
    <title>pipeline parallelism</title>
    <link>https://blog.vllbc.top/pipeline-parallelism/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/pipeline-parallelism/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">💥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>tensor_parallel</title>
    <link>https://blog.vllbc.top/tensor_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/tensor_parallel/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">💥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>zero</title>
    <link>https://blog.vllbc.top/zero/</link>
    <pubDate>Tue, 22 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/zero/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>flops分析</title>
    <link>https://blog.vllbc.top/flops%E5%88%86%E6%9E%90/</link>
    <pubDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/flops%E5%88%86%E6%9E%90/</guid>
    <description><![CDATA[<blockquote>
<p>FLOPs, floating point operations,
表示浮点数运算次数，衡量了计算量的大小。 如何计算矩阵乘法的 FLOPs 呢？
对于 <span class="math inline">\(A\in R^{1\times n},B\in
R^{n\times1}\)</span> ,计算 <span class="math inline">\(AB\)</span>
需要进行 <span class="math inline">\(n\)</span> 次乘法运算和 <span
class="math inline">\(n\)</span> 次加法运算，共计 <span
class="math inline">\(2n\)</span> 次浮点数运算，需要 <span
class="math inline">\(2n\)</span> 的 FLOPs。对于 <span
class="math inline">\(A\in R^{m\times n},B\in R^{n\times p}\)</span>
,计算 <span class="math inline">\(AB\)</span> 需要的浮点数运算次数为
<span class="math inline">\(2mnp\)</span> 。</p>]]></description>
</item>
<item>
    <title>ring-all-reduce</title>
    <link>https://blog.vllbc.top/ring-all-reduce/</link>
    <pubDate>Thu, 17 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ring-all-reduce/</guid>
    <description><![CDATA[<p>All-reduced=all-gather+reduce-scatter</p>
<ul>
<li><strong>All-Gather</strong> ：将分布式数据汇总到所有节点，适用于需要<strong>全局数据</strong>同步的场景。</li>
<li><strong>Reduce-Scatter</strong>：将分布式数据进行<strong>规约</strong>并<strong>分散</strong>到所有节点，适用于需要局部结果分发的场景。</li>
<li><strong>All-Reduce</strong> ： Reduce-Scatter 和 All-Gather
的组合。</li>
</ul>
<h2 id="all-gather">All-gather</h2>
<p><strong>核心功能</strong>：将每个节点的部分数据汇总到所有节点，最终所有节点拥有<strong>完整数据</strong>副本。<br />
<strong>适用场景</strong>：模型并行中的参数同步、全局统计信息聚合。</p>]]></description>
</item>
<item>
    <title>MHA</title>
    <link>https://blog.vllbc.top/mha/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mha/</guid>
    <description><![CDATA[<h2 id="self-attention">Self-attention</h2>
<p>首先介绍一下最主要的 self-attention，可以说是 self-attention
实现了上述的 token 之间交互的功能。</p>
<p>自注意力是模型的关键组成部分之一。注意和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。</p>]]></description>
</item>
<item>
    <title>dapo</title>
    <link>https://blog.vllbc.top/dapo/</link>
    <pubDate>Tue, 15 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dapo/</guid>
    <description><![CDATA[<p>DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling
Policy
Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2
个是主要亮点，是命名的来源）</p>]]></description>
</item>
</channel>
</rss>
