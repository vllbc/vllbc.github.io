<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>训练trick - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E8%AE%AD%E7%BB%83trick/</link>
        <description>训练trick - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 14 Jan 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E8%AE%AD%E7%BB%83trick/" rel="self" type="application/rss+xml" /><item>
    <title>温度超参数</title>
    <link>https://blog.vllbc.top/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/</link>
    <pubDate>Sun, 14 Jan 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/</guid>
    <description><![CDATA[<p>温度超参数t，一般为softmax结果除以该参数，或者在对比学习中，相似度除以参数t。
如图：  上图为无监督simcse中的损失函数。</p>
<p><code>t</code>越大，结果越平滑，<code>t</code>越小，得到的概率分布更“尖锐”。
当t趋于0时：  此时只关注最困难的负样本（smax）。 当t趋于∞时：  此时对比损失对所有负样本的权重都相同。</p>]]></description>
</item>
<item>
    <title>early-stopping</title>
    <link>https://blog.vllbc.top/early-stopping/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/early-stopping/</guid>
    <description><![CDATA[<h2 id="介绍">介绍</h2>
<p>早停止（Early
Stopping）是 <strong>当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段</strong>。</p>
<p>如何判断已经收敛？主要看以下几点： -
验证集上的Loss在模型多次迭代后，没有下降 - 验证集上的Loss开始上升。
这时就可以认为模型没有必要训练了，可以停止了，因为训练下去可能就会发生过拟合，所以早停法是一种防止模型过拟合的方法。</p>]]></description>
</item>
<item>
    <title>warmup</title>
    <link>https://blog.vllbc.top/warmup/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/warmup/</guid>
    <description><![CDATA[<p>在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning
rate，然后随着训练的批次增加，逐渐增大学习率，直到达到原本想要设置的学习率。</p>]]></description>
</item>
<item>
    <title>标签平滑</title>
    <link>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</guid>
    <description><![CDATA[<p>神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为onehot本身就是一个稀疏的向量，如果所有无关类别都为0的话，就可能会疏忽某些类别之间的联系。
具体的缺点有： -
真是标签与其它标签之间的关系被忽略了，很多有用的知识学不到了。 -
倾向于让模型更加武断，导致泛化性能差 -
面对有噪声的数据更容易收到影响。</p>]]></description>
</item>
<item>
    <title>调参技巧</title>
    <link>https://blog.vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</guid>
    <description><![CDATA[<ul>
<li>基本原则：快速试错。</li>
<li>小步试错，快速迭代</li>
<li>可以试试无脑的配置</li>
<li>实时打印一些结果</li>
<li>自动调参：网格搜索、random search、贝叶斯优化、</li>
<li>参数初始化</li>
<li>学习率warmup，慢慢增加，然后学习率衰减。</li>
</ul>
<h1 id="batch_size和lr">batch_size和lr</h1>
<p><strong>大的batchsize收敛到<a
href="https://zhida.zhihu.com/search?q=sharp+minimum&amp;zhida_source=entity&amp;is_preview=1">sharp
minimum</a>，而小的batchsize收敛到<a
href="https://zhida.zhihu.com/search?q=flat+minimum&amp;zhida_source=entity&amp;is_preview=1">flat
minimum</a>，后者具有更好的泛化能力。</strong>两者的区别就在于变化的趋势，一个快一个慢，如下图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp
minimum。</p>]]></description>
</item>
<item>
    <title>对抗训练</title>
    <link>https://blog.vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[<h1 id="min-max公式">Min-Max公式</h1>
<p>$$ <em>{} </em>{(x,y) }U[<em>{r</em>{adv}}L(,x+r_{adv},y)]</p>
<p>$$</p>
<ol type="1">
<li>内部max是为了找到worst-case的扰动，也就是攻击，其中， <span
class="math inline">\(L\)</span>为损失函数， <span
class="math inline">\(\mathbb{S}\)</span> 为扰动的范围空间。</li>
<li>外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 <span
class="math inline">\(\mathbb{D}\)</span> 是输入样本的分布。
简单理解就是<strong>在输入上进行梯度上升(增大loss)，在参数上进行梯度下降(减小loss)</strong></li>
</ol>
<h1 id="加入扰动后的损失函数">加入扰动后的损失函数</h1>
<p>$$ <em>{} -P(y |x+r</em>{adv};)</p>]]></description>
</item>
<item>
    <title>数据不平衡</title>
    <link>https://blog.vllbc.top/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/</guid>
    <description><![CDATA[<h2 id="数据不均衡">数据不均衡</h2>
<p>所谓的不平衡指的是不同类别的样本量差异非常大，或者少数样本代表了业务的关键数据（少量样本更重要），需要对少量样本的模式有很好的学习。样本类别分布不平衡主要出现在分类相关的建模问题上。样本类别分布不平衡从数据规模上可以分为大数据分布不平衡和小数据分布不平衡两种。</p>]]></description>
</item>
</channel>
</rss>
