<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>World-Model - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/world-model/</link>
        <description>World-Model - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 30 Jun 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/world-model/" rel="self" type="application/rss+xml" /><item>
    <title>world_model</title>
    <link>https://blog.vllbc.top/world_model/</link>
    <pubDate>Mon, 30 Jun 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/world_model/</guid>
    <description><![CDATA[<p>我理解的agent中的world
model即可以预测采取某个action之后state的变化，这样做的好处是可以降低试错带来的时间成本或者是其它潜在的成本、风险。</p>]]></description>
</item>
<item>
    <title>RLVR-World</title>
    <link>https://blog.vllbc.top/rlvr-world/</link>
    <pubDate>Mon, 16 Jun 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rlvr-world/</guid>
    <description><![CDATA[<p>这篇论文的核心思想，一言以蔽之，就是通过一种名为
<strong>RLVR（Reinforcement Learning with Verifiable
Rewards，可验证奖励的强化学习）</strong> 的技术，对 <strong>世界模型
(World Models)</strong>
进行“二次打磨”或“精加工”，从而使其更精准地服务于特定任务。这解决了传统训练方法（如最大似然估计
MLE）与最终应用目标之间存在的“貌合神离”的问题。</p>]]></description>
</item>
</channel>
</rss>
