<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>面经 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E9%9D%A2%E7%BB%8F/</link>
        <description>面经 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 10 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E9%9D%A2%E7%BB%8F/" rel="self" type="application/rss+xml" /><item>
    <title>北京百分点面经</title>
    <link>https://blog.vllbc.top/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/</link>
    <pubDate>Fri, 10 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/</guid>
    <description><![CDATA[<p>前几天试着投了简历，没想到有两家约了面试，一个是得物一个是北京百分点，得物面试没有怎么准备，太仓促了，二面挂了，百分点拿到了offer，但决定考研了就没去，记录一下面试的问题。岗位是nlp算法岗。</p>]]></description>
</item>
<item>
    <title>KMP</title>
    <link>https://blog.vllbc.top/kmp/</link>
    <pubDate>Wed, 08 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/kmp/</guid>
    <description><![CDATA[<p>KMP是字符串匹配问题的算法。“字符串A是否为字符串B的子串?如果是的话出现在B的哪些位置?”该问题就是字符串匹配问题，字符串A称为<strong>模式串</strong>，字符串B称为<strong>主串</strong>。</p>]]></description>
</item>
<item>
    <title>过拟合的解决方法</title>
    <link>https://blog.vllbc.top/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</guid>
    <description><![CDATA[<ol type="1">
<li>数据增强，即增加样本，也可以半监督如UDA。</li>
<li>正则化（Dropout等）</li>
<li>Batch
norm。本质是加快训练，让训练更稳定，但也可以缓解过拟合。配合relu也会缓解dead
relu问题。</li>
<li>early-stop，在过拟合之前停下来。</li>
<li>降低模型复杂度，与第2点类似。</li>
<li>学习率衰减，按照固定的epoch后衰减学习率。</li>
<li>特征选择，选择主要的特征进行训练，本质也是降低模型复杂度。</li>
</ol>]]></description>
</item>
<item>
    <title>OOV问题</title>
    <link>https://blog.vllbc.top/oov%E9%97%AE%E9%A2%98/</link>
    <pubDate>Mon, 13 Feb 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/oov%E9%97%AE%E9%A2%98/</guid>
    <description><![CDATA[<p>面试中经常被问到的一个问题就是out of
vocabulary，可能是因为当前数据集中出现了提前准备好的单词表中没有的word，也可能是因为test中出现了train中没有的word。
## 解决办法： 1. 直接Ignore 2. 将token分配为[unk] 3. 增大词表 4.
检查拼写 5. BPE算法或word piece
面试时可以展开说一下具体的算法过程，不再赘述。</p>]]></description>
</item>
<item>
    <title>kd树</title>
    <link>https://blog.vllbc.top/kd%E6%A0%91/</link>
    <pubDate>Mon, 21 Mar 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/kd%E6%A0%91/</guid>
    <description><![CDATA[<h1 id="kd树">kd树</h1>
<p>knn算法就是用kd树实现的</p>
<h2 id="二分查找">二分查找</h2>
<p>很简单 就不说了</p>
<h2 id="bst">BST</h2>
<p>很简单 就不说了</p>
<h2 id="多维数组">多维数组</h2>
<p>假设数组B为<span class="math inline">\([[6, 2], [6, 3], [3, 5], [5,
0], [1, 2], [4, 9], [8,
1]]\)</span>，有一个元素x，我们要找到数组B中距离x最近的元素，应该如何实现呢？比较直接的想法是用数组B中的每一个元素与x求距离，距离最小的那个元素就是我们要找的元素。假设x
= [1, 1]，那么用数组B中的所有元素与x求距离得到[5.0, 5.4, 4.5, 4.1, 1.0,
8.5, 7.0]，其中距离最小的是1，对应的元素是数组B中的[1, 2]，所以[1,
2]就是我们的查找结果。</p>]]></description>
</item>
<item>
    <title>前缀树</title>
    <link>https://blog.vllbc.top/%E5%89%8D%E7%BC%80%E6%A0%91/</link>
    <pubDate>Wed, 19 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%89%8D%E7%BC%80%E6%A0%91/</guid>
    <description><![CDATA[<h2 id="什么是前缀树">什么是前缀树？</h2>
<p>前缀树是<strong>N叉树的一种特殊形式</strong>。通常来说，一个前缀树是用来存储字符串的。前缀树的每一个节点代表一个字符串（前缀）。每一个节点会有多个子节点，通往不同子节点的路径上有着不同的字符。子节点代表的字符串是由节点本身的原始字符串，以及通往该子节点路径上所有的字符组成的。

在上图示例中，我们在节点中标记的值是该节点对应表示的字符串。例如，我们从根节点开始，选择第二条路径
‘b’，然后选择它的第一个子节点 ‘a’，接下来继续选择子节点
‘d’，我们最终会到达叶节点
“bad”。节点的值是由从根节点开始，与其经过的路径中的字符按顺序形成的。</p>]]></description>
</item>
<item>
    <title>B树</title>
    <link>https://blog.vllbc.top/b%E6%A0%91/</link>
    <pubDate>Sat, 07 Aug 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/b%E6%A0%91/</guid>
    <description><![CDATA[<h1 id="b树">B树</h1>
<p>B树就是B-树，以前还以为这是两种树，现在才知道这俩就是一个东西。</p>
<h2 id="基本概念">基本概念</h2>
<ol type="1">
<li>所有的叶子结点都出现在同一层上，并且不带信息(可以看做是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空)。</li>
<li>每个结点包含的关键字个数有上界和下界。用一个被称为
B-树的 <strong>最小度数</strong> 的固定整数 t≥2 来表示这些界
，其中 t 取决于磁盘块的大小：<br />
a.除根结点以外的每个结点必须至少有 t−1 个关键字。因此，除了根结点以外的每个内部结点有
t 个孩子。如果树非空，根结点至少有一个关键字。
<ol start="2" type="a">
<li>每个结点至多包含 2t−1 个关键字。</li>
</ol></li>
<li>一个包含x个关键字的结点有x+1个孩子。</li>
<li>一个结点中所有的关键字升序排列，两个关键字<span
class="math inline">\(k_1\)</span>和<span
class="math inline">\(k_2\)</span>之间的孩子结点的所有关键字key在<span
class="math inline">\((k_1, k_2)\)</span>的范围内。</li>
</ol>
<p>其中最小度数和B树的阶不一样：</p>]]></description>
</item>
<item>
    <title>LR为什么用交叉熵</title>
    <link>https://blog.vllbc.top/lr%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5/</link>
    <pubDate>Tue, 09 Mar 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/lr%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5/</guid>
    <description><![CDATA[<p>关于损失函数的问题，之前也有很多疑惑。看了网上的很多博客，有从很多角度出发来讲解的，看的也是云里雾里。现在大致做一下整理。</p>
<p>对于最小二乘，为什么损失函数是那种形式呢，这里可以假设误差符合正态分布，则y也符合正态分布，则从概率的角度来看，减小预测误差也就是最大化<span
class="math inline">\(P(Y|X,
w)\)</span>。可以看一下白板推导中的推导。</p>]]></description>
</item>
</channel>
</rss>
