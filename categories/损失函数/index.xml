<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>损失函数 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
        <description>损失函数 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" rel="self" type="application/rss+xml" /><item>
    <title>hinge loss</title>
    <link>https://blog.vllbc.top/hinge-loss/</link>
    <pubDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/hinge-loss/</guid>
    <description><![CDATA[<p>在机器学习中，<strong>hinge
loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p>
<p></p>
<p>其中 <span
class="math inline">\(\hat{y}\)</span> 表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。）， <span
class="math inline">\(y\)</span> 表示正确的类别。 - 如果 <span
class="math inline">\(\hat{y}y&lt;1\)</span> ，则损失为： <span
class="math inline">\(1-\hat{y}y\)</span> - 如果<span
class="math inline">\(\hat{y}y&gt;1\)</span> ，则损失为：0</p>]]></description>
</item>
<item>
    <title>focal loss</title>
    <link>https://blog.vllbc.top/focal-loss/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/focal-loss/</guid>
    <description><![CDATA[<h1 id="focal-loss">Focal Loss</h1>
<p>Focal Loss主要是为了解决类别不平衡的问题，Focal
Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例：</p>
<h3 id="原始loss">原始Loss</h3>
<p>原始的二分类： </p>
<p>其中 </p>]]></description>
</item>
<item>
    <title>交叉熵损失函数</title>
    <link>https://blog.vllbc.top/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
    <pubDate>Fri, 30 Apr 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
    <description><![CDATA[<h1 id="softmax理解">Softmax理解</h1>
<p>主要记录了在使用softmax这个函数中遇到的一些问题，比较基础，但确实困扰了一段时间。</p>
<p>在学习word2vec中, 使用的一般都是如下的损失函数：</p>]]></description>
</item>
<item>
    <title>Smooth L1 Loss</title>
    <link>https://blog.vllbc.top/smooth-l1-loss/</link>
    <pubDate>Mon, 19 Apr 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/smooth-l1-loss/</guid>
    <description><![CDATA[<h2 id="l1-loss">L1 Loss</h2>
<p>也称为<strong>Mean Absolute
Error</strong>，即平均绝对误差（<strong>MAE</strong>），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。</p>
<p><strong>优点：</strong> 对离群点（<strong>Outliers</strong>）或者异常值更具有鲁棒性。</p>]]></description>
</item>
</channel>
</rss>
