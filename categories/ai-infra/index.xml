<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>AI-Infra - 分类 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/categories/ai-infra/</link>
        <description>AI-Infra - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/categories/ai-infra/" rel="self" type="application/rss+xml" /><item>
    <title>3D并行</title>
    <link>http://localhost:1313/3d%E5%B9%B6%E8%A1%8C/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/3d%E5%B9%B6%E8%A1%8C/</guid>
    <description><![CDATA[<p>如果想将模型训练扩展到大的批次，则很快就会达到在单个 GPU
上可以做的极限。具体来说，会发生
<code>RuntimeError: CUDA out of memory</code>。 <a
href="梯度累计.md">梯度累计</a>、<a
href="Activation%20checkpointing.md">Activation checkpointing</a> 和 <a
href="CPU%20offloading.md">CPU offloading</a>
都可以一定程度上减少显存的占用，为了_有效地_扩展到更大的模型大小和不断增长的数据集，同时仍然在合理的时间内训练模型，我们需要将计算<strong>分布在</strong>一组机器上。</p>]]></description>
</item>
</channel>
</rss>
