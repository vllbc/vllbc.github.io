<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>优化器 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E4%BC%98%E5%8C%96%E5%99%A8/</link>
        <description>优化器 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E4%BC%98%E5%8C%96%E5%99%A8/" rel="self" type="application/rss+xml" /><item>
    <title>AdaGrad</title>
    <link>https://blog.vllbc.top/adagrad/</link>
    <pubDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adagrad/</guid>
    <description><![CDATA[<p>
AdaGrad
直接暴力累加平方梯度，这种做法的缺点就是累加的和会持续增长，会导致学习率变小最终变得无穷小，最后将无法获得额外信息。</p>]]></description>
</item>
<item>
    <title>AdamW</title>
    <link>https://blog.vllbc.top/adamw/</link>
    <pubDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adamw/</guid>
    <description><![CDATA[<p>AdamW相对与Adam的改动十分简单，其将权重衰减项从梯度的计算中拿出来直接加在了最后的权重更新步骤上（图1，式12）。其提出的动机在于：原先Adam的实现中如果采用了
<a
href="https://zhida.zhihu.com/search?content_id=231119964&amp;content_type=Article&amp;match_order=1&amp;q=L2%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F&amp;zhida_source=entity">L2权重衰减</a>，则相应的权重衰减项会被直接加在loss里，从而导致动量的一阶与二阶滑动平均均考虑了该权重衰减项，而这影响了Adam的优化效果，而将权重衰减与梯度的计算进行解耦能够显著提升Adam的效果。目前，AdamW现在已经成为<a
href="https://zhida.zhihu.com/search?content_id=231119964&amp;content_type=Article&amp;match_order=1&amp;q=transformer&amp;zhida_source=entity">transformer</a>训练中的默认优化器了。</p>]]></description>
</item>
<item>
    <title>RMSProp</title>
    <link>https://blog.vllbc.top/rmsprop/</link>
    <pubDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rmsprop/</guid>
    <description><![CDATA[<p>
RMSProp 和 Adagrad 算法的最大区别就是在于更新累积梯度值 r 的时候 RMSProp
考虑加入了一个权重系数 ρ 。
它使用了一个梯度平方的滑动平均。其主要思路就是考虑历史的梯度，对于离得近的梯度重点考虑，而距离比较远的梯度则逐渐忽略。注意图中的是内积。</p>]]></description>
</item>
<item>
    <title>SGD</title>
    <link>https://blog.vllbc.top/sgd/</link>
    <pubDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/sgd/</guid>
    <description><![CDATA[<p>作为机器学习的初学者必然会接触梯度下降算法以及
SGD，基本上形式如下：</p>
<p><span class="math display">\[
\theta_t = \theta_{t-1} - \alpha \;g(\theta)
\]</span> 其中 <span class="math inline">\(\alpha\)</span>
为学习率，<span class="math inline">\(g(\theta)\)</span> 为梯度。</p>]]></description>
</item>
<item>
    <title>SGD-Momentum</title>
    <link>https://blog.vllbc.top/sgd-momentum/</link>
    <pubDate>Sat, 12 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/sgd-momentum/</guid>
    <description><![CDATA[<p>带动量的随机梯度下降方法</p>
<p>它的思路就是计算前面梯度的该变量，每次迭代会考虑前面的计算结果。这样如果在某个维度上波动厉害的特征，会由于“momentum”的影响，而抵消波动的方向（因为波动剧烈的维度每次更新的方向是相反的，momentum
能抵消这种波动）。使得梯度下降更加的平滑，得到更快的收敛效率。而后续提出的
Adagrad，RMSProp 以及结合两者优点的 Adam
算法都考虑了这种“momentum”的思想。</p>]]></description>
</item>
<item>
    <title>Muon</title>
    <link>https://blog.vllbc.top/muon/</link>
    <pubDate>Fri, 11 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/muon/</guid>
    <description><![CDATA[<p>Muon 算法流程如下图所示：</p>
<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中最主要的部分是 NewtonSchulz 5 算法，流程如下：</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> newtonschulz5(G, steps<span class="op">=</span><span class="dv">5</span>, eps<span class="op">=</span><span class="fl">1e-7</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> G.ndim <span class="op">==</span> <span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    a, b, c <span class="op">=</span> (<span class="fl">3.4445</span>, <span class="op">-</span><span class="fl">4.7750</span>, <span class="fl">2.0315</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> G.bfloat16()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    X <span class="op">/=</span> (X.norm() <span class="op">+</span> eps)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> G.size(<span class="dv">0</span>) <span class="op">&gt;</span> G.size(<span class="dv">1</span>):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.T</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> X <span class="op">@</span> X.T</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> b <span class="op">*</span> A <span class="op">+</span> c <span class="op">*</span> A <span class="op">@</span> A</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> a <span class="op">*</span> X <span class="op">+</span> B <span class="op">@</span> X</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> G.size(<span class="dv">0</span>) <span class="op">&gt;</span> G.size(<span class="dv">1</span>):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.T</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X</span></code></pre></div>
<p>这个算法的作用是将 G 近似为一个最接近他的半正交矩阵，即：</p>]]></description>
</item>
<item>
    <title>Adam算法</title>
    <link>https://blog.vllbc.top/adam/</link>
    <pubDate>Sun, 11 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adam/</guid>
    <description><![CDATA[<h2 id="moment矩">moment(矩)</h2>
<p>矩在数学中的定义，一阶矩(first moment)就是样本的均值(mean),
二阶矩就是方差（variance）。 ## 滑动平均 滑动平均(exponential moving
average)，或者叫做指数加权平均(exponentially weighted moving
average)，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。在时间序列预测中也常用。</p>]]></description>
</item>
</channel>
</rss>
