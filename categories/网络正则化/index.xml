<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>网络正则化 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/</link>
        <description>网络正则化 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" rel="self" type="application/rss+xml" /><item>
    <title>Batch Norm</title>
    <link>https://blog.vllbc.top/batch-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/batch-norm/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>L1 L2正则化</title>
    <link>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid>
    <description><![CDATA[<h1 id="l1正则化">L1正则化</h1>
<h1 id="l2正则化">L2正则化</h1>
<h1 id="权重衰减">权重衰减</h1>
<h1 id="l2正则化和权重衰减的区别">L2正则化和权重衰减的区别</h1>
<p>L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。 </p>]]></description>
</item>
<item>
    <title>Layer Norm</title>
    <link>https://blog.vllbc.top/layer-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/layer-norm/</guid>
    <description><![CDATA[<h2 id="pre-norm">pre-norm</h2>
<p>Pre-norm:<span
class="math inline">\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\)</span></p>
<p><span class="math inline">\(先来看Pre-norm^{+},递归展开：\)</span>
<span class="math display">\[X_{t+1}=X_t+F_t(Norm(X_t))\]</span> <span
class="math inline">\(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\)</span>
其中，展开<span class="math inline">\(^{+}\)</span>后的每一项( <span
class="math inline">\(F_{1}( Norm( X_{1}) ) , \ldots\)</span>, <span
class="math inline">\(F_{t- 1}( Norm( X_{t- 1}) )\)</span>, <span
class="math inline">\(F_{t}( Norm( X_{t})
)\)</span>)之间都是同一量级的， 所以<span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\)</span>和 <span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))\)</span>之间的区别就像t和t-1的区别一样，我们可以将
其记为<span class="math inline">\(X_t+ 1= \mathscr{O} ( t+ 1)\)</span> .
这种特性就导致当t足够大的时候，<span
class="math inline">\(X_{t+1}\)</span>和<span
class="math inline">\(X_t\)</span>之间区别可以忽略不计（直觉上），那么就有：</p>]]></description>
</item>
<item>
    <title>Dropout正则化</title>
    <link>https://blog.vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</link>
    <pubDate>Thu, 21 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</guid>
    <description><![CDATA[<h1 id="dropout">Dropout</h1>
<p>在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值h以保留概率概率p由随机变量替换(即drop经过神经元后的值代替drop神经元)</p>]]></description>
</item>
</channel>
</rss>
