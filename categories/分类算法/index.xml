<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>分类算法 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</link>
        <description>分类算法 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="self" type="application/rss+xml" /><item>
    <title>最大熵模型</title>
    <link>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>决策树</title>
    <link>https://blog.vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/</link>
    <pubDate>Sun, 21 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
    <description><![CDATA[<p>参考：<a
href="https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html">https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html</a></p>
<p>《机器学习》周志华</p>
<h1 id="决策树">决策树</h1>
<p>决策树是什么？决策树(decision
tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision
block)，椭圆形成代表终止模块(terminating
block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed
edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf
node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root
node)。这样，结点说法就与模块说法对应上了，理解就好。</p>]]></description>
</item>
<item>
    <title>分类算法概述</title>
    <link>https://blog.vllbc.top/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Fri, 12 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[<h1 id="分类算法">分类算法</h1>
<p>主要区分一下生成模型和判别模型，首先要知道生成模型和判别模型都属于监督学习，即样本有其对应的标签的。还有一个概念就是硬分类和软分类，简单理解就是硬分类是直接分出类别，比如线性判别分析、感知机。而软分类是计算出概率，根据概率来得到类别，生成模型和判别模型都是软分类。</p>]]></description>
</item>
<item>
    <title>KNN</title>
    <link>https://blog.vllbc.top/knn/</link>
    <pubDate>Sat, 25 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/knn/</guid>
    <description><![CDATA[<h1 id="knn">KNN</h1>
<p>参考：<a
href="https://cuijiahua.com/blog/2017/11/ml_1_knn.html">https://cuijiahua.com/blog/2017/11/ml_1_knn.html</a></p>
<p>《统计学习方法》李航（kd树）</p>
<h2 id="简介">简介</h2>
<p>k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart
P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>]]></description>
</item>
<item>
    <title>线性判别分析</title>
    <link>https://blog.vllbc.top/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</link>
    <pubDate>Sat, 19 Feb 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</guid>
    <description><![CDATA[<h1 id="线性判别分析lda">线性判别分析(LDA)</h1>
<p>线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别
## 主要思想 1. 类内方差小 2. 类间方差大 ## 推导
这里以二类为例，即只有两个类别。</p>]]></description>
</item>
<item>
    <title>bayes</title>
    <link>https://blog.vllbc.top/bayes/</link>
    <pubDate>Wed, 16 Feb 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/bayes/</guid>
    <description><![CDATA[<h2 id="条件概率">条件概率</h2>
<p><span class="math inline">\(P(B|A) = \frac{P(AB)}{P(A)}\)</span></p>
<h2 id="乘法法则">乘法法则</h2>
<p>如果P(A) &gt; 0 <span class="math inline">\(P(AB) =
P(A)P(B|A)\)</span> 如果<span class="math inline">\(P(A_1 \dots
A_{n-1})\)</span> &gt; 0 则</p>
<p><span class="math display">\[
\begin{aligned}
P(A_1A_2\dots A_n) = P(A_1A_2\dots A_{n-1})P(A_n | A_1A_2\dots
A_{n-1})  \\\\ = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\dots P(A_n|A_1A_2\dots
A_{n-1})
\end{aligned}
\]</span></p>
<p>其中第一步使用了乘法公式，然后再对前者继续使用乘法公式，以此类推，就可以得到最后的结果。</p>]]></description>
</item>
<item>
    <title>Logistic回归</title>
    <link>https://blog.vllbc.top/logistic%E5%9B%9E%E5%BD%92/</link>
    <pubDate>Sat, 15 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/logistic%E5%9B%9E%E5%BD%92/</guid>
    <description><![CDATA[<h1 id="logistic回归">Logistic回归</h1>
<h2 id="线性回归">线性回归</h2>
<p>线性回归表达式：</p>
<p><span class="math display">\[
y = w^Tx+b
\]</span></p>
<p>广义回归模型：</p>
<p><span class="math display">\[
y = g^{-1}(w^Tx+b)
\]</span></p>
<h2 id="sigmoid函数">Sigmoid函数</h2>
<p>在分类任务中，需要找到一个联系函数，即g，将线性回归的输出值与实际的标签值联系起来。因此可以使用Sigmoid函数
即：</p>]]></description>
</item>
<item>
    <title>感知机算法</title>
    <link>https://blog.vllbc.top/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/</link>
    <pubDate>Tue, 16 Nov 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[<h1 id="感知机算法">感知机算法</h1>
<p>感知机印象中没有系统学习过但是是一个很简单的算法，最近看了一下李航老师的统计学习方法，发现感知机的思想和svm十分类似，并且比svm简单的多，不需要间隔最大，只需要分开就可以。同时老师在课堂上面讲的版本也有点不一样，主要是计算上的不同，本质还是一样的。然后就打算整理一下这一块。</p>]]></description>
</item>
<item>
    <title>SVM</title>
    <link>https://blog.vllbc.top/svm/</link>
    <pubDate>Mon, 20 Sep 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/svm/</guid>
    <description><![CDATA[<h1 id="svm">SVM</h1>
<h2 id="kernel">kernel</h2>
<h3 id="介绍">介绍</h3>
<p>其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。</p>
<p>一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超平面。</p>]]></description>
</item>
</channel>
</rss>
