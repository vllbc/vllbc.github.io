<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>分类算法 - 分类 - vllbc02</title>
        <link>https://blog.vllbc.top/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</link>
        <description>分类算法 - 分类 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" rel="self" type="application/rss+xml" /><item>
    <title>最大熵模型</title>
    <link>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>决策树</title>
    <link>https://blog.vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/</link>
    <pubDate>Sun, 21 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
    <description><![CDATA[参考：https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html 《机器学习》周]]></description>
</item>
<item>
    <title>分类算法概述</title>
    <link>https://blog.vllbc.top/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Fri, 12 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[分类算法 主要区分一下生成模型和判别模型，首先要知道生成模型和判别模型都属于监督学习，即样本有其对应的标签的。还有一个概念就是硬分类和软分类，]]></description>
</item>
<item>
    <title>KNN</title>
    <link>https://blog.vllbc.top/knn/</link>
    <pubDate>Sat, 25 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/knn/</guid>
    <description><![CDATA[KNN 参考：https://cuijiahua.com/blog/2017/11/ml_1_knn.html 《统计学习方法》李航（kd树） 简介 k]]></description>
</item>
<item>
    <title>线性判别分析</title>
    <link>https://blog.vllbc.top/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</link>
    <pubDate>Sat, 19 Feb 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</guid>
    <description><![CDATA[线性判别分析(LDA) 线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个]]></description>
</item>
<item>
    <title>bayes</title>
    <link>https://blog.vllbc.top/bayes/</link>
    <pubDate>Wed, 16 Feb 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/bayes/</guid>
    <description><![CDATA[条件概率 \(P(B|A) = \frac{P(AB)}{P(A)}\) 乘法法则 如果P(A) &gt; 0 \(P(AB) = P(A)P(B|A)\) 如果\(P(A_1 \dots A_{n-1})\) &gt; 0 则 \[ \begin{aligned} P(A_1A_2\dots A_n) = P(A_1A_2\dots A_{n-1})P(A_n | A_1A_2\dots A_{n-1}) \\\\ = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\dots P(A_n|A_1A_2\dots A_{n-1}) \end{aligned} \] 其中第一步使用了乘法公式，然后再对]]></description>
</item>
<item>
    <title>Logistic回归</title>
    <link>https://blog.vllbc.top/logistic%E5%9B%9E%E5%BD%92/</link>
    <pubDate>Sat, 15 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/logistic%E5%9B%9E%E5%BD%92/</guid>
    <description><![CDATA[Logistic回归 线性回归 线性回归表达式： \[ y = w^Tx+b \] 广义回归模型： \[ y = g^{-1}(w^Tx+b) \] Sigmoid函数 在分类任务中，需要找到一个联系函数，即g，将]]></description>
</item>
<item>
    <title>感知机算法</title>
    <link>https://blog.vllbc.top/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/</link>
    <pubDate>Tue, 16 Nov 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[感知机算法 感知机印象中没有系统学习过但是是一个很简单的算法，最近看了一下李航老师的统计学习方法，发现感知机的思想和svm十分类似，并且比sv]]></description>
</item>
<item>
    <title>SVM</title>
    <link>https://blog.vllbc.top/svm/</link>
    <pubDate>Mon, 20 Sep 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/svm/</guid>
    <description><![CDATA[SVM kernel 介绍 其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。 一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超]]></description>
</item>
</channel>
</rss>
