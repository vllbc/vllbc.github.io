<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reasoning - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/reasoning/</link>
        <description>Reasoning - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 05 Aug 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/reasoning/" rel="self" type="application/rss+xml" /><item>
    <title>信用分配</title>
    <link>https://blog.vllbc.top/%E4%BF%A1%E7%94%A8%E5%88%86%E9%85%8D/</link>
    <pubDate>Tue, 05 Aug 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E4%BF%A1%E7%94%A8%E5%88%86%E9%85%8D/</guid>
    <description><![CDATA[<p>最近涌现了很多关于信用分配的论文，因此整理一下</p>
<h2 id="first-return-entropy-eliciting-explore"><a
href="../../reading/Reasoning/First%20Return,%20Entropy-Eliciting%20Explore.md">First
Return, Entropy-Eliciting Explore</a></h2>
<h2
id="good-learners-think-their-thinkinggenerative-prm-makes-large-reasoning-model-more-efficient-math-learne"><a
href="../../reading/Reasoning/Good%20Learners%20Think%20Their%20Thinking：Generative%20PRM%20Makes%20Large%20Reasoning%20Model%20More%20Efficient%20Math%20Learne.md">Good
Learners Think Their Thinking：Generative PRM Makes Large Reasoning
Model More Efficient Math Learne</a></h2>
<h2 id="group-sequence-policy-optimization"><a
href="../../reading/LLM-RL/Group%20Sequence%20Policy%20Optimization.md">Group
Sequence Policy Optimization</a></h2>
<h2 id="process-reinforcement-through-implicit-rewards"><a
href="../../reading/Reasoning/PROCESS%20REINFORCEMENT%20THROUGH%20IMPLICIT%20REWARDS.md">PROCESS
REINFORCEMENT THROUGH IMPLICIT REWARDS</a></h2>
<h2
id="rlvmrreinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents"><a
href="../../reading/Planning/RLVMR：Reinforcement%20Learning%20with%20Verifiable%20Meta-Reasoning%20Rewards%20for%20Robust%20Long-Horizon%20Agents.md">RLVMR：Reinforcement
Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon
Agents</a></h2>
<h2
id="vapoefficient-and-reliable-reinforcement-learning-for-advanced-reasoning-tasks"><a
href="../../reading/LLM-RL/VAPO：Efficient%20and%20Reliable%20Reinforcement%20Learning%20for%20Advanced%20Reasoning%20Tasks.md">VAPO：Efficient
and Reliable Reinforcement Learning for Advanced Reasoning
Tasks</a></h2>
<h2 id="group-in-group-policy-optimization-for-llm-agent-training"><a
href="../../reading/Planning/Group-in-Group%20Policy%20Optimization%20for%20LLM%20Agent%20Training.md">Group-in-Group
Policy Optimization for LLM Agent Training</a></h2>
<h2
id="capotowards-enhancing-llm-reasoning-through-verifiable-generative-credit-assignment"><a
href="../../reading/Reasoning/CAPO：Towards%20Enhancing%20LLM%20Reasoning%20through%20Verifiable%20Generative%20Credit%20Assignment.md">CAPO：Towards
Enhancing LLM Reasoning through Verifiable Generative Credit
Assignment</a></h2>
<h2
id="beyond-policy-optimizationa-data-curation-flywheel-for-sparse-reward-long-horizon-planning"><a
href="../../reading/Planning/Beyond%20Policy%20Optimization：A%20Data%20Curation%20Flywheel%20for%20Sparse-Reward%20Long-Horizon%20Planning.md">Beyond
Policy Optimization：A Data Curation Flywheel for Sparse-Reward
Long-Horizon Planning</a></h2>
<h2
id="gtpo-and-grpo-stoken-and-sequence-level-reward-shaping-with-policy-entropy"><a
href="../../reading/Reasoning/GTPO%20and%20GRPO-S：Token%20and%20Sequence-Level%20Reward%20Shaping%20with%20Policy%20Entropy.md">GTPO
and GRPO-S：Token and Sequence-Level Reward Shaping with Policy
Entropy</a></h2>
<h2
id="gtpotrajectory-based-policy-optimization-in-large-language-models"><a
href="../../reading/Reasoning/GTPO：Trajectory-Based%20Policy%20Optimization%20in%20Large%20Language%20Models.md">GTPO：Trajectory-Based
Policy Optimization in Large Language Models</a></h2>]]></description>
</item>
<item>
    <title>entropy(reasoning)</title>
    <link>https://blog.vllbc.top/entropyreasoning/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/entropyreasoning/</guid>
    <description><![CDATA[<h2 id="熵坍塌">熵坍塌</h2>
<h3
id="ulorlan-ultra-long-output-reinforcement-learning-approach-for-advancing-large-language-models-reasoning-abilities"><a
href="../../reading/Reasoning/UloRL：An%20Ultra-Long%20Output%20Reinforcement%20Learning%20Approach%20for%20Advancing%20Large%20Language%20Models’%20Reasoning%20Abilities.md">UloRL：An
Ultra-Long Output Reinforcement Learning Approach for Advancing Large
Language Models’ Reasoning Abilities</a></h3>
<p>UloRL的核心创新 <strong>动态掩码熟练掌握正向词元（Dynamic Masking of
well-Mastered Positive Tokens,
DMMPTs）</strong>。论文作者敏锐地指出，熵坍塌的根源并非“训练正样本”，而是“过度训练已经熟练掌握的正向词元（MPTs）”。MPTs指的是那些在正确答案中，且模型已经能以极高概率（如&gt;99%）预测出来的词元。DMMPTs为此设计了一个“熵值恒温器”：
1. 设定一个理想的“目标熵值”。 2.
在训练时，如果模型的当前熵低于这个目标值，说明模型开始变得“僵化”了。此时，系统会自动“屏蔽”（mask）掉那些MPTs，不再对它们进行训练，迫使模型关注那些还未掌握好的部分。
3. 如果模型熵值高于或等于目标值，则正常进行训练。</p>]]></description>
</item>
<item>
    <title>思维链压缩</title>
    <link>https://blog.vllbc.top/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%80%9D%E7%BB%B4%E9%93%BE%E5%8E%8B%E7%BC%A9/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>MCTS和PRM</title>
    <link>https://blog.vllbc.top/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[<h2 id="核心总结">核心总结</h2>
<ul>
<li><strong>PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果</strong>。例如，
<ul>
<li>单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。</li>
<li>单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。</li>
</ul></li>
<li><strong>PRM 和 MCTS
既可以应用于优化训练数据，也可以用来预测用</strong>
<ul>
<li>用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward
model训练数据。</li>
<li>用于推理：很简单，推理用MCTS的方式把 test-scaling
做上来，再结合PRM的方式从众多路径中挑选最佳答案。</li>
</ul></li>
<li><strong>PRM和MCTS的缺点</strong><br />
这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。</li>
<li>Process Reward Model(PRM) 在实际应用中有三大局限：
<ul>
<li>第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。</li>
<li>第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。</li>
<li>第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy
model，但反而更多时间去优化 reward model 去了。</li>
</ul></li>
<li>对MCTS的看法：
<ul>
<li>文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。</li>
<li>MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。</li>
</ul></li>
</ul>
<h2 id="参考">参考</h2>
<p>https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs Can
Master Math Reasoning with Self-Evolved Deep Thinking</p>]]></description>
</item>
</channel>
</rss>
