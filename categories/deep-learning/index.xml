<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - 分类 - vllbc02</title>
        <link>https://vllbc.top/categories/deep-learning/</link>
        <description>Deep Learning - 分类 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 08 Nov 2022 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/categories/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>CNN</title>
    <link>https://vllbc.top/cnn/</link>
    <pubDate>Tue, 08 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/cnn/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>Smooth L1 Loss</title>
    <link>https://vllbc.top/smooth-l1-loss/</link>
    <pubDate>Mon, 07 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/smooth-l1-loss/</guid>
    <description><![CDATA[L1 Loss 也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。 优点： 对离群]]></description>
</item>
<item>
    <title>反向传播算法</title>
    <link>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</link>
    <pubDate>Mon, 07 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[反向传播算法遵循两个法则：梯度下降法则和链式求导法则。 梯度下降法则不用多说，记住一切的目的就是为了减小损失，即朝着局部最小值点移动。链式求导]]></description>
</item>
<item>
    <title>GRU</title>
    <link>https://vllbc.top/gru/</link>
    <pubDate>Thu, 03 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/gru/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>LSTM</title>
    <link>https://vllbc.top/lstm/</link>
    <pubDate>Thu, 03 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/lstm/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>单隐层多分类网络</title>
    <link>https://vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/</link>
    <pubDate>Thu, 27 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%8D%95%E9%9A%90%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C/</guid>
    <description><![CDATA[单隐层多分类神经网络（numpy实现） 使用Numpy实现，并且使用命令行的形式设定参数。 是一个作业里面的，实现的时候踩了一些坑，主要是训练里]]></description>
</item>
<item>
    <title>Dropout正则化</title>
    <link>https://vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</link>
    <pubDate>Tue, 30 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/dropout%E6%AD%A3%E5%88%99%E5%8C%96/</guid>
    <description><![CDATA[Dropout 在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值h以保留概率概率p由随机]]></description>
</item>
<item>
    <title>Adam算法</title>
    <link>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</link>
    <pubDate>Sun, 31 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/adam%E7%AE%97%E6%B3%95/</guid>
    <description><![CDATA[Adam算法 背景 作为机器学习的初学者必然会接触梯度下降算法以及SGD，基本上形式如下： $$ \theta_t = \theta_{t-1} - \alpha ;g(\theta) $$ 其中$\alpha$为学习率，$g(\]]></description>
</item>
<item>
    <title>梯度下降法</title>
    <link>https://vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</link>
    <pubDate>Sat, 30 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</guid>
    <description><![CDATA[梯度下降法 简介 批度梯度下降 其实就是一次将整个数据集进行梯度下降的迭代 随机梯度下降 就是对样本进行循环，每循环一个样本就更新一次参数，但是不容易]]></description>
</item>
<item>
    <title>softmax理解</title>
    <link>https://vllbc.top/softmax%E7%90%86%E8%A7%A3/</link>
    <pubDate>Mon, 27 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/softmax%E7%90%86%E8%A7%A3/</guid>
    <description><![CDATA[Softmax理解 主要记录了在使用softmax这个函数中遇到的一些问题，比较基础，但确实困扰了一段时间。 在学习word2vec中, 使用的一]]></description>
</item>
</channel>
</rss>
