<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - 分类 - vllbc02&#39;s blogs</title>
        <link>http://localhost:1313/categories/deep-learning/</link>
        <description>Deep Learning - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>vllbc02@163.com (vllbc)</managingEditor>
            <webMaster>vllbc02@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/categories/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Batch Norm</title>
    <link>http://localhost:1313/batch-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/batch-norm/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>L1 L2正则化</title>
    <link>http://localhost:1313/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid>
    <description><![CDATA[L1正则化 L2正则化 权重衰减 L2正则化和权重衰减的区别 L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。]]></description>
</item>
<item>
    <title>Layer Norm</title>
    <link>http://localhost:1313/layer-norm/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/layer-norm/</guid>
    <description><![CDATA[pre-norm Pre-norm:\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\) \(先来看Pre-norm^{+},递归展开：\) \[X_{t+1}=X_t+F_t(Norm(X_t))\] \(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\) 其中，展开\(^{+}\)后的每一项( \(F_{1}( Norm( X_{1}) ) , \ldots\), \(F_{t- 1}( Norm( X_{t- 1}) )\), \(F_{t}( Norm( X_{t}) )\))之间都]]></description>
</item>
<item>
    <title>hinge loss</title>
    <link>http://localhost:1313/hinge-loss/</link>
    <pubDate>Mon, 13 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/hinge-loss/</guid>
    <description><![CDATA[在机器学习中，hinge loss是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为： 其中 \(\hat{y}\) 表]]></description>
</item>
<item>
    <title>UDA</title>
    <link>http://localhost:1313/uda/</link>
    <pubDate>Wed, 08 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/uda/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>early-stopping</title>
    <link>http://localhost:1313/early-stopping/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/early-stopping/</guid>
    <description><![CDATA[介绍 早停止（Early Stopping）是 当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段。 如何判断已经收敛？]]></description>
</item>
<item>
    <title>focal loss</title>
    <link>http://localhost:1313/focal-loss/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/focal-loss/</guid>
    <description><![CDATA[Focal Loss Focal Loss主要是为了解决类别不平衡的问题，Focal Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例： 原始Loss 原始的二]]></description>
</item>
<item>
    <title>EDA</title>
    <link>http://localhost:1313/eda/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/eda/</guid>
    <description><![CDATA[NLP中的EDA 同义词替换，回译，近音字替换，随机插入，随机交换，随机删除 同义词替换 做法可以是维护一个同义词表，如哈工大的发布的同义词词典。]]></description>
</item>
<item>
    <title>warmup</title>
    <link>http://localhost:1313/warmup/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/warmup/</guid>
    <description><![CDATA[在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning r]]></description>
</item>
<item>
    <title>标签平滑</title>
    <link>http://localhost:1313/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>http://localhost:1313/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</guid>
    <description><![CDATA[神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为oneho]]></description>
</item>
</channel>
</rss>
