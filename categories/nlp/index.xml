<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - 分类 - vllbc02</title>
        <link>https://vllbc.top/categories/nlp/</link>
        <description>NLP - 分类 - vllbc02</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://vllbc.top/categories/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>Prompt</title>
    <link>https://vllbc.top/prompt/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/prompt/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>seq2seq</title>
    <link>https://vllbc.top/seq2seq/</link>
    <pubDate>Wed, 09 Nov 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/seq2seq/</guid>
    <description><![CDATA[Seq2Seq （本文只介绍最原始的seq2seq，带有注意力在attention文章中） RNN 有关RNN Seq2Seq是典型的Encoder-decoder]]></description>
</item>
<item>
    <title>tokenization</title>
    <link>https://vllbc.top/tokenization/</link>
    <pubDate>Mon, 17 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/tokenization/</guid>
    <description><![CDATA[Tokenization技术 本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子]]></description>
</item>
<item>
    <title>CoVe</title>
    <link>https://vllbc.top/cove/</link>
    <pubDate>Sun, 09 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/cove/</guid>
    <description><![CDATA[CoVe Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器， 模型训练 主要假设是，为了翻译一个]]></description>
</item>
<item>
    <title>Quick-Thought</title>
    <link>https://vllbc.top/quick-thought/</link>
    <pubDate>Mon, 03 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/quick-thought/</guid>
    <description><![CDATA[这是一种句向量的表示方式，即sentence2vec，实际上是对skip thought的改进，]]></description>
</item>
<item>
    <title>BM25</title>
    <link>https://vllbc.top/bm25/</link>
    <pubDate>Sun, 07 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/bm25/</guid>
    <description><![CDATA[BM25算法 BM25算法，通常用来作搜索相关性平分。一句话概况其主要思想：对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，]]></description>
</item>
<item>
    <title>GPT</title>
    <link>https://vllbc.top/gpt/</link>
    <pubDate>Wed, 27 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/gpt/</guid>
    <description><![CDATA[GPT 预训练(从左到右的 Transformer 语言模型) GPT 是一种基于 Transformer 的从左到右的语言模型。该架构是一个 12 层的 Transformer 解码器（没有解码器-编码器）。 ## 模型架构 就是12层的]]></description>
</item>
<item>
    <title>Dependency Parsing</title>
    <link>https://vllbc.top/dependency-parsing/</link>
    <pubDate>Mon, 27 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/dependency-parsing/</guid>
    <description><![CDATA[依存关系分析 对于句法结构分析，主要有两种方式：Constituency Parsing(成分分析)与Dependency Parsing(依存分]]></description>
</item>
<item>
    <title>主题模型</title>
    <link>https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Thu, 16 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 词袋模型 将所有词语装进一个袋子里，不考虑其]]></description>
</item>
<item>
    <title>Transformer</title>
    <link>https://vllbc.top/transformer/</link>
    <pubDate>Wed, 08 Jun 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/transformer/</guid>
    <description><![CDATA[Transformer 背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank]]></description>
</item>
</channel>
</rss>
