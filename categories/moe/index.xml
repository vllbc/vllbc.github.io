<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MoE - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/moe/</link>
        <description>MoE - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/moe/" rel="self" type="application/rss+xml" /><item>
    <title>MoE</title>
    <link>https://blog.vllbc.top/moe/</link>
    <pubDate>Wed, 07 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/moe/</guid>
    <description><![CDATA[<p>MoE 的思想类似于集成学习中的 <a
href="../../Machine%20Learning/集成学习/Ensemble%20Learning.md">Ensemble
Learning</a>。MoE 作用于原本 transformer 模型的 MLP 层，即：</p>
<p> 图片来自于Switch Transformers: Scaling to Trillion
Parameter Models with Simple and Efficient Sparsity 论文。</p>
<p>总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer
模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE
层由两个核心部分组成: <strong>一个路由器（或者叫门控网络）和若干数量的专家</strong>。</p>]]></description>
</item>
</channel>
</rss>
