<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Boosting - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/boosting/</link>
        <description>Boosting - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 01 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/boosting/" rel="self" type="application/rss+xml" /><item>
    <title>xgboost</title>
    <link>https://blog.vllbc.top/xgboost/</link>
    <pubDate>Wed, 01 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/xgboost/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>Adaboost</title>
    <link>https://blog.vllbc.top/adaboost/</link>
    <pubDate>Wed, 27 Apr 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adaboost/</guid>
    <description><![CDATA[<p>Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>]]></description>
</item>
<item>
    <title>GBDT</title>
    <link>https://blog.vllbc.top/gbdt/</link>
    <pubDate>Wed, 06 Jan 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gbdt/</guid>
    <description><![CDATA[<h1 id="梯度提升决策树gbdt">梯度提升决策树(GBDT)</h1>
<p>GBDT<strong>(Gradient Boosting Decision Tree)</strong>是一种迭代的<a
href="https://so.csdn.net/so/search?q=决策树&amp;spm=1001.2101.3001.7020">决策树</a>算法，由多棵决策树组成，所有树的结论累加起来作为最终答案。</p>
<h2 id="回归树">回归树</h2>
<p>选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小
值时的(j,s)对。其中Rm是被划分的输入空间， <span
class="math inline">\(\mathrm{cm}\)</span>
是空间Rm对应的固定输出值。</p>]]></description>
</item>
</channel>
</rss>
