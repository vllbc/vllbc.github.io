<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>大模型分布式 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/</link>
        <description>大模型分布式 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F/" rel="self" type="application/rss+xml" /><item>
    <title>device_mesh</title>
    <link>https://blog.vllbc.top/device_mesh/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/device_mesh/</guid>
    <description><![CDATA[<h2 id="verl中的device_mesh">verl中的device_mesh</h2>
<p>verl中有3个device_mesh，分别是： - 训练用的FSDP mesh（通常是一维） -
推理用的rollout mesh（包含tp维度） - Ulysses序列并行的mesh（dp×sp）</p>]]></description>
</item>
<item>
    <title>data_parallel</title>
    <link>https://blog.vllbc.top/data_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/data_parallel/</guid>
    <description><![CDATA[<p>如果想将模型训练扩展到大的批次，则很快就会达到在单个 GPU
上可以做的极限。具体来说，会发生
<code>RuntimeError: CUDA out of memory</code>。 <a
href="梯度累计.md">梯度累计</a>、<a
href="Activation%20checkpointing.md">Activation checkpointing</a> 和 <a
href="CPU%20offloading.md">CPU offloading</a>
都可以一定程度上减少显存的占用，为了_有效地_扩展到更大的模型大小和不断增长的数据集，同时仍然在合理的时间内训练模型，我们需要将计算<strong>分布在</strong>一组机器上。</p>]]></description>
</item>
<item>
    <title>pipeline parallelism</title>
    <link>https://blog.vllbc.top/pipeline-parallelism/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/pipeline-parallelism/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">💥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>tensor_parallel</title>
    <link>https://blog.vllbc.top/tensor_parallel/</link>
    <pubDate>Wed, 23 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/tensor_parallel/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<ul>
<li><a
href="https://cdn-lfs-us-1.hf.co/repos/e7/07/e7077a163ab0f314cedbb8ddd44667d765205ee536e8b4785fdd0872534107db/274a19a2577ed220cd3a102b4469c44310e4a7c8e8f8ebc36842d907cb51e127?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%3B+filename%3D%22The_Ultra-Scale_Playbook_Training_LLMs_on_GPU_Clusters.pdf%22%3B&amp;response-content-type=application%2Fpdf&amp;Expires=1751735939&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTczNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3LzA3L2U3MDc3YTE2M2FiMGYzMTRjZWRiYjhkZGQ0NDY2N2Q3NjUyMDVlZTUzNmU4YjQ3ODVmZGQwODcyNTM0MTA3ZGIvMjc0YTE5YTI1NzdlZDIyMGNkM2ExMDJiNDQ2OWM0NDMxMGU0YTdjOGU4ZjhlYmMzNjg0MmQ5MDdjYjUxZTEyNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&amp;Signature=jer8tObN1q6%7Eij8fX2vLIiox2VNNX0yAD9hjDxq9JXGDmzou6ONo7lnwIlrn%7ECbbaP-BXm80YdFMAgI2SbINgrxMfxLHTkp5IVwqppQ1INlC8K6JrZS3T8QlL4aY5jY7wX7SCUvweSuxEWA2QXMYwHWWV2Iy-OQAMkcdvvxDvjIZZwlYZqJ0tccDbpSYrOhNfkMcGYyxhp3HPgcEd6gVPydQE6g2wM8ErR04u-9dzwkJrIBowWrr8OSD9HJraRyr5XObTaBx3NEADn9De8Zyo%7EknwQs4MDxWSueQCYTlCfFElMF0%7EVMXYh%7EVfDSV5lZZiuxCFfke43Z12VSK5cMV%7EA__&amp;Key-Pair-Id=K24J24Z295AEI9">The
Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></li>
<li><a
href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">💥
Training Neural Nets on Larger Batches: Practical Tips for 1-GPU,
Multi-GPU &amp; Distributed setups | by Thomas Wolf | HuggingFace |
Medium</a></li>
<li><a href="https://www.jeremyjordan.me/distributed-training/">Training
extremely large neural networks across thousands of GPUs.</a></li>
</ul>]]></description>
</item>
<item>
    <title>zero</title>
    <link>https://blog.vllbc.top/zero/</link>
    <pubDate>Tue, 22 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/zero/</guid>
    <description><![CDATA[<p>分为zero1、zero2、zero3，虽然zero3对模型进行了分割，但是本质上还是属于数据并行，因为在前向传播和反向传播需要all-gather模型参数，需要完整的模型权重。</p>]]></description>
</item>
<item>
    <title>ring-all-reduce</title>
    <link>https://blog.vllbc.top/ring-all-reduce/</link>
    <pubDate>Thu, 17 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ring-all-reduce/</guid>
    <description><![CDATA[<p>All-reduced=all-gather+reduce-scatter</p>
<ul>
<li><strong>All-Gather</strong> ：将分布式数据汇总到所有节点，适用于需要<strong>全局数据</strong>同步的场景。</li>
<li><strong>Reduce-Scatter</strong>：将分布式数据进行<strong>规约</strong>并<strong>分散</strong>到所有节点，适用于需要局部结果分发的场景。</li>
<li><strong>All-Reduce</strong> ： Reduce-Scatter 和 All-Gather
的组合。</li>
</ul>
<h2 id="all-gather">All-gather</h2>
<p><strong>核心功能</strong>：将每个节点的部分数据汇总到所有节点，最终所有节点拥有<strong>完整数据</strong>副本。<br />
<strong>适用场景</strong>：模型并行中的参数同步、全局统计信息聚合。</p>]]></description>
</item>
<item>
    <title>ulysses_sequence_parallel</title>
    <link>https://blog.vllbc.top/ulysses_sequence_parallel/</link>
    <pubDate>Tue, 08 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ulysses_sequence_parallel/</guid>
    <description><![CDATA[<p>一句话：在sequence维度上进行切分</p>
<ul>
<li>将输入序列 X (长度 N) 沿序列维度切分为 SP 块，每个 GPU 分配到 N/SP
长度的子序列。
<ul>
<li>对于非注意力层 (如 MLP)，计算是完全局部的，每个 GPU
处理自己的子序列即可。
<ul>
<li>token 之间独立，token-level projection</li>
<li>Ulysses
SP的核心复杂性在于Attention层。为了让每个token在计算注意力时能够考虑到全局序列信息（或者说，让每个head在计算时能看到完整的序列，即使这个head只在当前rank计算），Attention模块前后需要进行两次精密的all-to-all数据重排。MLP层则没有这样的需求，数据在进入MLP时已经是按序列分片好的，可以直接进行本地计算。</li>
</ul></li>
<li>对于注意力层:
<ul>
<li>步骤 1 (计算 Q, K, V): 每个 GPU 基于其本地子序列计算出本地的
Q_local, K_local, V_local (维度约为 N/SP x d，d 是隐藏维度)。</li>
<li>步骤 2 (全局 K, V 收集 - 关键):
使用 <strong>All-to-All</strong> 通信操作（All-Gather??）。每个 GPU
将自己的 K_local, V_local 发送给所有其他 GPU，并接收来自所有其他 GPU 的
K, V 块。执行后，<strong>每个 GPU 拥有完整的全局 K 和 V 矩阵 (维度 N x
d)</strong>，但仍然只拥有本地的 Q_local (维度 N/SP x d)。
<ul>
<li><a
href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</a></li>
</ul></li>
<li>步骤 3 (本地注意力计算): 每个 GPU 使用其 Q_local 和完整的全局 K, V
计算其负责的那部分注意力输出 O_local (维度 N/SP x d)。计算公式为
Attention(Q_local, K_global, V_global)。这一步的计算量是 (N/SP) * N *
d，内存瓶颈在于存储临时的注意力分数矩阵，大小约为 <strong>(N/SP) *
N</strong>。相比原始的 **N*N**，内存显著降低。</li>
<li>步骤 4 (可选的输出重组):
如果后续层需要按序列拼接的完整输出，可能需要另一次通信（如 All-Gather
或另一次 All-to-All 的变种）来组合 O_local。但在 DeepSpeed
实现中，通常保持分布式状态，直接输入到下一个同样按序列并行的层。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="verl中的序列并行">verl中的序列并行</h2>
<p>在verl中，一般与remove_padding一起使用，即</p>]]></description>
</item>
<item>
    <title>fsdp</title>
    <link>https://blog.vllbc.top/fsdp/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/fsdp/</guid>
    <description><![CDATA[<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/1929115059113693341">RL
系统深思：FSDP 训练后端</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/694288870">PyTorch FSDP
设计解读</a></p>]]></description>
</item>
</channel>
</rss>
