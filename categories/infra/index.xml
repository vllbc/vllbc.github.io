<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Infra - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/infra/</link>
        <description>Infra - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/infra/" rel="self" type="application/rss+xml" /><item>
    <title>flops分析</title>
    <link>https://blog.vllbc.top/flops%E5%88%86%E6%9E%90/</link>
    <pubDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/flops%E5%88%86%E6%9E%90/</guid>
    <description><![CDATA[<blockquote>
<p>FLOPs, floating point operations,
表示浮点数运算次数，衡量了计算量的大小。 如何计算矩阵乘法的 FLOPs 呢？
对于 <span class="math inline">\(A\in R^{1\times n},B\in
R^{n\times1}\)</span> ,计算 <span class="math inline">\(AB\)</span>
需要进行 <span class="math inline">\(n\)</span> 次乘法运算和 <span
class="math inline">\(n\)</span> 次加法运算，共计 <span
class="math inline">\(2n\)</span> 次浮点数运算，需要 <span
class="math inline">\(2n\)</span> 的 FLOPs。对于 <span
class="math inline">\(A\in R^{m\times n},B\in R^{n\times p}\)</span>
,计算 <span class="math inline">\(AB\)</span> 需要的浮点数运算次数为
<span class="math inline">\(2mnp\)</span> 。</p>]]></description>
</item>
<item>
    <title>Activation checkpointing</title>
    <link>https://blog.vllbc.top/activation-checkpointing/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/activation-checkpointing/</guid>
    <description><![CDATA[<h2 id="为什么存储激活值">为什么存储激活值？</h2>
<p>首先回顾为什么要存储激活值。
简单来说，模型参数是根据导数更新的。为了有效地计算这些导数，必须缓存某些张量。激活内存是这些缓存张量的内存成本。
具体来说，以 <span class="math inline">\(f\)</span> 是矩阵乘法运算：</p>]]></description>
</item>
<item>
    <title>CPU offloading</title>
    <link>https://blog.vllbc.top/cpu-offloading/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/cpu-offloading/</guid>
    <description><![CDATA[<p><strong>CPU 卸载</strong>允许我们将一些状态传输到
CPU，因此我们不必将所有内容都保存在 GPU RAM 中。虽然 CPU作比
GPU作慢，但将不常访问的数据移动到 CPU 内存可以帮助我们保持在 GPU
内存限制范围内。</p>]]></description>
</item>
<item>
    <title>梯度累计</title>
    <link>https://blog.vllbc.top/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/</link>
    <pubDate>Thu, 10 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/</guid>
    <description><![CDATA[<p><strong>梯度累积</strong>使我们能够通过按顺序处理较小的批次来扩展到更大的有效批次。我们不是一次计算整个批次的梯度（这需要将所有激活存储在内存中），而是在更新模型参数之前将每个小批次的梯度相加。这减少了内存使用量，但需要更多的向前/向后传递。</p>]]></description>
</item>
<item>
    <title>remove_padding</title>
    <link>https://blog.vllbc.top/remove_padding/</link>
    <pubDate>Tue, 08 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/remove_padding/</guid>
    <description><![CDATA[<p>即
packing，将不同长度的序列紧凑存储，避免填充，减少不必要的计算和存储，提升效率。</p>
<h2 id="动机">动机</h2>
<p>sft进行微调，因为gpu是并行计算的，所以如果一个batch里面的数据，每条数据长度不相等，就需要对数据进行truncation（截断）和padding（pad数据到相同的seq_length）。显然，如果使用了padding，那么一个batch里面，就会有很多的pad_token，这些pad_token输入进入到了模型，但是却没有样本训练，造成了计算量的浪费。</p>]]></description>
</item>
<item>
    <title>transformer参数量分析</title>
    <link>https://blog.vllbc.top/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/</guid>
    <description><![CDATA[<p>进入大模型时代，基本上所有大模型都使用 decoder 部分，因此本文只分析
decoder 部分的参数量。 Transformer 的 decoder 每一层由 attention 和 mlp
组成，一般有 l 层。</p>
<h2 id="self-attention">Self-attention</h2>
<p>Self-attention 层由 <span class="math inline">\(W_{Q}\)</span>
、<span class="math inline">\(W_{K}\)</span>、<span
class="math inline">\(W_{V}\)</span> 和输出矩阵 <span
class="math inline">\(W_{O}\)</span> 和它们的偏置组成，权重矩阵的形状为
<span class="math inline">\([h,h]\)</span>，偏置形状为 <span
class="math inline">\([h]\)</span>，则 self-attention 部分的参数量为
<span class="math inline">\(4h^2+4h\)</span></p>]]></description>
</item>
<item>
    <title>显存占用计算</title>
    <link>https://blog.vllbc.top/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/</link>
    <pubDate>Sun, 06 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/</guid>
    <description><![CDATA[<h2 id="训练时">训练时</h2>
<ul>
<li><strong>模型参数</strong>：我们模型的可学习权重。</li>
<li><strong>Optimizer
states（优化器状态</strong>）：您需要跟踪的确切状态取决于您使用的优化器;例如，如果您使用的是 <a
href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?ref=jeremyjordan.me">AdamW</a>，则除了模型参数之外，您还需要跟踪第一和第二动量估计值。</li>
<li><strong>模型激活值</strong>：这将根据您的网络架构和批处理大小而有所不同，但会显著影响内存使用。<a
href="https://www.jeremyjordan.me/neural-networks-training">反向传播</a>需要此信息，以便我们能够有效地计算梯度。</li>
<li><strong>梯度</strong>：为模型的每个参数存储，与模型参数相同的内存占用。</li>
<li><strong>Input data</strong>：要传递给模型的 Importing
数据批次，内存占用取决于正在建模的数据的大小和类型。</li>
</ul>
<p>图示：  具体数值： 对于一个 transformer
来说，参数量可以由以下公式给出（详见 <a
href="transformer参数量分析.md">transformer参数量分析</a>）：</p>]]></description>
</item>
</channel>
</rss>
