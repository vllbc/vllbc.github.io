<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Reading - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/reading/</link>
        <description>Reading - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/reading/" rel="self" type="application/rss+xml" /><item>
    <title>Can Language Models Serve as Text-Based World Simulators?</title>
    <link>https://blog.vllbc.top/can-language-models-serve-as-text-based-world-simulators/</link>
    <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/can-language-models-serve-as-text-based-world-simulators/</guid>
    <description><![CDATA[<p>这篇论文发表于2024年6月，来自亚利桑那大学、微软研究院、艾伦人工智能研究所等顶尖机构，是一项关于大型语言模型（LLM）能力边界探索的严谨、扎实的量化研究。它并没有提出一个全新的、性能超群的模型，而是像一位严谨的实验物理学家，设计了一套精巧的实验装置，来精确测量并回答一个基础且重要的问题：<strong>当前最先进的语言模型，在多大程度上可以取代传统的手工编码，直接作为一个动态世界的“模拟器”？</strong></p>]]></description>
</item>
<item>
    <title>Group Sequence Policy Optimization</title>
    <link>https://blog.vllbc.top/group-sequence-policy-optimization/</link>
    <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/group-sequence-policy-optimization/</guid>
    <description><![CDATA[<p>这篇论文的核心贡献是提出了一种名为 <strong>组序列策略优化 (Group
Sequence Policy Optimization, GSPO)</strong>
的新型强化学习（RL）算法，旨在解决在训练大型语言模型（特别是<strong>混合专家模型,
Mixture-of-Experts,
MoE</strong>）时普遍存在的训练不稳定甚至模型崩溃的痛点。这不仅仅是一次微小的算法改进，而是一次对现有主流RL优化范式的根本性反思与重构，其核心思想是
<strong>“将优化的基本单元与奖励的基本单元对齐”</strong>。</p>]]></description>
</item>
<item>
    <title>Towards Effective Code-Integrated Reasoning</title>
    <link>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</link>
    <pubDate>Sat, 26 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/towards-effective-code-integrated-reasoning/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇名为《Towards
Effective Code-Integrated Reasoning》的论文
(arXiv:2505.24480v1)。这篇论文系统性地探讨了一个在当前大模型研究中至关重要的前沿方向：如何让模型更稳定、更有效地利用外部工具（特别是代码解释器）来完成复杂的推理任务。</p>]]></description>
</item>
<item>
    <title>Routine：A Structural Planning Framework for LLM Agent System in Enterprise</title>
    <link>https://blog.vllbc.top/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/</link>
    <pubDate>Fri, 25 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/</guid>
    <description><![CDATA[<h3
id="论文深度解读从混沌到有序routine框架如何驯服企业级ai智能体"><strong>论文深度解读：从“混沌”到“有序”，Routine框架如何驯服企业级AI智能体</strong></h3>
<p>当前，以大型语言模型（LLM）为核心的自主智能体（Autonomous
Agents）正以前所未有的速度发展，展现出在数据分析、人机交互等领域的巨大潜力。然而，当我们将这些通用智能体置于规则严密、流程复杂的企业环境中时，往往会遭遇“水土不服”的窘境。论文开篇就指出了这一核心挑战：</p>]]></description>
</item>
<item>
    <title>Search and Refine During Think：Autonomous Retrieval - Augmented Reasoning of LLMs</title>
    <link>https://blog.vllbc.top/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/</link>
    <pubDate>Sun, 20 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/</guid>
    <description><![CDATA[<h3
id="论文深度解读从思考时搜索到思考时搜索并提炼的范式革命">论文深度解读：从“思考时搜索”到“思考时搜索并提炼”的范式革命</h3>
<p>这篇论文的核心贡献在于，它挑战了传统检索增强生成（RAG）系统中一个根深蒂固的、看似理所当然的流程，并提出了一种更为精细、鲁棒和智能的替代范式。传统的
RAG
模型通常遵循一种“<strong>思考时搜索</strong>”（search-during-think）的模式：当模型在生成答案的过程中意识到知识不足时，它会触发一次或多次搜索，获取外部文档，然后直接基于这些（可能混杂着大量噪声的）文档来生成最终答案。这种方法的致命弱点在于，它假设模型能够自行从混杂、冗长甚至可能错误的信息中精准地“淘金”，而现实是，这种“信息投喂”常常导致“<strong>垃圾进，垃圾出</strong>”（garbage
in, garbage out）的困境。</p>]]></description>
</item>
<item>
    <title>Peri-LN：Revisiting Normalization Layer in the Transformer Architecture</title>
    <link>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</link>
    <pubDate>Sat, 19 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇关于
Transformer 架构中归一化策略的优秀论文——《Peri-LN: Revisiting
Normalization Layer in the Transformer Architecture》。</p>]]></description>
</item>
<item>
    <title>ZEROSEARCH：Incentivize the Search Capability of LLMs without Searching</title>
    <link>https://blog.vllbc.top/zerosearchincentivize-the-search-capability-of-llms-without-searching/</link>
    <pubDate>Fri, 18 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/zerosearchincentivize-the-search-capability-of-llms-without-searching/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇富有启发性的论文《ZEROSEARCH:
Incentivize the Search Capability of LLMs without Searching》。</p>
<p>这篇论文的核心思想极其巧妙，它直击了当前训练“搜索智能体（Search
Agent）”LLM 时最头疼的两个问题：高昂的 API
调用成本和不可控的搜索结果质量。传统的做法是让 LLM
在训练时与真实的搜索引擎（如谷歌）进行交互，通过强化学习（RL）来学习何时搜索、搜索什么以及如何利用搜索结果。但这就像让一个新手司机直接在高峰期的纽约街头学开车，不仅成本高昂（每次“练习”都要花钱），而且路况复杂多变（搜索结果时好时坏），很容易让模型“学坏”或者干脆放弃学习。</p>]]></description>
</item>
<item>
    <title>BRiTE：Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning</title>
    <link>https://blog.vllbc.top/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/</guid>
    <description><![CDATA[<p>好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇富有洞见的论文《BRITE:
Bootstrapping Reinforced Thinking Process to Enhance Language Model
Reasoning》。这篇论文确实触及了当前大模型领域一个核心且棘手的问题：如何让模型不仅能生成流畅的文本，更能进行可靠、严谨的逻辑推理。</p>]]></description>
</item>
<item>
    <title>PROCESS REINFORCEMENT THROUGH IMPLICIT REWARDS</title>
    <link>https://blog.vllbc.top/process-reinforcement-through-implicit-rewards/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/process-reinforcement-through-implicit-rewards/</guid>
    <description><![CDATA[<h3 id="论文深入解读">论文深入解读</h3>
<p>这篇名为《Process Reinforcement through Implicit
Rewards》(通过隐式奖励进行过程强化)
的论文，由来自清华大学、上海人工智能实验室、UIUC
等顶尖机构的研究者共同完成，为大语言模型（LLM）的强化学习（RL）领域带来了一个极具价值和创新性的解决方案——<strong>PRIME</strong>
框架。其核心贡献在于，它成功地将<strong>过程监督 (Process
Supervision)</strong> 的高效率与<strong>结果监督 (Outcome
Supervision)</strong>
的低成本相结合，解决了在复杂推理任务（如数学和编程）中应用强化学习时面临的关键瓶颈。</p>]]></description>
</item>
<item>
    <title>Search-R1：Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title>
    <link>https://blog.vllbc.top/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/</link>
    <pubDate>Wed, 16 Jul 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/</guid>
    <description><![CDATA[<p>好的，作为大模型领域的学术专家，我非常乐于为您深入解读这篇具有重要意义的论文《SEARCH-R
1: Training LLMs to Reason and Leverage Search Engines with
Reinforcement Learning》。</p>
<p>这篇论文的核心，是探索如何让大型语言模型（LLM）<strong>学会</strong>像人类专家一样，在解决复杂问题时，<strong>主动、智能、且迭代地使用搜索引擎</strong>。它不仅仅是简单地把搜索结果“喂”给模型，而是通过<strong>强化学习（Reinforcement
Learning,
RL）</strong>，训练模型形成一种内在的“研究”能力——知道<strong>什么时候</strong>需要信息，需要<strong>什么</strong>信息，以及如何<strong>整合</strong>这些信息来形成最终答案。</p>]]></description>
</item>
</channel>
</rss>
