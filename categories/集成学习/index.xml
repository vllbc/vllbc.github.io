<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>集成学习 - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link>
        <description>集成学习 - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 01 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" rel="self" type="application/rss+xml" /><item>
    <title>xgboost</title>
    <link>https://blog.vllbc.top/xgboost/</link>
    <pubDate>Wed, 01 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/xgboost/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>Ensemble Learning</title>
    <link>https://blog.vllbc.top/ensemble-learning/</link>
    <pubDate>Sat, 09 Jul 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/ensemble-learning/</guid>
    <description><![CDATA[<h1 id="集成学习">集成学习</h1>
<p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。
集成学习在各个规模的数据集上都有很好的策略。
数据集大：划分成多个小数据集，学习多个模型进行组合
数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</p>]]></description>
</item>
<item>
    <title>Adaboost</title>
    <link>https://blog.vllbc.top/adaboost/</link>
    <pubDate>Wed, 27 Apr 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/adaboost/</guid>
    <description><![CDATA[<p>Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>]]></description>
</item>
<item>
    <title>随机森林</title>
    <link>https://blog.vllbc.top/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</link>
    <pubDate>Tue, 15 Mar 2022 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</guid>
    <description><![CDATA[
]]></description>
</item>
<item>
    <title>Stacking</title>
    <link>https://blog.vllbc.top/stacking/</link>
    <pubDate>Thu, 25 Nov 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/stacking/</guid>
    <description><![CDATA[<h1 id="stacking">Stacking</h1>
<h2 id="思想简介">思想简介</h2>
<p>简单得理解，就是对于多个学习器，分别对结果进行预测，然后将预测的结果作为特征，再对结果进行预测。
上一张经典的图： 
以这个5折stacking为例：</p>]]></description>
</item>
<item>
    <title>GBDT</title>
    <link>https://blog.vllbc.top/gbdt/</link>
    <pubDate>Wed, 06 Jan 2021 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/gbdt/</guid>
    <description><![CDATA[<h1 id="梯度提升决策树gbdt">梯度提升决策树(GBDT)</h1>
<p>GBDT<strong>(Gradient Boosting Decision Tree)</strong>是一种迭代的<a
href="https://so.csdn.net/so/search?q=决策树&amp;spm=1001.2101.3001.7020">决策树</a>算法，由多棵决策树组成，所有树的结论累加起来作为最终答案。</p>
<h2 id="回归树">回归树</h2>
<p>选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小
值时的(j,s)对。其中Rm是被划分的输入空间， <span
class="math inline">\(\mathrm{cm}\)</span>
是空间Rm对应的固定输出值。</p>]]></description>
</item>
</channel>
</rss>
