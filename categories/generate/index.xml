<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Generate - 分类 - vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/categories/generate/</link>
        <description>Generate - 分类 - vllbc02&#39;s blogs</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 09 Mar 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.vllbc.top/categories/generate/" rel="self" type="application/rss+xml" /><item>
    <title>generate</title>
    <link>https://blog.vllbc.top/generate/</link>
    <pubDate>Sun, 09 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate/</guid>
    <description><![CDATA[<p>理论部分在这：<a
href="../../LLM/inference/generate相关.md">generate相关</a> ##
generate参数</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        inputs: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        generation_config: Optional[GenerationConfig] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        logits_processor: Optional[LogitsProcessorList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        stopping_criteria: Optional[StoppingCriteriaList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        prefix_allowed_tokens_fn: Optional[Callable[[<span class="bu">int</span>, torch.Tensor], List[<span class="bu">int</span>]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        synced_gpus: Optional[<span class="bu">bool</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        assistant_model: Optional[<span class="st">&quot;PreTrainedModel&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        streamer: Optional[<span class="st">&quot;BaseStreamer&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        negative_prompt_ids: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        negative_prompt_attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Union[GenerateOutput, torch.LongTensor]:</span></code></pre></div>
<p>在代码中可以看到在函数入口显式的定义了很多参数。他们的具体含义如下</p>]]></description>
</item>
<item>
    <title>frequency_penalty&amp;presence_penalty</title>
    <link>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</link>
    <pubDate>Thu, 05 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate%E7%9B%B8%E5%85%B3/</guid>
    <description><![CDATA[<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>LLM解码时采用的自回归采样，其过程如下：</p>
<ol type="1">
<li>小模型使用前缀作为输入，将输出结果处理+归一化成<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83&amp;zhida_source=entity">概率分布</a>后，采样生成下一个token。</li>
<li>将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。</li>
</ol>
<p>将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、<a
href="https://zhida.zhihu.com/search?content_id=232876036&amp;content_type=Article&amp;match_order=1&amp;q=top-k&amp;zhida_source=entity">top-k</a>和top-n等
# 贪心搜索
直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复
# 随机采样
按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
# beam search 维护一个大小为 k
的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k
个单词，然后保留总概率最高的 k
个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。
# top-k </p>]]></description>
</item>
</channel>
</rss>
