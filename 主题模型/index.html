<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>主题模型 - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="主题模型" />
<meta property="og:description" content="主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 词袋模型 将所有词语装进一个袋子里，不考虑其" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" /><meta property="og:image" content="https://vllbc.top/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-04-09T00:00:00+00:00" /><meta property="og:site_name" content="vllbc02" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://vllbc.top/logo.png"/>

<meta name="twitter:title" content="主题模型"/>
<meta name="twitter:description" content="主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 词袋模型 将所有词语装进一个袋子里，不考虑其"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" /><link rel="prev" href="https://vllbc.top/transformer/" /><link rel="next" href="https://vllbc.top/knn/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "主题模型",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/vllbc.top\/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "NLP, 主题模型","wordcount":  11377 ,
        "url": "https:\/\/vllbc.top\/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\/","datePublished": "2022-06-16T00:00:00+00:00","dateModified": "2023-04-09T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">主题模型</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-06-16">2022-06-16</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 11377 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 23 分钟&nbsp;<span id="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" class="leancloud_visitors" data-flag-title="主题模型">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#主题模型">主题模型</a>
      <ul>
        <li><a href="#词袋模型">词袋模型</a></li>
        <li><a href="#lsa">LSA</a>
          <ul>
            <li><a href="#单词-文本矩阵">单词-文本矩阵</a></li>
            <li><a href="#主题-文本矩阵">主题-文本矩阵</a></li>
            <li><a href="#从单词向量空间到主题向量空间的线性变换">从单词向量空间到主题向量空间的线性变换</a></li>
            <li><a href="#潜在语义分析">潜在语义分析</a></li>
          </ul>
        </li>
        <li><a href="#plsa">PLSA</a>
          <ul>
            <li><a href="#生成模型">生成模型</a></li>
            <li><a href="#共现模型">共现模型</a></li>
            <li><a href="#与潜在语义分析的关系">与潜在语义分析的关系</a></li>
            <li><a href="#概率潜在语义分析的算法">概率潜在语义分析的算法</a></li>
            <li><a href="#总结算法">总结算法</a></li>
            <li><a href="#用法">用法</a></li>
            <li><a href="#优点与不足">优点与不足</a></li>
          </ul>
        </li>
        <li><a href="#lda">LDA</a>
          <ul>
            <li><a href="#多项分布">多项分布</a></li>
            <li><a href="#狄利克雷分布">狄利克雷分布</a></li>
            <li><a href="#二项分布与贝塔分布">二项分布与贝塔分布</a></li>
            <li><a href="#基本想法">基本想法</a></li>
            <li><a href="#lda与plsa的关系">LDA与PLSA的关系</a></li>
            <li><a href="#模型定义">模型定义</a></li>
            <li><a href="#概率计算">概率计算</a></li>
            <li><a href="#吉布斯抽样">吉布斯抽样</a></li>
            <li><a href="#训练与推断">训练与推断</a></li>
            <li><a href="#代码">代码</a></li>
            <li><a href="#本质与使用条件">本质与使用条件</a></li>
          </ul>
        </li>
        <li><a href="#总结-1">总结</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content">
<h1 id="主题模型">主题模型</h1>
<p>主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的</p>
<h2 id="词袋模型">词袋模型</h2>
<p>将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的</p>
<p>例子：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>句子<span class="dv">1</span>：我 爱 北 京 天 安 门</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>转换为 [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>句子<span class="dv">2</span>：我 喜 欢 上 海</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>转换为 [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;This is the first document.&#39;</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;This document is the second document.&#39;</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;And this is the third one.&#39;</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Is this the first document?&#39;</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>vectorizer.fit_transform(corpus).toarray()</span></code></pre></div>
<p>结果：</p>
<pre><code>[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]</code></pre>
<h2 id="lsa">LSA</h2>
<p>LSA就是潜在语义分析。特点是通过矩阵分解发现文本与单词之间基于主题（话题）的语义关系。
首先要清楚几个概念：</p>
<h3 id="单词-文本矩阵">单词-文本矩阵</h3>
<p><span class="math display">\[
X=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 n} \\\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 n} \\\\
\vdots &amp; \vdots &amp; &amp; \vdots \\\\
x_{m 1} &amp; x_{m 2} &amp; \cdots &amp; x_{m n}
\end{array}\right]
\]</span></p>
<p>这是一个 <span class="math inline">\(m \times n\)</span> 矩阵, 元素
<span class="math inline">\(x_{i j}\)</span> 表示单词 <span class="math inline">\(w_i\)</span> 在文本 <span class="math inline">\(d_j\)</span> 中出现的频数或权值。由于单
词的种类很多, 而每个文本中出现单词的种类通常较少,
所以单词-文本矩阵是一个稀 疏矩阵。 权值通常用单词频率-逆文本频率 (term
frequency-inverse document frequency, TF-IDF）表示，其定义是</p>
<p><span class="math display">\[
\operatorname{TFIDF}_{i j}=\frac{\mathrm{tf}_{i j}}{\mathrm{tf}_{\bullet
j}} \log \frac{\mathrm{df}}{\mathrm{df}_i}, \quad i=1,2, \cdots, m ;
\quad j=1,2, \cdots, n
\]</span></p>
<p>直观上讲，可以直接用每一列作为文本语义表达，
因此可以通过余弦相似度等计算文本之间的相似性，并且矩阵稀疏，计算量较少。但其并不关心文本中词语出现的顺序等信息，因此需要改进。
### 单词-主题矩阵 假设所有文本共含有 <span class="math inline">\(k\)</span>
个话题。假设每个话题由一个定义在单词集合 <span class="math inline">\(W\)</span> 上的 <span class="math inline">\(m\)</span> 维向量表示, 称为话题向量, 即</p>
<p><span class="math display">\[
t_l=\left[\begin{array}{c}
t_{1 l} \\\\
t_{2 l} \\\\
\vdots \\\\
t_{m l}
\end{array}\right], \quad l=1,2, \cdots, k
\]</span></p>
<p>其中 <span class="math inline">\(t_{i l}\)</span> 是单词 <span class="math inline">\(w_i\)</span> 在话题 <span class="math inline">\(t_l\)</span> 的权值, <span class="math inline">\(i=1,2, \cdots, m\)</span>, 权值越大,
该单词在该话题中 的重要度就越高。这 <span class="math inline">\(k\)</span> 个话题向量 <span class="math inline">\(t_1, t_2, \cdots, t_k\)</span>
张成一个话题向量空间 (topic vector 话题向量空间 <span class="math inline">\(T\)</span> 也可以表示为一个矩阵, 称为单词-主题矩阵
(word-topic matrix）, 记作</p>
<p><span class="math display">\[
T=\left[\begin{array}{cccc}
t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1 k} \\\\
t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2 k} \\\\
\vdots &amp; \vdots &amp; &amp; \vdots \\\\
t_{m 1} &amp; t_{m 2} &amp; \cdots &amp; t_{m k}
\end{array}\right]
\]</span></p>
<h3 id="主题-文本矩阵">主题-文本矩阵</h3>
<p>将单词-文本矩阵中的文本<span class="math inline">\(x_j\)</span>投影到主题向量空间<span class="math inline">\(J\)</span>中，得到在主题空间中的一个向量<span class="math inline">\(y_j\)</span>。</p>
<p><span class="math display">\[
Y=\left[\begin{array}{cccc}
y_{11} &amp; y_{12} &amp; \cdots &amp; y_{1 n} \\\\
y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2 n} \\\\
\vdots &amp; \vdots &amp; &amp; \vdots \\\\
y_{k 1} &amp; y_{k 2} &amp; \cdots &amp; y_{k n}
\end{array}\right]
\]</span></p>
<h3 id="从单词向量空间到主题向量空间的线性变换">从单词向量空间到主题向量空间的线性变换</h3>
<p>单词-文本矩阵<span class="math inline">\(X\)</span>可以近似表示为单词-主题矩阵<span class="math inline">\(T\)</span>与主题-文本矩阵<span class="math inline">\(Y\)</span>的乘积，这就是潜在语义分析：</p>
<p><span class="math display">\[
X\approx TY
\]</span></p>
<h3 id="潜在语义分析">潜在语义分析</h3>
<p>给定单词-文本矩阵<span class="math inline">\(X\)</span>，每一行代表一个单词，每一列代表一个文本。其中的元素代表单词在文本中的权重或者频数（词袋模型）。</p>
<h4 id="截断奇异值分析">截断奇异值分析</h4>
<p><span class="math display">\[
X \approx U_k \Sigma_k V_k^{\mathrm{T}}=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 &amp; 0 &amp; 0 &amp; 0 \\\\
0 &amp; \sigma_2 &amp; 0 &amp; 0 \\\\
0 &amp; 0 &amp; \ddots &amp; 0 \\\\
0 &amp; 0 &amp; 0 &amp; \sigma_k
\end{array}\right]\left[\begin{array}{c}
v_1^{\mathrm{T}} \\\\
v_2^{\mathrm{T}} \\\\
\vdots \\\\
v_k^{\mathrm{T}}
\end{array}\right]
\]</span></p>
<p>接下来考虑文本在主题空间中的表示。</p>
<p><span class="math display">\[
\begin{aligned}
X &amp;=\left[\begin{array}{llll}
x_1 &amp; x_2 &amp; \cdots &amp; x_n
\end{array}\right] \approx U_k \Sigma_k V_k^{\mathrm{T}} \\\\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 &amp; &amp; &amp; \\\\
&amp; \sigma_2 &amp; 0 &amp; \\\\
0 &amp; \ddots &amp; \\\\
&amp; &amp; \sigma_k
\end{array}\right]\left[\begin{array}{cccc}
v_{11} &amp; v_{21} &amp; \cdots &amp; v_{n 1} \\\\
v_{12} &amp; v_{22} &amp; \cdots &amp; v_{n 2} \\\\
\vdots &amp; \vdots &amp; &amp; \vdots \\\\
v_{1 k} &amp; v_{2 k} &amp; \cdots &amp; v_{n k}
\end{array}\right] \\\\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 v_{11} &amp; \sigma_1 v_{21} &amp; \cdots &amp; \sigma_1 v_{n
1} \\\\
\sigma_2 v_{12} &amp; \sigma_2 v_{22} &amp; \cdots &amp; \sigma_2 v_{n
2} \\\\
\vdots &amp; \vdots &amp; &amp; \vdots \\\\
\sigma_k v_{1 k} &amp; \sigma_k v_{2 k} &amp; \cdots &amp; \sigma_k v_{n
k}
\end{array}\right]
\end{aligned}
\]</span></p>
<p>其中:</p>
<p><span class="math display">\[
u_l = \begin{bmatrix}u_{1l} \\\\u_{2l} \\\\ \vdots \\\\u_{ml}
\end{bmatrix}, \quad l= 1, 2, \dots, k
\]</span></p>
<p>代表单词对主题的权重。</p>
<p>由式知, 矩阵 <span class="math inline">\(X\)</span> 的第 <span class="math inline">\(j\)</span> 列向量 <span class="math inline">\(x_j\)</span> 满足</p>
<p><span class="math display">\[
\begin{aligned}
x_j &amp; \approx U_k\left(\Sigma_k V_k^{\mathrm{T}}\right)\_j \\\\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{c}
\sigma_1 v_{j 1} \\\\
\sigma_2 v_{j 2} \\\\
\vdots \\\\
\sigma_k v_{j k}
\end{array}\right] \\\\
&amp;=\sum_{l=1}^k \sigma_l v_{j l} u_l, \quad j=1,2, \cdots, n
\end{aligned}
\]</span></p>
<p>则<span class="math inline">\(\Sigma_kV_k^T\)</span>每一个列向量是一个文本在主题向量空间中的表示。</p>
<h2 id="plsa">PLSA</h2>
<h3 id="生成模型">生成模型</h3>
<p>假设有单词集合 <span class="math inline">\(W={w_1, w_2, \cdots,
w_M}\)</span>, 其中 <span class="math inline">\(M\)</span> 是单词个数;
文本 (指标) 集 合 <span class="math inline">\(D={d_1, d_2, \cdots,
d_N}\)</span>, 其中 <span class="math inline">\(N\)</span> 是文本个数;
话题集合 <span class="math inline">\(Z={z_1, z_2, \cdots, z_K}\)</span>,
其中 <span class="math inline">\(K\)</span>
是预先设定的话题个数。随机变量 <span class="math inline">\(w\)</span>
取值于单词集合; 随机变量 <span class="math inline">\(d\)</span>
取值于文本集 合, 随机变量 <span class="math inline">\(z\)</span>
取值于话题集合。概率分布 <span class="math inline">\(P(d)\)</span>
、条件概率分布 <span class="math inline">\(P(z \mid d)\)</span>
、条件概率分 布 <span class="math inline">\(P(w \mid z)\)</span>
皆属于多项分布, 其中 <span class="math inline">\(P(d)\)</span>
表示生成文本 <span class="math inline">\(d\)</span> 的概率, <span class="math inline">\(P(z \mid d)\)</span> 表示文本 <span class="math inline">\(d\)</span> 生 成话题 <span class="math inline">\(z\)</span> 的概率, <span class="math inline">\(P(w
\mid z)\)</span> 表示话题 <span class="math inline">\(z\)</span>
生成单词 <span class="math inline">\(w\)</span> 的概率。</p>
<p>每个文本 <span class="math inline">\(d\)</span>
拥有自己的话题概率分布 <span class="math inline">\(P(z \mid d)\)</span>,
每个话题 <span class="math inline">\(z\)</span> 拥有自己的单词概率分 布
<span class="math inline">\(P(w \mid z)\)</span>;
也就是说一个文本的内容由其相关话题决定,
一个话题的内容由其相关单词决定。</p>
<p>生成模型通过以下步骤生成文本-单词共现数据: (1) 依据概率分布 <span class="math inline">\(P(d)\)</span>, 从文本 (指标)
集合中随机选取一个文本 <span class="math inline">\(d\)</span>, 共生成
<span class="math inline">\(N\)</span> 个文本; 针对每个文本,
执行以下操作; (2) 在文本 <span class="math inline">\(d\)</span>
给定条件下, 依据条件概率分布 <span class="math inline">\(P(z \mid
d)\)</span>, 从话题集合随机选取一个 话题 <span class="math inline">\(z\)</span>, 共生成 <span class="math inline">\(L\)</span> 个话题, 这里 <span class="math inline">\(L\)</span> 是文本长度; (3) 在话题 <span class="math inline">\(z\)</span> 给定条件下, 依据条件概率分布 <span class="math inline">\(P(w \mid z)\)</span>, 从单词集合中随机选取一
个单词 <span class="math inline">\(w\)</span> 。</p>
<p>生成模型中, 单词变量 <span class="math inline">\(w\)</span>
与文本变量 <span class="math inline">\(d\)</span> 是观测变量, 话题变量
<span class="math inline">\(z\)</span> 是隐变量。也就
是说模型生成的是单词-话题-文本三元组 <span class="math inline">\((w, z,
d)\)</span> 的集合, 但观测到的是单词-文本二 元组 <span class="math inline">\((w, d)\)</span> 的集合,
观测数据表示为单词-文本矩阵 <span class="math inline">\(T\)</span>
的形式, 矩阵 <span class="math inline">\(T\)</span> 的行表示单词,
列表示文本, 元素表示单词-文本对 <span class="math inline">\((w,
d)\)</span> 的出现次数。</p>
<p>从数据的生成过程可以推出, 文本-单词共现数据 <span class="math inline">\(T\)</span> 的生成概率为所有单词-文本 对 <span class="math inline">\((w, d)\)</span> 的生成概率的乘积,</p>
<p><span class="math display">\[
P(T)=\prod_{(w, d)} P(w, d)^{n(w, d)}
\]</span></p>
<p>这里 <span class="math inline">\(n(w, d)\)</span> 表示 <span class="math inline">\((w, d)\)</span> 的出现次数,
单词-文本对出现的总次数是 <span class="math inline">\(N \times
L\)</span> 。每个单 词-文本对 <span class="math inline">\((w,
d)\)</span> 的生成概率由以下公式决定:</p>
<p><span class="math display">\[
\begin{aligned}
P(w, d) &amp;=P(d) P(w \mid d) \\\\
&amp;=P(d) \sum_z P(w, z \mid d) \\\\
&amp;=P(d) \sum_z P(z \mid d) P(w \mid z)
\end{aligned}
\]</span></p>
<p>即生成模型的定义。 生成模型假设在话题 <span class="math inline">\(z\)</span> 给定条件下, 单词 <span class="math inline">\(w\)</span> 与文本 <span class="math inline">\(d\)</span> 条件独立, 即</p>
<p><span class="math display">\[
P(w, z \mid d)=P(z \mid d) P(w \mid z)
\]</span></p>
<p><img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221012121216.png"/></p>
<h3 id="共现模型">共现模型</h3>
<p><span class="math display">\[
P(T)=\prod_{(w, d)} P(w, d)^{n(w, d)}
\]</span></p>
<p>每个单词-文本对 <span class="math inline">\((w, d)\)</span>
的概率由以下公式决定:</p>
<p><span class="math display">\[
P(w, d)=\sum_{z \in Z} P(z) P(w \mid z) P(d \mid z)
\]</span></p>
<p>式 (18.5) 即共现模型的定义。容易验证, 生成模型 (18.2) 和共现模型
(18.5) 是等价的。 共现模型假设在话题 <span class="math inline">\(z\)</span> 给定条件下, 单词 <span class="math inline">\(w\)</span> 与文本 <span class="math inline">\(d\)</span> 是条件独立的, 即</p>
<p><span class="math display">\[
P(w, d \mid z)=P(w \mid z) P(d \mid z)
\]</span></p>
<p>直观解释： <img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221014225958.png"/></p>
<h3 id="与潜在语义分析的关系">与潜在语义分析的关系</h3>
<p>共现模型也可以表示为三个矩阵乘积的形式。这样, 概率潜在语义分析与
潜在语义分析的对应关系可以从中看得很清楚。下面是共现模型的矩阵乘积形式:</p>
<p><span class="math display">\[
\begin{aligned}
X^{\prime} &amp;=U^{\prime} \Sigma^{\prime} V^{\prime \mathrm{T}} \\\\
X^{\prime} &amp;=[P(w, d)]\_{M \times N} \\\\
U^{\prime} &amp;=[P(w \mid z)]\_{M \times K} \\\\
\Sigma^{\prime} &amp;=[P(z)]\_{K \times K} \\\\
V^{\prime} &amp;=[P(d \mid z)]\_{N \times K}
\end{aligned}
\]</span></p>
<h3 id="概率潜在语义分析的算法">概率潜在语义分析的算法</h3>
<p>Plsa是含有隐变量的模型，其学习通常使用EM算法。
E步是计算Q函数，M步是极大化Q函数。</p>
<p>设单词集合为 <span class="math inline">\(W={w_1, w_2, \cdots,
w_M}\)</span>, 文本集合为 <span class="math inline">\(D={d_1, d_2,
\cdots, d_N}\)</span>, 话 题集合为 <span class="math inline">\(Z={z_1,
z_2, \cdots, z_K}\)</span> 。给定单词-文本共现数据 <span class="math inline">\(T={n\left(w_i, d_j\right)}, i=\)</span> <span class="math inline">\(1,2, \cdots, M, j=1,2, \cdots, N\)</span>,
目标是估计概率潜在语义分析模型（生成模型）的 参数。如果使用极大似然估计,
对数似然函数是</p>
<p><span class="math display">\[
\begin{aligned}
L &amp;=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \log
P\left(w_i, d_j\right) \\\\
&amp;=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \log
\left[\sum_{k=1}^K P\left(w_i \mid z_k\right) P\left(z_k \mid
d_j\right)\right]
\end{aligned}
\]</span></p>
<p>但是模型含有隐变量, 对数似然函数的优化无法用解析方法求解, 这时使用
EM算法。 应用 EM算法的核心是定义 <span class="math inline">\(Q\)</span>
函数。 <span class="math inline">\(\mathrm{E}\)</span> 步：计算 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q\)</span>
函数为完全数据的对数似然函数对不完全数据的条件分布的期望。针对概率潜
在语义分析的生成模型, <span class="math inline">\(Q\)</span> 函数是</p>
<p><span class="math display">\[
Q=\sum_{k=1}^K{\sum_{j=1}^N n\left(d_j\right)\left[\log
P\left(d_j\right)+\sum_{i=1}^M \frac{n\left(w_i,
d_j\right)}{n\left(d_j\right)} \log P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)\right]} P\left(z_k \mid w_i, d_j\right)
\]</span></p>
<p>式中 <span class="math inline">\(n\left(d_j\right)=\sum_{i=1}^M
n\left(w_i, d_j\right)\)</span> 表示文本 <span class="math inline">\(d_j\)</span> 中的单词个数, <span class="math inline">\(n\left(w_i, d_j\right)\)</span> 表示单词 <span class="math inline">\(w_i\)</span> 在文本 <span class="math inline">\(d_j\)</span> 中出现的次数。条件概率分布 <span class="math inline">\(P\left(z_k \mid w_i, d_j\right)\)</span>
代表不完全数据, 是已知变量。条件概 率分布 <span class="math inline">\(P\left(w_i \mid z_k\right)\)</span> 和 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span>
的乘积代表完全数据, 是末知变量。 由于可以从数据中直接统计得出 <span class="math inline">\(P\left(d_j\right)\)</span> 的估计, 这里只考虑
<span class="math inline">\(P\left(w_i \mid z_k\right), P\left(z_k \mid
d_j\right)\)</span> 的估计, 可将 <span class="math inline">\(Q\)</span>
函数简化为函数 <span class="math inline">\(Q^{\prime}\)</span></p>
<p><span class="math display">\[
Q^{\prime}=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \sum_{k=1}^K
P\left(z_k \mid w_i, d_j\right) \log \left[P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)\right]
\]</span></p>
<p><span class="math inline">\(Q^{\prime}\)</span> 函数中的 <span class="math inline">\(P\left(z_k \mid w_i, d_j\right)\)</span>
可以根据贝叶斯公式计算</p>
<p><span class="math display">\[
P\left(z_k \mid w_i, d_j\right)=\frac{P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)}{\sum_{k=1}^K P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)}
\]</span></p>
<p>其中 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span>
和 <span class="math inline">\(P\left(w_i \mid z_k\right)\)</span>
由上一步迭代得到。 <span class="math inline">\(\mathrm{M}\)</span> 步:
极大化 <span class="math inline">\(Q\)</span> 函数。 通过约束最优化求解
<span class="math inline">\(Q\)</span> 函数的极大值, 这时 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span> 和 <span class="math inline">\(P\left(w_i \mid z_k\right)\)</span> 是变量。因为
变量 <span class="math inline">\(P\left(w_i \mid z_k\right), P\left(z_k
\mid d_j\right)\)</span> 形成概率分布, 满足约束条件</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_{i=1}^M P\left(w_i \mid z_k\right)=1, \quad k=1,2, \cdots, K
\\\\
&amp;\sum_{k=1}^K P\left(z_k \mid d_j\right)=1, \quad j=1,2, \cdots, N
\end{aligned}
\]</span></p>
<p>应用拉格朗日法, 引入拉格朗日乘子 <span class="math inline">\(\tau_k\)</span> 和 <span class="math inline">\(\rho_j\)</span>, 定义拉格朗日函数 <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\Lambda=Q^{\prime}+\sum_{k=1}^K \tau_k\left(1-\sum_{i=1}^M P\left(w_i
\mid z_k\right)\right)+\sum_{j=1}^N \rho_j\left(1-\sum_{k=1}^K
P\left(z_k \mid d_j\right)\right)
\]</span></p>
<p>将拉格朗日函数 <span class="math inline">\(\Lambda\)</span> 分别对
<span class="math inline">\(P\left(w_i \mid z_k\right)\)</span> 和 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span> 求偏导数,
并令其等于 0 , 得到下面 的方程组</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_{j=1}^N n\left(w_i, d_j\right) P\left(z_k \mid w_i,
d_j\right)-\tau_k P\left(w_i \mid z_k\right)=0, \quad i=1,2, \cdots, M ;
\quad k=1,2, \cdots, K \\\\
&amp;\sum_{i=1}^M n\left(w_i, d_j\right) P\left(z_k \mid w_i,
d_j\right)-\rho_j P\left(z_k \mid d_j\right)=0, \quad j=1,2, \cdots, N ;
\quad k=1,2, \cdots, K
\end{aligned}
\]</span></p>
<p>解方程组得到 <span class="math inline">\(M\)</span>
步的参数估计公式:</p>
<p><span class="math display">\[
P\left(w_i \mid z_k\right)=\frac{\sum_{j=1}^N n\left(w_i, d_j\right)
P\left(z_k \mid w_i, d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N n\left(w_m,
d_j\right) P\left(z_k \mid w_m, d_j\right)}
\]</span></p>
<p><span class="math display">\[
P(z_k\mid d_j) = \frac{\sum_{i=1}^Mn(w_i, d_j)P(z_k\mid
w_i,d_j)}{n(d_j)}
\]</span></p>
<h3 id="总结算法">总结算法</h3>
<p>输入: 设单词集合为 <span class="math inline">\(W={w_1, w_2, \cdots,
w_M}\)</span>, 文本集合为 <span class="math inline">\(D={d_1, d_2,
\cdots, d_N}\)</span>, 话题集合为 <span class="math inline">\(Z={z_1,
z_2, \cdots, z_K}\)</span>, 共现数据 <span class="math inline">\({n\left(w_i, d_j\right)}, i=1,2, \cdots, M,
j=1\)</span>, <span class="math inline">\(2, \cdots, N\)</span>; 输出:
<span class="math inline">\(P\left(w_i \mid z_k\right)\)</span> 和 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span> 。 (1)
设置参数 <span class="math inline">\(P\left(w_i \mid z_k\right)\)</span>
和 <span class="math inline">\(P\left(z_k \mid d_j\right)\)</span>
的初始值。 (2) 迭代执行以下 <span class="math inline">\(\mathrm{E}\)</span> 步, <span class="math inline">\(\mathrm{M}\)</span> 步, 直到收敛为止。 <span class="math inline">\(\mathrm{E}\)</span> 步:</p>
<p><span class="math display">\[
P\left(z_k \mid w_i, d_j\right)=\frac{P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)}{\sum_{k=1}^K P\left(w_i \mid z_k\right)
P\left(z_k \mid d_j\right)}
\]</span></p>
<p>M 步:</p>
<p><span class="math display">\[
\begin{aligned}
P\left(w_i \mid z_k\right) &amp;=\frac{\sum_{j=1}^N n\left(w_i,
d_j\right) P\left(z_k \mid w_i, d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N
n\left(w_m, d_j\right) P\left(z_k \mid w_m, d_j\right)} \\\\
P\left(z_k \mid d_j\right) &amp;=\frac{\sum_{i=1}^M n\left(w_i,
d_j\right) P\left(z_k \mid w_i, d_j\right)}{n\left(d_j\right)}
\end{aligned}
\]</span></p>
<h3 id="用法">用法</h3>
<p>与LSA类似，可以把文档对各个主题的概率看作是文档的表示，最后用到的就是<span class="math inline">\(P(z_k\mid d_j)\)</span>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221018125347.png"/>
k就是我们自己设定的主题数，一般来说K远远小于文档个数和词汇表大小，这样也达到了降维的目的。</p>
<h3 id="优点与不足">优点与不足</h3>
<h4 id="优点">优点</h4>
<p>pLSA是在一套比较完整的思想的基础上提出来的，模型中各项参数有明确的物理含义，可解释性比较强。相比LSA，pLSA对人类生成文本机制的刻画更加细致、更加符合我们的常识，比如，pLSA基于条件概率，引入了一个“隐含变量”（相对于可以看到的文档和词语，是不可观测变的），即主题，来描述文本生成的过程。
#### 不足 pLSA的理论与我们的实践不是那么的统一: (1)
我们说话的时候，根本不会考虑” 我说这段话的概率大小”，即 <span class="math inline">\(p\left(d_t\right)\)</span> (2)
pLSA认为，我们说话时面向的主题分布，取决于 “文档”
（实际上是文档ID)。这个假设显然是不合理的，小说家不会因为自己写到第666回而调整
主题。 (3) 类似
(2)，随着上下文的变化，我们围绕一个主题说话的内容和方式也
会发生改变。在主题模型中，这种改变的体现，就是一个主题下的词语概率分
布会发生改变。而pLSA忽略了这样的事实。</p>
<p>从计算复杂度的角度看pLSA有两个比较大的缺陷: (1) pLSA中，对文档
出现的概率估计，来自对训练语料的学习。而对于一个
末知文档，我们是无法估计它出现的概率的一一因此pLSA无法对训练语料之
外的文档进行处理。pLSA的这个特点决定了，在在线(online) 场景中(数据是
持续增加的)，那么文档处理系统就需要定时使用pLSA对整个语料库进行计
算。因此，pLSA比较适合允许一定时滞的离线计算。 (2)
pLSA认为一个文档对各个主题的隶属度是一定的——而一个主题对各个词语的隶属度也是一定的，因此pLSA在生成一个文档的各个词语时、使用了相同的词语概率分布。这样，pLSA需要为每一个文档记录一个专门的随着语料数据集规模的增加，pLSA的参数规模也会增加，导致模型训练越来越困难。</p>
<h2 id="lda">LDA</h2>
<p>LDA模型是文本集合的生成概率模型。</p>
<p>LDA 的文本集合的生成过程如下: 首先随机生成一个文本的话题分布,
之后在该 文本的每个位置, 依据该文本的话题分布随机生成一个话题,
然后在该位置依据该话 题的单词分布随机生成一个单词,
直至文本的最后一个位置, 生成整个文本。重复以 上过程生成所有文本。</p>
<p>LDA 模型是含有隐变量的概率图模型。模型中, 每个话题的单词分布, 每个文
本的话题分布, 文本的每个位置的话题是隐变量; 文本的每个位置的单词是观测变
量。LDA 模型的学习与推理无法直接求解, 通常使用吉布斯抽样 (Gibbs
sampling) 和 变分 EM算法 (variational EM algorithm), 前者是蒙特卡罗法,
而后者是近似算法。</p>
<h3 id="多项分布">多项分布</h3>
<p>(多项分布) 若多元离散随机变量 <span class="math inline">\(X=\left(X_1, X_2, \cdots, X_k\right)\)</span>
的概率质 量函数为</p>
<p><span class="math display">\[
\begin{aligned}
P\left(X_1=n_1, X_2=n_2, \cdots, X_k=n_k\right) &amp;=\frac{n !}{n_{1} !
n_{2} ! \cdots n_{k} !} p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k} \\\\
&amp;=\frac{n !}{\prod_{i=1}^k n_{i} !} \prod_{i=1}^k p_i^{n_i}
\end{aligned}
\]</span></p>
<p>其中 <span class="math inline">\(p=\left(p_1, p_2, \cdots,
p_k\right), p_i \geqslant 0, i=1,2, \cdots, k, \sum_{i=1}^k p_i=1,
\sum_{i=1}^k n_i=n\)</span>, 则称随机变 量 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\((n, p)\)</span> 的多项分布, 记作 <span class="math inline">\(X \sim \operatorname{Mult}(n, p)\)</span> 。</p>
<p>当试验的次数 <span class="math inline">\(n\)</span> 为 1 时,
多项分布变成类别分布 (categorical distribution)。类
别分布表示试验可能出现的 <span class="math inline">\(k\)</span>
种结果的概率。显然多项分布包含类别分布。</p>
<h3 id="狄利克雷分布">狄利克雷分布</h3>
<p>狄利克雷分布 (Dirichlet distribution)
是一种多元连续随机变量的概率分布, 是 贝塔分布 (beta distribution)
的扩展。在贝叶斯学习中, 狄利克雷分布常作为多项分 布的先验分布使用。
(狄利克雷分布) 若多元连续随机变量 <span class="math inline">\(\theta=\left(\theta_1, \theta_2, \cdots,
\theta_k\right)\)</span> 的概率密 度函数为</p>
<p><span class="math display">\[
p(\theta \mid \alpha)=\frac{\Gamma\left(\sum_{i=1}^k
\alpha_i\right)}{\prod_{i=1}^k \Gamma\left(\alpha_i\right)}
\prod_{i=1}^k \theta_i^{\alpha_i-1}
\]</span></p>
<p>其中 <span class="math inline">\(\sum_{i=1}^k \theta_i=1, \theta_i
\geqslant 0, \alpha=\left(\alpha_1, \alpha_2, \cdots, \alpha_k\right),
\alpha_i&gt;0, i=1,2, \cdots, k\)</span>, 则称随机变量 <span class="math inline">\(\theta\)</span> 服从参数为 <span class="math inline">\(\alpha\)</span> 的狄利克雷分布, 记作 <span class="math inline">\(\theta \sim \operatorname{Dir}(\alpha)\)</span> 。
式中 <span class="math inline">\(\Gamma(s)\)</span> 是伽马函数,
定义为</p>
<p><span class="math display">\[
\Gamma(s)=\int_0^{\infty} x^{s-1} \mathrm{e}^{-x} \mathrm{~d} x, \quad
s&gt;0
\]</span></p>
<p>具有性质：</p>
<p><span class="math display">\[
\Gamma(s+1) = s\Gamma(s)
\]</span></p>
<p>当s为自然数时，有：</p>
<p><span class="math display">\[
\Gamma(s+1) = s!
\]</span></p>
<p>令</p>
<p><span class="math display">\[
\mathrm{B}(\alpha)=\frac{\prod_{i=1}^k
\Gamma\left(\alpha_i\right)}{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}
\]</span></p>
<p>则狄利克雷分布的密度函数可以写成</p>
<p><span class="math display">\[
p(\theta \mid \alpha)=\frac{1}{\mathrm{~B}(\alpha)} \prod_{i=1}^k
\theta_i^{\alpha_i-1}
\]</span></p>
<p><span class="math inline">\(\mathrm{B}(\alpha)\)</span> 是规范化因子,
称为多元贝塔函数 (或扩展的贝塔函数)。由密度函数的性质</p>
<p><span class="math display">\[
\int \frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k
\Gamma\left(\alpha_i\right)} \prod_{i=1}^{\alpha_i-1} \mathrm{~d}
\theta=\frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k
\Gamma\left(\alpha_i\right)} \int \prod_{i=1}^k \theta_i^{\alpha_i-1}
\mathrm{~d} \theta=1
\]</span></p>
<p>得</p>
<p><span class="math display">\[
\mathrm{B}(\alpha)=\int \prod_{i=1}^k \theta_i^{\alpha_i-1} \mathrm{~d}
\theta
\]</span></p>
<h3 id="二项分布与贝塔分布">二项分布与贝塔分布</h3>
<p>二项分布是多项分布的特殊情况, 贝塔分布是狄利克雷分布的特殊情况。
二项分布是指如下概率分布。 <span class="math inline">\(X\)</span>
为离散随机变量, 取值为 <span class="math inline">\(m\)</span>,
其概率质量函数为</p>
<p><span class="math display">\[
P(X=m)=\left(\begin{array}{c}
n \\\\
m
\end{array}\right) p^m(1-p)^{n-m}, \quad m=0,1,2, \cdots, n
\]</span></p>
<p>其中 <span class="math inline">\(n\)</span> 和 <span class="math inline">\(p(0 \leqslant p \leqslant 1)\)</span> 是参数。</p>
<p>贝塔分布是指如下概率分布, <span class="math inline">\(X\)</span>
为连续随机变量, 取值范围为 <span class="math inline">\([0,1]\)</span>,
其概率密度 函数为</p>
<p><span class="math display">\[
p(x)= \begin{cases}\frac{1}{\mathrm{~B}(s, t)} x^{s-1}(1-x)^{t-1}, &amp;
0 \leqslant x \leqslant 1 \\\\ 0, &amp; \text { 其他 }\end{cases}
\]</span></p>
<p>其中 <span class="math inline">\(s&gt;0\)</span> 和 <span class="math inline">\(t&gt;0\)</span> 是参数, <span class="math inline">\(\mathrm{B}(s, t)=\frac{\Gamma(s)
\Gamma(t)}{\Gamma(s+t)}\)</span> 是贝塔函数, 定义为</p>
<p><span class="math display">\[
\mathrm{B}(s, t)=\int_0^1 x^{s-1}(1-x)^{t-1} \mathrm{~d} x =
\frac{\Gamma(s)\Gamma(t)}{\Gamma(s+t)}
\]</span></p>
<p>当 <span class="math inline">\(s, t\)</span> 是自然数时(<span class="math inline">\(\Gamma(s+1) = s!\)</span>),</p>
<p><span class="math display">\[
\mathrm{B}(s, t)=\frac{(s-1) !(t-1) !}{(s+t-1) !}
\]</span></p>
<p>当 <span class="math inline">\(n\)</span> 为 1 时,
二项分布变成伯努利分布（Bernoulli distribution）或 0-1 分布。
伯努利分布表示试验可能出现的 2
种结果的概率。显然二项分布包含伯努利分布。给出几种概率分布的关系。 <img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221017225107.png"/></p>
<h3 id="基本想法">基本想法</h3>
<p>在LDA主题模型下，一篇文章由词语的序列组成。首先以一定概率选择一个主题，其次以一定概率在这个主题中选择一个词。如果一篇文章由1000个词组成，那么就把上述方式重复1000遍，就能组成这篇文章。那么值得注意的是，以一定概率选择一个主题是服从多项式分布的，而多项式分布的参数是服从Dirichlet分布的。以一定概率在特定主题中选择一个词也是服从多项式分布的，多项式分布的参数是服从Dirichlet分布的。为什么呢？因为Dirichlet分布是多项式分布的共轭分布，也就是说由贝叶斯估计得到的后验分布仍然是Dirichlet分布。
<img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221017233927.png"/></p>
<h3 id="lda与plsa的关系">LDA与PLSA的关系</h3>
<p>二者都是概率模型，都是利用概率生成模型对文本集合进行主题分析的无监督学习方法。</p>
<p>PLSA是用了频率派的方法，利用极大似然进行学习，而LDA使用了贝叶斯派的方法，进行贝叶斯推断。</p>
<p>二者都假设存在两个分布：话题是单词的多项分布，文本是话题的多项分布，不同的在于LDA认为多项分布的参数也服从一个分布，而不是固定不变的，使用狄利克雷分布作为多项分布的先验分布，也就是多项分布的参数服从狄利克雷分布。</p>
<p>引入先验概率的作用可以防止过拟合。为啥选择狄利克雷分布呢？因为它是多项分布的共轭先验分布，先验分布与后验分布形式相同，便于由先验分布得到后验分布。</p>
<p>LDA是在Plsa的基础上，为单词分布和主题分布增加了两个狄利克雷先验。
<img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221020102411.png"/>
<img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221020102417.png"/></p>
<h3 id="模型定义">模型定义</h3>
<h4 id="模型要素">模型要素</h4>
<p>潜在狄利克雷分配 (LDA) 使用三个集合: 一是单词集合 <span class="math inline">\(W={w_1, \cdots, w_v, \cdots}\)</span>, , 其中
<span class="math inline">\(w_v\)</span> 是第 <span class="math inline">\(v\)</span> 个单词, <span class="math inline">\(v=1,2, \cdots, V, V\)</span>
是单词的个数。二是文本集合 <span class="math inline">\(D={\mathbf{w}_1,
\cdots, \mathbf{w}_m, \cdots, \mathbf{w}_M}\)</span>, 其中 <span class="math inline">\(\mathbf{w}_m\)</span> 是第 <span class="math inline">\(m\)</span> 个文本, <span class="math inline">\(m=1,2, \cdots, M, M\)</span> 是文本 的个数。文本
<span class="math inline">\(\mathbf{w}_m\)</span> 是一个单词序列 <span class="math inline">\(\mathbf{w}_m=\left(w_{m 1}, \cdots, w_{m n},
\cdots, w_{m N_m}\right)\)</span>, 其中 <span class="math inline">\(w_{m
n}\)</span> 是 文本 <span class="math inline">\(\mathbf{w}_m\)</span>
的第 <span class="math inline">\(n\)</span> 个单词, <span class="math inline">\(n=1,2, \cdots, N_m, N_m\)</span> 是文本 <span class="math inline">\(\mathbf{w}_m\)</span>
中单词的个数。三是主题集合集合 <span class="math inline">\(Z={z_1,
\cdots, z_k, \cdots, z_K}\)</span>, 其中 <span class="math inline">\(z_k\)</span> 是第 <span class="math inline">\(k\)</span> 个话题, <span class="math inline">\(k=1,2, \cdots, K, K\)</span> 是话题的个数。</p>
<ul>
<li>每一个话题 <span class="math inline">\(z_k\)</span>
由一个单词的条件概率分布 <span class="math inline">\(p\left(w \mid
z_k\right)\)</span> 决定, <span class="math inline">\(w \in W\)</span>
。分布 <span class="math inline">\(p\left(w \mid z_k\right)\)</span>
服从多项分布 (严格意义上类别分布), 其参数为 <span class="math inline">\(\varphi_k\)</span> 。参数 <span class="math inline">\(\varphi_k\)</span> 服从狄利克雷分布 (先验分布),
其超参数为 <span class="math inline">\(\beta\)</span> 。参数 <span class="math inline">\(\varphi_k\)</span> 是一个 <span class="math inline">\(V\)</span> 维向量 <span class="math inline">\(\varphi_k=\left(\varphi_{k 1}, \varphi_{k 2},
\cdots, \varphi_{k V}\right)\)</span>, 其中 <span class="math inline">\(\varphi_{k v}\)</span> 表示话题 <span class="math inline">\(z_k\)</span> 生成单词 <span class="math inline">\(w_v\)</span> 的概率。所有话题的参数向量构成一个
<span class="math inline">\(K \times V\)</span> 矩阵 <span class="math inline">\(\varphi=\{\varphi_k\}_{k=1}^K\)</span> 。超参数
<span class="math inline">\(\beta\)</span> 也是一个 <span class="math inline">\(V\)</span> 维向量 <span class="math inline">\(\beta=\left(\beta_1, \beta_2, \cdots,
\beta_V\right)\_{\text {。 }}\)</span>(对于话题<span class="math inline">\(z_k\)</span>其生成单词<span class="math inline">\(w_v\)</span>先验服从狄利克雷分布，因此是一个V维向量)</li>
<li>每一个文本 <span class="math inline">\(\mathbf{w}_m\)</span>
由一个话题的条件概率分布 <span class="math inline">\(p\left(z \mid
\mathbf{w}_m\right)\)</span> 决定, <span class="math inline">\(z \in
Z_{\text {。 }}\)</span> 分布 <span class="math inline">\(p\left(z \mid
\mathbf{w}_m\right)\)</span> 服从多项分布 (严格意义上类别分布), 其参数为
<span class="math inline">\(\theta_m\)</span> 。参数 <span class="math inline">\(\theta_m\)</span> 服从狄利克雷分布 (先验分布),
其超参数为 <span class="math inline">\(\alpha\)</span> , 参数 <span class="math inline">\(\theta_m\)</span> 是一个 <span class="math inline">\(K\)</span> 维向量 <span class="math inline">\(\theta_m=\left(\theta_{m 1}, \theta_{m 2}, \cdots,
\theta_{m K}\right)\)</span>, 其中 <span class="math inline">\(\theta_{m
k}\)</span> 表示文本 <span class="math inline">\(\mathrm{w}_m\)</span>
生成话题 <span class="math inline">\(z_k\)</span>
的概率。所有文本的参数向量构成一个 <span class="math inline">\(M \times
K\)</span> 矩阵 <span class="math inline">\(\theta=\{\theta_m\}_{m=1}^M\)</span> 。超参数
<span class="math inline">\(\alpha\)</span> 也是一个 <span class="math inline">\(K\)</span> 维向量 <span class="math inline">\(\alpha=\left(\alpha_1, \alpha_2, \cdots,
\alpha_K\right)\)</span> 。</li>
<li>每一个文本 <span class="math inline">\(\mathbf{w}_m\)</span>
中的每一个单词 <span class="math inline">\(w_{m n}\)</span>
由该文本的话题分布 <span class="math inline">\(p\left(z \mid
\mathbf{w}_m\right)\)</span> 以及所有话 题的单词分布 <span class="math inline">\(p\left(w \mid z_k\right)\)</span> 决定。</li>
</ul>
<h4 id="生成过程">生成过程</h4>
<p>LDA 文本集合的生成过程如下: 给定单词集合 <span class="math inline">\(W\)</span>, 文本集合 <span class="math inline">\(D\)</span>, 话题集合 <span class="math inline">\(Z\)</span>, 狄利克雷分布的超参数 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 。</p>
<p>1.生成单词分布 随机生成 <span class="math inline">\(K\)</span>
个话题的单词分布。具体过程如下, 按照狄利克雷分布 <span class="math inline">\(\operatorname{Dir}(\beta)\)</span> 随机
生成一个参数向量 <span class="math inline">\(\varphi_k, \varphi_k \sim
\operatorname{Dir}(\beta)\)</span>, 作为话题 <span class="math inline">\(z_k\)</span> 的单词分布 <span class="math inline">\(p\left(w \mid z_k\right), w \in W, k=\)</span>
<span class="math inline">\(1,2, \cdots, K\)</span> 。</p>
<p>2.生成主题分布 随机生成 <span class="math inline">\(M\)</span>
个文本的主题分布。具体过程如下: 按照狄利克雷分布 <span class="math inline">\(\operatorname{Dir}(\alpha)\)</span> 随
机生成一个参数向量 <span class="math inline">\(\theta_m, \theta_m \sim
\operatorname{Dir}(\alpha)\)</span>, 作为文本 <span class="math inline">\(\mathbf{w}_m\)</span> 的主题分布 <span class="math inline">\(p\left(z \mid \mathbf{w}_m\right), m=\)</span>
<span class="math inline">\(1,2, \cdots, M_{}\)</span> 。</p>
<p>3.生成文本的单词序列 随机生成 <span class="math inline">\(M\)</span>
个文本的 <span class="math inline">\(N_m\)</span> 个单词。文本 <span class="math inline">\(\mathbf{w}_m(m=1,2, \cdots, M)\)</span> 的单词
<span class="math inline">\(w_{m n}(n=\)</span> <span class="math inline">\(\left.1,2, \cdots, N_m\right)\)</span>
的生成过程如下:</p>
<p>3.1 首先按照多项分布 <span class="math inline">\(\operatorname{Mult}\left(\theta_m\right)\)</span>
随机生成一个话题 <span class="math inline">\(z_{m n}, z_{m n} \sim
\operatorname{Mult}\left(\theta_m\right)\)</span> 3.2 然后按照多项分布
<span class="math inline">\(\operatorname{Mult}\left(\varphi_{z_{m
n}}\right)\)</span> 随机生成一个单词 <span class="math inline">\(w_{m
n}, w_{m n} \sim \operatorname{Mult}\left(\varphi_{z_{m
n}}\right)\_{\text {。 }}\)</span> 文本 <span class="math inline">\(\mathbf{w}_m\)</span> 本身是单词序列 <span class="math inline">\(\mathbf{w}_m=\left(w_{m 1}, w_{m 2}, \cdots, w_{m
N_m}\right)\)</span>, 对应着隐式的话题序列 <span class="math inline">\(\mathbf{z}_m=\left(z_{m 1}, z_{m 2}, \cdots, z_{m
N_m}\right) 。\)</span></p>
<p>引用一下LDA数学八卦的图： <img src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221019104516.png"/></p>
<ul>
<li><span class="math inline">\(\vec{\alpha} \rightarrow \vec{\theta}_m
\rightarrow z_{m, n}\)</span>, 这个过程表示在生成第 <span class="math inline">\(m\)</span>
篇文档的时候，先从第一个坛子中抽了一个doc-topic 骰子 <span class="math inline">\(\vec{\theta}_m\)</span>,然后投这个骰子生成了文档<span class="math inline">\(m\)</span>中第 <span class="math inline">\(n\)</span> 个词的topic编号 <span class="math inline">\(z_{m, n}\)</span> ；</li>
<li><span class="math inline">\(\vec{\beta} \rightarrow \vec{\varphi}_k
\rightarrow w_{m, n} \mid k=z_{m, n}\)</span>,
这个过程表示用如下动作生成语料中第 <span class="math inline">\(m\)</span> 篇文档的第 <span class="math inline">\(n\)</span> 个词: 在上帝手头的 <span class="math inline">\(K\)</span> 个topic-word 骰子 <span class="math inline">\(\vec{\varphi}_k\)</span> 中，挑选编号为 <span class="math inline">\(k=z_{m, n}\)</span> 的那个骰子进行投掷，然后生成
word <span class="math inline">\(w_{m, n}\)</span> ;</li>
</ul>
<p>理解 LDA最重要的就是理解这两个物理过程。LDA 模型在基于 <span class="math inline">\(K\)</span> 个 topic 生成语料中的 <span class="math inline">\(M\)</span> 篇文档的过程中， 由于是 bag-of-words
模型，有一些物理过程是相互独立可交换的。由此，LDA生成模型中， <span class="math inline">\(M\)</span> 篇文档会对应 于 <span class="math inline">\(M\)</span> 个独立的 Dirichlet-Multinomial
共轭结构；K个 个 topic 会对应于 <span class="math inline">\(K\)</span>
个独立的 Dirichlet-Multinomial 共轭结 构。所以理解 LDA
所需要的所有数学就是理解 Dirichlet-Multiomail
共轭，其它都就是理解物理过程。</p>
<h4 id="总结">总结</h4>
<ol type="1">
<li>对于话题 <span class="math inline">\(z_k(k=1,2, \cdots, K)\)</span>
: 生成多项分布参数 <span class="math inline">\(\varphi_k \sim
\operatorname{Dir}(\beta)\)</span>, 作为话题的单词分布 <span class="math inline">\(p\left(w \mid z_k\right)\)</span>;</li>
<li>对于文本 <span class="math inline">\(\mathbf{w}_m(m=1,2, \cdots,
M)\)</span>; 生成多项分布参数 <span class="math inline">\(\theta_m \sim
\operatorname{Dir}(\alpha)\)</span>, 作为文本的话题分布 <span class="math inline">\(p\left(z \mid \mathbf{w}_m\right)\)</span>;</li>
<li>对于文本 <span class="math inline">\(\mathbf{w}_m\)</span> 的单词
<span class="math inline">\(w_{m n}\left(m=1,2, \cdots, M, n=1,2,
\cdots, N_m\right)\)</span> :
<ol type="a">
<li>采样生成话题 <span class="math inline">\(z_{m n} \sim
\operatorname{Mult}\left(\theta_m\right)\)</span>,
作为单词对应的话题;</li>
<li>采样生成单词 <span class="math inline">\(w_{m n} \sim
\operatorname{Mult}\left(\varphi_{z_{m n}}\right)\)</span> 。</li>
</ol></li>
</ol>
<p>LDA 的文本生成过程中, 假定话题个数 <span class="math inline">\(K\)</span> 给定, 实际通常通过实验选定。狄利
克雷分布的超参数 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span>
通常也是事先给定的。在没有其他先验知识的情况下, 可以 假设向量 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 的所有分量均为 1 ,
这时的文本的话题分布 <span class="math inline">\(\theta_m\)</span>
是对称的, 话题的单 词分布 <span class="math inline">\(\varphi_k\)</span>
也是对称的。</p>
<p>（帮助理解：主题数为3，假设<span class="math inline">\(\theta_m\)</span> = {0.4, 0.5,
0.1}，则说明主题<span class="math inline">\(z_2\)</span>出现在文档m当中的概率为0.5，这就是多项分布的参数，再根据多项分布进行采样得到主题。）</p>
<h3 id="概率计算">概率计算</h3>
<p>LDA 模型整体是由观测变量和隐变量组成的联合概率分布, 可以表为</p>
<p><span class="math display">\[
p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha,
\beta)=\prod_{k=1}^K p\left(\varphi_k \mid \beta\right) \prod_{m=1}^M
p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m} p\left(z_{m n} \mid
\theta_m\right) p\left(w_{m n} \mid z_{m n}, \varphi\right)
\]</span></p>
<p>(其中M为文本数，<span class="math inline">\(N_m\)</span>为文档m的长度，K为主题数) 其中观测变量
<span class="math inline">\(\mathrm{w}\)</span>
表示所有文本中的单词序列, 隐变量 <span class="math inline">\(\mathrm{z}\)</span> 表示所有文本中的话题序列,
隐变量 <span class="math inline">\(\theta\)</span>
表示所有文本的话题分布的参数, 隐变量 <span class="math inline">\(\varphi\)</span> 表示所有话题的单词分布的参 数,
<span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 是超参数。</p>
<ul>
<li><span class="math inline">\(p\left(\varphi_k \mid
\beta\right)\)</span> 表示超参数 <span class="math inline">\(\beta\)</span> 给定条件下第 <span class="math inline">\(k\)</span> 个话题的单词分布的参数 <span class="math inline">\(\varphi_k\)</span> 的生成概率;</li>
<li><span class="math inline">\(p\left(\theta_m \mid
\alpha\right)\)</span> 表示超参数 <span class="math inline">\(\alpha\)</span> 给定条件下第 <span class="math inline">\(m\)</span> 个文本的话题分布的 参数 <span class="math inline">\(\theta_m\)</span> 的生成概率;</li>
<li><span class="math inline">\(p\left(z_{m n} \mid
\theta_m\right)\)</span> 表示第 <span class="math inline">\(m\)</span>
个文本的话题分布 <span class="math inline">\(\theta_m\)</span>
给定条件下文本的 第 <span class="math inline">\(n\)</span> 个位置的话题
<span class="math inline">\(z_{m n}\)</span> 的生成概率;</li>
<li><span class="math inline">\(p\left(w_{m n} \mid z_{m n},
\varphi\right)\)</span> 表示在第 <span class="math inline">\(m\)</span>
个文本的第 <span class="math inline">\(n\)</span> 个位 置的话题 <span class="math inline">\(z_{m n}\)</span> 及所有话题的单词分布的参数 <span class="math inline">\(\varphi\)</span> 给定条件下第 <span class="math inline">\(m\)</span> 个文本的第 <span class="math inline">\(n\)</span> 个位 置的单词 <span class="math inline">\(w_{m n}\)</span> 的生成概率。</li>
</ul>
<p>第 <span class="math inline">\(m\)</span>
个文本的联合概率分布可以表为</p>
<p><span class="math display">\[
p\left(\mathbf{w}_m, \mathbf{z}_m, \theta_m, \varphi \mid \alpha,
\beta\right)=\prod_{k=1}^K p\left(\varphi_k \mid \beta\right)
p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m} p\left(z_{m n} \mid
\theta_m\right) p\left(w_{m n} \mid z_{m n}, \varphi\right)
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{w}_m\)</span>
表示该文本中的单词序列, <span class="math inline">\(\mathbf{z}_m\)</span> 表示该文本的话题序列, <span class="math inline">\(\theta_m\)</span> 表示该文本的话 题分布参数。 LDA
模型的联合分布含有隐变量, 对隐变量进行积分得到边缘分布。 参数 <span class="math inline">\(\theta_m\)</span> 和 <span class="math inline">\(\varphi\)</span> 给定条件下第 <span class="math inline">\(m\)</span> 个文本的生成概率是</p>
<p><span class="math display">\[
p\left(\mathbf{w}_m \mid \theta_m,
\varphi\right)=\prod_{n=1}^{N_m}\left[\sum_{k=1}^K p\left(z_{m n}=k \mid
\theta_m\right) p\left(w_{m n} \mid \varphi_k\right)\right]
\]</span></p>
<p>超参数 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 给定条件下第 <span class="math inline">\(m\)</span> 个文本的生成概率是</p>
<p><span class="math display">\[
p\left(\mathbf{w}_m \mid \alpha, \beta\right)=\prod_{k=1}^K \int
p\left(\varphi_k \mid \beta\right)\left[\int p\left(\theta_m \mid
\alpha\right) \prod_{n=1}^{N_m}\left[\sum_{l=1}^K p\left(z_{m n}=l \mid
\theta_m\right) p\left(w_{m n} \mid \varphi_l\right)\right] \mathrm{d}
\theta_m\right] \mathrm{d} \varphi_k
\]</span></p>
<p>超参数 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 给定条件下所有文本的生成概率是</p>
<p><span class="math display">\[
p(\mathbf{w} \mid \alpha, \beta)=\prod_{k=1}^K \int p\left(\varphi_k
\mid \beta\right)\left[\prod_{m=1}^M \int p\left(\theta_m \mid
\alpha\right) \prod_{n=1}^{N_m}\left[\sum_{l=1}^K p\left(z_{m n}=l \mid
\theta_m\right) p\left(w_{m n} \mid \varphi_l\right)\right] \mathrm{d}
\theta_m\right] \mathrm{d} \varphi_k
\]</span></p>
<h3 id="吉布斯抽样">吉布斯抽样</h3>
<h4 id="基本思想">基本思想</h4>
<p>有三个主要目标： - 话题序列的集合<span class="math inline">\(z=(z_1,
z_2, \cdots, z_M)\)</span>的后验概率分布，其中<span class="math inline">\(z_m\)</span>是第m个文本的主题序列，<span class="math inline">\(z_m=(z_{m1}, \cdots, z_{mN_{m}})\)</span>; -
参数<span class="math inline">\(\theta=(\theta_1, \cdots,
\theta_{M})\)</span>，其中<span class="math inline">\(\theta_m\)</span>是第m个文本的主题分布的参数； -
参数<span class="math inline">\(\varphi=(\varphi_1, \cdots,
\varphi_K)\)</span>，其中<span class="math inline">\(\varphi_k\)</span>是第k个主题的单词分布的参数。</p>
<p>对<span class="math inline">\(p(\mathbf{w}, \mathbf{z}, \theta,
\varphi \mid \alpha, \beta)\)</span>进行估计</p>
<p>吉布斯抽样, 这是一种常用的马尔可夫链蒙特卡罗法。为了估计 多元随机变量
<span class="math inline">\(x\)</span> 的联合分布 <span class="math inline">\(p(x)\)</span>, 吉布斯抽样法选择 <span class="math inline">\(x\)</span> 的一个分量, 固定其他分量,
按照其条件概率分布进行随机抽样, 依次循环对每一个分量执行这个操作,
得到联合 分布 <span class="math inline">\(p(x)\)</span> 的一个随机样本,
重复这个过程, 在燃烧期之后, 得到联合概率分布 <span class="math inline">\(p(x)\)</span> 的 样本集合。</p>
<p>LDA 模型的学习通常采用收缩的吉布斯抽样 (collapsed Gibbs sampling) ,
基本想法是, 通过对隐变量 <span class="math inline">\(\theta\)</span> 和
<span class="math inline">\(\varphi\)</span> 积分, 得到边缘概率分布
<span class="math inline">\(p(\mathbf{w}, \mathbf{z} \mid \alpha,
\beta)\)</span> (也是联合分 布), 其中变量 <span class="math inline">\(\mathbf{w}\)</span> 是可观测的, 变量 <span class="math inline">\(\mathbf{z}\)</span> 是不可观测的; 对后验概率分布
<span class="math inline">\(p(\mathbf{z} \mid \mathbf{w}, \alpha,
\beta)\)</span> 进 行吉布斯抽样, 得到分布 <span class="math inline">\(p(\mathbf{z} \mid \mathbf{w}, \alpha,
\beta)\)</span> 的样本集合; 再利用这个样本集合对参数 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\varphi\)</span> 进行估计, 最终得到 LDA 模型 <span class="math inline">\(p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid
\alpha, \beta)\)</span> 的所有参数估计。 #### 算法流程</p>
<p>输入: 文本的单词序列 <span class="math inline">\(\mathbf{w}=\{\mathbf{w}_1, \cdots, \mathbf{w}_m,
\cdots, \mathbf{w}_M\}, \mathbf{w}_m=\left(w_{m 1}, \cdots, w_{m n},
\cdots\right.\)</span>, <span class="math inline">\(\left.w_{m_{N_m}}\right)\)</span>;</p>
<p>输出: 文本的话题序列 <span class="math inline">\(\mathrm{z}=\{\mathbf{z}_1, \cdots, \mathbf{z}_m,
\cdots, \mathbf{z}_M\}, \mathbf{z}_m=\left(z_{m 1}, \cdots, z_{m n},
\cdots, z_{m_{N_m}}\right)\)</span> 的后验概率分布 <span class="math inline">\(p(\mathbf{z} \mid \mathbf{w}, \alpha,
\beta)\)</span> 的样本计数, 模型的参数 <span class="math inline">\(\varphi\)</span> 和 <span class="math inline">\(\theta\)</span> 的估计值; 参数: 超参数 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span>, 话题个数 <span class="math inline">\(K\)</span> 。</p>
<ol type="1">
<li><p>设所有计数矩阵的元素 <span class="math inline">\(n_{m k}, n_{k
v}\)</span>, 计数向量的元素 <span class="math inline">\(n_m,
n_k\)</span> 初值为 0 ;</p></li>
<li><p>对所有文本 <span class="math inline">\(\mathbf{w}_m, m=1,2,
\cdots, M\)</span> 对第 <span class="math inline">\(m\)</span>
个文本中的所有单词 <span class="math inline">\(w_{m n}, n=1,2, \cdots,
N_m\)</span></p></li>
</ol>
<ol type="a">
<li>抽样话题 <span class="math inline">\(z_{m n}=z_k \sim
\operatorname{Mult}\left(\frac{1}{K}\right)\)</span>;(对于文本m，其多项分布的参数为<span class="math inline">\(\frac{1}{K}\)</span>，由<span class="math inline">\(\alpha\)</span>生成，即<span class="math inline">\(\theta_m \sim Dir(\alpha)\)</span>，<span class="math inline">\(\theta_m\)</span>为长度为K的向量。)</li>
</ol>
<p>增加文本-话题计数 <span class="math inline">\(n_{m k}=n_{m
k}+1\)</span>, 增加文本-话题和计数 <span class="math inline">\(n_m=n_m+1\)</span>, 增加话题-单词计数 <span class="math inline">\(n_{k v}=n_{k v}+1\)</span>, 增加话题-单词和计数
<span class="math inline">\(n_k=n_k+1\)</span>;</p>
<p>（3）循环执行以下操作, 直到进入燃烧期 对所有文本 <span class="math inline">\(\mathbf{w}_m, m=1,2, \cdots, M\)</span> 对第 <span class="math inline">\(m\)</span> 个文本中的所有单词 <span class="math inline">\(w_{m n}, n=1,2, \cdots, N_m\)</span></p>
<ol type="a">
<li><p>当前的单词 <span class="math inline">\(w_{m n}\)</span> 是第
<span class="math inline">\(v\)</span> 个单词, 话题指派 <span class="math inline">\(z_{m n}\)</span> 是第 <span class="math inline">\(k\)</span> 个话题; 减少计数 <span class="math inline">\(n_{m k}=n_{m k}-1, n_m=n_m-1, n_{k v}=n_{k v}-1,
n_k=n_k-1\)</span>;</p></li>
<li><p>按照满条件分布进行抽样</p></li>
</ol>
<p><span class="math display">\[
p\left(z_i \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right)
\propto \frac{n_{k v}+\beta_v}{\sum_{v=1}^V\left(n_{k v}+\beta_v\right)}
\cdot \frac{n_{m k}+\alpha_k}{\sum_{k=1}^K\left(n_{m k}+\alpha_k\right)}
\]</span></p>
<p>得到新的第 <span class="math inline">\(k^{\prime}\)</span> 个话题,
分配给 <span class="math inline">\(z_{m n}\)</span>;</p>
<ol start="3" type="a">
<li><p>增加计数 <span class="math inline">\(n_{m k^{\prime}}=n_{m
k^{\prime}}+1, n_m=n_m+1, n_{k^{\prime} v}=n_{k^{\prime} v}+1,
n_{k^{\prime}}=n_{k^{\prime}}+1\)</span>;</p></li>
<li><p>得到更新的两个计数矩阵 <span class="math inline">\(N_{K \times
V}=\left[n_{k v}\right]\)</span> 和 <span class="math inline">\(N_{M
\times K}=\left[n_{m k}\right]\)</span>, 表示后验 概率分布 <span class="math inline">\(p(\mathbf{z} \mid \mathbf{w}, \alpha,
\beta)\)</span> 的样本计数;</p></li>
</ol>
<ol start="4" type="1">
<li>利用得到的样本计数, 计算模型参数</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\theta_{m k} &amp;=\frac{n_{m k}+\alpha_k}{\sum_{k=1}^K\left(n_{m
k}+\alpha_k\right)} \\\\
\varphi_{k v} &amp;=\frac{n_{k v}+\beta_v}{\sum_{v=1}^V\left(n_{k
v}+\beta_v\right)}
\end{aligned}
\]</span></p>
<h3 id="训练与推断">训练与推断</h3>
<p>有了LDA模型，我们的目标有两个：</p>
<ul>
<li>估计模型中的参数<span class="math inline">\(\varphi_1, \cdots,
\varphi_K\)</span>和<span class="math inline">\(\theta_1, \cdots,
\theta_M\)</span>;</li>
<li>对于新来的一篇doc，我们能够计算这篇文档的topic分布<span class="math inline">\(\theta_{new}\)</span>。</li>
</ul>
<p>有了吉布斯采样公式就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic语义分析，训练的过程就是通过Gibbs
Samping获取语料中的（z,w）样本，而模型中的所有参数可以基于采样的样本进行估计。</p>
<p>训练流程如下：</p>
<ul>
<li>随机初始化：对语料中的每篇文档的每个词w，随机赋一个topic编号z。</li>
<li>重新扫描语料库，对每个词按照吉布斯采样公式重新采样它的topic，在语料中进行更新。</li>
<li>重复以上语料库的重新采样过程直到吉布斯采样收敛。</li>
<li>统计语料库的topic-word共现频率矩阵，就是LDA的模型</li>
</ul>
<p>由这个矩阵我们可以计算每一个<span class="math inline">\(p(word\mid
topic)\)</span>概率，从而计算出模型参数<span class="math inline">\(\varphi_1, \cdots,
\varphi_K\)</span>，也可以计算另一个参数<span class="math inline">\(\theta_1, \cdots,
\theta_M\)</span>，只要在吉布斯抽样收敛后统计每篇文章的topic频率分布，就可以计算每一个<span class="math inline">\(p(topic\mid
doc)\)</span>概率，由于它是和训练语料的每篇文章相关的，对于我们理解新的文档毫无用处，所以一般没有必要保留这个概率。</p>
<p>如何对新的文档进行推断呢？其实和训练过程完全相似，对于新的文档，认为<span class="math inline">\(\varphi_{kt}\)</span>是稳定不变的，是由训练语料得到的模型提供的。采样过程只估计该文档的topic分布<span class="math inline">\(\theta_{new}\)</span>就好了。</p>
<p>推断过程如下：</p>
<ul>
<li>随机初始化：对当前文档的每个词w，随机的赋一个topic编号z；</li>
<li>重新扫描当前文档，按照吉布斯抽样公式，对每个词w，重新采样它的topic；</li>
<li>重复以上过程直到吉布斯采样收敛</li>
<li>统计文档中的topic分布，该分布就是<span class="math inline">\(\theta_{new}\)</span></li>
</ul>
<h3 id="代码">代码</h3>
<p>实现了吉布斯推断的python代码：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">&#34;&#34;&#34;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">LDA implementation in Python</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">  </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">@author: Michael Zhang</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">&#34;&#34;&#34;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LDA(<span class="bu">object</span>):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tdm, T, alpha <span class="op">=</span> <span class="fl">1.</span>, beta<span class="op">=</span><span class="fl">1.</span>, iteration<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#34;&#34;&#34;</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="st">        tdm: the copus, of (D, Num_words_in_corpus),</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="st">            the value of each entry is the counts of corresponding words in this the corresponding document.</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="st">            e.g.</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="st">            tdm[d, w] = number of word w appears in document d.</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="st">        T: the number of topics</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="st">        &#34;&#34;&#34;</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tdm <span class="op">=</span> tdm</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.D, <span class="va">self</span>.W <span class="op">=</span> <span class="va">self</span>.tdm.shape            </span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha<span class="op">=</span> alpha <span class="co"># count for expected value for hyper parameter alpha of theta, i.e. document-topic distribution.</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta <span class="co"># count for expected value for hyper parameter beta topic-word distribution.</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iteration <span class="op">=</span> iteration</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z must take in (d,w,i) as input, corresponding to</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># topic indicator for i-th obserevation of word w in doc d</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z <span class="op">=</span> {}</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.topic_word_matrix <span class="op">=</span> np.zeros((<span class="va">self</span>.T, <span class="va">self</span>.W)) <span class="co"># initialize the topic-word matrix.</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.doc_topic_matrix <span class="op">=</span> np.zeros((<span class="va">self</span>.D, <span class="va">self</span>.T)) <span class="co"># initialize the documnet-topic matrix.</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.topic_counts <span class="op">=</span> np.zeros(<span class="va">self</span>.T) <span class="co"># initialize the topic counter for after sampling process, should be sum of value in self.topic_word_matrix</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.doc_counts <span class="op">=</span> np.zeros(<span class="va">self</span>.D) <span class="co"># initialize the doc counter for after sampling process, should be sum of value in self.doc_topic_matrix</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log_likelihood <span class="op">=</span> np.zeros(<span class="va">self</span>.iteration) <span class="co"># store the value of log likelihood at each iteration</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_matrix()</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># @pysnooper.snoop(&#39;init.log&#39;)    </span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_matrix(<span class="va">self</span>):</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#34;&#34;&#34;</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="st">        for all words</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="st">        1. sample a topic randomly from T topics for each word</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="st">        2. increment topic word count, self.topic_word_matrix</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a><span class="st">        3. increment document topic count,  self.doc_topic_matrix</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a><span class="st">        4. update the topic indicator z.</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="st">        &#34;&#34;&#34;</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.D):</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>            doc <span class="op">=</span> scipy.sparse.coo_matrix(<span class="va">self</span>.tdm[d])</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>            word_freq_topic <span class="op">=</span> <span class="bu">zip</span>(doc.col, doc.data)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> w, frequency <span class="kw">in</span> word_freq_topic: <span class="co"># (word, freq)</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(frequency):</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>                    <span class="co">############ Finish the following initialization steps #############</span></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># 1. sample a topic randomly from T topics for each word</span></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>                    topic <span class="op">=</span> np.random.randint(<span class="va">self</span>.T)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># 2. increment topic word count, self.topic_word_matrix</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.topic_word_matrix[topic, w] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># 3. increment document topic count,  self.doc_topic_matrix</span></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.doc_topic_matrix[d, topic] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># 4. update the topic indicator z.</span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.z[(d, w, i)] <span class="op">=</span> topic <span class="co"># d: document ID; w: word ID: i: instance ID，即在d中第几个w</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.topic_counts <span class="op">=</span> <span class="va">self</span>.topic_word_matrix.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.doc_counts <span class="op">=</span> <span class="va">self</span>.doc_topic_matrix.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># @pysnooper.snoop(&#39;fit.log&#39;)    </span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>):</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.iteration):</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>            <span class="co"># iterate over all the documents</span></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.D):</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>            <span class="co"># iterate over all the words in d</span></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.tdm[d].indices:</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># iterate over number of times observed word w in doc d</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.tdm[d, w]):</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># we apply the hidden-varible method of Gibbs sampler, the hidden variable is z[(d,w,i)]</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.doc_topic_matrix[d,<span class="va">self</span>.z[(d,w,i)]] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.doc_counts[d] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.topic_word_matrix[<span class="va">self</span>.z[(d,w,i)],w] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.topic_counts[<span class="va">self</span>.z[(d,w,i)]] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># estimation of phi and theta for the current corpus</span></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>                        phi_hat <span class="op">=</span> (<span class="va">self</span>.topic_word_matrix[:,w] <span class="op">+</span> <span class="va">self</span>.beta) <span class="op">/</span> (<span class="va">self</span>.topic_counts <span class="op">+</span> <span class="va">self</span>.beta <span class="op">*</span> <span class="va">self</span>.W)</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>                        theta_hat <span class="op">=</span> (<span class="va">self</span>.doc_topic_matrix[d,:] <span class="op">+</span> <span class="va">self</span>.alpha) <span class="op">/</span> (<span class="va">self</span>.doc_counts[d] <span class="op">+</span> <span class="va">self</span>.alpha <span class="op">*</span> <span class="va">self</span>.T)</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># calculate the full conditional distribution</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>                        full_conditional <span class="op">=</span> phi_hat <span class="op">*</span> theta_hat</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># normalize full_conditional such that it summation equals to 1.</span></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>                        full_conditional <span class="op">=</span> full_conditional <span class="op">/</span> full_conditional.<span class="bu">sum</span>()</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># sample a topic for i-th obserevation of word w in doc d based on full_conditional</span></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>                        new_topic <span class="op">=</span> np.random.multinomial(<span class="dv">1</span>, full_conditional).argmax()</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># update z, doc_topic_matrix, doc_counts, topic_word_matrix, topic_counts here.</span></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.z[(d,w,i)] <span class="op">=</span> new_topic</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.doc_topic_matrix[d,<span class="va">self</span>.z[(d,w,i)]] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.topic_word_matrix[<span class="va">self</span>.z[(d,w,i)],w] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.doc_counts[d] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.topic_counts[<span class="va">self</span>.z[(d,w,i)]] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>                        <span class="co">############################################################</span></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Equation 2  log P(w|z)  for each iteration based on Equation [2]</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>            <span class="co">## +++++++++ insert code below ++++++++++++++++++++++++</span><span class="al">###</span></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log_likelihood[it] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.T):</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> w <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.W):</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.log_likelihood[it] <span class="op">+=</span> <span class="va">self</span>.topic_word_matrix[k,w] <span class="op">*</span> np.log((<span class="va">self</span>.topic_word_matrix[k,w] <span class="op">+</span> <span class="va">self</span>.beta) <span class="op">/</span> (<span class="va">self</span>.topic_counts[k] <span class="op">+</span> <span class="va">self</span>.beta <span class="op">*</span> <span class="va">self</span>.W))</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>            <span class="co">############################################################</span></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;Iteration </span><span class="sc">%i</span><span class="ch">\t</span><span class="st"> LL: </span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> (it,<span class="va">self</span>.log_likelihood[it]))</span></code></pre></div>
<h3 id="本质与使用条件">本质与使用条件</h3>
<p>本质上说，主题模型根本上是实现文本数据的结构化，结构化的文档可以彼此比较和查询，实现传统的任务。
LDA主题模型本质上解决了两类问题： - 文档聚类 - 词汇聚类</p>
<p>主要价值在于： 1）文档的结构化，相比于传统的词袋模型达到了降维的效果
2）完成了文档的聚类和词汇的聚类，实现文本信息的抽象化分析，帮助分析者探索隐含的语义内容。</p>
<p>实践中数据要有以下性质才会有较好的结果：</p>
<ol type="1">
<li>文档足够多</li>
<li>文档足够长</li>
<li>词汇特征够多</li>
<li>词频足够大</li>
</ol>
<h2 id="总结-1">总结</h2>
<p>历时好几周，终于完结了主题模型，主要是概率论没有学好，跟着推导的过程过于痛苦，不过也算是稍微理解了一点LDA，复述一下：</p>
<p><strong>LDA理解可以类比于PLSA，大体的思想都是根据文档生成主题分布，再根据主题分布和单词分布得到文档中的各个单词。不同的是LDA是贝叶斯派的思想，对于两种分布加入了狄利克雷先验概率。LDA的生成过程可以看成上帝掷骰子，从M个骰子中选取一个作为文本m的主题分布，从K个骰子中选取一个作为主题k的单词分布，（注意这里的多项分布的参数就是多项分布中的概率p，其服从狄利克雷分布，比如对于<span class="math inline">\(\theta_m\)</span>，它其实就是文本m生成不同主题k的概率<span class="math inline">\(p(z\mid d_m)\)</span>，是个K维的向量。对于<span class="math inline">\(\varphi_k\)</span>，是由主题k生成不同单词v的概率<span class="math inline">\(p(w\mid
z_k)\)</span>，是个V维的向量。也就是根据狄利克雷分布采样得到的是一些概率，这些概率也是我们最终要求的参数，这些概率作为多项分布的参数再采样生成主题或者单词，还有就是<span class="math inline">\(p(z_k\mid d_m)\)</span>与<span class="math inline">\(z_{mn}\)</span>的理解，前者就是相当于<span class="math inline">\(\theta_{mk}\)</span>，后者肯定是主题集合中的一个，不过是根据参数为<span class="math inline">\(\theta_m\)</span>的多项分布在位置n采样得到的。这就是LDA的整个的理解，当然模型的求解是使用吉布斯抽样的方法，与上面写的步骤不同。写这些是便于理解）。</strong></p>
<p>由主题分布可以对文本的每个位置赋值一个主题，再根据主题-单词分布可以生成整个文本。一切的一切都是和PLSA一样，求两个分布，以至于可以生成我们的文档。LDA也可以得到文档的主题分布，得到了主题分布和单词分布可以应用于各种任务当中。具体可以参考《LDA漫游指南》。</p>
<p>现在知道了LDA是怎么一回事了，但还是感觉模模糊糊的，感觉如“通俗理解LDA主题模型”这篇文章开头所说的那样陷入了LDA的细枝末节中，所以写了一些主题，加深自己的印象与理解，经过代码的洗礼，又理解深入了一些，但感觉还没有掌握的很好，可能需要消化消化，那就先告一段落了。以后常看看就行。
## 参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/374924140">https://zhuanlan.zhihu.com/p/374924140</a>
<a href="https://www.cnblogs.com/gasongjian/p/7631978.html">https://www.cnblogs.com/gasongjian/p/7631978.html</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-04-09</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型" data-hashtags="NLP,主题模型"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-hashtag="NLP"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">主题模型</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/transformer/" class="prev" rel="prev" title="Transformer"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Transformer</a>
            <a href="/knn/" class="next" rel="next" title="KNN">KNN<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
