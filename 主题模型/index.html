<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>主题模型 - vllbc02</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="主题模型" />
<meta property="og:description" content="主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 词袋模型 将所有词语装进一个袋子里，不考虑其" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" /><meta property="og:image" content="https://vllbc.top/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-11-12T20:48:52+08:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://vllbc.top/logo.png"/>

<meta name="twitter:title" content="主题模型"/>
<meta name="twitter:description" content="主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 词袋模型 将所有词语装进一个袋子里，不考虑其"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" /><link rel="prev" href="https://vllbc.top/word-embedding/" /><link rel="next" href="https://vllbc.top/transformer/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "主题模型",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/vllbc.top\/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "NLP","wordcount":  11531 ,
        "url": "https:\/\/vllbc.top\/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\/","datePublished": "2022-04-25T00:00:00+00:00","dateModified": "2022-11-12T20:48:52+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/vllbc.top\/images\/avatar.png",
                    "width":  1080 ,
                    "height":  1080 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">主题模型</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-04-25">2022-04-25</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 11531 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 24 分钟&nbsp;<span id="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" class="leancloud_visitors" data-flag-title="主题模型">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#词袋模型">词袋模型</a></li>
    <li><a href="#lsa">LSA</a>
      <ul>
        <li><a href="#单词-文本矩阵">单词-文本矩阵</a></li>
        <li><a href="#单词-主题矩阵">单词-主题矩阵</a></li>
        <li><a href="#主题-文本矩阵">主题-文本矩阵</a></li>
        <li><a href="#从单词向量空间到主题向量空间的线性变换">从单词向量空间到主题向量空间的线性变换</a></li>
        <li><a href="#潜在语义分析">潜在语义分析</a>
          <ul>
            <li><a href="#截断奇异值分析">截断奇异值分析</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#plsa">PLSA</a>
      <ul>
        <li><a href="#生成模型">生成模型</a></li>
        <li><a href="#共现模型">共现模型</a></li>
        <li><a href="#与潜在语义分析的关系">与潜在语义分析的关系</a></li>
        <li><a href="#概率潜在语义分析的算法">概率潜在语义分析的算法</a></li>
        <li><a href="#总结算法">总结算法</a></li>
        <li><a href="#用法">用法</a></li>
        <li><a href="#优点与不足">优点与不足</a>
          <ul>
            <li><a href="#优点">优点</a></li>
            <li><a href="#不足">不足</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#lda">LDA</a>
      <ul>
        <li><a href="#多项分布">多项分布</a></li>
        <li><a href="#狄利克雷分布">狄利克雷分布</a></li>
        <li><a href="#二项分布与贝塔分布">二项分布与贝塔分布</a></li>
        <li><a href="#基本想法">基本想法</a></li>
        <li><a href="#lda与plsa的关系">LDA与PLSA的关系</a></li>
        <li><a href="#模型定义">模型定义</a>
          <ul>
            <li><a href="#模型要素">模型要素</a></li>
            <li><a href="#生成过程">生成过程</a></li>
            <li><a href="#总结">总结</a></li>
          </ul>
        </li>
        <li><a href="#概率计算">概率计算</a></li>
        <li><a href="#吉布斯抽样">吉布斯抽样</a>
          <ul>
            <li><a href="#基本思想">基本思想</a></li>
            <li><a href="#算法流程">算法流程</a></li>
          </ul>
        </li>
        <li><a href="#训练与推断">训练与推断</a></li>
        <li><a href="#代码">代码</a></li>
        <li><a href="#本质与使用条件">本质与使用条件</a></li>
      </ul>
    </li>
    <li><a href="#总结-1">总结</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="主题模型">主题模型</h1>
<p>主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的</p>
<h2 id="词袋模型">词袋模型</h2>
<p>将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的</p>
<p>例子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">句子1</span><span class="err">：</span><span class="n">我</span> <span class="n">爱</span> <span class="n">北</span> <span class="n">京</span> <span class="n">天</span> <span class="n">安</span> <span class="n">门</span>
</span></span><span class="line"><span class="cl"><span class="n">转换为</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">句子2</span><span class="err">：</span><span class="n">我</span> <span class="n">喜</span> <span class="n">欢</span> <span class="n">上</span> <span class="n">海</span>
</span></span><span class="line"><span class="cl"><span class="n">转换为</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</span></span><span class="line"><span class="cl"><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;This document is the second document.&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;And this is the third one.&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[[0 1 1 1 0 0 1 0 1]
</span></span><span class="line"><span class="cl"> [0 2 0 1 0 1 1 0 1]
</span></span><span class="line"><span class="cl"> [1 0 0 1 1 0 1 1 1]
</span></span><span class="line"><span class="cl"> [0 1 1 1 0 0 1 0 1]]
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="lsa">LSA</h2>
<p>LSA就是潜在语义分析。特点是通过矩阵分解发现文本与单词之间基于主题（话题）的语义关系。
首先要清楚几个概念：</p>
<h3 id="单词-文本矩阵">单词-文本矩阵</h3>
<p>$$
X=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 n} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
x_{m 1} &amp; x_{m 2} &amp; \cdots &amp; x_{m n}
\end{array}\right]
$$</p>
<p>这是一个 $m \times n$ 矩阵, 元素 $x_{i j}$ 表示单词 $w_i$ 在文本 $d_j$ 中出现的频数或权值。由于单 词的种类很多, 而每个文本中出现单词的种类通常较少, 所以单词-文本矩阵是一个稀 疏矩阵。
权值通常用单词频率-逆文本频率 (term frequency-inverse document frequency, TF-IDF）表示，其定义是</p>
<p>$$
\operatorname{TFIDF}_{i j}=\frac{\mathrm{tf}_{i j}}{\mathrm{tf}_{\bullet j}} \log \frac{\mathrm{df}}{\mathrm{df}_i}, \quad i=1,2, \cdots, m ; \quad j=1,2, \cdots, n
$$</p>
<p>直观上讲，可以直接用每一列作为文本语义表达， 因此可以通过余弦相似度等计算文本之间的相似性，并且矩阵稀疏，计算量较少。但其并不关心文本中词语出现的顺序等信息，因此需要改进。</p>
<h3 id="单词-主题矩阵">单词-主题矩阵</h3>
<p>假设所有文本共含有 $k$ 个话题。假设每个话题由一个定义在单词集合 $W$ 上的 $m$ 维向量表示, 称为话题向量, 即</p>
<p>$$
t_l=\left[\begin{array}{c}
t_{1 l} \\
t_{2 l} \\
\vdots \\
t_{m l}
\end{array}\right], \quad l=1,2, \cdots, k
$$</p>
<p>其中 $t_{i l}$ 是单词 $w_i$ 在话题 $t_l$ 的权值, $i=1,2, \cdots, m$, 权值越大, 该单词在该话题中 的重要度就越高。这 $k$ 个话题向量 $t_1, t_2, \cdots, t_k$ 张成一个话题向量空间 (topic vector
话题向量空间 $T$ 也可以表示为一个矩阵, 称为单词-主题矩阵 (word-topic matrix）, 记作</p>
<p>$$
T=\left[\begin{array}{cccc}
t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1 k} \\
t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2 k} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
t_{m 1} &amp; t_{m 2} &amp; \cdots &amp; t_{m k}
\end{array}\right]
$$</p>
<h3 id="主题-文本矩阵">主题-文本矩阵</h3>
<p>将单词-文本矩阵中的文本$x_j$投影到主题向量空间$J$中，得到在主题空间中的一个向量$y_j$。</p>
<p>$$
Y=\left[\begin{array}{cccc}
y_{11} &amp; y_{12} &amp; \cdots &amp; y_{1 n} \\
y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2 n} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
y_{k 1} &amp; y_{k 2} &amp; \cdots &amp; y_{k n}
\end{array}\right]
$$</p>
<h3 id="从单词向量空间到主题向量空间的线性变换">从单词向量空间到主题向量空间的线性变换</h3>
<p>单词-文本矩阵$X$可以近似表示为单词-主题矩阵$T$与主题-文本矩阵$Y$的乘积，这就是潜在语义分析：</p>
<p>$$
X\approx TY
$$</p>
<h3 id="潜在语义分析">潜在语义分析</h3>
<p>给定单词-文本矩阵$X$，每一行代表一个单词，每一列代表一个文本。其中的元素代表单词在文本中的权重或者频数（词袋模型）。</p>
<h4 id="截断奇异值分析">截断奇异值分析</h4>
<p>$$
X \approx U_k \Sigma_k V_k^{\mathrm{T}}=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \sigma_2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_k
\end{array}\right]\left[\begin{array}{c}
v_1^{\mathrm{T}} \\
v_2^{\mathrm{T}} \\
\vdots \\
v_k^{\mathrm{T}}
\end{array}\right]
$$</p>
<p>接下来考虑文本在主题空间中的表示。</p>
<p>$$
\begin{aligned}
X &amp;=\left[\begin{array}{llll}
x_1 &amp; x_2 &amp; \cdots &amp; x_n
\end{array}\right] \approx U_k \Sigma_k V_k^{\mathrm{T}} \\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 &amp; &amp; &amp; \\
&amp; \sigma_2 &amp; 0 &amp; \\
0 &amp; \ddots &amp; \\
&amp; &amp; \sigma_k
\end{array}\right]\left[\begin{array}{cccc}
v_{11} &amp; v_{21} &amp; \cdots &amp; v_{n 1} \\
v_{12} &amp; v_{22} &amp; \cdots &amp; v_{n 2} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
v_{1 k} &amp; v_{2 k} &amp; \cdots &amp; v_{n k}
\end{array}\right] \\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{cccc}
\sigma_1 v_{11} &amp; \sigma_1 v_{21} &amp; \cdots &amp; \sigma_1 v_{n 1} \\
\sigma_2 v_{12} &amp; \sigma_2 v_{22} &amp; \cdots &amp; \sigma_2 v_{n 2} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
\sigma_k v_{1 k} &amp; \sigma_k v_{2 k} &amp; \cdots &amp; \sigma_k v_{n k}
\end{array}\right]
\end{aligned}
$$</p>
<p>其中:</p>
<p>$$
u_l = \begin{bmatrix}u_{1l} \\u_{2l} \\ \vdots \\u_{ml} \end{bmatrix}, \quad l= 1, 2, \dots, k
$$</p>
<p>代表单词对主题的权重。</p>
<p>由式知, 矩阵 $X$ 的第 $j$ 列向量 $x_j$ 满足</p>
<p>$$
\begin{aligned}
x_j &amp; \approx U_k\left(\Sigma_k V_k^{\mathrm{T}}\right)_j \\
&amp;=\left[\begin{array}{llll}
u_1 &amp; u_2 &amp; \cdots &amp; u_k
\end{array}\right]\left[\begin{array}{c}
\sigma_1 v_{j 1} \\
\sigma_2 v_{j 2} \\
\vdots \\
\sigma_k v_{j k}
\end{array}\right] \\
&amp;=\sum_{l=1}^k \sigma_l v_{j l} u_l, \quad j=1,2, \cdots, n
\end{aligned}
$$</p>
<p>则$\Sigma_kV_k^T$每一个列向量是一个文本在主题向量空间中的表示。</p>
<h2 id="plsa">PLSA</h2>
<h3 id="生成模型">生成模型</h3>
<p>假设有单词集合 $W=\left\{w_1, w_2, \cdots, w_M\right\}$, 其中 $M$ 是单词个数; 文本 (指标) 集 合 $D=\left\{d_1, d_2, \cdots, d_N\right\}$, 其中 $N$ 是文本个数; 话题集合 $Z=\left\{z_1, z_2, \cdots, z_K\right\}$, 其中 $K$ 是预先设定的话题个数。随机变量 $w$ 取值于单词集合; 随机变量 $d$ 取值于文本集 合, 随机变量 $z$ 取值于话题集合。概率分布 $P(d)$ 、条件概率分布 $P(z \mid d)$ 、条件概率分 布 $P(w \mid z)$ 皆属于多项分布, 其中 $P(d)$ 表示生成文本 $d$ 的概率, $P(z \mid d)$ 表示文本 $d$ 生 成话题 $z$ 的概率, $P(w \mid z)$ 表示话题 $z$ 生成单词 $w$ 的概率。</p>
<p>每个文本 $d$ 拥有自己的话题概率分布 $P(z \mid d)$, 每个话题 $z$ 拥有自己的单词概率分 布 $P(w \mid z)$; 也就是说一个文本的内容由其相关话题决定, 一个话题的内容由其相关单词决定。</p>
<p>生成模型通过以下步骤生成文本-单词共现数据:
(1) 依据概率分布 $P(d)$, 从文本 (指标) 集合中随机选取一个文本 $d$, 共生成 $N$ 个文本; 针对每个文本, 执行以下操作;
(2) 在文本 $d$ 给定条件下, 依据条件概率分布 $P(z \mid d)$, 从话题集合随机选取一个 话题 $z$, 共生成 $L$ 个话题, 这里 $L$ 是文本长度;
(3) 在话题 $z$ 给定条件下, 依据条件概率分布 $P(w \mid z)$, 从单词集合中随机选取一 个单词 $w$ 。</p>
<p>生成模型中, 单词变量 $w$ 与文本变量 $d$ 是观测变量, 话题变量 $z$ 是隐变量。也就 是说模型生成的是单词-话题-文本三元组 $(w, z, d)$ 的集合, 但观测到的是单词-文本二 元组 $(w, d)$ 的集合, 观测数据表示为单词-文本矩阵 $T$ 的形式, 矩阵 $T$ 的行表示单词, 列表示文本, 元素表示单词-文本对 $(w, d)$ 的出现次数。</p>
<p>从数据的生成过程可以推出, 文本-单词共现数据 $T$ 的生成概率为所有单词-文本 对 $(w, d)$ 的生成概率的乘积,</p>
<p>$$
P(T)=\prod_{(w, d)} P(w, d)^{n(w, d)}
$$</p>
<p>这里 $n(w, d)$ 表示 $(w, d)$ 的出现次数, 单词-文本对出现的总次数是 $N \times L$ 。每个单 词-文本对 $(w, d)$ 的生成概率由以下公式决定:</p>
<p>$$
\begin{aligned}
P(w, d) &amp;=P(d) P(w \mid d) \\
&amp;=P(d) \sum_z P(w, z \mid d) \\
&amp;=P(d) \sum_z P(z \mid d) P(w \mid z)
\end{aligned}
$$</p>
<p>即生成模型的定义。
生成模型假设在话题 $z$ 给定条件下, 单词 $w$ 与文本 $d$ 条件独立, 即</p>
<p>$$
P(w, z \mid d)=P(z \mid d) P(w \mid z)
$$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221012121216.png" width="432" height="163" /></p>
<h3 id="共现模型">共现模型</h3>
<p>$$
P(T)=\prod_{(w, d)} P(w, d)^{n(w, d)}
$$</p>
<p>每个单词-文本对 $(w, d)$ 的概率由以下公式决定:</p>
<p>$$
P(w, d)=\sum_{z \in Z} P(z) P(w \mid z) P(d \mid z)
$$</p>
<p>式 (18.5) 即共现模型的定义。容易验证, 生成模型 (18.2) 和共现模型 (18.5) 是等价的。 共现模型假设在话题 $z$ 给定条件下, 单词 $w$ 与文本 $d$ 是条件独立的, 即</p>
<p>$$
P(w, d \mid z)=P(w \mid z) P(d \mid z)
$$</p>
<p>直观解释：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221014225958.png" width="409" height="234" /></p>
<h3 id="与潜在语义分析的关系">与潜在语义分析的关系</h3>
<p>共现模型也可以表示为三个矩阵乘积的形式。这样, 概率潜在语义分析与 潜在语义分析的对应关系可以从中看得很清楚。下面是共现模型的矩阵乘积形式:</p>
<p>$$
\begin{aligned}
X^{\prime} &amp;=U^{\prime} \Sigma^{\prime} V^{\prime \mathrm{T}} \\
X^{\prime} &amp;=[P(w, d)]<em>{M \times N} \\
U^{\prime} &amp;=[P(w \mid z)]</em>{M \times K} \\
\Sigma^{\prime} &amp;=[P(z)]<em>{K \times K} \\
V^{\prime} &amp;=[P(d \mid z)]</em>{N \times K}
\end{aligned}
$$</p>
<h3 id="概率潜在语义分析的算法">概率潜在语义分析的算法</h3>
<p>Plsa是含有隐变量的模型，其学习通常使用EM算法。
E步是计算Q函数，M步是极大化Q函数。</p>
<p>设单词集合为 $W=\{w_1, w_2, \cdots, w_M\}$, 文本集合为 $D=\{d_1, d_2, \cdots, d_N\}$, 话 题集合为 $Z=\left\{z_1, z_2, \cdots, z_K\right\}$ 。给定单词-文本共现数据 $T=\left\{n\left(w_i, d_j\right)\right\}, i=$ $1,2, \cdots, M, j=1,2, \cdots, N$, 目标是估计概率潜在语义分析模型（生成模型）的 参数。如果使用极大似然估计, 对数似然函数是</p>
<p>$$
\begin{aligned}
L &amp;=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \log P\left(w_i, d_j\right) \\
&amp;=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \log \left[\sum_{k=1}^K P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)\right]
\end{aligned}
$$</p>
<p>但是模型含有隐变量, 对数似然函数的优化无法用解析方法求解, 这时使用 EM算法。 应用 EM算法的核心是定义 $Q$ 函数。
$\mathrm{E}$ 步：计算 $Q$ 函数
$Q$ 函数为完全数据的对数似然函数对不完全数据的条件分布的期望。针对概率潜 在语义分析的生成模型, $Q$ 函数是</p>
<p>$$
Q=\sum_{k=1}^K\left\{\sum_{j=1}^N n\left(d_j\right)\left[\log P\left(d_j\right)+\sum_{i=1}^M \frac{n\left(w_i, d_j\right)}{n\left(d_j\right)} \log P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)\right]\right\} P\left(z_k \mid w_i, d_j\right)
$$</p>
<p>式中 $n\left(d_j\right)=\sum_{i=1}^M n\left(w_i, d_j\right)$ 表示文本 $d_j$ 中的单词个数, $n\left(w_i, d_j\right)$ 表示单词 $w_i$ 在文本 $d_j$ 中出现的次数。条件概率分布 $P\left(z_k \mid w_i, d_j\right)$ 代表不完全数据, 是已知变量。条件概 率分布 $P\left(w_i \mid z_k\right)$ 和 $P\left(z_k \mid d_j\right)$ 的乘积代表完全数据, 是末知变量。
由于可以从数据中直接统计得出 $P\left(d_j\right)$ 的估计, 这里只考虑 $P\left(w_i \mid z_k\right), P\left(z_k \mid d_j\right)$ 的估计, 可将 $Q$ 函数简化为函数 $Q^{\prime}$</p>
<p>$$
Q^{\prime}=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i, d_j\right) \sum_{k=1}^K P\left(z_k \mid w_i, d_j\right) \log \left[P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)\right]
$$</p>
<p>$Q^{\prime}$ 函数中的 $P\left(z_k \mid w_i, d_j\right)$ 可以根据贝叶斯公式计算</p>
<p>$$
P\left(z_k \mid w_i, d_j\right)=\frac{P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)}{\sum_{k=1}^K P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)}
$$</p>
<p>其中 $P\left(z_k \mid d_j\right)$ 和 $P\left(w_i \mid z_k\right)$ 由上一步迭代得到。
$\mathrm{M}$ 步: 极大化 $Q$ 函数。
通过约束最优化求解 $Q$ 函数的极大值, 这时 $P\left(z_k \mid d_j\right)$ 和 $P\left(w_i \mid z_k\right)$ 是变量。因为 变量 $P\left(w_i \mid z_k\right), P\left(z_k \mid d_j\right)$ 形成概率分布, 满足约束条件</p>
<p>$$
\begin{aligned}
&amp;\sum_{i=1}^M P\left(w_i \mid z_k\right)=1, \quad k=1,2, \cdots, K \\
&amp;\sum_{k=1}^K P\left(z_k \mid d_j\right)=1, \quad j=1,2, \cdots, N
\end{aligned}
$$</p>
<p>应用拉格朗日法, 引入拉格朗日乘子 $\tau_k$ 和 $\rho_j$, 定义拉格朗日函数 $A$</p>
<p>$$
\Lambda=Q^{\prime}+\sum_{k=1}^K \tau_k\left(1-\sum_{i=1}^M P\left(w_i \mid z_k\right)\right)+\sum_{j=1}^N \rho_j\left(1-\sum_{k=1}^K P\left(z_k \mid d_j\right)\right)
$$</p>
<p>将拉格朗日函数 $\Lambda$ 分别对 $P\left(w_i \mid z_k\right)$ 和 $P\left(z_k \mid d_j\right)$ 求偏导数, 并令其等于 0 , 得到下面 的方程组</p>
<p>$$
\begin{aligned}
&amp;\sum_{j=1}^N n\left(w_i, d_j\right) P\left(z_k \mid w_i, d_j\right)-\tau_k P\left(w_i \mid z_k\right)=0, \quad i=1,2, \cdots, M ; \quad k=1,2, \cdots, K \\
&amp;\sum_{i=1}^M n\left(w_i, d_j\right) P\left(z_k \mid w_i, d_j\right)-\rho_j P\left(z_k \mid d_j\right)=0, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K
\end{aligned}
$$</p>
<p>解方程组得到 $M$ 步的参数估计公式:</p>
<p>$$
P\left(w_i \mid z_k\right)=\frac{\sum_{j=1}^N n\left(w_i, d_j\right) P\left(z_k \mid w_i, d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N n\left(w_m, d_j\right) P\left(z_k \mid w_m, d_j\right)}
$$</p>
<p>$$
P(z_k\mid d_j) = \frac{\sum_{i=1}^Mn(w_i, d_j)P(z_k\mid w_i,d_j)}{n(d_j)}
$$</p>
<h3 id="总结算法">总结算法</h3>
<p>输入: 设单词集合为 $W=\left\{w_1, w_2, \cdots, w_M\right\}$, 文本集合为 $D=\left\{d_1, d_2, \cdots, d_N\right\}$, 话题集合为 $Z=\left\{z_1, z_2, \cdots, z_K\right\}$, 共现数据 $\left\{n\left(w_i, d_j\right)\right\}, i=1,2, \cdots, M, j=1$, $2, \cdots, N$;
输出: $P\left(w_i \mid z_k\right)$ 和 $P\left(z_k \mid d_j\right)$ 。
(1) 设置参数 $P\left(w_i \mid z_k\right)$ 和 $P\left(z_k \mid d_j\right)$ 的初始值。
(2) 迭代执行以下 $\mathrm{E}$ 步, $\mathrm{M}$ 步, 直到收敛为止。
$\mathrm{E}$ 步:</p>
<p>$$
P\left(z_k \mid w_i, d_j\right)=\frac{P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)}{\sum_{k=1}^K P\left(w_i \mid z_k\right) P\left(z_k \mid d_j\right)}
$$</p>
<p>M 步:</p>
<p>$$
\begin{aligned}
P\left(w_i \mid z_k\right) &amp;=\frac{\sum_{j=1}^N n\left(w_i, d_j\right) P\left(z_k \mid w_i, d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N n\left(w_m, d_j\right) P\left(z_k \mid w_m, d_j\right)} \\
P\left(z_k \mid d_j\right) &amp;=\frac{\sum_{i=1}^M n\left(w_i, d_j\right) P\left(z_k \mid w_i, d_j\right)}{n\left(d_j\right)}
\end{aligned}
$$</p>
<h3 id="用法">用法</h3>
<p>与LSA类似，可以把文档对各个主题的概率看作是文档的表示，最后用到的就是$P(z_k\mid d_j)$。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221018125347.png" width="374" height="36" />
k就是我们自己设定的主题数，一般来说K远远小于文档个数和词汇表大小，这样也达到了降维的目的。</p>
<h3 id="优点与不足">优点与不足</h3>
<h4 id="优点">优点</h4>
<p>pLSA是在一套比较完整的思想的基础上提出来的，模型中各项参数有明确的物理含义，可解释性比较强。相比LSA，pLSA对人类生成文本机制的刻画更加细致、更加符合我们的常识，比如，pLSA基于条件概率，引入了一个“隐含变量”（相对于可以看到的文档和词语，是不可观测变的），即主题，来描述文本生成的过程。</p>
<h4 id="不足">不足</h4>
<p>pLSA的理论与我们的实践不是那么的统一:
(1) 我们说话的时候，根本不会考虑&quot; 我说这段话的概率大小&quot;，即 $p\left(d_t\right)$
(2) pLSA认为，我们说话时面向的主题分布，取决于 &ldquo;文档&rdquo; （实际上是文档ID)。这个假设显然是不合理的，小说家不会因为自己写到第666回而调整 主题。
(3) 类似 (2)，随着上下文的变化，我们围绕一个主题说话的内容和方式也 会发生改变。在主题模型中，这种改变的体现，就是一个主题下的词语概率分 布会发生改变。而pLSA忽略了这样的事实。</p>
<p>从计算复杂度的角度看pLSA有两个比较大的缺陷:
(1) pLSA中，对文档 出现的概率估计，来自对训练语料的学习。而对于一个 末知文档，我们是无法估计它出现的概率的一一因此pLSA无法对训练语料之 外的文档进行处理。pLSA的这个特点决定了，在在线(online) 场景中(数据是 持续增加的)，那么文档处理系统就需要定时使用pLSA对整个语料库进行计 算。因此，pLSA比较适合允许一定时滞的离线计算。
(2) pLSA认为一个文档对各个主题的隶属度是一定的——而一个主题对各个词语的隶属度也是一定的，因此pLSA在生成一个文档的各个词语时、使用了相同的词语概率分布。这样，pLSA需要为每一个文档记录一个专门的随着语料数据集规模的增加，pLSA的参数规模也会增加，导致模型训练越来越困难。</p>
<h2 id="lda">LDA</h2>
<p>LDA模型是文本集合的生成概率模型。</p>
<p>LDA 的文本集合的生成过程如下: 首先随机生成一个文本的话题分布, 之后在该 文本的每个位置, 依据该文本的话题分布随机生成一个话题, 然后在该位置依据该话 题的单词分布随机生成一个单词, 直至文本的最后一个位置, 生成整个文本。重复以 上过程生成所有文本。</p>
<p>LDA 模型是含有隐变量的概率图模型。模型中, 每个话题的单词分布, 每个文 本的话题分布, 文本的每个位置的话题是隐变量; 文本的每个位置的单词是观测变 量。LDA 模型的学习与推理无法直接求解, 通常使用吉布斯抽样 (Gibbs sampling) 和 变分 EM算法 (variational EM algorithm), 前者是蒙特卡罗法, 而后者是近似算法。</p>
<h3 id="多项分布">多项分布</h3>
<p>(多项分布) 若多元离散随机变量 $X=\left(X_1, X_2, \cdots, X_k\right)$ 的概率质 量函数为</p>
<p>$$
\begin{aligned}
P\left(X_1=n_1, X_2=n_2, \cdots, X_k=n_k\right) &amp;=\frac{n !}{n_{1} ! n_{2} ! \cdots n_{k} !} p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k} \\
&amp;=\frac{n !}{\prod_{i=1}^k n_{i} !} \prod_{i=1}^k p_i^{n_i}
\end{aligned}
$$</p>
<p>其中 $p=\left(p_1, p_2, \cdots, p_k\right), p_i \geqslant 0, i=1,2, \cdots, k, \sum_{i=1}^k p_i=1, \sum_{i=1}^k n_i=n$, 则称随机变 量 $X$ 服从参数为 $(n, p)$ 的多项分布, 记作 $X \sim \operatorname{Mult}(n, p)$ 。</p>
<p>当试验的次数 $n$ 为 1 时, 多项分布变成类别分布 (categorical distribution)。类 别分布表示试验可能出现的 $k$ 种结果的概率。显然多项分布包含类别分布。</p>
<h3 id="狄利克雷分布">狄利克雷分布</h3>
<p>狄利克雷分布 (Dirichlet distribution) 是一种多元连续随机变量的概率分布, 是 贝塔分布 (beta distribution) 的扩展。在贝叶斯学习中, 狄利克雷分布常作为多项分 布的先验分布使用。
(狄利克雷分布) 若多元连续随机变量 $\theta=\left(\theta_1, \theta_2, \cdots, \theta_k\right)$ 的概率密 度函数为</p>
<p>$$
p(\theta \mid \alpha)=\frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \Gamma\left(\alpha_i\right)} \prod_{i=1}^k \theta_i^{\alpha_i-1}
$$</p>
<p>其中 $\sum_{i=1}^k \theta_i=1, \theta_i \geqslant 0, \alpha=\left(\alpha_1, \alpha_2, \cdots, \alpha_k\right), \alpha_i&gt;0, i=1,2, \cdots, k$, 则称随机变量 $\theta$ 服从参数为 $\alpha$ 的狄利克雷分布, 记作 $\theta \sim \operatorname{Dir}(\alpha)$ 。
式中 $\Gamma(s)$ 是伽马函数, 定义为</p>
<p>$$
\Gamma(s)=\int_0^{\infty} x^{s-1} \mathrm{e}^{-x} \mathrm{~d} x, \quad s&gt;0
$$</p>
<p>具有性质：</p>
<p>$$
\Gamma(s+1) = s\Gamma(s)
$$</p>
<p>当s为自然数时，有：</p>
<p>$$
\Gamma(s+1) = s!
$$</p>
<p>令</p>
<p>$$
\mathrm{B}(\alpha)=\frac{\prod_{i=1}^k \Gamma\left(\alpha_i\right)}{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}
$$</p>
<p>则狄利克雷分布的密度函数可以写成</p>
<p>$$
p(\theta \mid \alpha)=\frac{1}{\mathrm{~B}(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1}
$$</p>
<p>$\mathrm{B}(\alpha)$ 是规范化因子, 称为多元贝塔函数 (或扩展的贝塔函数)。由密度函数的性质</p>
<p>$$
\int \frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \Gamma\left(\alpha_i\right)} \prod_{i=1}^{\alpha_i-1} \mathrm{~d} \theta=\frac{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \Gamma\left(\alpha_i\right)} \int \prod_{i=1}^k \theta_i^{\alpha_i-1} \mathrm{~d} \theta=1
$$</p>
<p>得</p>
<p>$$
\mathrm{B}(\alpha)=\int \prod_{i=1}^k \theta_i^{\alpha_i-1} \mathrm{~d} \theta
$$</p>
<h3 id="二项分布与贝塔分布">二项分布与贝塔分布</h3>
<p>二项分布是多项分布的特殊情况, 贝塔分布是狄利克雷分布的特殊情况。
二项分布是指如下概率分布。 $X$ 为离散随机变量, 取值为 $m$, 其概率质量函数为</p>
<p>$$
P(X=m)=\left(\begin{array}{c}
n \\
m
\end{array}\right) p^m(1-p)^{n-m}, \quad m=0,1,2, \cdots, n
$$</p>
<p>其中 $n$ 和 $p(0 \leqslant p \leqslant 1)$ 是参数。</p>
<p>贝塔分布是指如下概率分布, $X$ 为连续随机变量, 取值范围为 $[0,1]$, 其概率密度 函数为</p>
<p>$$
p(x)= \begin{cases}\frac{1}{\mathrm{~B}(s, t)} x^{s-1}(1-x)^{t-1}, &amp; 0 \leqslant x \leqslant 1 \\ 0, &amp; \text { 其他 }\end{cases}
$$</p>
<p>其中 $s&gt;0$ 和 $t&gt;0$ 是参数, $\mathrm{B}(s, t)=\frac{\Gamma(s) \Gamma(t)}{\Gamma(s+t)}$ 是贝塔函数, 定义为</p>
<p>$$
\mathrm{B}(s, t)=\int_0^1 x^{s-1}(1-x)^{t-1} \mathrm{~d} x = \frac{\Gamma(s)\Gamma(t)}{\Gamma(s+t)}
$$</p>
<p>当 $s, t$ 是自然数时($\Gamma(s+1) = s!$),</p>
<p>$$
\mathrm{B}(s, t)=\frac{(s-1) !(t-1) !}{(s+t-1) !}
$$</p>
<p>当 $n$ 为 1 时, 二项分布变成伯努利分布（Bernoulli distribution）或 0-1 分布。 伯努利分布表示试验可能出现的 2 种结果的概率。显然二项分布包含伯努利分布。给出几种概率分布的关系。
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017225107.png" width="784" height="167" /></p>
<h3 id="基本想法">基本想法</h3>
<p>在LDA主题模型下，一篇文章由词语的序列组成。首先以一定概率选择一个主题，其次以一定概率在这个主题中选择一个词。如果一篇文章由1000个词组成，那么就把上述方式重复1000遍，就能组成这篇文章。那么值得注意的是，以一定概率选择一个主题是服从多项式分布的，而多项式分布的参数是服从Dirichlet分布的。以一定概率在特定主题中选择一个词也是服从多项式分布的，多项式分布的参数是服从Dirichlet分布的。为什么呢？因为Dirichlet分布是多项式分布的共轭分布，也就是说由贝叶斯估计得到的后验分布仍然是Dirichlet分布。
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221017233927.png" width="775" height="437" /></p>
<h3 id="lda与plsa的关系">LDA与PLSA的关系</h3>
<p>二者都是概率模型，都是利用概率生成模型对文本集合进行主题分析的无监督学习方法。</p>
<p>PLSA是用了频率派的方法，利用极大似然进行学习，而LDA使用了贝叶斯派的方法，进行贝叶斯推断。</p>
<p>二者都假设存在两个分布：话题是单词的多项分布，文本是话题的多项分布，不同的在于LDA认为多项分布的参数也服从一个分布，而不是固定不变的，使用狄利克雷分布作为多项分布的先验分布，也就是多项分布的参数服从狄利克雷分布。</p>
<p>引入先验概率的作用可以防止过拟合。为啥选择狄利克雷分布呢？因为它是多项分布的共轭先验分布，先验分布与后验分布形式相同，便于由先验分布得到后验分布。</p>
<p>LDA是在Plsa的基础上，为单词分布和主题分布增加了两个狄利克雷先验。
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102411.png" width="490" height="348" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221020102417.png" width="490" height="348" /></p>
<h3 id="模型定义">模型定义</h3>
<h4 id="模型要素">模型要素</h4>
<p>潜在狄利克雷分配 (LDA) 使用三个集合: 一是单词集合 $W=\left\{w_1, \cdots, w_v, \cdots\right.$, $\left.w_V\right\}$, 其中 $w_v$ 是第 $v$ 个单词, $v=1,2, \cdots, V, V$ 是单词的个数。二是文本集合 $D=\left\{\mathbf{w}_1, \cdots, \mathbf{w}_m, \cdots, \mathbf{w}_M\right\}$, 其中 $\mathbf{w}_m$ 是第 $m$ 个文本, $m=1,2, \cdots, M, M$ 是文本 的个数。文本 $\mathbf{w}_m$ 是一个单词序列 $\mathbf{w}_m=\left(w_{m 1}, \cdots, w_{m n}, \cdots, w_{m N_m}\right)$, 其中 $w_{m n}$ 是 文本 $\mathbf{w}_m$ 的第 $n$ 个单词, $n=1,2, \cdots, N_m, N_m$ 是文本 $\mathbf{w}_m$ 中单词的个数。三是主题集合集合 $Z=\left\{z_1, \cdots, z_k, \cdots, z_K\right\}$, 其中 $z_k$ 是第 $k$ 个话题, $k=1,2, \cdots, K, K$ 是话题的个数。</p>
<ul>
<li>每一个话题 $z_k$ 由一个单词的条件概率分布 $p\left(w \mid z_k\right)$ 决定, $w \in W$ 。分布 $p\left(w \mid z_k\right)$ 服从多项分布 (严格意义上类别分布), 其参数为 $\varphi_k$ 。参数 $\varphi_k$ 服从狄利克雷分布 (先验分布), 其超参数为 $\beta$ 。参数 $\varphi_k$ 是一个 $V$ 维向量 $\varphi_k=\left(\varphi_{k 1}, \varphi_{k 2}, \cdots, \varphi_{k V}\right)$, 其中 $\varphi_{k v}$ 表示话题 $z_k$ 生成单词 $w_v$ 的概率。所有话题的参数向量构成一个 $K \times V$ 矩阵 $\varphi=\left\{\varphi_k\right\}_{k=1}^K$ 。超参数 $\beta$ 也是一个 $V$ 维向量 $\beta=\left(\beta_1, \beta_2, \cdots, \beta_V\right)_{\text {。 }}$(对于话题$z_k$其生成单词$w_v$先验服从狄利克雷分布，因此是一个V维向量)</li>
<li>每一个文本 $\mathbf{w}_m$ 由一个话题的条件概率分布 $p\left(z \mid \mathbf{w}_m\right)$ 决定, $z \in Z_{\text {。 }}$ 分布 $p\left(z \mid \mathbf{w}_m\right)$ 服从多项分布 (严格意义上类别分布), 其参数为 $\theta_m$ 。参数 $\theta_m$ 服从狄利克雷分布 (先验分布), 其超参数为 $\alpha$ , 参数 $\theta_m$ 是一个 $K$ 维向量 $\theta_m=\left(\theta_{m 1}, \theta_{m 2}, \cdots, \theta_{m K}\right)$, 其中 $\theta_{m k}$ 表示文本 $\mathrm{w}_m$ 生成话题 $z_k$ 的概率。所有文本的参数向量构成一个 $M \times K$ 矩阵 $\theta=\left\{\theta_m\right\}_{m=1}^M$ 。超参数 $\alpha$ 也是一个 $K$ 维向量 $\alpha=\left(\alpha_1, \alpha_2, \cdots, \alpha_K\right)$ 。</li>
<li>每一个文本 $\mathbf{w}_m$ 中的每一个单词 $w_{m n}$ 由该文本的话题分布 $p\left(z \mid \mathbf{w}_m\right)$ 以及所有话 题的单词分布 $p\left(w \mid z_k\right)$ 决定。</li>
</ul>
<h4 id="生成过程">生成过程</h4>
<p>LDA 文本集合的生成过程如下:
给定单词集合 $W$, 文本集合 $D$, 话题集合 $Z$, 狄利克雷分布的超参数 $\alpha$ 和 $\beta$ 。</p>
<p>1.生成单词分布
随机生成 $K$ 个话题的单词分布。具体过程如下, 按照狄利克雷分布 $\operatorname{Dir}(\beta)$ 随机 生成一个参数向量 $\varphi_k, \varphi_k \sim \operatorname{Dir}(\beta)$, 作为话题 $z_k$ 的单词分布 $p\left(w \mid z_k\right), w \in W, k=$ $1,2, \cdots, K$ 。</p>
<p>2.生成主题分布
随机生成 $M$ 个文本的主题分布。具体过程如下: 按照狄利克雷分布 $\operatorname{Dir}(\alpha)$ 随 机生成一个参数向量 $\theta_m, \theta_m \sim \operatorname{Dir}(\alpha)$, 作为文本 $\mathbf{w}_m$ 的主题分布 $p\left(z \mid \mathbf{w}_m\right), m=$ $1,2, \cdots, M_{}$ 。</p>
<p>3.生成文本的单词序列
随机生成 $M$ 个文本的 $N_m$ 个单词。文本 $\mathbf{w}_m(m=1,2, \cdots, M)$ 的单词 $w_{m n}(n=$ $\left.1,2, \cdots, N_m\right)$ 的生成过程如下:</p>
<p>3.1 首先按照多项分布 $\operatorname{Mult}\left(\theta_m\right)$ 随机生成一个话题 $z_{m n}, z_{m n} \sim \operatorname{Mult}\left(\theta_m\right)$
3.2 然后按照多项分布 $\operatorname{Mult}\left(\varphi_{z_{m n}}\right)$ 随机生成一个单词 $w_{m n}, w_{m n} \sim \operatorname{Mult}\left(\varphi_{z_{m n}}\right)_{\text {。 }}$
文本 $\mathbf{w}_m$ 本身是单词序列 $\mathbf{w}_m=\left(w_{m 1}, w_{m 2}, \cdots, w_{m N_m}\right)$, 对应着隐式的话题序列 $\mathbf{z}_m=\left(z_{m 1}, z_{m 2}, \cdots, z_{m N_m}\right) 。$</p>
<p>引用一下LDA数学八卦的图：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png"
        data-srcset="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png 1.5x, /%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png 2x"
        data-sizes="auto"
        alt="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png"
        title="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/image/Pasted%20image%2020221019104516.png" width="361" height="307" /></p>
<ul>
<li>$\vec{\alpha} \rightarrow \vec{\theta}_m \rightarrow z_{m, n}$, 这个过程表示在生成第 $m$ 篇文档的时候，先从第一个坛子中抽了一个doc-topic 骰子 $\vec{\theta}_m$,然后投这个骰子生成了文档$m$中第 $n$ 个词的topic编号 $z_{m, n}$ ；</li>
<li>$\vec{\beta} \rightarrow \vec{\varphi}_k \rightarrow w_{m, n} \mid k=z_{m, n}$, 这个过程表示用如下动作生成语料中第 $m$ 篇文档的第 $n$ 个词: 在上帝手头的 $K$ 个topic-word 骰子 $\vec{\varphi}_k$ 中，挑选编号为 $k=z_{m, n}$ 的那个骰子进行投掷，然后生成 word $w_{m, n}$ ;</li>
</ul>
<p>理解 LDA最重要的就是理解这两个物理过程。LDA 模型在基于 $K$ 个 topic 生成语料中的 $M$ 篇文档的过程中， 由于是 bag-of-words 模型，有一些物理过程是相互独立可交换的。由此，LDA生成模型中， $M$ 篇文档会对应 于 $M$ 个独立的 Dirichlet-Multinomial 共轭结构；K个 个 topic 会对应于 $K$ 个独立的 Dirichlet-Multinomial 共轭结 构。所以理解 LDA 所需要的所有数学就是理解 Dirichlet-Multiomail 共轭，其它都就是理解物理过程。</p>
<h4 id="总结">总结</h4>
<p>(1) 对于话题 $z_k(k=1,2, \cdots, K)$ :
生成多项分布参数 $\varphi_k \sim \operatorname{Dir}(\beta)$, 作为话题的单词分布 $p\left(w \mid z_k\right)$;
(2) 对于文本 $\mathbf{w}_m(m=1,2, \cdots, M)$;
生成多项分布参数 $\theta_m \sim \operatorname{Dir}(\alpha)$, 作为文本的话题分布 $p\left(z \mid \mathbf{w}_m\right)$;
(3) 对于文本 $\mathbf{w}_m$ 的单词 $w_{m n}\left(m=1,2, \cdots, M, n=1,2, \cdots, N_m\right)$ :
(a) 采样生成话题 $z_{m n} \sim \operatorname{Mult}\left(\theta_m\right)$, 作为单词对应的话题;
(b) 采样生成单词 $w_{m n} \sim \operatorname{Mult}\left(\varphi_{z_{m n}}\right)$ 。</p>
<p>LDA 的文本生成过程中, 假定话题个数 $K$ 给定, 实际通常通过实验选定。狄利 克雷分布的超参数 $\alpha$ 和 $\beta$ 通常也是事先给定的。在没有其他先验知识的情况下, 可以 假设向量 $\alpha$ 和 $\beta$ 的所有分量均为 1 , 这时的文本的话题分布 $\theta_m$ 是对称的, 话题的单 词分布 $\varphi_k$ 也是对称的。</p>
<p>（帮助理解：主题数为3，假设$\theta_m$ = {0.4, 0.5, 0.1}，则说明主题$z_2$出现在文档m当中的概率为0.5，这就是多项分布的参数，再根据多项分布进行采样得到主题。）</p>
<h3 id="概率计算">概率计算</h3>
<p>LDA 模型整体是由观测变量和隐变量组成的联合概率分布, 可以表为</p>
<p>$$
p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha, \beta)=\prod_{k=1}^K p\left(\varphi_k \mid \beta\right) \prod_{m=1}^M p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m} p\left(z_{m n} \mid \theta_m\right) p\left(w_{m n} \mid z_{m n}, \varphi\right)
$$</p>
<p>(其中M为文本数，$N_m$为文档m的长度，K为主题数)
其中观测变量 $\mathrm{w}$ 表示所有文本中的单词序列, 隐变量 $\mathrm{z}$ 表示所有文本中的话题序列, 隐变量 $\theta$ 表示所有文本的话题分布的参数, 隐变量 $\varphi$ 表示所有话题的单词分布的参 数, $\alpha$ 和 $\beta$ 是超参数。</p>
<ul>
<li>$p\left(\varphi_k \mid \beta\right)$ 表示超参数 $\beta$ 给定条件下第 $k$ 个话题的单词分布的参数 $\varphi_k$ 的生成概率;</li>
<li>$p\left(\theta_m \mid \alpha\right)$ 表示超参数 $\alpha$ 给定条件下第 $m$ 个文本的话题分布的 参数 $\theta_m$ 的生成概率;</li>
<li>$p\left(z_{m n} \mid \theta_m\right)$ 表示第 $m$ 个文本的话题分布 $\theta_m$ 给定条件下文本的 第 $n$ 个位置的话题 $z_{m n}$ 的生成概率;</li>
<li>$p\left(w_{m n} \mid z_{m n}, \varphi\right)$ 表示在第 $m$ 个文本的第 $n$ 个位 置的话题 $z_{m n}$ 及所有话题的单词分布的参数 $\varphi$ 给定条件下第 $m$ 个文本的第 $n$ 个位 置的单词 $w_{m n}$ 的生成概率。</li>
</ul>
<p>第 $m$ 个文本的联合概率分布可以表为</p>
<p>$$
p\left(\mathbf{w}_m, \mathbf{z}_m, \theta_m, \varphi \mid \alpha, \beta\right)=\prod_{k=1}^K p\left(\varphi_k \mid \beta\right) p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m} p\left(z_{m n} \mid \theta_m\right) p\left(w_{m n} \mid z_{m n}, \varphi\right)
$$</p>
<p>其中 $\mathbf{w}_m$ 表示该文本中的单词序列, $\mathbf{z}_m$ 表示该文本的话题序列, $\theta_m$ 表示该文本的话 题分布参数。
LDA 模型的联合分布含有隐变量, 对隐变量进行积分得到边缘分布。
参数 $\theta_m$ 和 $\varphi$ 给定条件下第 $m$ 个文本的生成概率是</p>
<p>$$
p\left(\mathbf{w}_m \mid \theta_m, \varphi\right)=\prod_{n=1}^{N_m}\left[\sum_{k=1}^K p\left(z_{m n}=k \mid \theta_m\right) p\left(w_{m n} \mid \varphi_k\right)\right]
$$</p>
<p>超参数 $\alpha$ 和 $\beta$ 给定条件下第 $m$ 个文本的生成概率是</p>
<p>$$
p\left(\mathbf{w}_m \mid \alpha, \beta\right)=\prod_{k=1}^K \int p\left(\varphi_k \mid \beta\right)\left[\int p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m}\left[\sum_{l=1}^K p\left(z_{m n}=l \mid \theta_m\right) p\left(w_{m n} \mid \varphi_l\right)\right] \mathrm{d} \theta_m\right] \mathrm{d} \varphi_k
$$</p>
<p>超参数 $\alpha$ 和 $\beta$ 给定条件下所有文本的生成概率是</p>
<p>$$
p(\mathbf{w} \mid \alpha, \beta)=\prod_{k=1}^K \int p\left(\varphi_k \mid \beta\right)\left[\prod_{m=1}^M \int p\left(\theta_m \mid \alpha\right) \prod_{n=1}^{N_m}\left[\sum_{l=1}^K p\left(z_{m n}=l \mid \theta_m\right) p\left(w_{m n} \mid \varphi_l\right)\right] \mathrm{d} \theta_m\right] \mathrm{d} \varphi_k
$$</p>
<h3 id="吉布斯抽样">吉布斯抽样</h3>
<h4 id="基本思想">基本思想</h4>
<p>有三个主要目标：</p>
<ul>
<li>话题序列的集合$z=(z_1, z_2, \cdots, z_M)$的后验概率分布，其中$z_m$是第m个文本的主题序列，$z_m=(z_{m1}, \cdots, z_{mN_{m}})$;</li>
<li>参数$\theta=(\theta_1, \cdots, \theta_{M})$，其中$\theta_m$是第m个文本的主题分布的参数；</li>
<li>参数$\varphi=(\varphi_1, \cdots, \varphi_K)$，其中$\varphi_k$是第k个主题的单词分布的参数。</li>
</ul>
<p>对$p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha, \beta)$进行估计</p>
<p>吉布斯抽样, 这是一种常用的马尔可夫链蒙特卡罗法。为了估计 多元随机变量 $x$ 的联合分布 $p(x)$, 吉布斯抽样法选择 $x$ 的一个分量, 固定其他分量, 按照其条件概率分布进行随机抽样, 依次循环对每一个分量执行这个操作, 得到联合 分布 $p(x)$ 的一个随机样本, 重复这个过程, 在燃烧期之后, 得到联合概率分布 $p(x)$ 的 样本集合。</p>
<p>LDA 模型的学习通常采用收缩的吉布斯抽样 (collapsed Gibbs sampling) , 基本想法是, 通过对隐变量 $\theta$ 和 $\varphi$ 积分, 得到边缘概率分布 $p(\mathbf{w}, \mathbf{z} \mid \alpha, \beta)$ (也是联合分 布), 其中变量 $\mathbf{w}$ 是可观测的, 变量 $\mathbf{z}$ 是不可观测的; 对后验概率分布 $p(\mathbf{z} \mid \mathbf{w}, \alpha, \beta)$ 进 行吉布斯抽样, 得到分布 $p(\mathbf{z} \mid \mathbf{w}, \alpha, \beta)$ 的样本集合; 再利用这个样本集合对参数 $\theta$ 和 $\varphi$ 进行估计, 最终得到 LDA 模型 $p(\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha, \beta)$ 的所有参数估计。</p>
<h4 id="算法流程">算法流程</h4>
<p>输入: 文本的单词序列 $\mathbf{w}=\left\{\mathbf{w}_1, \cdots, \mathbf{w}_m, \cdots, \mathbf{w}_M\right\}, \mathbf{w}_m=\left(w_{m 1}, \cdots, w_{m n}, \cdots\right.$, $\left.w_{m_{N_m}}\right)$;</p>
<p>输出: 文本的话题序列 $\mathrm{z}=\left\{\mathbf{z}_1, \cdots, \mathbf{z}_m, \cdots, \mathbf{z}_M\right\}, \mathbf{z}_m=\left(z_{m 1}, \cdots, z_{m n}, \cdots, z_{m_{N_m}}\right)$ 的后验概率分布 $p(\mathbf{z} \mid \mathbf{w}, \alpha, \beta)$ 的样本计数, 模型的参数 $\varphi$ 和 $\theta$ 的估计值;
参数: 超参数 $\alpha$ 和 $\beta$, 话题个数 $K$ 。</p>
<p>(1) 设所有计数矩阵的元素 $n_{m k}, n_{k v}$, 计数向量的元素 $n_m, n_k$ 初值为 0 ;</p>
<p>(2) 对所有文本 $\mathbf{w}_m, m=1,2, \cdots, M$
对第 $m$ 个文本中的所有单词 $w_{m n}, n=1,2, \cdots, N_m$
(a) 抽样话题 $z_{m n}=z_k \sim \operatorname{Mult}\left(\frac{1}{K}\right)$;(对于文本m，其多项分布的参数为$\frac{1}{K}$，由$\alpha$生成，即$\theta_m \sim Dir(\alpha)$，$\theta_m$为长度为K的向量。)</p>
<p>增加文本-话题计数 $n_{m k}=n_{m k}+1$,
增加文本-话题和计数 $n_m=n_m+1$,
增加话题-单词计数 $n_{k v}=n_{k v}+1$,
增加话题-单词和计数 $n_k=n_k+1$;</p>
<p>（3）循环执行以下操作, 直到进入燃烧期
对所有文本 $\mathbf{w}_m, m=1,2, \cdots, M$
对第 $m$ 个文本中的所有单词 $w_{m n}, n=1,2, \cdots, N_m$</p>
<p>(a) 当前的单词 $w_{m n}$ 是第 $v$ 个单词, 话题指派 $z_{m n}$ 是第 $k$ 个话题;
减少计数 $n_{m k}=n_{m k}-1, n_m=n_m-1, n_{k v}=n_{k v}-1, n_k=n_k-1$;</p>
<p>(b) 按照满条件分布进行抽样</p>
<p>$$
p\left(z_i \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right) \propto \frac{n_{k v}+\beta_v}{\sum_{v=1}^V\left(n_{k v}+\beta_v\right)} \cdot \frac{n_{m k}+\alpha_k}{\sum_{k=1}^K\left(n_{m k}+\alpha_k\right)}
$$</p>
<p>得到新的第 $k^{\prime}$ 个话题, 分配给 $z_{m n}$;</p>
<p>(c) 增加计数 $n_{m k^{\prime}}=n_{m k^{\prime}}+1, n_m=n_m+1, n_{k^{\prime} v}=n_{k^{\prime} v}+1, n_{k^{\prime}}=n_{k^{\prime}}+1$;</p>
<p>(d) 得到更新的两个计数矩阵 $N_{K \times V}=\left[n_{k v}\right]$ 和 $N_{M \times K}=\left[n_{m k}\right]$, 表示后验 概率分布 $p(\mathbf{z} \mid \mathbf{w}, \alpha, \beta)$ 的样本计数;</p>
<p>(4) 利用得到的样本计数, 计算模型参数</p>
<p>$$
\begin{aligned}
\theta_{m k} &amp;=\frac{n_{m k}+\alpha_k}{\sum_{k=1}^K\left(n_{m k}+\alpha_k\right)} \\
\varphi_{k v} &amp;=\frac{n_{k v}+\beta_v}{\sum_{v=1}^V\left(n_{k v}+\beta_v\right)}
\end{aligned}
$$</p>
<h3 id="训练与推断">训练与推断</h3>
<p>有了LDA模型，我们的目标有两个：</p>
<ul>
<li>估计模型中的参数$\varphi_1, \cdots, \varphi_K$和$\theta_1, \cdots, \theta_M$;</li>
<li>对于新来的一篇doc，我们能够计算这篇文档的topic分布$\theta_{new}$。</li>
</ul>
<p>有了吉布斯采样公式就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic语义分析，训练的过程就是通过Gibbs Samping获取语料中的（z,w）样本，而模型中的所有参数可以基于采样的样本进行估计。</p>
<p>训练流程如下：</p>
<ul>
<li>随机初始化：对语料中的每篇文档的每个词w，随机赋一个topic编号z。</li>
<li>重新扫描语料库，对每个词按照吉布斯采样公式重新采样它的topic，在语料中进行更新。</li>
<li>重复以上语料库的重新采样过程直到吉布斯采样收敛。</li>
<li>统计语料库的topic-word共现频率矩阵，就是LDA的模型</li>
</ul>
<p>由这个矩阵我们可以计算每一个$p(word\mid topic)$概率，从而计算出模型参数$\varphi_1, \cdots, \varphi_K$，也可以计算另一个参数$\theta_1, \cdots, \theta_M$，只要在吉布斯抽样收敛后统计每篇文章的topic频率分布，就可以计算每一个$p(topic\mid doc)$概率，由于它是和训练语料的每篇文章相关的，对于我们理解新的文档毫无用处，所以一般没有必要保留这个概率。</p>
<p>如何对新的文档进行推断呢？其实和训练过程完全相似，对于新的文档，认为$\varphi_{kt}$是稳定不变的，是由训练语料得到的模型提供的。采样过程只估计该文档的topic分布$\theta_{new}$就好了。</p>
<p>推断过程如下：</p>
<ul>
<li>随机初始化：对当前文档的每个词w，随机的赋一个topic编号z；</li>
<li>重新扫描当前文档，按照吉布斯抽样公式，对每个词w，重新采样它的topic；</li>
<li>重复以上过程直到吉布斯采样收敛</li>
<li>统计文档中的topic分布，该分布就是$\theta_{new}$</li>
</ul>
<h3 id="代码">代码</h3>
<p>实现了吉布斯推断的python代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">LDA implementation in Python
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">@author: Michael Zhang
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scipy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LDA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tdm</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">iteration</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        tdm: the copus, of (D, Num_words_in_corpus),
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">            the value of each entry is the counts of corresponding words in this the corresponding document.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">            e.g.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">            tdm[d, w] = number of word w appears in document d.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        T: the number of topics
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">tdm</span> <span class="o">=</span> <span class="n">tdm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tdm</span><span class="o">.</span><span class="n">shape</span>            
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">=</span> <span class="n">alpha</span> <span class="c1"># count for expected value for hyper parameter alpha of theta, i.e. document-topic distribution.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="c1"># count for expected value for hyper parameter beta topic-word distribution.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="n">iteration</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># z must take in (d,w,i) as input, corresponding to</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># topic indicator for i-th obserevation of word w in doc d</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">))</span> <span class="c1"># initialize the topic-word matrix.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="c1"># initialize the documnet-topic matrix.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># initialize the topic counter for after sampling process, should be sum of value in self.topic_word_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">doc_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">)</span> <span class="c1"># initialize the doc counter for after sampling process, should be sum of value in self.doc_topic_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">)</span> <span class="c1"># store the value of log likelihood at each iteration</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_init_matrix</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># @pysnooper.snoop(&#39;init.log&#39;)    </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_init_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        for all words
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. sample a topic randomly from T topics for each word
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. increment topic word count, self.topic_word_matrix
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. increment document topic count,  self.doc_topic_matrix
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. update the topic indicator z.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">doc</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">coo_matrix</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tdm</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">word_freq_topic</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">col</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="n">word_freq_topic</span><span class="p">:</span> <span class="c1"># (word, freq)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">frequency</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1">############ Finish the following initialization steps #############</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># 1. sample a topic randomly from T topics for each word</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="n">topic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># 2. increment topic word count, self.topic_word_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[</span><span class="n">topic</span><span class="p">,</span> <span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># 3. increment document topic count,  self.doc_topic_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">topic</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># 4. update the topic indicator z.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">topic</span> <span class="c1"># d: document ID; w: word ID: i: instance ID，即在d中第几个w</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">doc_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># @pysnooper.snoop(&#39;fit.log&#39;)    </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># iterate over all the documents</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># iterate over all the words in d</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tdm</span><span class="p">[</span><span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># iterate over number of times observed word w in doc d</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tdm</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># we apply the hidden-varible method of Gibbs sampler, the hidden variable is z[(d,w,i)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span><span class="p">[</span><span class="n">d</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)]]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">doc_counts</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)],</span><span class="n">w</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)]]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># estimation of phi and theta for the current corpus</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">phi_hat</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[:,</span><span class="n">w</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">theta_hat</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span><span class="p">[</span><span class="n">d</span><span class="p">,:]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">doc_counts</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># calculate the full conditional distribution</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">full_conditional</span> <span class="o">=</span> <span class="n">phi_hat</span> <span class="o">*</span> <span class="n">theta_hat</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># normalize full_conditional such that it summation equals to 1.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">full_conditional</span> <span class="o">=</span> <span class="n">full_conditional</span> <span class="o">/</span> <span class="n">full_conditional</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># sample a topic for i-th obserevation of word w in doc d based on full_conditional</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">new_topic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">full_conditional</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1"># update z, doc_topic_matrix, doc_counts, topic_word_matrix, topic_counts here.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_topic</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">doc_topic_matrix</span><span class="p">[</span><span class="n">d</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)]]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)],</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">doc_counts</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)]]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="c1">############################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2  log P(w|z)  for each iteration based on Equation [2]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">## +++++++++ insert code below ++++++++++++++++++++++++###</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">w</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_word_matrix</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">w</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_counts</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">############################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%i</span><span class="se">\t</span><span class="s1"> LL: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">it</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="n">it</span><span class="p">]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="本质与使用条件">本质与使用条件</h3>
<p>本质上说，主题模型根本上是实现文本数据的结构化，结构化的文档可以彼此比较和查询，实现传统的任务。
LDA主题模型本质上解决了两类问题：</p>
<ul>
<li>文档聚类</li>
<li>词汇聚类</li>
</ul>
<p>主要价值在于：
1）文档的结构化，相比于传统的词袋模型达到了降维的效果
2）完成了文档的聚类和词汇的聚类，实现文本信息的抽象化分析，帮助分析者探索隐含的语义内容。</p>
<p>实践中数据要有以下性质才会有较好的结果：</p>
<ol>
<li>文档足够多</li>
<li>文档足够长</li>
<li>词汇特征够多</li>
<li>词频足够大</li>
</ol>
<h2 id="总结-1">总结</h2>
<p>历时好几周，终于完结了主题模型，主要是概率论没有学好，跟着推导的过程过于痛苦，不过也算是稍微理解了一点LDA，复述一下：</p>
<p><strong>LDA理解可以类比于PLSA，大体的思想都是根据文档生成主题分布，再根据主题分布和单词分布得到文档中的各个单词。不同的是LDA是贝叶斯派的思想，对于两种分布加入了狄利克雷先验概率。LDA的生成过程可以看成上帝掷骰子，从M个骰子中选取一个作为文本m的主题分布，从K个骰子中选取一个作为主题k的单词分布，（注意这里的多项分布的参数就是多项分布中的概率p，其服从狄利克雷分布，比如对于$\theta_m$，它其实就是文本m生成不同主题k的概率$p(z\mid d_m)$，是个K维的向量。对于$\varphi_k$，是由主题k生成不同单词v的概率$p(w\mid z_k)$，是个V维的向量。也就是根据狄利克雷分布采样得到的是一些概率，这些概率也是我们最终要求的参数，这些概率作为多项分布的参数再采样生成主题或者单词，还有就是$p(z_k\mid d_m)$与$z_{mn}$的理解，前者就是相当于$\theta_{mk}$，后者肯定是主题集合中的一个，不过是根据参数为$\theta_m$的多项分布在位置n采样得到的。这就是LDA的整个的理解，当然模型的求解是使用吉布斯抽样的方法，与上面写的步骤不同。写这些是便于理解）。</strong></p>
<p>由主题分布可以对文本的每个位置赋值一个主题，再根据主题-单词分布可以生成整个文本。一切的一切都是和PLSA一样，求两个分布，以至于可以生成我们的文档。LDA也可以得到文档的主题分布，得到了主题分布和单词分布可以应用于各种任务当中。具体可以参考《LDA漫游指南》。</p>
<p>现在知道了LDA是怎么一回事了，但还是感觉模模糊糊的，感觉如“通俗理解LDA主题模型”这篇文章开头所说的那样陷入了LDA的细枝末节中，所以写了一些主题，加深自己的印象与理解，经过代码的洗礼，又理解深入了一些，但感觉还没有掌握的很好，可能需要消化消化，那就先告一段落了。以后常看看就行。</p>
<h2 id="参考">参考</h2>
<p><a href="https://zhuanlan.zhihu.com/p/374924140" target="_blank" rel="noopener noreffer ">https://zhuanlan.zhihu.com/p/374924140</a>
<a href="https://www.cnblogs.com/gasongjian/p/7631978.html" target="_blank" rel="noopener noreffer ">https://www.cnblogs.com/gasongjian/p/7631978.html</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-11-12&nbsp;<a class="git-hash" href="https://github.com/dillonzq/LoveIt/commit/eab55b5fbc02156d3384de3b8010756633a8629f" target="_blank" title="commit by vllbc(1683070754@qq.com) eab55b5fbc02156d3384de3b8010756633a8629f: 公式看着没问题了 先保存 在玩火">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>eab55b5</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型" data-hashtags="NLP"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-hashtag="NLP"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" data-title="主题模型"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/word-embedding/" class="prev" rel="prev" title="Word Embedding"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Word Embedding</a>
            <a href="/transformer/" class="next" rel="next" title="Transformer">Transformer<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript><div id="gitalk" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/gitalk/gitalk"></a>Gitalk</a>.
            </noscript><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript><div id="fb-root" class="comment"></div>
            <div
                class="fb-comments"
                data-href="https://vllbc.top/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"
                data-width="100%"
                data-numposts="10"
            ></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://developers.facebook.com/docs/plugins/comments/"></a>Facebook</a>.
            </noscript><div id="telegram-comments" class="comment"><script type="text/javascript" src="https://comments.app/js/widget.js?3" defer data-comments-app-website="" data-limit="5" data-colorful="1"></script><noscript>
                    Please enable JavaScript to view the comments powered by <a href="https://comments.app/">Telegram Comments</a>.
                </noscript>
            </div><div id="commento" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://commento.io/">Commento</a>.
            </noscript><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript><div id="giscus" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://giscus.app">Giscus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.css"><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://connect.facebook.net/zh_CN/sdk.js#xfbml=1&version=v5.0&appId=&autoLogAppEvents=1" defer></script><script type="text/javascript" src="https://cdn.commento.io/js/commento.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"giscus":{"category":"Announcements","categoryId":"","darkTheme":"dark","emitMetadata":"0","inputPosition":"bottom","lang":"zh-CN","lazyLoading":false,"lightTheme":"light","mapping":"pathname","reactionsEnabled":"1","repo":"","repoId":""},"gitalk":{"admin":[""],"clientID":"","clientSecret":"","id":"2022-04-25T00:00:00Z","owner":"","repo":"","title":"主题模型"},"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"","lightTheme":"github-light","repo":""},"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"你的评论 ...","recordIP":true,"serverURLs":"https://leancloud.hugoloveit.com","visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.zh-cn","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
