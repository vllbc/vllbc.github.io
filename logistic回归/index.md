# Logisticå›å½’



# Logisticå›å½’
## çº¿æ€§å›å½’
çº¿æ€§å›å½’è¡¨è¾¾å¼ï¼š

$$
y = w^Tx+b
$$

å¹¿ä¹‰å›å½’æ¨¡å‹ï¼š

$$
y = g^{-1}(w^Tx+b)
$$

## Sigmoidå‡½æ•°
åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œéœ€è¦æ‰¾åˆ°ä¸€ä¸ªè”ç³»å‡½æ•°ï¼Œå³gï¼Œå°†çº¿æ€§å›å½’çš„è¾“å‡ºå€¼ä¸å®é™…çš„æ ‡ç­¾å€¼è”ç³»èµ·æ¥ã€‚å› æ­¤å¯ä»¥ä½¿ç”¨Sigmoidå‡½æ•°
å³ï¼š

$$
\delta(z) = \frac{1}{1+e^{-z}}
$$

å¯¹æ•°å‡ ç‡å…¶å®æ˜¯ä¸€ç§â€œsigmoid"å‡½æ•°ï¼Œå®ƒå°†zå€¼è½¬åŒ–ä¸ºä¸€ä¸ªæ¥è¿‘ 0 æˆ– 1 çš„ $y$ å€¼:

$$
y=\frac{1}{1+e^{-\left(w^{T} x+b\right)}} \rightarrow \operatorname{In} \frac{y}{1-y}=w^{T} x+b
$$

è‹¥å°†yè§†ä¸ºæ ·æœ¬ $x$ ä½œä¸ºæ­£ä¾‹çš„å¯èƒ½æ€§ï¼Œåˆ™1-yæ˜¯å…¶åä¾‹çš„å¯èƒ½æ€§ï¼Œä¸¤è€…çš„æ¯”å€¼ $\frac{y}{1-y}$ ç§°ä¸ºâ€œå‡ ç‡â€ï¼Œåæ˜ äº†xä½œä¸ºæ­£ä¾‹çš„ç›¸å¯¹å¯èƒ½æ€§ï¼Œå¯¹å‡ ç‡å–å¯¹ æ•°åˆ™å¾—åˆ° $\operatorname{In} \frac{y}{1-y}$ ï¼Œå¯ä»¥çœ‹å‡ºï¼Œä¸Šå¼å…¶å®æ˜¯åœ¨ç”¨çº¿æ€§å›å½’æ¨¡å‹çš„é¢„æµ‹ç»“æœå»é€¼è¿‘çœŸå®æ ‡è®°çš„å¯¹æ•°å‡ ç‡ã€‚æ‰€ä»¥è¯¥æ¨¡å‹ä¹Ÿè¢«ç§°ä½œâ€œå¯¹æ•°å‡ ç‡å› å½’â€ã€‚
## æŸå¤±å‡½æ•°
$$
J = -\frac{1}{m}\sum_{i=1}^my_i\log(\hat{y_i})+(1-y_i)\log(1-\hat{y})
$$

å®é™…ä¸Šå¯ä»¥çœ‹ä½œä¸‹é¢äº¤å‰ç†µæŸå¤±å‡½æ•°å½¢å¼åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸Šçš„å½¢å¼ï¼š

$$
J = -\frac{1}{m}\sum_{i=1}^my_i\log(\hat{y_i})
$$

è¿™é‡Œçš„$y_i$ä¸$\hat{y_i}$éƒ½æ˜¯å‘é‡ï¼Œå…¶é•¿åº¦å°±æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚å…¶ä¸­$y_i$ä»£è¡¨å®é™…åˆ†å¸ƒï¼Œå½¢å¼ä¸Šä¸ºonehotå‘é‡ã€‚$\hat{y_i}$æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œä¸ºé¢„æµ‹çš„å€¼ã€‚

å…¶å®è¿™é‡Œå¯ä»¥æƒ³ä¸€ä¸‹ç¥ç»ç½‘ç»œï¼Œå¯¹äºsigmoidæ¥è¯´ï¼Œè¾“å‡ºå±‚çš„ç¥ç»å…ƒå¯ä»¥æ˜¯ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸¤ä¸ªï¼Œå¦‚æœæ˜¯ä¸€ä¸ªçš„è¯å°±å¯ä»¥ç”¨ä¸Šé¢çš„å½¢å¼ï¼Œå¦‚æœæ˜¯ä¸¤ä¸ªçš„è¯å¯ä»¥ç”¨ä¸‹é¢çš„è¿™ç§å½¢å¼ã€‚

ä¹Ÿå¯ä»¥è¿™æ ·ç†è§£ï¼Œå¯¹äºsoftmaxçš„è¿™ç§å½¢å¼ï¼Œå¯¹äºäºŒåˆ†ç±»æˆ‘ä»¬å¯ä»¥æ‹†åˆ†æˆè¿™æ ·

$$
\begin{cases}
\log \hat{y_i}, \quad y_i=1 \\\\
\log (1-\hat{y_i}), \quad y_i=0
\end{cases}
$$

å†ç»“åˆèµ·æ¥ï¼Œè¿™æ ·å°±å¯ä»¥å¾—åˆ°é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°çš„ç»“æœã€‚
## ä¸æå¤§ä¼¼ç„¶ä¼°è®¡çš„å…³ç³»

$$
h(x;\theta) = p(y=1|x;\theta) = \frac{1}{1+e^{-\theta x+b}}
$$


$$
p(y=0|x;\theta) = 1-p(y=1|x;\theta)
$$

åˆ™å¯¹äºå•ä¸ªæ ·æœ¬ï¼š

$$
p(y|x;\theta) = h(x;\theta)^y(1-h(x;\theta))^{(1-y)}
$$

æ¥ä¸‹æ¥ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ä¼°è®¡å‡ºå‚æ•°$\theta$

$$
\begin{aligned}
L(\theta) = \prod_{i=1}^mp(y_i|x_i;\theta)\\\\ =\prod_{i=1}^mh(x_i;\theta)^{y_i}(1-h(x_i;\theta)) ^{1-y_i}
\end{aligned}
$$

åˆ™ï¼š

$$
l(\theta) = \ln L(\theta) = \sum_{i=1}^my_i\ln (h(x_i;\theta ))+(1-y_i)ln(1-h(x_i|\theta))
$$

æå¤§è¿™ä¸ªå‡½æ•°ï¼Œä¹Ÿå°±æ˜¯æœ€å°åŒ–è¿™ä¸ªå‡½æ•°çš„è´Ÿæ•°ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢çš„æŸå¤±å‡½æ•°ã€‚

## pythonå®ç°

```python
class LogisticRegression:
    def __init__(self):
        pass
    def sigmoid(self,a):
        res = []
        for x in a:
            if x >= 0:
                res.append(1/(1+np.exp(-x)))
            else:
                res.append(np.exp(x) / (np.exp(x) + 1))
        return np.array(res)
    def train(self, X, y_true, n_iters=100, learning_rate=1):
        """
        æ ¹æ®ç»™å®šçš„è®­ç»ƒé›†Xå’Œyæ¥è®­ç»ƒé€»è¾‘å›å½’
        """
        # ç¬¬é›¶æ­¥ï¼šåˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        #ğŸ‘†æ ·æœ¬æ•°må’Œç‰¹å¾é‡æ•°nåˆ†åˆ«èµ‹å€¼ä¸ºXçš„è¡Œæ•°å’Œåˆ—æ•°
        self.weights = np.zeros((n_features,1))
        self.bias = 0
        costs = []

        for i in range(n_iters):
            # ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥ï¼šè®¡ç®—è¾“å…¥çš„ç‰¹å¾é‡å’Œæƒå€¼çš„çº¿æ€§ç»„åˆï¼Œä½¿ç”¨sigmoidå‡½æ•°
            y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias)
            # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—ä»£ä»·å€¼ï¼Œç”¨äºä¹‹åè®¡ç®—ä»£ä»·å‡½æ•°å€¼
            cost = (-1/n_samples)*np.sum(y_true*np.log(y_predict+1e-5)+(1-y_true)*(np.log(1-y_predict+1e-5)))
            # ç¬¬å››æ­¥ï¼šè®¡ç®—æ¢¯åº¦
            dw = (1/n_samples)*np.dot(X.T,(y_predict - y_true))
            db = (1/n_samples)*np.sum(y_predict-y_true)
            # ç¬¬äº”æ­¥ï¼›æ›´æ–°å‚æ•°
            self.weights = self.weights - learning_rate * dw
            self.bias = self.bias - learning_rate * db

            costs.append(cost)
            if i%10 == 0:
                print(f"Cost after iteration {i}:{cost}")

        # return self.weights,self.bias,costs

    def predict(self,X):
        """
        å¯¹äºæµ‹è¯•é›†Xï¼Œé¢„æµ‹äºŒå…ƒåˆ†ç±»æ ‡ç­¾
        """
        y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias)
        return np.array(y_predict)
```
