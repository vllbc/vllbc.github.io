# LR为什么用交叉熵



关于损失函数的问题，之前也有很多疑惑。看了网上的很多博客，有从很多角度出发来讲解的，看的也是云里雾里。现在大致做一下整理。

对于最小二乘，为什么损失函数是那种形式呢，这里可以假设误差符合正态分布，则y也符合正态分布，则从概率的角度来看，减小预测误差也就是最大化$P(Y|X, w)$。可以看一下白板推导中的推导。

![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220726164436.png)
可以看到最终的结果就是我们熟悉的MSE损失函数，也就是说mse是正态分布的极大似然估计。

而对于LR来说(这里以多分类为例，对于二分类问题，使用的是logit loss)，交叉熵是假设模型分布为多项式分布，即为多项式分布的极大似然估计。

下面说明为啥分类问题中使用交叉熵会更好：

- MSE无差别关注全部类别上预测概率和真实概率的差。
- 交叉熵关注的是正确类别的预测概率。

分类问题中,模型的输出空间是概率分布,但目标输出空间是样例的类别,也就是说我们最终目标是获得正确的类别. 因此使用交叉熵的效果会更好。
