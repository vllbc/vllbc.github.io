[{"categories":["others"],"content":"贴一下可以玩的shortcode。 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:0:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"音乐播放 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"播放列表 夏日口袋专辑： ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:1","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"播放单曲 最爱的一首（我是紬厨）： ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:2","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"视频播放 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:2:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"bilibili 有多P可以选择集数 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:2:1","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"admonition 类型有：note、abstract、info、tip、success、question、warning、failure、danger、bug、example、quote。 技巧 一个 技巧 横幅 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:3:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"typeit ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:4:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"简单内容 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:4:1","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"代码内容 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:4:2","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"mapbox ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:5:0","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"默认样式 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:5:1","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["others"],"content":"自定义样式 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:5:2","tags":["others","shortcode(置顶)"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["LLM","NLP","reasoning"],"content":"核心总结 PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1\u003e2的效果。例如， 单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。 单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。 PRM 和 MCTS 既可以应用于优化训练数据，也可以用来预测用 用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward model训练数据。 用于推理：很简单，推理用MCTS的方式把 test-scaling 做上来，再结合PRM的方式从众多路径中挑选最佳答案。 PRM和MCTS的缺点 这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。 Process Reward Model(PRM) 在实际应用中有三大局限： 第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。 第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。 第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy model，但反而更多时间去优化 reward model 去了。 对MCTS的看法： 文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。 MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。 ## 参考 https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking ","date":"2025-04-04","objectID":"/mcts%E5%92%8Cprm/:1:0","tags":["LLM","NLP","reasoning"],"title":"MCTS和PRM","uri":"/mcts%E5%92%8Cprm/"},{"categories":["","python","coding","LEGB"],"content":"a = 'global' def outer(): # def len(in_var): # print('called my len() function: ', end=\"\") # l = 0 # for i in in_var: # l += 1 # return l a = 'local' def inner(): nonlocal a a += ' variable' inner() print('a is', a) # print(len(a)) outer() # print(len(a)) print('a is', a) 此时为nonlocal a，会按照local-闭包-global的顺序找到闭包变量a。a的值为local variable a = 'global' def outer(): # def len(in_var): # print('called my len() function: ', end=\"\") # l = 0 # for i in in_var: # l += 1 # return l a = 'local' def inner(): global a a += ' variable' inner() print('a is', a) # print(len(a)) outer() # print(len(a)) print('a is', a) 此时为global，会从全局变量中寻找a，a的值为global variable ","date":"2025-03-24","objectID":"/legb/:0:0","tags":["python","coding","LEGB"],"title":"LEGB","uri":"/legb/"},{"categories":["","coding","python"],"content":"python调试工具，类似于vscode的调试工具，使用命令行进行调试。 ","date":"2025-03-23","objectID":"/debugger/:0:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"使用方法 ","date":"2025-03-23","objectID":"/debugger/:1:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"插入式 import pdb; pdb.set_trace() 或者 breakpoint() ","date":"2025-03-23","objectID":"/debugger/:1:1","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"非插入式 python -m pdb [-c command] (-m module | pyfile) [args ...] ","date":"2025-03-23","objectID":"/debugger/:1:2","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"常用命令 ","date":"2025-03-23","objectID":"/debugger/:2:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"h 即help，可用命令如下 ","date":"2025-03-23","objectID":"/debugger/:2:1","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"p p x 即print(x)，用于打印变量。 pp x，使用pprint打印 ","date":"2025-03-23","objectID":"/debugger/:2:2","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"w 即where，查看当前调用栈。 ","date":"2025-03-23","objectID":"/debugger/:2:3","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"u和d u即up，回到上一个frame d即down，到下一个frame ","date":"2025-03-23","objectID":"/debugger/:2:4","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"l 即lst l 查看前后12行代码 ll查看当前函数全部代码 ","date":"2025-03-23","objectID":"/debugger/:2:5","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"b 即break，进行打断点 b x，在第x行打断点。 b 查看所有断点。 相同的有tbreak，与break的区别是第一次到该断点后会自动移除断点。即temporary breakpoint ","date":"2025-03-23","objectID":"/debugger/:2:6","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"n 即next，执行下一条语句，但忽视函数调用内部细节 ","date":"2025-03-23","objectID":"/debugger/:2:7","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"s 即step，执行下一条语句，如果有函数调用，则调用新frame，进入函数内部。 ","date":"2025-03-23","objectID":"/debugger/:2:8","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"c 即continue，继续程序的执行直到下一个断点 image.png ","date":"2025-03-23","objectID":"/debugger/:2:9","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"r 即return，直接跳转到当前函数return语句 ","date":"2025-03-23","objectID":"/debugger/:2:10","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"until until n，使程序继续执行直到执行到行数为n的语句。 ","date":"2025-03-23","objectID":"/debugger/:2:11","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"cl 即clear clear n，清除编号为n的断点 clear，清除所有断点。 ### j 即jump，向前或向后跳转，与until区别是，jump不会执行中间的语句，而是忽略他们。 ","date":"2025-03-23","objectID":"/debugger/:2:12","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"display 相当于一个监视器，用于监视变量的变化 ### retval 打印当前函数最后一次返回的返回值 ### q 即quit，退出pdb调试。 ","date":"2025-03-23","objectID":"/debugger/:2:13","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["coding","LLM","generate"],"content":"理论部分在这：generate相关 ## generate参数 def generate( self, inputs: Optional[torch.Tensor] = None, generation_config: Optional[GenerationConfig] = None, logits_processor: Optional[LogitsProcessorList] = None, stopping_criteria: Optional[StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[\"PreTrainedModel\"] = None, streamer: Optional[\"BaseStreamer\"] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs, ) -\u003e Union[GenerateOutput, torch.LongTensor]: 在代码中可以看到在函数入口显式的定义了很多参数。他们的具体含义如下 inputs：tensor 形式的 token_id，通常先准备文本形式的提示词和输入，使用tokenizer转化为对应 id，这里维度通常为 [batch_size, seq_len] generation_config：一个用 GenerationConfig 类创建的对象，存储着模型生成的超参数，可以提前创建该对象并传入 .generate() logits_processor：高级功能，logits_processor 可以在每个 step 的输出概率计算完成后，对分数进行进一步的干预，改变输出的概率分布，从而影响生成的结果，例如最常见的，重复惩罚，就是使用 logits_processor 完成的。（不懂的话可以看后面如何具体实现的） stopping_criteria：高级功能，允许用户通过 stopping_criteria 自定义生成停止条件（不懂的话可以看后面如何具体实现的） prefix_allowed_tokens_fn：解码策略的一个超参数，用于前缀 token 约束（感觉没必要放在这里） synced_gpus： DeepSpeed ZeRO Stage-3 多GPU时使用（ZeRO-3包括优化器状态+梯度+权重并行优化，而推理阶段只使用权重并行），此时需要将 synced_gpus 设置成 Ture。. 否则，如果一个 GPU 在另一个 GPU 之前完成生成，整个系统就会挂起，因为其余 GPU 尚未从最先完成的 GPU 接收到权重分片。 transformers\u003e=4.28 在生成时检测到多个 GPU 会自动设置 synced_gpus=True，transformers\u003c4.28 需要手动设置，本文代码环境transformers=4.41.1 assistant_model：高级功能，辅助生成模型，另一个词表完全相同的小模型，有些token使用辅助模型生成更快 streamer：流式输出控制器，现在的大模型平台都是一个字一个字显示出来的，这就是流式输出，否则的话会等所有生成完成再显示出来。这个可以自定义流式输出的方式 negative_prompt_ids：负面提示，一些前沿研究会用到，不用管 negative_prompt_attention_mask：负面提示的 attention_mask **kwargs 以上输入都太高大上了，只有 inputs 会每次传入，其他的对于常规输出根本用不到（其实 inputs 也可以不用输入，通过tokenizer()得到model_inputs后，使用**model_inputs方式也可以传入） 回想一下别人的代码，会看到这里经常传入 temperature=0.7, top_k=20, max_new_tokens=512等参数，都是通过**kwargs传入进来的 其实传入的这些都是输入参数 generation_config 的属性（可以进入对应类中看一下有哪些属性，from transformers.generation.configuration_utils import GenerationConfig），你可以创建该对象并覆盖某些参数，也可以通过参数形式在调用.generate()时传进来 在后面会将传入的这些参数覆盖掉generation_config中对应的属性 ","date":"2025-03-09","objectID":"/generate/:0:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["coding","LLM","generate"],"content":"inputs处理 def _prepare_model_inputs( self, inputs: Optional[torch.Tensor] = None, bos_token_id: Optional[torch.Tensor] = None, model_kwargs: Optional[Dict[str, torch.Tensor]] = None, ) -\u003e Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]: \"\"\" This function extracts the model-specific `inputs` for generation. \"\"\" # 1.有一些 encoder-decoder 模型的输入有不同的名称，这里首先确认名称 if ( self.config.is_encoder_decoder and hasattr(self, \"encoder\") and self.encoder.main_input_name != self.main_input_name ): input_name = self.encoder.main_input_name else: input_name = self.main_input_name # 从 model_kwargs 中去掉 input_name: None 的键值对 model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name} # 2.这里确保 model.generate() 输入参数中的 inputs 和 kwargs 中的 input_name 只输入一个 inputs_kwarg = model_kwargs.pop(input_name, None) if inputs_kwarg is not None and inputs is not None: raise ValueError( f\"`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. \" f\"Make sure to either pass {inputs} or {input_name}=...\" ) elif inputs_kwarg is not None: inputs = inputs_kwarg # 3.如果 input_name != inputs_embeds， 这里确保 input_name 和 inputs_embeds 只输入一个 if input_name == \"input_ids\" and \"inputs_embeds\" in model_kwargs: # 如果是 decoder-only 模型，先看看模型 .forward() 函数的参数中，是否包含 inputs_embeds，如果不包含就弹出异常 if not self.config.is_encoder_decoder: has_inputs_embeds_forwarding = \"inputs_embeds\" in set( inspect.signature(self.prepare_inputs_for_generation).parameters.keys() ) if not has_inputs_embeds_forwarding: raise ValueError( f\"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} \" \"doesn't have its forwarding implemented. See the GPT2 implementation for an example \" \"(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!\" ) # In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of # the attention mask) can rely on the actual model input. model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation( inputs, bos_token_id, model_kwargs=model_kwargs ) else: if inputs is not None: raise ValueError(\"You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.\") inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\" # 4. 如果 `inputs` 还是 None，尝试用 BOS token 创建 `input_ids` inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs) return inputs, input_name, model_kwargs 若传入了 inputs，就不要在 kwargs 中再次定义 input_ids 若 inputs 为 None，且 model_kwargs 不包含 input_ids 或 input_ids 也为 None，则创建一个 [batch_size, 1] 大小的tensor，里面的值都为 bos_token_id ","date":"2025-03-09","objectID":"/generate/:1:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["coding","LLM","generate"],"content":"参考 https://blog.csdn.net/qq_41496421/article/details/142346738?spm=1001.2014.3001.5502 https://blog.csdn.net/qq_41496421/article/details/142580960?spm=1001.2014.3001.5501 ","date":"2025-03-09","objectID":"/generate/:2:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["","einops"],"content":" einops.einsum calls einsum operations with einops-style named axes indexing, computing tensor products with an arbitrary number of tensors. Unlike typical einsum syntax, here you must pass tensors first, and then the pattern. Also, note that rearrange operations such as \"(batch chan) out\", or singleton axes (), are not currently supported. 爱因斯坦求和 ","date":"2025-01-11","objectID":"/einsum/:0:0","tags":["einops"],"title":"einsum","uri":"/einsum/"},{"categories":["einops"],"content":"pack Packs several tensors into one. See einops tutorial for introduction into packing (and how it replaces stack and concatenation). ## unpack \u003eUnpacks a single tensor into several by splitting over a selected axes. See einops tutorial for introduction into packing (and how it replaces stack and concatenation). image.png ","date":"2025-01-11","objectID":"/pack-and-unpack/:1:0","tags":["einops"],"title":"pack and unpack","uri":"/pack-and-unpack/"},{"categories":["","einops"],"content":" einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. 代替reshape，给维度命名。可以用…代表不想动的维度。 ","date":"2025-01-11","objectID":"/rearrange/:0:0","tags":["einops"],"title":"rearrange","uri":"/rearrange/"},{"categories":["","einops"],"content":" einops.reduce combines rearrangement and reduction using reader-friendly notation. reduce会使维度减少。 ","date":"2025-01-11","objectID":"/reduce/:0:0","tags":["einops"],"title":"reduce","uri":"/reduce/"},{"categories":["","einops"],"content":" einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, and broadcast functions. repeat是使维度增加，与reduce相反。 ## 应用 比如说repeat_kv函数就可以用einops.repeat很方便的实现 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\" bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) 等价于 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: einops.repeat(x, 'bs slen kvheads dim-\u003e bs slen (kvheads n_rep) dim', n_rep=n_rep).shape ","date":"2025-01-11","objectID":"/repeat/:0:0","tags":["einops"],"title":"repeat","uri":"/repeat/"},{"categories":["","einops"],"content":" Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/jax/etc.) to numpy.ndarray image.png ","date":"2025-01-08","objectID":"/asnumpy/:0:0","tags":["einops"],"title":"asnumpy","uri":"/asnumpy/"},{"categories":["einops"],"content":" Parse a tensor shape to dictionary mapping axes names to their lengths. # Use underscore to skip the dimension in parsing. \u003e\u003e\u003e x = np.zeros([2, 3, 5, 7]) \u003e\u003e\u003e parse_shape(x, 'batch _ h w') {'batch': 2, 'h': 5, 'w': 7} # `parse_shape` output can be used to specify axes_lengths for other operations: \u003e\u003e\u003e y = np.zeros([700]) \u003e\u003e\u003e rearrange(y, '(b c h w) -\u003e b c h w', **parse_shape(x, 'b _ h w')).shape (2, 10, 5, 7) 也就是把维度的维数映射到对应的命名。与数据无关，只看得到维度。 ","date":"2025-01-08","objectID":"/parse_shape/:0:0","tags":["einops"],"title":"parse_shape","uri":"/parse_shape/"},{"categories":["","task planning","survey"],"content":"该工作主要梳理了LLM-based Agent 中的规划（planning）能力，原文链接： Understanding the planning of LLM agents: A survey 文章中，作者将planning能力进一步细分为了五个维度： 任务分解（Task Decomposition） 规划选择（Plan Selection） 外部辅助规划（External Planner） 反馈和改进（Reflection and Refinement） 记忆（Memory） ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:0:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"引言 (Introduction) 自主智能代理：被定义为能够完成特定任务的智能实体。它们通过感知环境、规划和执行动作来实现目标。 规划的重要性：规划是代理最关键的能力之一，它要求代理进行复杂的理解、推理和决策过程。 传统方法的局限性：以往的工作主要依赖于符号方法或基于强化学习的方法，如规划领域定义语言（PDDL）或策略学习。这些传统方法有其局限性，例如符号方法需要将自然语言描述的问题转换为符号建模，这可能需要人类专家的努力，而且缺乏容错性。强化学习方法通常需要与环境的大量样本（交互）来学习有效策略，这在数据收集耗时或成本高昂的场景中可能不切实际。 LLM的潜力：近年来，大型语言模型（LLM）的出现标志着一个范式的转变。LLM在多个领域取得了显著的成功，展示了在推理、工具使用、规划和指令跟随方面的重要智能。这种智能为将LLM作为代理的认知核心提供了可能性，从而有潜力提高规划能力。 本文工作：尽管已有调查尝试总结LLM的技术，但文献中往往缺乏对规划能力的详细分析。本调查旨在分析最新的研究工作，讨论优势和局限性，并提供对基于LLM的代理规划能力的系统性视角。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:1:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"方法 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:2:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"任务分解 (Task Decomposition) image.png 现实世界中的任务通常是复杂和多步骤的，直接通过单步规划过程来解决复杂任务是一项巨大挑战。任务分解通过将复杂任务分解为多个简单子任务，使得规划过程更加可行。 分解方法分类：任务分解方法主要分为两类： - 分解优先方法（Decomposition-First Methods）：首先将任务分解为子目标，然后依次为每个子目标制定计划。 - 交错分解方法（Interleaved Decomposition Methods）：在任务分解和子任务规划之间进行交错，每次只揭示当前状态的一个或两个子任务。 分解优先方法的代表工作 HuggingGPT：LLM作为控制器，负责将人类输入的任务分解为子任务，选择模型，并生成最终响应。 Plan-and-Solve：将原始的“让我们逐步思考”转变为两步提示指令：“我们首先制定计划”和“我们执行计划”。 ProgPrompt：将自然语言描述的任务转化为编码问题，将每个动作形式化为函数，每个对象表示为变量。 #### 交错分解方法的代表工作： Chain-of-Thought (CoT)：通过构建的轨迹指导LLM对复杂问题进行推理，利用LLM的推理能力进行任务分解。 Zero-shot CoT：使用“让我们逐步思考”的指令，解锁LLM的零样本推理能力。 ReAct：将推理和规划解耦，交替进行推理（思考步骤）和规划（行动步骤）。 #### 讨论 分解优先方法的优势在于创建了子任务与原始任务之间的强关联，降低了任务遗忘和幻觉的风险。但需要额外的调整机制，以避免某个步骤的错误导致整体失败。 交错分解方法可以根据环境反馈动态调整分解，提高了容错性。但对于复杂任务，过长的轨迹可能导致LLM产生幻觉，偏离原始目标。 挑战：尽管任务分解显著提高了LLM代理解决复杂任务的能力，但仍存在挑战，包括任务分解引入的额外开销、时间成本和计算成本，以及LLM的上下文长度限制。 ### 多计划选择 (Multi-Plan Selection) 由于任务的复杂性和LLM固有的不确定性，对于给定任务，LLM代理可能会生成多种不同的计划。多计划生成涉及利用生成模型解码过程中的不确定性，通过不同的采样策略来产生多个候选计划。 - Self-consistency：采用简单的直觉，即复杂问题的解决方案很少是唯一的。通过温度采样、top-k采样等策略，获得多个不同的推理路径。 - Tree-of-Thought (ToT)：提出“采样”和“提议”两种策略来生成计划。LLM在解码过程中会采样多个计划，并通过少量示例提示生成各种计划。 - Graph-of-Thought (GoT)：在ToT的基础上增加了思想的转换，支持任意思想的聚合。 - LLM-MCTS 和 RAP：利用LLM作为启发式策略函数，通过蒙特卡洛树搜索（MCTS）算法来获取多个潜在动作。 最优计划选择：在候选计划中选择最优计划时，采用了多种启发式搜索算法。 - Self-consistency：使用简单的多数投票策略，将得票最多的计划视为最优选择。 - Tree-of-Thought (ToT)：支持树搜索算法，如广度优先搜索（BFS）和深度优先搜索（DFS），使用LLM评估多个动作并选择最优动作。 - LLM-MCTS 和 RAP：也使用树结构辅助多计划搜索，但它们采用MCTS算法进行搜索。 - LLM A：利用经典的A算法协助LLM搜索，使用当前位置到目标位置的切比雪夫距离作为启发式成本函数来选择最优路径。 #### 讨论 多计划选择的可扩展性显著优势在于提供了在广阔搜索空间中更广泛探索潜在解决方案的能力。 然而，这种优势伴随着计算需求的增加，尤其是对于具有大量token计数或计算的模型，这在资源受限的情况下尤为重要。 LLM在计划评估中的作用引入了新的挑战，因为LLM在任务排名方面的表现仍在审查中，需要进一步验证和微调其在此特定情境下的能力。 LLM的随机性质为选择过程增加了随机性，可能影响所选计划的一致性和可靠性。 外部规划器辅助规划 (External Planner-Aided Planning) 尽管LLM在推理和任务分解方面展现出了强大的能力，但在面对具有复杂约束的环境时，例如数学问题求解或生成可执行动作，仍然面临挑战。 方法分类：根据引入的规划器类型，这些方法可以分为两类： - 符号规划器（Symbolic Planner）：基于形式化模型，如PDDL，使用符号推理来找到从初始状态到目标状态的最优路径。 - 神经规划器（Neural Planner）：通过强化学习或模仿学习技术训练的深度模型，针对特定领域展现出有效的规划能力。 #### 符号规划器的代表工作 - LLM+P：通过结合基于PDDL的符号规划器，使用LLM将问题组织成PDDL语言格式，并利用Fast Downward solver进行规划。 - LLM-DP：特别为动态交互环境设计，将环境反馈信息形式化为PDDL语言，并使用BFS solver生成计划。 - LLM+PDDL：在LLM生成的PDDL模型中增加手动验证步骤，并提出使用LLM生成的计划作为局部搜索规划器的初始启发式解。 - LLM+ASP：将自然语言描述的任务转换为ASP问题，然后使用ASP求解器CLINGO生成计划。 #### 神经规划器的代表工作 - CALM：结合了语言模型和基于RL的神经规划器，使用语言模型生成候选动作，然后通过DRRN策略网络重新排序以选择最优动作。 - SwiftSage：将规划过程分为快速思考和慢速思考，快速思考通过模仿学习训练的DT模型实现，慢速思考则涉及LLM基于当前状态的推理和规划。 #### 讨论 在这些策略中，LLM主要扮演支持角色，其主要功能包括解析文本反馈并提供额外的推理信息以协助规划，特别是在解决复杂问题时。 传统的符号AI系统在构建符号模型时复杂且依赖于人类专家，而LLM可以加速这一过程，有助于更快更优地建立符号模型。 符号系统的优势包括理论完备性、稳定性和可解释性。将统计AI与LLM结合，有望成为未来人工智能发展的主要趋势。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:2:1","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"反思和精炼 (Reflection and Refinement) 反思和精炼是规划过程中不可或缺的组成部分，它们增强了LLM代理规划的容错能力和错误纠正能力。由于LLM在规划过程中可能产生幻觉或在复杂问题上推理能力不足，导致错误或陷入“思维循环”，反思和总结失败有助于代理纠正错误并在后续尝试中打破循环。 Self-refine： 利用迭代过程，包括生成、反馈和精炼。在每次生成后，LLM为计划产生反馈，促进基于反馈的调整。 Reflexion： 扩展了ReAct方法，通过引入评估器来评估轨迹。LLM在检测到错误时生成自我反思，帮助纠正错误。 CRITIC： 使用外部工具，如知识库和搜索引擎，来验证LLM生成的动作。然后利用外部知识进行自我纠正，显著减少事实错误。 InteRecAgent： 使用称为ReChain的自我纠正机制。LLM用于评估由交互推荐代理生成的响应和工具使用计划，总结错误反馈，并决定是否重新规划。 LEMA： 首先收集错误的规划样本，然后使用更强大的GPT-4进行纠正。这些纠正后的样本随后用于微调LLM代理，从而在各种规模的LLaMA模型上实现显著的性能提升。 #### 讨论 自我反思策略类似于强化学习的原则，其中代理作为决策者，环境反馈触发策略网络的更新。然而，与深度强化学习通过修改模型参数来更新不同，在LLM代理中，这种更新是通过LLM自身的自我反思来实现的，最终形成文本形式的反馈。 -这些文本反馈可以作为长期和短期记忆，通过提示影响代理后续的规划输出。然而，目前还没有确凿的证据证明这种文本形式的更新最终能够使LLM代理达到特定目标。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:2:2","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"记忆增强规划 (Memory-Augmented Planning) 记忆是提升代理规划能力的关键途径，可以帮助代理从经验中学习并适应新的情境。 #### RAG-based Memory（基于RAG的记忆) - 概念：使用检索增强生成（Retrieval-Augmented Generation, RAG）技术，将记忆以文本形式存储，并在需要时检索出来辅助规划。 - 方法：如MemoryBank、TiM 和 RecMind，这些方法通过文本编码模型将记忆编码为向量，并建立索引结构，以便在规划时检索与当前任务相关的经验。 #### Embodied Memory（体现记忆）： - 概念：通过微调（fine-tuning）LLM，将代理的历史经验样本嵌入到模型参数中，从而增强记忆能力。 - 方法：如CALM 和 TDT，这些方法使用从代理与环境交互中收集的数据来微调模型，使其能够记住与规划相关的信息，并在规划任务中表现更好。 #### 记忆更新方式： - RAG-based：提供了实时、低成本的外部记忆更新，但依赖于检索算法的准确性。 - Finetuning：提供了更大的记忆容量，但记忆更新成本较高，并且在保留细节方面存在挑战。 讨论： 记忆增强的LLM代理在规划中表现出更强的增长潜力和容错能力，但记忆生成在很大程度上依赖于LLM自身的生成能力。 通过自我生成的记忆来提升较弱LLM代理的能力仍然是一个具有挑战性的领域。 #### 挑战 尽管记忆增强LLM代理在规划方面表现出优势，但它们在记忆生成上仍然面临挑战，特别是在自我生成记忆方面。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:2:3","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"评估 (Evaluation) 评估代理的规划能力是研究领域中的一个关键问题。作者调查了几种主流的基准测试方法，并将它们分为以下几类： 交互式游戏环境（Interactive Gaming Environments）： 提供基于代理动作的实时多模态反馈，如文本和视觉反馈。 例如Minecraft，代理需要收集材料制作工具以获得更多奖励，常用评价指标是代理创建的工具数量。 ### 基于文本的交互环境（Text-based interactive environments）： 代理位于用自然语言描述的环境中，动作和位置有限。 常用评价指标是成功率或获得的奖励，例如ALFWorld和ScienceWorld。 ### 交互式检索环境（Interactive Retrieval Environments）： 模拟人类在现实生活信息检索和推理的过程。 代理可以与搜索引擎和其他网络服务交互，通过搜索关键词或执行点击、前进、后退操作来获取更多信息，完成问答任务或信息检索任务。 ### 交互式编程环境（Interactive Programming Environments）： 模拟程序员与计算机之间的交互，测试代理解决计算机相关问题的规划能力。 代理需要与计算机交互，通过编写代码或指令来解决问题。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:3:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"实验 作者在四个基准测试上进行了实验，以验证代表性方法的性能。这些基准测试包括==ALFWorld==、==ScienceWorld==、==HotPotQA==和==FEVER==，涵盖了交互式游戏和问答基准测试。 实验结果显示，性能随着成本的增加而提高，表明更详细的思考（即消耗更多的token）可以带来性能上的提升。 另外，对于复杂任务，示例（例如Zero-shot CoT和Few-shot CoT）对于LLM进一步理解任务至关重要。 反思（Reﬂexion）在提高成功率方面发挥了关键作用，尤其是在复杂任务中，显示了LLM具备错误纠正能力。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:4:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"讨论： 现有的基准测试大多依赖于任务的最终完成状态，缺乏细粒度的逐步评估。 环境反馈通常是规则化的、简化的，并且与现实世界场景有距离。 未来的发展方向可能包括利用高智能模型如LLM来设计更现实的评估环境。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:4:1","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","task planning","survey"],"content":"结论和未来方向 (Conclusions and Future Directions) 进展总结：自LLM展现出智能以来，使用LLM增强代理规划能力的研究受到了越来越多的关注。作者概述了主要的研究方向，并在前文中对各种方法进行了详细比较和分析。 实验结果：作者在四个基准测试上进行了实验，比较了几种代表性方法的有效性，并指出随着投入成本的增加，性能也随之提高。 ### 挑战 幻觉问题（Hallucinations）：LLM在规划过程中可能会产生幻觉，导致非理性的计划或无法遵循复杂指令。 生成计划的可行性：与基于符号的人工智能相比，LLM在优化过程中可能难以遵守复杂约束，导致生成的计划缺乏可行性。 计划的效率：现有LLM代理的规划过程可能没有考虑生成计划的效率，未来的发展可能需要引入额外的效率评估模块。 ### 未来方向： 多模态环境反馈：考虑集成多模态大型模型的发展，并重新审视相关的规划策略，以处理包括图像、音频等在内的多模态反馈。 细粒度评估：利用高智能模型如LLM设计更现实的评估环境，提供更细致的逐步评估，以更好地模拟现实世界场景。 ","date":"2024-12-20","objectID":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/:5:0","tags":["task_planning"],"title":"5种方法","uri":"/5%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["","coding","torch"],"content":"gather 参数： input (Tensor) – the source tensor dim (int) – the axis along which to index index (LongTensor) – the indices of elements to gather out (Tensor_,__optional_) – the destination tensor sparse_grad (bool,optional) – If True, gradient w.r.t. input will be a sparse tensor. \u003e gather操作是scatter操作的逆操作，如果说scatter是根据index和src求self(input)，那么gather操作是根据self(input)和index求src。具体来说gather操作是根据index指出的索引，沿dim指定的轴收集input的值。 out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 对于gather操作来说，有三个约束需要满足： （1）对于所有的维度d != dim，有input.size(d) == index.size(d)，对于维度dim来说，有index.size(d) \u003e= 1； （2）张量out的维度大小必须和index相同； （3）和scatter一样，index中的索引值必须在input.size(dim)范围内。 ### example ### code example import torch t = torch.Tensor([[1, 2], [3, 4]]) # t = 1 2 # 3 4 index = torch.LongTensor([[0, 0], [1, 0]]) # index = 0 0 # 1 0 # dim = 0 : [[1,2],[3,2]] # dim = 1 : [[1,1],[4,3]] # index = 0 # 1 # dim = 0 : [[1],[3]] # dim = 1 : [[1],[4]] # index = 0 1 # dim = 0 : [[1, 4]] # dim = 1 : [[1, 2]] ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:1:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"scatter Writes all values from the tensor into at the indices specified in the tensor. For each value in , its output index is specified by its index in for and by the corresponding value in for .`src``self``index``src``src``dimension != dim``index``dimension = dim` For a 3-D tensor, is updated as:`self` 参数： - dim (int) – the axis along which to index - index (LongTensor) – the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity - src (Tensor) – the source element(s) to scatter, incase value is not specified - value (float) – the source element(s) to scatter, incase src is not specified self[index[i][j][k]][j][k] = src[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k] # if dim == 2 看了上面这个操作就理解了。 由此可以得出以下约束： 1. 张量self，张量index和张量src的维度数量必须相同（即三者的.dim()必须相等，注意不是维度大小）； 2. 对于每一个维度d，有index.size(d)\u003c=src.size(d)； 3. 对于每一个维度d，如果d!=dim，有index.size(d)\u003c=self.size(d)； 对于index也有一些约束： 1. 张量index中的每一个值大小必须在[0, self.size(dim)-1]之间； 2. 张量index沿dim维的那一行中所有值都必须是唯一的（弱约束，违反不会报错，但是会造成没有意义的操作）。 ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:2:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"example ### code example import torch a = torch.arange(10).reshape(2,5).float() print(f\"a: \\n{a}\") b = torch.zeros(3, 5) print(f\"b: \\n{b}\") b_= b.scatter(dim=0, index=torch.LongTensor([[1, 2, 1, 1, 2], [2, 0, 2, 1, 0]]), src=a) print(f\"b_: \\n{b_}\") # tensor([[0, 6, 0, 0, 9], # [0, 0, 2, 8, 0], # [5, 1, 7, 0, 4]]) ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:2:1","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"scatter_add_ 这个函数和scatter基本上没有任何区别，区别在于上图中的对于self中同一位置的填入是随机的，self[3,0]不确定是7还是9，self[0,1]不确定是8还是10，但是使用scatter_add就将所有即将填入同一位置的数相加，例子如下： ### example ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:3:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"参考 https://zhuanlan.zhihu.com/p/158993858 ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:4:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","LLM","NLP"],"content":"LLaMA介绍 LLaMA 是目前为止，效果最好的开源 LLM 之一。 论文的核心思想：相比于GPT，更小的模型+更多的训练数据**也可以获得可比的效果 基于更多 tokens 的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为 LLaMA，参数范围从 7B 到 65B 不等，与现有最佳 LLM 相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于 GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM 的使用和研究平民化，因为它可以在单个 GPU 上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如 Chinchilla 或 PaLM-540B）相竞争的能力。 LLaMA1、2的主要差别在训练上下文长度、训练token数、注意力机制以及对齐方法上。 模型 训练长度 分词器 词表大小 位置编码 激活层 标准化 训练token数 链接 精度 注意力机制 有无chat版本 Alignment LLaMA 2,048 BPE（Sentence-Piece） 32k ROPE SwiGLU 基于 RMSNorm 的 Pre-Norm 1万亿(6.7B,13B) 1.4万亿（32.5B,65.2B） http://arxiv.org/abs/2302.13971 fp16 MHA 0 LLaMA2 4,096 同上 32k ROPE 同上 同上 2万亿 https://arxiv.org/abs/2307.09288 bf16 34B,70B GQA, 其他MHA 1 SFT+RLHF(拒绝采样+PPO) （表来自LLaMA家族） LLaMA1 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:0:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"训练数据 image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:1:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"训练参数 image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:2:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"RMSnorm 与 Layer Norm 相比，RMS Norm的主要区别在于去掉了减去均值的部分，计算公式为： \\[\\overline{a}_{i}=\\frac{a_{i}}{RMS(a)}\\] 其中 \\[RMS(a)=\\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}a_{i}^{2}} \\\\ \\] 此外RMSNorm 还可以引入可学习的缩放因子g，从而得到 \\[\\overline{a}_i=\\frac{a_i}{RMS(\\boldsymbol{a})}g_i\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"Pre-norm和Post-norm 注意其使用的是Pre-norm结构，与Post-norm结构差异如下： 关于Pre Norm的效果和Post Norm效果差异，相关分析在这两篇文章中： 模型优化漫谈：BERT的初始标准差为什么是0.02？ 为什么Pre Norm的效果不如Post Norm？ 总结来说就是Pre-norm加深的是模型的宽度，而不是深度，从而导致训练效果不如Post-norm，但可以缓解Post-norm的梯度消失。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaRMSNorm(nn.Module): def __init__(self, hidden_size, eps=1e-6): \"\"\" LlamaRMSNorm is equivalent to T5LayerNorm \"\"\" super().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.variance_epsilon = eps # eps 防止取倒数之后分母为0 def forward(self, hidden_states): input_dtype = hidden_states.dtype variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True) hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) # rsqrt 即sqrt后取倒数 # weight 是末尾乘的可训练参数, 即g_i return (self.weight * hidden_states).to(input_dtype) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"RoPE RoPE 的核心思想是“通过绝对位置编码的方式实现相对位置编码”，可以说是具备了绝对位置编码的方便性，同时可以表示不同 token 之间的相对位置关系。RoPE 是将位置编码和 query或者key进行相乘。 \\[\\begin{bmatrix}\\cos m\\theta_0\u0026-\\sin m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\\\sin m\\theta_0\u0026\\cos m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\cos m\\theta_1\u0026-\\sin m\\theta_1\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\sin m\\theta_1\u0026\\cos m\\theta_1\u0026\\cdots\u00260\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\cos m\\theta_{d/2-1}\u0026-\\sin m\\theta_{d/2-1}\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\sin m\\theta_{d/2-1}\u0026\\cos m\\theta_{d/2-1}\\end{bmatrix}\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\] 由于矩阵太稀疏，会造成浪费，因此计算时是这么做的： \\[\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix}\\] 此外，角度的计算方式如下： \\[\\theta_j=10000^{-2j/d},j\\in[1,2,\\dots,d/2]\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:4:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaRotaryEmbedding(torch.nn.Module): def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None): super().__init__() inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim)) self.register_buffer(\"inv_freq\", inv_freq) # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. # Keep the logic here just in case. if seq_len \u003e self.max_seq_len_cached: self.max_seq_len_cached = seq_len t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False) return ( self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype), self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype), ) def rotate_half(x): \"\"\"Rotates half the hidden dims of the input.\"\"\" x1 = x[..., : x.shape[-1] // 2] x2 = x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) def apply_rotary_pos_emb(q, k, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:4:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"SwiGLU \\[\\begin{aligned} \\mathrm{FFN}_{\\mathrm{SwiGLU}}(x,W,V,W_{2})\u0026=\\mathrm{SwiGLU}(x,W,V)W_{2}\\\\\\mathrm{SwiGLU}(x,W,V)\u0026=\\mathrm{Swish}_{\\beta}(xW)\\otimes xV\\\\\\mathrm{Swish}_{\\beta}(x)\u0026=x\\sigma(\\beta x) \\end{aligned}\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:5:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaMLP(nn.Module): def __init__( self, hidden_size: int, intermediate_size: int, hidden_act: str, ): super().__init__() self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False) self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False) self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False) # config 中 hidden_act = 'silu' # 'silu' 和 'swish' 对应的激活函数均为：SiLUActivation # https://github.com/huggingface/transformers/blob/717dadc6f36be9f50abc66adfd918f9b0e6e3502/src/transformers/activations.py#L229 self.act_fn = ACT2FN[hidden_act] def forward(self, x): # 对应上述公式的 SwiGLU return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:5:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"实验结果 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"常识推理任务 image.png LLaMA - 13B模型虽然比GPT - 3小10倍，但在大多数基准上也优于GPT - 3。 除BoolQ外，LLaMA - 65B在所有报告的基准上都优于Chinchilla-70B。 除了在BoolQ和WinoGrande上，LLaMA-65B在所有地方都超过了PaLM540B。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"阅读理解任务 image.png 可以看到，LLaMA-13B比GPT-3高出了几个百分点。 LLaMA-65B的表现已经接近甚至超越PaLM-540B的表现。 LLaMA2 Llama1只做了预训练，Llama2做了预训练+SFT+RLHF ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"KV Cache image.png LLM推理过程分为Prefill和Decode两个阶段。 Prefill阶段会对Prompt中所有的token做并行计算，得到Prompt中所有Tokens的KV Cache以及计算得到生成的第一个Token。Prompt阶段Token计算得到的KV Cache会保存下来，留给Decode阶段复用。 Decode阶段是一个自回归过程，每decode一个新的Token，都需要用到所有之前计算得到的KV Cache来计算当前query token的Attention。因此，当输出长度越来越大或者context很长时，KV Cache将会占用大量的显存。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"使用KV cache时位置信息怎么注入？ 初次学习KV cache时，虽然原理比较简单易懂，但是对于后续的输入只有一个token这里产生了些许困惑，后续只输入一个token的话，位置编码该怎么办呢？于是我比较简单粗暴地猜测位置index随着推理不断更新，当时翻了各种资料也没有得到解释，后面翻了翻llama的源码，发现我的猜测还真是正确的。 def forward(self, tokens: torch.Tensor, start_pos: int): \"\"\" Perform a forward pass through the Transformer model. Args: tokens (torch.Tensor): Input token indices. start_pos (int): Starting position for attention caching. Returns: torch.Tensor: Output logits after applying the Transformer model. \"\"\" _bsz, seqlen = tokens.shape h = self.tok_embeddings(tokens) self.freqs_cis = self.freqs_cis.to(h.device) freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] mask = None if seqlen \u003e 1: mask = torch.full( (seqlen, seqlen), float(\"-inf\"), device=tokens.device ) mask = torch.triu(mask, diagonal=1) # When performing key-value caching, we compute the attention scores # only for the new sequence. Thus, the matrix of scores is of size # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for # j \u003e cache_len + i, since row i corresponds to token cache_len + i. mask = torch.hstack([ torch.zeros((seqlen, start_pos), device=tokens.device), mask ]).type_as(h) for layer in self.layers: h = layer(h, start_pos, freqs_cis, mask) h = self.norm(h) output = self.output(h).float() return output 可以看到forward函数中的start_pos参数代表着位置信息，freqs_cis是实现RoPE位置编码需要用到的。 注意 freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]这一行，即是实现了rope相对位置编码的kv cache的核心。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class Attention(nn.Module): # ... self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): # 假设当前x为(1, 1, dim)，也就是上一个预测的token # self-attention的输入，标准的(bs, seqlen, hidden_dim) bsz, seqlen, _ = x.shape # 计算当前token的qkv # q k v分别进行映射，注意这里key, value也需要先由输入进行映射再和kv_cache里面的key, value进行拼接 xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # 对当前输入的query和key进行RoPE，注意kv_cache里面的key已经做过了RoPE xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # 缓存当前token的kv self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos: start_pos + seqlen] = xk self.cache_v[:bsz, start_pos: start_pos + seqlen] = xv # 取出前seqlen个token的kv缓存 # 取出全部缓存的key和value（包括之前在cache里面的和本次输入的），作为最终的key和value keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # 将kv重复填充，使kv和q的头数个数相同 # repeat k/v heads if n_kv_heads \u003c n_heads，对齐头的数量 keys = repeat_kv(keys, self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values = repeat_kv(values, self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) # 计算当前token的attention score，，注意mask需要加上，另外维度要对应上 xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim) keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_local_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1).type_as(xq) output = torch.matmul(scores, values) # (bs, n_local_heads, seqlen, head_dim) output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) return self.wo(output) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"MQA\u0026GQA image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"为什么不继续使用MHA？ 标准的mha中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"MQA 标准的MHA中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 MQA比较极端，只保留一个KV Head，多个Query Heads共享相同的KV Head。这相当于不同Head的Attention差异，全部都放在了Query上，需要模型仅从不同的Query Heads上就能够关注到输入hidden states不同方面的信息。这样做的好处是，极大地降低了KV Cache的需求，但是会导致模型效果有所下降。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"GQA GQA就是在MHA和MQA之间做了一个平衡。对query heads进行分组，分成几组就对应多少个kv heads，然后每一组内的query Heads共享相同的KV head。 GQA可以在减少计算量和KV Cache同时确保模型效果不受到大的影响。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:3","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"SFT 监督微调（Supervised Fine-Tuning, SFT）是对已经预训练的模型进行特定任务的训练，以提高其在该任务上的表现。预训练模型通常在大量通用数据上进行训练，学到广泛的语言知识和特征。在SFT过程中，利用特定任务的数据，对模型进行进一步调整，使其更适合该任务。 SFT数据一般就是\u003cprompt, response\u003e数据对。在训练方式上和pretrain没有任何区别，即得到当前token对应的logit，以next token作为标签计算交叉熵损失。 pretrain 是在背书，纯粹的学习知识；sft 则是在做题，学习的是指令 follow 能力。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:9:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"一些要点 少量高质量数据集训练模型的效果，要好于大量低质量数据集的训练效果。分析数据和清洗数据就是 sft 阶段 90% 的工作量。 sft 会让模型见到最重要的 eos_token，pretrain 模型因为没见过该 token 而无法停止生成。 sft 的 prompt 不做 loss，但这并不是说它不能做 loss。主要原因是 prompt 的同质化比较严重，不做 loss_mask 的话，同样的一句话会被翻来覆去的学，但如果你能保证你的每条 prompt 都是独一无二的，就完全可以省去 prompt 的 loss_mask 环节。 为了提高模型训练效率，将多组数据进行拼接，尽量填满4096。但对于分类任务会出现问题，详见https://zhuanlan.zhihu.com/p/809229182。 经过一通分析后，我们发现，新的训练方式改变了短 answer 数据的 loss 占比，毕竟模型在计算 loss 的时候，是先算一个句子内每个 token 的 平均 loss，再算一个 batch_size 内的平均 loss。 分类任务的 answer 通常只有 1 个 token：不 concat 的时候，它的 loss 贡献就是 1 / batch_size；concat 的时候，它就需要先和别的 answer 的 token 算平均 loss，再贡献 1 / batch_size。 这也就是说，采用 llama2 提到的 先 concat 语料再做 sft 训练，会对短 answer 数据很不公平，也就更容易造成短 answer 数据的欠拟合，pretrain 由于所有 token 都算 loss 则没有这个现象。最终，我们通过上采样短 answer 数据，成功的避免了分类任务的效果下滑。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:9:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"实验结果 - Llama 2模型优于Llama 1模型。 - Llama 2-70B比Llama 1-65B在MMLU和BBH上的结果分别提高了≈5和≈8个点。 - Llama 2-7B和30B模型在除代码基准以外的所有类别上都优于相应大小的MPT模型。 - Llama 2-7B和34B在所有类别的基准测试集上都优于Falcon-7B和40B模型。 参考 [KV Cache优化]🔥MQA/GQA/YOCO/CLA/MLKV笔记: 层内和层间KV Cache共享 - 知乎 (zhihu.com) Transformers KV Caching Explained | by João Lages | Medium https://zhuanlan.zhihu.com/p/679640407 LLaMA家族 https://zhuanlan.zhihu.com/p/809229182 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:10:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM"],"content":"LLM解码时采用的自回归采样，其过程如下： 小模型使用前缀作为输入，将输出结果处理+归一化成概率分布后，采样生成下一个token。 将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。 将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、top-k和top-n等 # 贪心搜索 直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复 # 随机采样 按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。 # beam search 维护一个大小为 k 的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k 个单词，然后保留总概率最高的 k 个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。 # top-k 选取前k个token，然后再重新生成概率分布，再进行抽样 它可以与其他解码策略结合使用，例如温度调节（Temperature Scaling）、重复惩罚（Repetition Penalty）、长度惩罚（Length Penalty）等，来进一步优化生成的效果。 代码: import torch from labml_nn.sampling import Sampler # Top-k Sampler class TopKSampler(Sampler): # k is the number of tokens to pick # sampler is the sampler to use for the top-k tokens # sampler can be any sampler that takes a logits tensor as input and returns a token tensor; e.g. `TemperatureSampler`. def __init__(self, k: int, sampler: Sampler): self.k = k self.sampler = sampler # Sample from logits def __call__(self, logits: torch.Tensor): # New logits filled with −∞; i.e. zero probability zeros = logits.new_ones(logits.shape) * float('-inf') # Pick the largest k logits and their indices values, indices = torch.topk(logits, self.k, dim=-1) # Set the values of the top-k selected indices to actual logits. # Logits of other tokens remain −∞ zeros.scatter_(-1, indices, values) # Sample from the top-k logits with the specified sampler. return self.sampler(zeros) top-p top-k 有一个缺陷，那就是“k 值取多少是最优的？”非常难确定。于是出现了动态设置 token 候选列表大小策略——即核采样（Nucleus Sampling）。 top-p 采样的思路是，在每一步，只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的单词。这种方法也被称为核采样（nucleus sampling），因为它只关注概率分布的核心部分，而忽略了尾部部分。例如，如果 p=0.9，那么我们只从累积概率达到 0.9 的最小单词集合中选择一个单词，而不考虑其他累积概率小于 0.9 的单词。这样可以避免采样到一些不合适或不相关的单词，同时也可以保留一些有趣或有创意的单词。 import torch from torch import nn from labml_nn.sampling import Sampler class NucleusSampler(Sampler): \"\"\" ## Nucleus Sampler \"\"\" def __init__(self, p: float, sampler: Sampler): \"\"\" :param p: is the sum of probabilities of tokens to pick $p$ :param sampler: is the sampler to use for the selected tokens \"\"\" self.p = p self.sampler = sampler # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits self.softmax = nn.Softmax(dim=-1) def __call__(self, logits: torch.Tensor): \"\"\" Sample from logits with Nucleus Sampling \"\"\" # Get probabilities $P(x_i | x_{1:i-1})$ probs = self.softmax(logits) # Sort probabilities in descending order sorted_probs, indices = torch.sort(probs, dim=-1, descending=True) # Get the cumulative sum of probabilities in the sorted order cum_sum_probs = torch.cumsum(sorted_probs, dim=-1) # Find the cumulative sums less than $p$. nucleus = cum_sum_probs \u003c self.p # Prepend ones so that we add one token after the minimum number # of tokens with cumulative probability less that $p$. nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1) # Get log probabilities and mask out the non-nucleus sorted_log_probs = torch.log(sorted_probs) sorted_log_probs[~nucleus] = float('-inf') # Sample from the sampler sampled_sorted_indexes = self.sampler(sorted_log_probs) # Get the actual indexes res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1)) # return res.squeeze(-1) Temperature采样 详见温度超参数 speculative decoding 大型语言模型（LLM）的推理通常需要使用自回归采样。它们的推理过程相当缓慢，需要逐个token地进行串行解码。因此，大型模型的推理过程往往受制于访存速度，生成每个标记都需要将所有参数从存储单元传输到计算单元，因此内存访问带宽成为严重的瓶颈。 为了解决推理速度慢的问题，已经进行了许多针对推理的工程优化，例如改进的计算核心实现、多卡并行计算、批处理策略等等。然而，这些方法并没有从根本上解决LLM解码过程是受制于访存带宽的问题。 投机采样是一种可以从根本上解码计算访存比的方法，保证和使用原始模型的采样分布完全相同。它使用两个模型：一个是原始目标模型，另一个是比原始模型小得多的近似模型。近似模型用于进行自回归串行采样，而大型模型则用于评估采样结果。解码过程中，某些token的解码相对容易，某些token的解码则很困难。因此，简单的token生成可以交给小型模型处理，而困难的token则交给大型模型处理。这里的小型模型可以采用与原始模型相同的结构，但参数更少，或者干脆使用n-gram模型。小型模型不仅计算量较小，更重要的是减少了内存访问的需求。 ## 采样过程 投机采样过程如下： 用小模型Mq做自回归采样连续生成 γ 个tokens。 把生成的γ个tokens和前缀拼接一起送进大模Mp执行一次forwards。 使用大、小模型logits结果做比对，如果发现某个token小模型生成的不好，重新采样这个token。重复步骤1。 如果小模型生成结果都满意，则用大模型采样下一个token。重复步骤1。 第2步，将γ个tokens和前缀拼成一起作为大模型输入，和自回归相比，尽管计算量一样，但是γ个tokens可以同时参与计算，计算访存比显著提升。 第3步，如何评价一个token生成的不好？如果q(x) \u003e p(x)（p，q表示在大小模型采样概率，也就是logits归一化后的概率分布）","date":"2024-09-05","objectID":"/generate%E7%9B%B8%E5%85%B3/:0:0","tags":["LLM"],"title":"frequency_penalty\u0026presence_penalty","uri":"/generate%E7%9B%B8%E5%85%B3/"},{"categories":["","LLM"],"content":"线性Transformer \\[V_i'=\\frac{\\sum_{j=1}^N sim(Q_i,K_j)V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\] 注意下标i。 其中 \\[sim(Q_{i},K_{j})=\\phi(Q_{i},K_{j})\\] 此时有： \\[V_{i}^{\\prime}=\\frac{\\phi(Q_{i})\\sum_{j=1}^{i}\\phi(K_{j})^{T}V_{j}}{\\phi(Q_{i})\\sum_{j=1}^{i}\\phi(K_{j})^{T}}\\] 注意可以将\\(\\phi(Q_{i})\\)提出来。 原始Transformer的计算复杂度随序列长N呈二次方增长，这是因为attention的计算包含两层for循环，外层是对于每一个Query，我们需要计算它对应token的新表征；内层for循环是为了计算每一个Query对应的新表征，需要让该Query与每一个Key进行计算。 所以外层是 for q in Queries，内层是 for k in Keys。Queries数量和Keys数量都是N，所以复杂度是 O(N^2) 。而Linear Transformer，它只有外层for q in Queries这个循环了。因为求和项的计算与i无关，所以所有的 Qi 可以共享求和项的值。换言之，求和项的值可以只计算一次，然后存在内存中供所有 Qi 去使用。所以Linear Transformer的计算复杂度是O(N) 。 Attention Free Transformer \\[V_i'=\\sigma(Q_i)\\odot\\frac{\\sum_{j-1}^iexp(K_j+w_{i,j})\\odot V_j}{\\sum_{j=1}^iexp(K_j+w_{i,j})}\\] 其中σ是sigmoid函数\\(^{+};\\odot\\)是逐元素相乘 (element-wise product); wi,j是待训练的参数。 AFT采用的形式和上面的Linear Transformer不一样。首先是attention score, Linear Transformer仍然是同Transformer一样，为每一个Value赋予一个weight。而AFT会为每个 dimension\\(^{+}\\)赋予weight。换言之，在Linear Transformer中，同一个Value中不同dimension的weight是一致的；而AFT同一Value中不同dimension的weight不同(\\(w_{i,j}\\))。此外，attention score的计算也变得格外简单，用K去加一个可训练的bias\\(^{+}\\)。Q的用法很像一个gate。 可以很容易仿照公式(5)把AFT也写成递归形式，这样容易看出，AFT也可以像Linear Transformer,在inference阶段复用前面时刻的计算结果，表现如RNN形式，从而相比于 Transformer变得更加高效。 RWKV RWKV的特点如下： 改造AFT，通过Liner Transformer变换将self-attention复杂度由O(N^2)降为 O(N) 。 保留AFT简单的“attention”形式和Sequential Decoding，具有RNN表现形式。 ","date":"2024-09-04","objectID":"/rwkv/:0:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"Time-Mixing image.png def time_mixing(x, last_x, last_num, last_den, decay, bonus, mix_k, mix_v, mix_r, Wk, Wv, Wr, Wout): k = Wk @ ( x * mix_k + last_x * (1 - mix_k) ) v = Wv @ ( x * mix_v + last_x * (1 - mix_v) ) r = Wr @ ( x * mix_r + last_x * (1 - mix_r) ) wkv = (last_num + exp(bonus + k) * v) / \\ (last_den + exp(bonus + k)) rwkv = sigmoid(r) * wkv num = exp(-exp(decay)) * last_num + exp(k) * v den = exp(-exp(decay)) * last_den + exp(k) return Wout @ rwkv, (x,num,den) ","date":"2024-09-04","objectID":"/rwkv/:1:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"Channel-Mixing image.png def channel_mixing(x, last_x, mix_k, mix_r, Wk, Wr, Wv): k = Wk @ ( x * mix_k + last_x * (1 - mix_k) ) r = Wr @ ( x * mix_r + last_x * (1 - mix_r) ) vk = Wv @ np.maximum(k, 0)**2 return sigmoid(r) * vk, x 参考 How the RWKV language model works | The Good Minima (johanwind.github.io) ","date":"2024-09-04","objectID":"/rwkv/:2:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"证明 核心思想就是找到一个转换，可以通过点积操作将位置信息注入，即： \\[\u003cf_q\\left(x_m,m\\right),f_k\\left(x_n,n\\right)\u003e=g\\left(x_m,x_n,m-n\\right)\\] 而通过复数的一些性质，找到了满足上述操作的转换： \\[\\begin{aligned} \u0026f_{q}\\left(\\boldsymbol{x}_{m},m\\right)=\\left(\\boldsymbol{W}_{q}\\boldsymbol{x}_{m}\\right)e^{im\\theta} \\\\ \u0026f_{k}\\left(\\boldsymbol{x}_{n},n\\right)=\\left(\\boldsymbol{W}_{k}\\boldsymbol{x}_{n}\\right)e^{in\\theta} \\\\ \u0026g\\left(\\boldsymbol{x}_{m},\\boldsymbol{x}_{n},m-n\\right)=\\mathrm{Re}\\left[\\left(\\boldsymbol{W}_{q}\\boldsymbol{x}_{m}\\right)\\left(\\boldsymbol{W}_{k}\\boldsymbol{x}_{n}\\right)^{*}e^{i(m-n)\\theta}\\right] \\end{aligned}\\] 可以发现g函数中存在相对位置信息。 欧拉公式：\\(e^{ix}=\\cos x+i\\sin x\\) \\[\\begin{aligned}\u0026\\text{基于上面面1点结论,可知}\\\\\u0026f_{q}\\left(x_{m},m\\right)=\\left(W_{q}x_{m}\\right)e^{im\\theta}=q_{m}e^{im\\theta}\\\\\u0026\\text{然后将}q_{m\\text{表示成复数形式（torch.view\\_as\\_complex）,可得}}\\\\\u0026q_{m}=\\left[q_{m}^{(1)},q_{m}^{(2)}\\right]=\\left[q_{m}^{(1)}+iq_{m}^{(2)}\\right]\\\\\u0026\\text{从而有}\\\\\u0026f_{q}\\left(x_{m},m\\right)=q_{m}e^{im\\theta}=\\left[q_{m}^{(1)}+iq_{m}^{(2)}\\right]e^{im\\theta}\\\\\u0026\\text{基于欧拉公式,可知}f_{q}\\left(x_{m},m\\right)_{\\text{即是两个复数相乘}}\\\\\u0026f_{q}\\left(x_{m},m\\right)=q_{m}e^{im\\theta}=\\left(q_{m}^{(1)}+iq_{m}^{(2)}\\right)*\\left(\\cos(m\\theta)+i\\sin(m\\theta)\\right)\\end{aligned}\\] 根据复数的计算，可得： \\[\\begin{aligned}q_{m}e^{im\\theta}=\\left(q_{m}^{(1)}+iq_{m}^{(2)}\\right)*(\\cos(m\\theta)+i\\sin(m\\theta))\\\\=\\left(q_{m}^{(1)}\\cos(m\\theta) -q_{m}^{(2)}\\sin(m\\theta)\\right)+i\\left(q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right)\\end{aligned}\\] 再将结果写成向量的形式，即： \\[q_{m}e^{im\\theta}=\\left[q_{m}^{(1)}\\cos(m\\theta)-q_{m}^{(2)}\\sin(m\\theta),q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right]\\] 即是query向量乘了一个旋转矩阵： \\[\\begin{gathered} f_{q}\\left(x_{m},m\\right)=\\left(W_{q}x_{m}\\right)e^{im\\theta}=q_{m}e^{im\\theta} \\\\ =\\left|q_{m}^{(1)}\\cos(m\\theta)-q_{m}^{(2)}\\sin(m\\theta),q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right| \\\\ =\\left(\\begin{array}{cc}{\\cos(m\\theta)}\u0026{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}\u0026{\\cos(m\\theta)}\\end{array}\\right)\\left(\\begin{array}{c}{q_{m}^{(1)}}\\\\{q_{m}^{(2)}}\\end{array}\\right) \\end{gathered}\\] 后续的证明看一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)-CSDN博客 将二维推广，有： \\[\\boldsymbol{R}_{\\Theta,m}^{d}=\\underbrace{\\left(\\begin{array}{ccccccc}{\\cos m\\theta_{0}}\u0026{-\\sin m\\theta_{0}}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{\\sin m\\theta_{0}}\u0026{\\cos m\\theta_{0}}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{0}\u0026{0}\u0026{\\cos m\\theta_{1}}\u0026{-\\sin m\\theta_{1}}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{0}\u0026{0}\u0026{\\sin m\\theta_{1}}\u0026{\\cos m\\theta_{1}}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{\\vdots}\u0026{\\vdots}\u0026{\\vdots}\u0026{\\vdots}\u0026{\\ddots}\u0026{\\vdots}\u0026{\\vdots}\\\\{0}\u0026{0}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{\\cos m\\theta_{d/2-1}}\u0026{-\\sin m\\theta_{d/2-1}}\\\\{0}\u0026{0}\u0026{0}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{\\sin m\\theta_{d/2-1}}\u0026{\\cos m\\theta_{d/2-1}}\\end{array}\\right)}\\] 则计算旋转编码，即有： \\[\\begin{bmatrix}\\cos m\\theta_0\u0026-\\sin m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\\\sin m\\theta_0\u0026\\cos m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\cos m\\theta_1\u0026-\\sin m\\theta_1\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\sin m\\theta_1\u0026\\cos m\\theta_1\u0026\\cdots\u00260\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\cos m\\theta_{d/2-1}\u0026-\\sin m\\theta_{d/2-1}\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\sin m\\theta_{d/2-1}\u0026\\cos m\\theta_{d/2-1}\\end{bmatrix}\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\] 由于矩阵太稀疏，会造成浪费，因此计算时是这么做的： \\[\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix}\\] 此外，角度的计算方式如下： \\[\\theta_j=10000^{-2j/d},j\\in[1,2,\\dots,d/2]\\] 代码 ","date":"2024-08-31","objectID":"/rope/:0:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["","LLM"],"content":"llama实现 llama实现比较简单，但是一开始很不容易理解，实现如下： def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0): \"\"\" Precompute the frequency tensor for complex exponentials (cis) with given dimensions. This function calculates a frequency tensor with complex exponentials using the given dimension 'dim' and the end index 'end'. The 'theta' parameter scales the frequencies. The returned tensor contains complex values in complex64 data type. Args: dim (int): Dimension of the frequency tensor. end (int): End index for precomputing frequencies. theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0. Returns: torch.Tensor: Precomputed frequency tensor with complex exponentials. \"\"\" # dim = 128 # end = 4096 # torch.arange(0, dim, 2) [0, 2, 4, 6, 8, 10,..., 124, 126] 共64个 # torch.arange(0, dim, 2)[: (dim // 2)] 保证是64个 freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # rope中的角度 # freqs = [1/10000.0^(0/128), 1/10000.0^(2/128), 1/10000.0^(4/128), ..., 1/10000.0^(126/128)] t = torch.arange(end, device=freqs.device) # postition idx # t = [0, 1, 2, ..., 4095] freqs = torch.outer(t, freqs).float() # type: ignore # freqs 得到 freqs和t的笛卡尔积，维度为（4096，64） # freqs = [[0, 0, 0,..., 0], # [1/10000.0^(0/128), 1/10000.0^(2/128), 1/10000.0^(4/128), ..., 1/10000.0^(126/128)], # [2/10000.0^(0/128), 2/10000.0^(2/128), 2/10000.0^(4/128), ..., 2/10000.0^(126/128)], # ..., # [4095/10000.0^(0/128), 4095/10000.0^(2/128), 4095/10000.0^(4/128), ..., 4095/10000.0^(126/128)]] freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # complex64 # freqs_cis的维度为(4096,64)，相当于半径为1，角度为freqs的极坐标的复数表示，如公式6所示。 return freqs_cis def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor): # 将除了position和dim其他的维度变为1 \"\"\" Reshape frequency tensor for broadcasting it with another tensor. This function reshapes the frequency tensor to have the same shape as the target tensor 'x' for the purpose of broadcasting the frequency tensor during element-wise operations. Args: freqs_cis (torch.Tensor): Frequency tensor to be reshaped. x (torch.Tensor): Target tensor for broadcasting compatibility. Returns: torch.Tensor: Reshaped frequency tensor. Raises: AssertionError: If the frequency tensor doesn't match the expected shape. AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions. \"\"\" # freqs_cis.shape = [1024, 64] # x.shape = [2, 1024, 32, 64] ndim = x.ndim assert 0 \u003c= 1 \u003c ndim assert freqs_cis.shape == (x.shape[1], x.shape[-1]) # 将freqs_cis.shape变为[1, 1024, 1, 64] shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] return freqs_cis.view(*shape) def apply_rotary_emb( xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, ): \"\"\" Apply rotary embeddings to input tensors using the given frequency tensor. This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are returned as real tensors. Args: xq (torch.Tensor): Query tensor to apply rotary embeddings. xk (torch.Tensor): Key tensor to apply rotary embeddings. freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials. Returns: Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings. \"\"\" # 将xq和xk的最后一个维度进行复数运算，得到新的xq和xk # 为了进行复数运算，需要将xq和xk的最后一个维度展开为2维 # 例如，xq的形状为[2, seq_len, 32, 128], reshape后为[2, seq_len, 32 , 64, 2] # view_as_complex函数可以将张量中的最后一维的两个元素作为实部和虚部合成一个复数xq的形状变为[2, seq_len, 32, 64] xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)) xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) # 将freqs_cis广播到xq和xk的最后一个维度 freqs_cis = reshape_for_broadcast(freqs_cis, xq_) # freqs_cis.shape = [1, 1024, 1, 64] # view_as_real和view_as_complex相反，可以将张量中最后一维的复数拆出实部和虚部 # (xq_ ","date":"2024-08-31","objectID":"/rope/:1:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["","LLM"],"content":"另一种实现 另一种实现(transformers)利用了下面这个式子： \\[ \\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix} \\] class LlamaRotaryEmbedding(torch.nn.Module): def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None): super().__init__() inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim)) self.register_buffer(\"inv_freq\", inv_freq) # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. # Keep the logic here just in case. if seq_len \u003e self.max_seq_len_cached: self.max_seq_len_cached = seq_len t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False) return ( self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype), self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype), ) def rotate_half(x): \"\"\"Rotates half the hidden dims of the input.\"\"\" x1 = x[..., : x.shape[-1] // 2] x2 = x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) def apply_rotary_pos_emb(q, k, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed 相对于llama的版本比较容易理解。 Long-term decay of RoPE 公式不看了，结论就是RoPE有长距离衰减的特性，相对距离越远的token之间的关注度也会降低，表现为attention score减小，这是个很好的特性。”This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection.“ # 参考 一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)-CSDN博客 LLM—llama2结构和源码解读 - 知乎 (zhihu.com) ","date":"2024-08-31","objectID":"/rope/:2:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["","文献和源码阅读"],"content":"Data Engineering for Scaling Language Models to 128K Context ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:0:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["","文献和源码阅读"],"content":"💡 Meta Data Title Data Engineering for Scaling Language Models to 128K Context Journal Authors Yao Fu; Rameswar Panda; Xinyao Niu; Xiang Yue; Hannaneh Hajishirzi; Yoon Kim; Hao Peng Pub. date 2024-02-15 期刊标签 DOI 10.48550/arXiv.2402.10171 附件 Fu et al_2024_Data Engineering for Scaling Language Models to 128K Context.pdf ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:1:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["","文献和源码阅读"],"content":"📜 研究背景 \u0026 基础 \u0026 目的 论文主要研究了如何通过数据工程的方法，将语言模型的上下文长度扩展到128K个token。这项研究的重点在于数据工程，作者们提出了一个假设：长上下文建模的能力，特别是利用任意输入位置信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过轻量级的持续预训练在适当的数据混合上扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。 ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:2:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["","文献和源码阅读"],"content":"📊 研究内容 “(1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context;” (Fu 等, 2024, p. 1) 数据量较少 “(2) for quality, our results equally emphasize domain balance and length upsampling. Concretely, we find that na ̈ıvely upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important.” (Fu 等, 2024, p. 1) 对于质量来说使用上采样可以大幅提高能力 “which asks the model to precisely recite the information in a given sentence where the sentence (the “needle”) is placed in an arbitrary location of a 128K long document (the “haystack”).” (Fu 等, 2024, p. 1) 干草堆测试的定义 “attention has quadratic complexity” (Fu 等, 2024, p. 1) transformer原始注意力就是一个平方复杂度的注意力 “We hypothesize that the capability to utilize information at arbitrary locations within long context length is (mostly) already acquired during pretraining, even for models pretrained on substantially shorter 4K contexts.” (Fu 等, 2024, p. 1) 作者认为模型在预训练过程就学习到了利用位置信息的能力 “because, as we observe, this results in perplexiy degradations in other domains (Table 5).” (Fu 等, 2024, p. 2) 只是单独对一个邻域的数据上采样会使得其它邻域的性能下降。 “we use 80K compared to Together’s 32K, which does not generalizes beyond 32K;” (Fu 等, 2024, p. 3) 使用80k的上下文进行训练 “data mixture: we use SlimPajama which has balanced domains compared to YaRN, which uses book-only PG19;” (Fu 等, 2024, p. 3) 使用邻域数据更平衡的混合数据集 “length upsampling: we upsample long sequences compared to LongLoRA, which does not.” (Fu 等, 2024, p. 3) 邻域数据进行平衡性的长文本上采样 “Another important related work is the previous LLaMA Long (Xiong et al., 2023) work and the concurrent XVERSE (XVerse, 2024) work, which continue pretraining the model on 32K sequences for about 500 billion tokens. These works are implicitly motivated by the view that longcontext modeling is a new capability that must be “injected” through large-scale training. We instead hypothesize that the base model has mostly already acquired this capability through large-scale pretraining, and thus a lightweight continual pretraining on relatively small data (e.g., 5B tokens) is enough to extend these capabilities to much longer context lengths (Fig. 3).” (Fu 等, 2024, p. 3) 与另一种观点对比，另一种观点是大模型的长上下文能力是通过大规模的继续预训练注入的。 “We use the SlimPajama (Soboleva et al., 2023) dataset for continual pretraining. This dataset is an open-source reproduction of the LLaMA (Touvron et al., 2023a) pretraining data mixture, consisting of 82% web data (67% from CommonCrawl and 15% from C4), 4.5% code (Github), 4.5% Wikipedia, 4.5% books, 2.5% Arxiv, and 2.0% StackExchange.” (Fu 等, 2024, p. 3) 本文使用的SlimPajama数据集的构成 “Since this dataset closely mirrors that used to pretrain the LLaMA models, there is less concern of distribution shift during continual pretraining; it is therefore used by many recent works like Fuzhao Xue \u0026 You (2023).” (Fu 等, 2024, p. 3) 即本文继续预训练使用的数据集与llama预训练使用的数据集相比分布比较接近，偏移较少，对预训练权重影响不大。 “Directly upsampling long data changes the domain mixture, e.g., upsampling sequences longer than 100K will increase the portion of the books domain. Likewise, changes in the domain mixture will result in shifts of the length distribution.” (Fu 等, 2024, p. 3) 不能直接上采样，会改变不同邻域数据的混合比例 🔤直接上采样长数据会改变域混合，例如，上采样序列大于100K会增加图书域的比例。同样，域混合的变化也会导致长度分布的变化。🔤 “Per-source Upsampling: This retains the domain mixture, then upsamples long documents within each domain.” (Fu 等, 2024, p. 3) 核心做法 “For training, we use a constant learning rate 2e-5. We modify the base of RoPE positional encoding to adjust it to longer context, as in Xiong et al. (2023). We pack all data to 80K chunks regardless of the document boundary, following common practice (Raffel et al., 2020; Touvron et al., 2023a). We set the batch size to be 4M tokens. Note that this batch size is the same as training on 4K context length, as we increase the length of a chunk but decrease the number of chunks in a batch. We train the model on 5B tok","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:3:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["","文献和源码阅读"],"content":"🚩 研究结论 论文总结了研究成果，指出通过持续预训练可以有效地扩展语言模型的上下文长度，并为未来的长上下文指令微调研究奠定了基础。 ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:4:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["","文献和源码阅读"],"content":"📌 感想 \u0026 疑问 这篇论文提出了一种通过数据工程的方法来进行长下文建模，主要是通过加长继续预训练的上下文长度，并且选用新的混合邻域数据集，对每一个邻域都进行长文本的上采样，从而提高了数据质量，实验证明通过这种方法得到的模型在干草堆实验上的效果与chatgpt接近，并且不会损失太多在短文本上的性能。 ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:5:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":[""],"content":"KV cache LLM推理过程分为Prefill和Decode两个阶段，其中Prefill阶段会对Prompt中所有的token做并行计算，得到Prompt中所有Tokens的KV Cache以及计算得到首Token。Prompt阶段Token计算得到的KV Cache会保存下来，留给Decode阶段复用，Decode阶段是一个自回归过程，每decode一个新的Token，都需要用到所有之前计算得到的KV Cache来计算当前query token的Attention。因此，当输出长度越来越大或者context很长时，KV Cache将会占用大量的显存。如何优化KV Cache的显存占用，一直都是LLM推理的核心主题之一。 之前一直疑惑kv cache既然每次只输入生成token就可以，那么位置信息该怎么注入呢？翻了翻llama的源码，找到了答案： def forward(self, tokens: torch.Tensor, start_pos: int): \"\"\" Perform a forward pass through the Transformer model. Args: tokens (torch.Tensor): Input token indices. start_pos (int): Starting position for attention caching. Returns: torch.Tensor: Output logits after applying the Transformer model. \"\"\" _bsz, seqlen = tokens.shape h = self.tok_embeddings(tokens) self.freqs_cis = self.freqs_cis.to(h.device) freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] mask = None if seqlen \u003e 1: mask = torch.full( (seqlen, seqlen), float(\"-inf\"), device=tokens.device ) mask = torch.triu(mask, diagonal=1) # When performing key-value caching, we compute the attention scores # only for the new sequence. Thus, the matrix of scores is of size # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for # j \u003e cache_len + i, since row i corresponds to token cache_len + i. mask = torch.hstack([ torch.zeros((seqlen, start_pos), device=tokens.device), mask ]).type_as(h) for layer in self.layers: h = layer(h, start_pos, freqs_cis, mask) h = self.norm(h) output = self.output(h).float() return output 注意freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]这一行，即是实现了rope相对位置编码的kv cache的核心。 kv cache代码 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\" bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) class Attention(nn.Module): \"\"\"Multi-head attention module.\"\"\" def __init__(self, args: ModelArgs): \"\"\" Initialize the Attention module. Args: args (ModelArgs): Model configuration parameters. Attributes: n_kv_heads (int): Number of key and value heads. n_local_heads (int): Number of local query heads. n_local_kv_heads (int): Number of local key and value heads. n_rep (int): Number of repetitions for local heads. head_dim (int): Dimension size of each attention head. wq (ColumnParallelLinear): Linear transformation for queries. wk (ColumnParallelLinear): Linear transformation for keys. wv (ColumnParallelLinear): Linear transformation for values. wo (RowParallelLinear): Linear transformation for output. cache_k (torch.Tensor): Cached keys for attention. cache_v (torch.Tensor): Cached values for attention. \"\"\" super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads // model_parallel_size self.n_local_kv_heads = self.n_kv_heads // model_parallel_size self.n_rep = self.n_local_heads // self.n_local_kv_heads self.head_dim = args.dim // args.n_heads self.wq = ColumnParallelLinear( args.dim, args.n_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wk = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wv = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wo = RowParallelLinear( args.n_heads * self.head_dim, args.dim, bias=False, input_is_parallel=True, init_method=lambda x: x, ) # kv_cache是缓存键值对，在训练过程中，我们只保存最近n个键值对 self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[","date":"2024-08-07","objectID":"/kv-cache/:0:0","tags":null,"title":"KV cache","uri":"/kv-cache/"},{"categories":[""],"content":"Low-Rank Adaption (LoRA)，即“低秩适配”，实现了预训练模型的参数高效微调，且不会增加模型的推理延迟。 ## 内在维度 2020年，A. Aghajanyan等人研究了这一现象，发现预训练模型存在一个较低的”内在维度”,使用少量样本微调时，实际上是在更新低维空间中的参数。把预训练模型的全部参数看成一个D维参数向量，记为\\(\\Theta^\\mathrm{(D)}\\),模型的原始参数为\\(\\Theta_0^\\mathrm{(D)}\\),设\\(\\Theta^{(d)}\\)是d维子空间中的一个向量，d\u003cD,利用一个固定的D*d映射矩阵P 把d维空间中的向量映射到D维空间，\\(\\Theta^{(\\mathrm{D})}\\)可写为： \\[\\mathrm{\\theta^{(D)}=\\theta_0^{(D)}+P\\theta^{(d)}}\\] 下图中，以D=3,d=2为例： 左图直接在3维空间中训练模型，直接优化原始模型参数\\(\\Theta_{0}^{(\\mathrm{D})}\\),把它更新为\\(\\Theta^{(\\mathrm{D})}\\)。右图冻结\\(\\Theta_{0}^{(\\mathrm{D})}\\),转而在2维空间中寻找一个\\(\\Theta^{(\\mathrm{d})}\\),再用矩阵P把\\(\\Theta^{(\\mathrm{d})}\\)映射到3维空间。如果用右图的方式可以把模型优化到良好的效果，例如，达到了全量参数微调效果的90%,则该模型的内在维度\\(\\mathrm{d}_{90}=2\\)。 实验表明，仅训练200个参数，就可以使RoBERTa-large在MRPC数据集上的效果达到全量参数微调效果的90%。 ## 低秩适配 预训练模型的权重矩阵通常具有满秩，这意味着权重矩阵的各个列向量之间线性无关，这样的矩阵没有冗余信息，是无法被压缩的。但是，“内在维度”现象表明，微调模型时只需更新少量参数，这启发我们微调时产生的权重增量矩阵\\(\\Delta\\)W可能包含大量冗余参数，\\(\\Delta\\)W很可能不是满秩的。对低秩矩阵做分解，可以利用较少的参数重建或近似原矩阵。这就是LoRA的核心思想。 设输入为x，微调时得到增量\\(\\Delta W\\),与原始权重\\(\\mathcal{W}_{0}\\)相加得到更新后的权重，输出h= ( \\(W_0\\)+ \\(\\Delta\\)W) x。根据矩阵的乘法分配律，有h= \\(W_0\\)x+ \\(\\Delta\\)Wx,这意味着微调时可以保持\\(W_0\\)不变，分别将\\(W_0\\)、\\(\\Delta\\)W与x相乘，最后把两个乘积相加即可得到输出h。 设\\(\\mathcal{W}_0\\in\\mathbb{R}^{\\mathrm{dxk}}\\),\\(\\Delta \\mathcal{W} _{\\mathrm{f} }\\)的秩为r。\\(\\Delta \\mathcal{W}= \\mathcal{B} \\mathcal{A}\\)是\\(\\Delta\\mathcal{W}\\)的一个满秩分解，其中 \\(\\mathcal{B} \\in \\mathbb{R} ^{\\mathrm{dxr}}, \\mathcal{A} \\in \\mathbb{R} ^{\\mathrm{rxk}}, \\mathcal{r} \\ll \\min ( \\mathcal{d} , \\mathcal{k} )\\)。训练时，分别用随机高斯和零矩阵初始化A和B，确保初始化时 BA是零矩阵，对模型效果没有影响。训练过程中冻结\\(\\mathcal{W}_0\\),只更新矩阵B和A，共r(d+k)个参数，从而实 现“参数高效”微调。推理时，分别计算\\(\\mathbf{W_{\\mathrm{n} }x}\\)和 BAx并相加，得到输出h，如下图所示： 实际上，r是一个超参，训练时可任意设定，\\(\\Delta\\)W真正的秩未必等于r。如果r恰好等于\\(\\Delta\\)W的秩，甚至大于\\(\\Delta\\)的秩(例如等于预训练权重矩阵\\(W_{0}\\)的秩),利用学到的B和A可以完全重建\\(\\Delta\\)W,这时，LoRA的效果近似于全量微调。如果r小于\\(\\Delta\\)W的秩，BA就是\\(\\Delta\\)W的一个低秩近似，利用矩阵B和A可以恢复矩阵\\(\\Delta W\\)中的部分信息。 ## 代码 class LoraModel(torch.nn.Module): \"\"\" Creates Low Rank Adapter (Lora) model from a pretrained transformers model. Args: model ([`transformers.PreTrainedModel`]): The model to be adapted. config ([`LoraConfig`]): The configuration of the Lora model. Returns: `torch.nn.Module`: The Lora model. Example:: \u003e\u003e\u003e from transformers import AutoModelForSeq2SeqLM, LoraConfig \u003e\u003e\u003e from peft import LoraModel, LoraConfig \u003e\u003e\u003e config = LoraConfig( peft_type=\"LORA\", task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.01, ) \u003e\u003e\u003e model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") \u003e\u003e\u003e lora_model = LoraModel(config, model) **Attributes**: - **model** ([`transformers.PreTrainedModel`]) -- The model to be adapted. - **peft_config** ([`LoraConfig`]): The configuration of the Lora model. \"\"\" def __init__(self, config, model): super().__init__() self.peft_config = config self.model = model self._find_and_replace() mark_only_lora_as_trainable(self.model, self.peft_config.bias) def _find_and_replace(self): kwargs = { \"r\": self.peft_config.r, \"lora_alpha\": self.peft_config.lora_alpha, \"lora_dropout\": self.peft_config.lora_dropout, \"fan_in_fan_out\": self.peft_config.fan_in_fan_out, \"merge_weights\": self.peft_config.merge_weights, } key_list = [key for key, _ in self.model.named_modules()] for key in key_list: if any(key.endswith(target_key) for target_key in self.peft_config.target_modules): # 对特定的层插入lora层 parent, target, target_name = self._get_submodules(key) bias = target.bias is not None if isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None: new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs) elif self.peft_config.enable_lora is not None: kwargs.update({\"enable_lora\": self.peft_config.enable_lora}) if isinstance(target, Conv1D): in_features, out_features = target.weight.shape else: in_features, out_features = target.in_features, target.out_features if kwargs[\"fan_in_fan_out\"]: warnings.warn( \"fan_in_fan_out is set to True but the target module is not a Conv1D. \" \"Setting fan_in_fan_out to False.\" ) kwargs[\"fan_in_fan_out\"] = False new_module = MergedLinear(in_features, out_","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:0:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":[""],"content":"参考 https://snailcoder.github.io/2023/08/06/parameter-efficient-llm-fine-tuning-lora.html ","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:1:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":["","文献和源码阅读"],"content":"Transformer Feed-Forward Layers Are Key-Value Memories ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:0:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","文献和源码阅读"],"content":"💡 Meta Data Title Transformer Feed-Forward Layers Are Key-Value Memories Journal Authors Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy Pub. date 2021-09-05 期刊标签 DOI 10.48550/arXiv.2012.14913 附件 Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:1:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","文献和源码阅读"],"content":"📜 研究背景 \u0026 基础 \u0026 目的 前馈层占据了 Transformer 模型参数的三分之二，但其在网络中的作用尚未被充分探索。作者发现 Transformer 语言模型中的前馈层可以作为键值记忆（key-value memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。作者发现 Transformer 语言模型中的前馈层可以作为键值记忆（key-value memories）来操作。每个键（key）与训练示例中的文本模式相关联，每个值（value）则诱导输出词汇表上的概率分布。前馈层的输出是其记忆的组合，并通过模型层的残差连接逐步细化，以产生最终的输出分布。 ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:2:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","文献和源码阅读"],"content":"📊 研究内容 前馈层与键值神经记忆非常相似，唯一的区别是神经记忆使用 softmax 作为非线性函数，而 Transformer 中的前馈层不使用归一化函数。 其中 “We posit that the key vectors K in feed-forward layers act as pattern detectors over the input sequence, where each individual key vector ki corresponds to a specific pattern over the input prefix x1, . . . , xj. To test our claim, we analyze the keys of a trained language model’s feed-forward layers. We first retrieve the training examples (prefixes of a sentence) most associated with a given key, that is, the input texts where the memory coefficient is highest.” (Geva 等, 2021, p. 2) 表明神经元中的\\(k_i\\)代表了一种模式，而对应的参数矩阵K（某一列向量）充当这个模式的模式检测器。当检测到有对应模式时，则表现为对应的\\(k_i\\)值较高，相当于注意力中的得分，而其对应的第二层参数矩阵V（某一列向量）代表了这种模式对应的token的概率分布（乘嵌入矩阵进行转换），而将得分与V向量概率分布相乘后得到的概率分布即是最终要得到token的概率分布。即这是一种混合响应输出。 “Comparing equations 1 and 2 shows that feedforward layers are almost identical to key-value neural memories; the only difference is that neural memory uses softmax as the non-linearity f (·), while the canonical transformer does not use a normalizing function in the feed-forward layer. The hidden dimension dm is essentially the number of memories in the layer, and the activation m = f (x · K\u003e), commonly referred to as the hidden layer, is a vector containing an unnormalized non-negative coefficient for each memory. We refer to each mi as the memory coefficient of the ith memory cell.” (Geva 等, 2021, p. 2) ffn中的kv解释与self attention中kv的区别 “We assume that patterns stored in memory cells originate from examples the model was trained on. Therefore, given a key kithat corresponds to the i-th hidden dimension of the-th feed-forward layer, we compute the memory coefficient ReLU(xj· ki) for every prefix x1, . . . , xj of every sentence from the WikiText103’s training set.3” (Geva 等, 2021, p. 3) 与key向量相乘后得到的是一个数值，即memory coefficient，用前缀中最后一个token的输入向量乘以神经元对应的模式检测器ki，即得到这个前缀相对于这个输入模式的匹配程度。 “Then, we retrieve the top-t trigger examples, that is, the t prefixes whose representation at layer yielded the highest inner product with ki.” (Geva 等, 2021, p. 3) 对于每一层的每一个ki都找到前t个memory coefficient最大的句子（前缀） “For every layer and memory dimension i, we compare the top-ranked token according to v i , (argmax(pi)) to the next token w i in the top1 trigger example according to ki(the example whose memory coefficient for ki is the highest).” (Geva 等, 2021, p. 4) 即对应vi得到的概率分布的对应token与前面ki中得到的最高memory coeffcient的句子的下一个token进行对比。 “Next, we take the next token of ki’s top-1 trigger example (w i ), and find where it ranks in the value vector’s distribution pi. Figure 5 shows that the rank of the next token of a trigger example increases through the layers, meaning that w i tends to get higher probability in the upper layers.” (Geva 等, 2021, p. 5) 对于ki得分最高的句子，其下一个token即要预测的token在ki对应的vi的概率分布中的位置（rank）。 “Here, the validation set is used (rather than the training set used to find trigger examples) since we are trying to characterize the model’s behavior at inference time, not find the examples it “memorizes” during training.” (Geva 等, 2021, p. 6) 🔤为什么用验证集的原因🔤 “While there are cases where a single memory cell dominates the output of a layer, the majority of outputs are clearly compositional. We count the number of instances where the feed-forward layer’s top prediction is different from all of the memories’ top predictions.” (Geva 等, 2021, p. 6) 意思为原始vi得到的概率分布对应的token与和Key计算加权后的yi得到的概率分布对应的token是否一致。这里的token都是预测的前缀的下一个token。 “The fraction of examples in a random sample of 4,000 examples where the layer’s prediction is different from the prediction of all of its memories.” (Geva 等, 2021, p. 7) 图8表明了v的混合相应输出与原始v完全不同，而且相同的例子都是一些停用词。因此混合相应输出非常有意义。 “Figure 9 shows that roughly a third of the model’s predictions are determined in the bottom few layers. This number grows rapidly from layer 10 onwards, implying that the majority of “hard” decisions occur before the final layer.” (Geva 等, 2021, p. 7) 即某一层得到的最终输出等于经过整个模型得到的最终输出。 ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:3:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","文献和源码阅读"],"content":"🚩 研究结论 前馈层的作用 作者提出前馈层模拟键值记忆，并展示了实验结果，表明键与可解释的输入模式相关联，值在模型上层诱导与下一个标记分布相关的输出词汇表分布。 研究意义 这些发现为理解 Transformer 语言模型的工作原理提供了新的视角，并为现代 NLP 模型的研究开辟了新的研究方向。 ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:4:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","文献和源码阅读"],"content":"📌 感想 \u0026 疑问 该文从kv角度解读了transformer中前馈层的作用，很具有启发性，并得出了深层学习句子的高级特征，浅层学习句子的表面特征(即句子以某个word为结尾)的结论。 ","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:5:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","算法题"],"content":" Problem: ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:0:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"思路 注意拓扑排序最好是邻接表（哈系表实现），并用队列处理后续入度为0的点 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:1:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"解题方法 描述你的解题方法 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:2:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:3:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"Code class Solution: def canFinish(self, numCourses: int, prerequisites: List[List[int]]) -\u003e bool: if not prerequisites: return True def get_zero(numCourses, prerequisites, temp): queue = [] for prerequisite in prerequisites: temp[prerequisite[0]] += 1 for i in range(len(temp)): if temp[i] == 0: queue.append(i) return queue temp = [0 for _ in range(numCourses)] queue = get_zero(numCourses, prerequisites, temp) if queue == []: return False from collections import defaultdict d = defaultdict(list) for prerequisite in prerequisites: d[prerequisite[1]].append(prerequisite[0]) while queue: idx = queue.pop(0) nexs = d[idx] if nexs: for nex in nexs: temp[nex] -= 1 if temp[nex] == 0: queue.append(nex) numCourses -= 1 return numCourses == 0 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:4:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":[""],"content":" Problem: ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:0:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"思路 讲述看到这一题的思路 ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:1:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"解题方法 描述你的解题方法 ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:2:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:3:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"Code ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:4:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":["","贪心","算法题"],"content":"跳跃游戏 Problem: ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:0:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"思路 讲述看到这一题的思路 ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:1:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"解题方法 描述你的解题方法 ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:2:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:3:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"Code ```Python3 [] # 跳跃游戏ii # 划分字母区间 [763. 划分字母区间 - 力扣（LeetCode）](https://leetcode.cn/problems/partition-labels/description/?envType=study-plan-v2\u0026envId=top-100-liked) 本题预处理完毕后思路和跳跃游戏2类似，当然也可以使用合并区间的思路来，都是贪心算法。 ## Code ```python class Solution: def partitionLabels(self, s: str) -\u003e List[int]: from collections import defaultdict d = defaultdict(list) for i, char in enumerate(s): d[char].append(i) # 也可以考虑合并区间做了，下面的解法类似跳跃游戏2 res = [] start = 0 max_jump = 0 for i, char in enumerate(s): max_jump = max(max_jump, d[char][-1]) if i == max_jump: res.append(i - start + 1) start = i + 1 max_jump = 0 return res ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:4:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","算法题"],"content":"41. 缺失的第一个正数 - 力扣（LeetCode） 空间复杂度o(n)很好想，但o(1)不好想，还是个408考研真题 注意O(n) == O(2n)，即相较于边遍历边判断，还是遍历两次更加方便且不会有太多损失。类似思想：73. 矩阵置零 - 力扣（LeetCode） class Solution: def firstMissingPositive(self, nums: List[int]) -\u003e int: # 将原数组当作哈希表使用。 for i, num in enumerate(nums): if num \u003c= 0: # 先把小于0的先一步处理，以便于使用标记 nums[i] = len(nums) + 1 for i, num in enumerate(nums): num = abs(num) if num \u003e= 1 and num \u003c= len(nums): if nums[num-1] \u003e 0: # 不重复添加 nums[num-1] = -nums[num-1] res = len(nums) + 1 for i, num in enumerate(nums): if num \u003e 0: res = i + 1 break return res ","date":"2024-03-16","objectID":"/%E7%BC%BA%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%95%B0/:0:0","tags":["算法题"],"title":"缺失的第一个正数","uri":"/%E7%BC%BA%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%95%B0/"},{"categories":["","NLP","LLM"],"content":"ICL即In-contexting Learning。 ICL 包含三种分类： - Few-shot learning，允许输入数条示例和一则任务说明； - One-shot learning，只允许输入一条示例和一则任务说明； - Zero-shot learning，不允许输入任何示例，只允许输入一则任务说明。 训练时 推理时 与微调不同，使用ICL推理时并不更新参数。可以简单理解为带有Tuning的都会更新参数，因为叫“微调”，而ICL并不更新参数，但训练时的ICL会更新参数（将ICL模板作为语料），这显而易见。 参考 In-context Learning学习笔记 - 知乎 (zhihu.com) ","date":"2024-03-14","objectID":"/icl/:0:0","tags":["NLP","LLM","ICL"],"title":"ICL","uri":"/icl/"},{"categories":["","算法题"],"content":"题目地址 # 思路 通过前缀和+哈希表，并有简单的数学变换。前缀和即 \\(y[i]=y[i-1]+x[i]\\) 类比于accumlate函数，注意前缀和思想也可以应用为“前缀积、后缀和、后缀积”等思想。238. 除自身以外数组的乘积 - 力扣（LeetCode） \u003e 使用前缀和的方法可以解决这个问题，因为我们需要找到和为k的连续子数组的个数。通过计算前缀和，我们可以将问题转化为求解两个前缀和之差等于k的情况。 \u003e假设数组的前缀和数组为prefixSum，其中prefixSum[i]表示从数组起始位置到第i个位置的元素之和。那么对于任意的两个下标i和j（i \u003c j），如果prefixSum[j] - prefixSum[i] = k，即从第i个位置到第j个位置的元素之和等于k，那么说明从第i+1个位置到第j个位置的连续子数组的和为k。 通过遍历数组，计算每个位置的前缀和，并使用一个哈希表来存储每个前缀和出现的次数。在遍历的过程中，我们检查是否存在prefixSum[j] - k的前缀和，如果存在，说明从某个位置到当前位置的连续子数组的和为k，我们将对应的次数累加到结果中。 这样，通过遍历一次数组，我们可以统计出和为k的连续子数组的个数，并且时间复杂度为O(n)，其中n为数组的长度。 # 代码 class Solution: def subarraySum(self, nums: List[int], k: int) -\u003e int: from collections import defaultdict d = defaultdict(int) d[0] = 1 prefix = 0 res = 0 for num in nums: prefix += num temp = prefix - k if temp in d: res += d[temp] d[prefix] += 1 return res ","date":"2024-03-14","objectID":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:0:0","tags":["算法题"],"title":"和为K的子数组","uri":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["","算法题"],"content":"题目地址 # 思路 通过前缀和+哈希表，并有简单的数学变换。前缀和即 \\(y[i]=y[i-1]+x[i]\\) 类比于accumlate函数，注意前缀和思想也可以应用为“前缀积、后缀和、后缀积”等思想。238. 除自身以外数组的乘积 - 力扣（LeetCode） \u003e 使用前缀和的方法可以解决这个问题，因为我们需要找到和为k的连续子数组的个数。通过计算前缀和，我们可以将问题转化为求解两个前缀和之差等于k的情况。 \u003e假设数组的前缀和数组为prefixSum，其中prefixSum[i]表示从数组起始位置到第i个位置的元素之和。那么对于任意的两个下标i和j（i \u003c j），如果prefixSum[j] - prefixSum[i] = k，即从第i个位置到第j个位置的元素之和等于k，那么说明从第i+1个位置到第j个位置的连续子数组的和为k。 通过遍历数组，计算每个位置的前缀和，并使用一个哈希表来存储每个前缀和出现的次数。在遍历的过程中，我们检查是否存在prefixSum[j] - k的前缀和，如果存在，说明从某个位置到当前位置的连续子数组的和为k，我们将对应的次数累加到结果中。 这样，通过遍历一次数组，我们可以统计出和为k的连续子数组的个数，并且时间复杂度为O(n)，其中n为数组的长度。 # 代码 class Solution: def subarraySum(self, nums: List[int], k: int) -\u003e int: from collections import defaultdict d = defaultdict(int) d[0] = 1 prefix = 0 res = 0 for num in nums: prefix += num temp = prefix - k if temp in d: res += d[temp] d[prefix] += 1 return res ","date":"2024-03-14","objectID":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84-1/:0:0","tags":["算法题"],"title":"和为K的子数组","uri":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84-1/"},{"categories":["","算法题"],"content":"def find(x): if (p[x] != x): p[x] = find(p[x]) return p[x] 上面是y总的模板，实现了路径压缩。 ","date":"2024-03-13","objectID":"/%E5%B9%B6%E6%9F%A5%E9%9B%86/:0:0","tags":["算法题"],"title":"并查集","uri":"/%E5%B9%B6%E6%9F%A5%E9%9B%86/"},{"categories":["","others"],"content":" note abstract, summary, tldr info, todo tip, hint, important success, check, done question, help, faq warning, caution, attention failure, fail, missing danger, error bug example quote, cite 与本主题的shortcode类似 类型有：note、abstract、info、tip、success、question、warning、failure、danger、bug、example、quote。 技巧 一个 技巧 横幅 note [!note] 这是一个note abstract [!abstract] 这是一个abstract tip [!tip] 这是一个tip success [!success] 这是一个success question [!question] 这是一个question warning [!warning] this is a warning failure [!failure] this is a failure danger [!danger ] this is a danger bug [!bug] this is a bug example [!example] this is a example quote [!quote] this is a quote ","date":"2024-03-09","objectID":"/obsidian%E4%B8%AD%E7%9A%84callout/:0:0","tags":["others"],"title":"obsidian中的callout","uri":"/obsidian%E4%B8%AD%E7%9A%84callout/"},{"categories":["others"],"content":" 指定光标位置：$x 多光标编辑：$x $x 指定 placeholder 文本：${x:placeholder} 指定多选值：${x|aaa,bbb|} 取变量：$VariableName 对变量做转换：${VariableName/正则/替换的文本/i} \"当前文件： $TM_FILENAME\", \"当前日期： $CURRENT_YEAR/$CURRENT_MONTH/$CURRENT_DATE\" 参考 一个案例学会 VSCode Snippets，极大提高开发效率 - 知乎 (zhihu.com) ","date":"2024-03-03","objectID":"/vscode-snippets/:0:0","tags":["others"],"title":"vscode-snippets","uri":"/vscode-snippets/"},{"categories":["信息检索","文本匹配"],"content":"无监督 ","date":"2024-01-16","objectID":"/simcse/:0:0","tags":["信息检索","文本匹配"],"title":"SimCSE","uri":"/simcse/"},{"categories":["信息检索","文本匹配"],"content":"info Noise Contrastive Estimation loss 有监督 复现代码 只贴最核心的损失函数代码 def simcse_unsup_loss(y_pred, device, temp=0.05): \"\"\"无监督的损失函数 y_pred (tensor): bert的输出, [batch_size * 2, 768] ,2为句子个数，即一个句子对 \"\"\" # 得到y_pred对应的label, [1, 0, 3, 2, ..., batch_size-1, batch_size-2] y_true = torch.arange(y_pred.shape[0], device=device) y_true = (y_true - y_true % 2 * 2) + 1 # batch内两两计算相似度, 得到相似度矩阵(batch_size*batch_size) sim = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=-1) print(sim) print(sim.shape) # 将相似度矩阵对角线置为很小的值, 消除自身的影响 sim = sim - torch.eye(y_pred.shape[0], device=device) * 1e12 # 相似度矩阵除以温度系数 sim = sim / temp # 计算相似度矩阵与y_true的交叉熵损失 # 计算交叉熵，每个case都会计算与其他case的相似度得分，得到一个得分向量，目的是使得该得分向量中正样本的得分最高，负样本的得分最低 loss = F.cross_entropy(sim, y_true) return torch.mean(loss) \"\"\" 苏神keras源码 def simcse_loss(y_true, y_pred): idxs = K.arange(0, K.shape(y_pred)[0]) #生成batch内句子的编码 [0,1,2,3,4,5]为例子 idxs_1 = idxs[None, :] # 给idxs添加一个维度，变成： [[0,1,2,3,4,5]] idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None] # 这个意思就是说，如果一个句子id为奇数，那么和它同义的句子的id就是它的上一句，如果一个句子id为偶数，那么和它同义的句子的id就是它的下一句。 [:, None] 是在列上添加一个维度。初步生成了label。[[1], [0], [3], [2], [5], [4]] y_true = K.equal(idxs_1, idxs_2) # equal会让idxs1和idxs2都映射到6*6,idxs1垂直，idxs2水平 y_true = K.cast(y_true, K.floatx()) # 生成label y_pred = K.l2_normalize(y_pred, axis=1) # 对句向量各个维度做了一个L2正则，使其变得各项同性，避免下面计算相似度时，某一个维度影响力过大。 similarities = K.dot(y_pred, K.transpose(y_pred)) # 计算batch内每句话和其他句子的内积相似度。 similarities = similarities - tf.eye(K.shape(y_pred)[0]) * 1e12 # 将和自身的相似度变为0(后面的softmax之后)。 similarities = similarities * 20 # 将所有相似度乘以20，这个目的是想计算softmax概率时，更加有区分度。 loss = K.categorical_crossentropy(y_true, similarities, from_logits=True) return K.mean(loss) \"\"\" def simcse_sup_loss(y_pred, device, lamda=0.05): \"\"\" 有监督损失函数 \"\"\" similarities = F.cosine_similarity(y_pred.unsqueeze(0), y_pred.unsqueeze(1), dim=2) row = torch.arange(0, y_pred.shape[0], 3) col = torch.arange(0, y_pred.shape[0]) col = col[col % 3 != 0] similarities = similarities[row, :] similarities = similarities[:, col] similarities = similarities / lamda y_true = torch.arange(0, len(col), 2, device=device) loss = F.cross_entropy(similarities, y_true) return loss 参考文献 SIMCSE算法源码分析 - 知乎 (zhihu.com) SimCSE论文及源码解读 | Swift’s Blog (transformerswsz.github.io) ","date":"2024-01-16","objectID":"/simcse/:1:0","tags":["信息检索","文本匹配"],"title":"SimCSE","uri":"/simcse/"},{"categories":["库学习","transformers"],"content":"基本用法 下面是使用的一个例子，重点是TrainingArg和data_collator。 dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path='./text.txt', block_size=512) data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm=True, mlm_probability=0.15 ) training_args = TrainingArguments( output_dir='./outputs/', overwrite_output_dir=True, num_train_epochs=100, per_device_train_batch_size=16, save_steps=5000, ) trainer = Trainer( model=model, args=training_args, data_collator=data_collator, train_dataset=dataset, ) trainer.train() trainer.save_model('./outputs/') 的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其它的附图。 [0089] 图1为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的整体流程图； [0090] 图2为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的Decoder-only模型架构示意图； [0091] 图3为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的利用现有大语言模型训练流程图； [0092] 图4为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的预训练大语言模型流程图； [0093] 图5为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的推理流程图； [0094] 图6为本发明第一个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的虚拟字符检索流程图； [0095] 图7为本发明第二个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的各个模型的推理性能对比图； [0096] 图8为本发明第二个实施例提供的一种基于大语言模型自身对上下文进行压缩的 方法的部分压缩示例图。 ","date":"2024-01-16","objectID":"/trainer/:0:0","tags":["库学习","Transformer","transformers"],"title":"trainer","uri":"/trainer/"},{"categories":["库学习","transformers"],"content":"具体实施方式 [0097] 为使本发明的上述目的、特征和优点能够更加明显易懂，下面结合说明书附图对本发明的具体实施方式做详细的说明，显然所描述的实施例是本发明的一部分实施例，而不是全部实施例。基于本发明中的实施例，本领域普通人员在没有做出创造性劳动前提下所获得的所有其他实施例，都应当属于本发明的保护的范围。 [0098] 实施例1 [0099] 参照图1~图6，为本发明的一个实施例，提供了一种基于大语言模型自身对上下文 进行压缩的方法，包括： [0100] Sl:获取待压缩文本，添加任务描述、分隔符和压缩槽。 [0101] 添加任务描述、分隔符和压缩槽包括，将任务描述、待压缩文本和连续掩码序列拼 接成一个新的序列 sequence: AN \\(ESTAC\\) \\(sequence=(x_{p},x_{c},x_{m})\\) d\\(x= \\frac 12\\) d\\(x= \\frac 12\\) d\\(x= \\frac 12\\) [0102] \\[x_m=[M][M]\\cdotp\\cdotp\\cdotp[M]\\] [0103] 其中，\\(x_p\\)表示任务描述，\\(x_\\mathrm{c}\\)表示待压缩文本，\\([M]\\)表示压缩槽，\\(x_m\\)表示连 续掩码序列。 [0104] 应说明的是，任务描述来源于预设的数据库。 ","date":"2024-01-16","objectID":"/trainer/:0:1","tags":["库学习","Transformer","transformers"],"title":"trainer","uri":"/trainer/"},{"categories":["训练trick"],"content":"温度超参数t，一般为softmax结果除以该参数，或者在对比学习中，相似度除以参数t。 如图： 上图为无监督simcse中的损失函数。 t越大，结果越平滑，t越小，得到的概率分布更“尖锐”。 当t趋于0时： 此时只关注最困难的负样本（smax）。 当t趋于∞时： 此时对比损失对所有负样本的权重都相同。 因此t越大，可以避免陷入局部最优解。(局部最优是过早地确定了优化的梯度方向，失去了在其他方向上探索的机会。较大的温度使得各个方向上的梯度差异没有那么明显，从而获得了在早期更多的探索机会。) [!note] 温度系数的作用是调节对困难样本的关注程度：越小的温度系数越关注于将本样本和最相似的困难样本分开，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小 考虑两个极端情况，温度系数趋向于0时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。 可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。对比损失也是这样的。这种性质更有利于形成在超球面均匀分布的特征。 语言建模 (lena-voita.github.io)中有个小游戏可以看到随着温度的变化导致概率分布的变化 参考 CVPR2021自监督学习论文: 理解对比损失的性质以及温度系数的作用 - 知乎 (zhihu.com) ","date":"2024-01-14","objectID":"/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/:0:0","tags":["训练trick","温度超参数"],"title":"温度超参数","uri":"/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/"},{"categories":["NLP"],"content":"领域自适应之继续预训练 ","date":"2024-01-12","objectID":"/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/:0:0","tags":["NLP","继续预训练"],"title":"继续预训练","uri":"/%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83/"},{"categories":["算法题"],"content":"leetcode地址：953. 验证外星语词典 - 力扣（LeetCode） ","date":"2024-01-08","objectID":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/:0:0","tags":["算法题","验证外星语词典"],"title":"验证外星语词典","uri":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/"},{"categories":["算法题"],"content":"简单方法 python列表之间也可以进行比较（太灵活了），比如[1, 2, 3] \u003c [2, 2, 3]成立，即按照字典序进行比较，与其是一样的比较规则。因此对于本题可以利用python的特性轻松解决。 好久没写python了，变得很生疏，一开始写的很蠢： class Solution: def isAlienSorted(self, words: List[str], order: str) -\u003e bool: d = dict(zip(order, range(len(order)))) words = list(map(lambda s: [d[i] for i in s], words)) print([1, 2, 3] \u003c [2, 2, 3]) return words == sorted(words) 后来想起来了sorted中还有个key参数，并且列表还有个index方法（我基本上没用过），于是改成了一行 class Solution: def isAlienSorted(self, words: List[str], order: str) -\u003e bool: return words == sorted(words, key=lambda w:[order.index(x) for x in w]) ","date":"2024-01-08","objectID":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/:1:0","tags":["算法题","验证外星语词典"],"title":"验证外星语词典","uri":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/"},{"categories":["Deep Learning","网络正则化"],"content":"L1正则化 L2正则化 权重衰减 L2正则化和权重衰减的区别 L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。 ","date":"2023-03-22","objectID":"/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/:0:0","tags":["Deep Learning","网络正则化","L1 L2正则化"],"title":"L1 L2正则化","uri":"/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/"},{"categories":["Deep Learning","网络正则化"],"content":"pre-norm Pre-norm:\\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\\) \\(先来看Pre-norm^{+},递归展开：\\) \\[X_{t+1}=X_t+F_t(Norm(X_t))\\] \\(=X_{0}+F_{1}(Norm(X_{1}))+\\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\\) 其中，展开\\(^{+}\\)后的每一项( \\(F_{1}( Norm( X_{1}) ) , \\ldots\\), \\(F_{t- 1}( Norm( X_{t- 1}) )\\), \\(F_{t}( Norm( X_{t}) )\\))之间都是同一量级的， 所以\\(F_1(Norm(X_1))+\\ldots F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\\)和 \\(F_1(Norm(X_1))+\\ldots F_{t-1}(Norm(X_{t-1}))\\)之间的区别就像t和t-1的区别一样，我们可以将 其记为\\(X_t+ 1= \\mathscr{O} ( t+ 1)\\) . 这种特性就导致当t足够大的时候，\\(X_{t+1}\\)和\\(X_t\\)之间区别可以忽略不计（直觉上），那么就有： \\[F_t(X_t)+F_{t+1}(X_{t+1})\\approx F_t(X_t)+F_{t+1}(X_t)=(F_t\\bigoplus F_{t+1})(X_t)\\] 这就是所谓的增加宽度，而没有增加深度。从而导致pre-norm的精度不高。 # post-norm Post-norm:\\(X_{t+1}=Norm(X_{t}+F_{t}(x_{t}))\\) 本来layernorm是为了缓解梯度消失，但是在post-norm这里却成为了梯度消失的罪魁祸首。也导致了收敛较难、需要大量调参。 \\[X_{t+1}=Norm(X_t+F_t(X_t))=\\frac{X_t+F_t(X_t)}{\\sqrt{2}}\\] \\[=\\frac{X_0}{\\sqrt{2}^{t+1}}+\\frac{F_0(X_0)}{\\sqrt{2}^{t+1}}+\\ldots+\\frac{F_{t-1}(X_{t-1})}{\\sqrt{2}^2}+\\frac{F_t(X_t)}{\\sqrt{2}}\\:(\\] 这个结构跟pre-norm比起来充分考虑了所有分支 (残差\\(^{+})\\) 的输出，做到了真正增加深度，自然精度会相对好一些。 不过它也有它很显然的问题，当t足够大、也就是叠加的attention层足够多以后，底层那些分支(残差)的影响力被衰减掉了，残差有利于解决梯度消失，但是在Post Norm中，残差这条通道被严重削弱了，越靠近输入，削弱得越严重，残差“名存实亡”，那么势必会有梯度消失的问题，这也就是文章开头所说的postnorm难收敛、参数难调的原因。本来我们做Norm也是为了处理梯度消失，但从分析看来，transformer结构中的layernorm\\(^{+}\\)并没有完全实现它的作用。那这就意味着transformer原始结构的失败吗？并不是的，因为这种梯度消失的问题在整个结构上来看(配合上adam系优化器和学习率warmup，warmup对于post-norm极为重要) 是并不明显的。 离输入层的残差影响力弱这一特性，也有它的用武之地，比如在finetune的时候，我们就希望不要过多调整靠近输入层的参数、以免破坏预训练的效果。 ","date":"2023-03-22","objectID":"/layer-norm/:0:0","tags":["Deep Learning","网络正则化","Layer Norm"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"warmup的重要性 Post-LN Transformer在训练的初始阶段，输出层附近的期望梯度非常大，所以，如果没有warm-up，模型优化过程就会炸裂，非常不稳定。 模型对越靠后的层越敏感，也就是越靠后的层学习得越快，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。 很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但 Adam 的更新量是常数量级的，梯度不准，但更新量依然是常数量级，意味着可能就是一个常数量级的随机噪声了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。 从上图中就可以看出来，post-ln在开始阶段层数越高梯度越大，此时需要小学习率，而当warmup完后，梯度变得很小（绿色部分）。此时可以使用大学习率。 ","date":"2023-03-22","objectID":"/layer-norm/:1:0","tags":["Deep Learning","网络正则化","Layer Norm"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"Adam如何缓解梯度消失 其实。最关键的原因是，在当前的各种自适应优化技术“下，我们已经不大担心梯度消失问题了。这是因为，当前 NLP 中主流的优化器是 Adam 及其变种。对于 Adam 来说，由于包含了动量和二阶矩校正，所以近似来看，它的更新量大致上为 \\[\\Delta\\theta=-\\eta\\frac{\\mathbb{E}_{t}[g_{t}]}{\\sqrt{\\mathbb{E}_{t}[g_{t}^{2}]}}\\] 可以看到，分子分母是都是同量纲的，因此分式结果其实就是 (1)的量级，而更新量就是 (n)量级。也就是说，理论上只要梯度的绝对值大于随机误差，那么对应的参数都会有常数量级的更新量（意思就是参数的更新量与梯度的关系不是很大，因此受梯度消失影响较小）；这跟 SGD 不一样，SGD 的更新量是正比于梯度的，只要梯度小，更新量也会很小，如果梯度 过小，那么参数几乎会没被更新。 所以，Post Norm 的残差虽然被严重削弱，但是在 base、large 级别的模型中，它还不至于削弱到小于随机误差的地步，因此配合 Adam 等优化器，它还是可以得到有效更新的，也就有可能成功训练了。当然，只是有可能，事实上越深的 Post Norm 模型确实越难训练，比如要仔细调节学习率和 Warmup 等。 # Deep-norm \\(最后再提一下DeepNet中结合Post-LN^+的良好性能以及Pre-LN的训练稳定性做出的改良\\)。 \\[X_{t+1}=Norm(\\alpha X_t+F_t(X_t))\\text{(6)}\\] \\(它在add norm之前给输入乘了一个up-scale^+的常数系数 α\u003e1\\)。 现在 (5) 的展开为： \\[X_{t+1}=\\frac{\\alpha^{t+1}X_{0}}{\\sqrt{2}^{t+1}}+\\frac{\\alpha^{t}F_{0}(X_{0})}{\\sqrt{2}^{t+1}}+\\ldots+\\frac{\\alpha F_{t-1}(X_{t-1})}{\\sqrt{2}^{2}}+\\frac{F_{t}(X_{t})}{\\sqrt{2}}\\] 因为\\(\\alpha\u003e1\\) ,所以它能够在保留post-norm真正增加了深度这优点的同时，一定程度避免了梯度 消失。（本质还是post-norm） 参考 Transformer梳理（一）：Post-Norm VS Pre-Norm - 知乎 (zhihu.com) 模型优化漫谈：BERT的初始标准差为什么是0.02？ - 科学空间|Scientific Spaces (kexue.fm) 为什么Pre Norm的效果不如Post Norm？ - 科学空间|Scientific Spaces (kexue.fm) 香侬读 | Transformer中warm-up和LayerNorm的重要性探究 - 知乎 (zhihu.com) Bert/Transformer 被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com) ","date":"2023-03-22","objectID":"/layer-norm/:2:0","tags":["Deep Learning","网络正则化","Layer Norm"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["others"],"content":"早就配置好了，但是之前使用的是tab打开pdf，感觉有点狭窄，于是换成了外部pdf，使用的就是经典的sumatra pdf，具体的配置过程可以看：https://zhuanlan.zhihu.com/p/142963562，但是在配置反向搜索的时候出现了问题，于是查看网上的一些解决方法，一番折腾下终于解决了这个问题，下面是最终方案，记录一下。 首先是使用code.cmd，参考了这个文章：https://zhuanlan.zhihu.com/p/112108701不过直接使用的一个问题就是反向搜索时会弹出cmd窗口，比较烦人，在评论区有人写了脚本解决了这个问题，现在就可以比较完美的进行反向搜索了https://blog.csdn.net/he_yang_/article/details/129210746?spm=1001.2014.3001.5501 ","date":"2023-03-21","objectID":"/vscode%E9%85%8D%E7%BD%AElatex/:0:0","tags":["others","vscode配置latex"],"title":"vscode配置latex","uri":"/vscode%E9%85%8D%E7%BD%AElatex/"},{"categories":["Deep Learning","损失函数"],"content":"在机器学习中，hinge loss是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为： 其中 \\(\\hat{y}\\) 表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。）， \\(y\\) 表示正确的类别。 - 如果 \\(\\hat{y}y\u003c1\\) ，则损失为： \\(1-\\hat{y}y\\) - 如果\\(\\hat{y}y\u003e1\\) ，则损失为：0 ","date":"2023-03-13","objectID":"/hinge-loss/:0:0","tags":["Deep Learning","损失函数","hinge loss"],"title":"hinge loss","uri":"/hinge-loss/"},{"categories":["面经"],"content":"前几天试着投了简历，没想到有两家约了面试，一个是得物一个是北京百分点，得物面试没有怎么准备，太仓促了，二面挂了，百分点拿到了offer，但决定考研了就没去，记录一下面试的问题。岗位是nlp算法岗。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:0:0","tags":["面经","北京百分点面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"一面：技术面 先自我介绍，然后介绍了一下项目。根据印象提问有以下内容： 有没有数据不平衡问题，是怎么解决的？欠采样，过采样，focal loss，着重介绍了一下focal loss的参数含义。 对分类问题最后一层要怎么做：max、mean，全连接 有没有尝试过别的模型：没有，然后面试官说其实textcnn和fasttext效果不一定比bert效果差。 对传统的神经网络了解吗？了解。介绍一下lstm。按照三个门分析就行，哪个门是什么作用，注意一下激活函数的不同。 介绍一下bert，从输入开始介绍就行，把两个任务也展开说一下。 介绍一下tfidf，这个很简单，说一下tf和idf的含义和公式就行。 数据增强怎么做的？同义词替换、回译。 介绍一下提示学习。说一下主要思想，还有离散prompts和连续prompts。 大体就记得问了这些，总体来说不是很难，而且大多数都是神经网络相关的，准备了一些传统的机器学习方法也没有用上。 然后是两道算法题： 第一题：已知一随机发生器，产生0的概率是p，产生1的概率是1-p， 现在要你构造一个发生器， 使得它构造0和1的概率均为 1/2； def res(): a = random() b = random() if a == 0 and b == 1: return 0 if a == 1 and b == 0: return 1 else: return res() 由于需要产生1/2，而用1位0，或1位1无法产生等概率， 因此，考虑将随机数扩展成2位： 00 pp 01 p(1-p) 10 (1-p)p 11 (1-p)(1-p) 有上述分析知道，01和10是等概率的，因此我们只需要产生01和10就行了。 于是可以，遇到00和11就丢弃，只记录01和10。可以令，01表示0,10表示1，则等概率1/2产生0和1了。 当时面试官问了这个问题的时候有点懵，因为之前从来没有做过，但是经过面试官的提示也是写出来了。 第二题是LCS问题，就是最长公共子序列问题，当时想到了使用动态规划，但是懒得在纸上推转移方程了，就直接暴力解决了。 def LCS(s1, s2): dp = [[0 for _ in range(len(s1))] for _ in range(len(s2))] res = 0 for i in range(len(s1)): for j in range(i, len(s2)): if s1[i:j+1] in s2: res = max(res, j+1-i) return res print(LCS(\"baced\", \"acefg\")) ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:1:0","tags":["面经","北京百分点面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"二面综合面 上来也是自我介绍加项目介绍，有些问题和一面重复了，就不赘述。 介绍几种bert的变体，albert、roberta，就介绍了这两种。 bert的mask策略。 具体的也忘了，当时也没有想直接记录下来，后面就等到了口头offer，过几天也发了正式offer，但是因为要考研等一些原因就拒了，等考完研再找吧。这次只在软件上投了投简历，没有内推等，还是挺有收获的。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:2:0","tags":["面经","北京百分点面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"总结 这几轮面试还是很有收获的，也算是模拟了以后的考研复试，虽然以后大概率是线下复试了。还有就是简历上最最重要的就是实习经历，我之前也没有实习经历，因此没有实习经历的情况下最重要的就是项目，别的基本上都没有问，不过还是要保证你写上去的都是真的会的。因此重要的就是实习和项目，获奖经历可能只是给面试的机会更大一些。这次拒绝offer少了一次实习经历还算有点可惜，不过相对而言还是考研重要一些。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:3:0","tags":["面经","北京百分点面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"KMP是字符串匹配问题的算法。“字符串A是否为字符串B的子串?如果是的话出现在B的哪些位置?”该问题就是字符串匹配问题，字符串A称为模式串，字符串B称为主串。 ## BF算法 BF算法就是暴力匹配，即对主串从头开始慢慢移动模式串，直到找到相匹配的位置。 代码很简单暴力： 假设n为主串长度，m为模式串长度。 每一轮字符串比较：最差的情况为模式串最后一个字与主串不同其他都相同（如模式串为AAB，主串对应部分为AAC），必须走完整个字符串才能得出结果，因此复杂度为O(m)。 所有轮字符串比较：最差的情况是移动到最后一次比较才寻找得到，总共需要n-m+1次，主串通常比模式串长很多，故Brute-Force时间复杂度为O(nm) 在匹配上没有办法进行优化，因此可以从模式串的移动上入手，由此引入了Kmp算法。 ","date":"2023-03-08","objectID":"/kmp/:0:0","tags":["面经","KMP"],"title":"KMP","uri":"/kmp/"},{"categories":["面经"],"content":"KMP KMP 算法的不同之处在于，它会花费空间来记录一些信息。目的就是为了减少匹配的趟数，算法的核心就是每次匹配过程中推断出后续完全不可能匹配成功的匹配过程，从而减少比较的趟数。 ","date":"2023-03-08","objectID":"/kmp/:1:0","tags":["面经","KMP"],"title":"KMP","uri":"/kmp/"},{"categories":["面经"],"content":"Next数组 next数组实质上就是找出模式串中前后字符重复出现的个数，为了能够跳跃不可能匹配的步骤。 next数组的定义为：next[i]表示模式串A[0]至A[i]这个字串，使得前k个字符等于后k个字符的最大值，特别的k不能取i+i,因为字串一共才i+1个字符，自己跟自己相等毫无意义。 如何确定在移动过程中需要跳过多少步呢？下图更直观的体现了跳跃的过程： 也就是跳到模式串中的后缀相同字符串开始。因为这时可以确定前面的字符串肯定无法匹配了，过的趟数=匹配上字符串中间字符长度-重复字符串长度 在实际代码编写中，移动的实际上是模式串的匹配位置。前面展示的只是理解减小匹配次数这一过程。实际上跳动就是模式串比较指针的移动。模式串向右移动也就是比较指针向左移动，移动的距离就是跳过的趟数 移动后的指针为：\\(j=j-(j-next[j-1]) =next[j-1]\\) 其中原来的j为匹配成功的字符串长度，也就是匹配失败的指针位置。next[j-1]就是匹配成功的字符串的最长相同前后缀数，也就是要跳转到的指针的下标。（下标从0开始的，如果下标从1开始则在原来基础上+1就是要跳转到的位置，这也是为什么书上要+1）。 即下图所示。 然后再用移动后的指针和主串的上一轮匹配错误的位置相比，这时已经保证了前面的字符已经匹配了，即当前指针前面的子串已经和上一轮匹配成功的子串的最长后缀相同了。 因此可以看到，Kmp算法的核心就是构造Next数组。 最简单的方法就是根据定义暴力构造时间复杂度为O(m2法)：： 第二种构建方案，是一种递推的方式进行构建，时间复杂度为O(n+m): 考虑：如果next[0], next[1], … next[x-1]均已知，那么如何求出 next[x] ？我们已经知道next[x-1],标记next[x-1]=temp,则可以讨论A[temp]和A[x]的值，分2种情况讨论： 第一种情况：A[temp]等于A[x]，也就是说在前一个next结果上又多了一个字符串相同的长度，因此next[x]为next[x-1]+1 这种情况说明了x-1时最长前缀的后一位和x位置的字符相同，则说明了前缀和后缀都增加了相同的1位，next[x] = next[x-1] + 1。 （这里给一下我自己的理解，因为next中保存的是前后缀最大相同的长度，因此通常代表着最大相同前缀的后一位） 第二种情况：当A[temp]和A[x]不相等的时候，我们需要缩小temp,把temp变成next[temp-1]，直到A[temp]=A[x]为止。A[now]=A[x]时，就可以直接向右扩展了。 如何理解这张图，当发现A[temp] != A[x]的时候，就一直缩小temp，也就是temp = next[temp-1]，也就是找原来的最长前缀中对应的next[temp-1]，因为前缀中的相同前后缀最大长度（图中为2）肯定也与后缀中的相同前后缀最大长度（图中为2）相同，也说明了前缀中的前缀（图中为0和1上的A和B）等于后缀中的后缀（图中为10和11上的A和B），此时还是原来的思路temp为前缀中的前缀的下一位，判断是否与后缀中的后缀的下一位相同（就是当前的A[x]），如果相同就是第一种情况，next[x] = temp + 1，如果一直没找到则设为0，说明没有。 学到这不得不感叹kmp三位大佬的恐怖。。 大体的思想就是这样，不过如果对于做题来说的话，有很多不一样的地方，比如王道书上的next数组是从下标1开始的，对于本文的内容，需要得到匹配失败位置-1的next数组值，因此书上将整体右移了一位，开头补上了-1，这样匹配失败的位置就是对应的next数组值。又下一次匹配要从最长前缀的下一位开始，而对于下标从0开始，next数组值就是下一位（前面解释了），而对于下标从1开始，需要再+1才是对应下标，因此也对next数组最后进行了+1操作。不管怎么样，next的含义都是最长前缀的下一个位置。主要思想都是一样的，随机应变即可。 ","date":"2023-03-08","objectID":"/kmp/:1:1","tags":["面经","KMP"],"title":"KMP","uri":"/kmp/"},{"categories":["面经"],"content":"代码： class KMP: def __init__(self, text, pattern) -\u003e None: self.text = text self.pattern = pattern self.next = [] def init_next(self): self.next.append(0) # first for i, s in enumerate(self.pattern[1:], 1): tmp = self.next[i-1] if s == self.pattern[tmp]: self.next.append(tmp + 1) else: while self.pattern[tmp] != s and tmp != 0: tmp = self.next[tmp-1] if tmp == 0 and self.pattern[tmp] != s: self.next.append(0) continue self.next.append(tmp + 1) def search(self): i = 0 # text index j = 0 # partten index while i \u003c len(self.text): if self.text[i] == self.pattern[j]: i += 1 j += 1 elif j: j = self.next[j-1] else: i += 1 if j == len(self.pattern): return i - j return -1 kmp = KMP(\"hello\", \"ll\") kmp.init_next() print(kmp.search()) print(kmp.next) ","date":"2023-03-08","objectID":"/kmp/:1:2","tags":["面经","KMP"],"title":"KMP","uri":"/kmp/"},{"categories":["面经"],"content":"nextval 还有一种改进的next，因为匹配错误的位置重新移动指针后的位置的值可能与原来的值相同，因此在构建next数组的时候可以直接跳转到不相同的位置，这样就减小了重复的比较。 比如aaabaaaab和aaaab，当在主串中第一个b位置出错时，此时模式串为a，会比较str[next[2]]与b，但此时str[next[2]]也是a，再次比较也会错误，但再移动后发现还是a还是错误，这些比较都是没有意义的，最后要移动到模式串开头，然后将主串指针+1再比较，所以构建nextval减少了这些比较。 我是根据已经构建好的next数组推理来的。代码如下： def create_nextval(self): length = len(self.pattern) self.nextval = [0] * length for i in range(1, len(self.next)): tmp = self.next[i-1] if self.pattern[tmp] != self.pattern[i]: self.nextval[i-1] = tmp else: while tmp and self.pattern[tmp] == self.pattern[i]: tmp = self.next[tmp-1] self.nextval[i-1] = tmp 因为用不到最后一位的数值，所以考研书上就直接全部右移一位，理论上都是相同的。原理就是匹配错误的位置的字符和要跳转到的位置的字符相同的话就要继续跳转直到等于0或者不相同为止。 ","date":"2023-03-08","objectID":"/kmp/:1:3","tags":["面经","KMP"],"title":"KMP","uri":"/kmp/"},{"categories":["Machine Learning"],"content":"特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。 根据特征选择的形式，可分为三大类： - Filter(过滤法)：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选 - Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征 - Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的） ## 过滤式 基本想法是：分别对每个特征 \\(x_i\\) ，计算 \\(x_i\\) 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征\\(x_i\\) 。 - Pearson相关系数 - 卡方验证 - 互信息和最大信息系数 - 距离相关系数 - 方差选择法 ### ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:0:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Machine Learning"],"content":"包裹式 ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:1:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Machine Learning"],"content":"嵌入式 ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:2:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Deep Learning","训练trick"],"content":"介绍 早停止（Early Stopping）是 当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段。 如何判断已经收敛？主要看以下几点： - 验证集上的Loss在模型多次迭代后，没有下降 - 验证集上的Loss开始上升。 这时就可以认为模型没有必要训练了，可以停止了，因为训练下去可能就会发生过拟合，所以早停法是一种防止模型过拟合的方法。 ","date":"2023-03-06","objectID":"/early-stopping/:1:0","tags":["Deep Learning","训练trick","early-stopping"],"title":"early-stopping","uri":"/early-stopping/"},{"categories":["Deep Learning","训练trick"],"content":"代码 import numpy as np import torch import os class EarlyStopping: \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\" def __init__(self, save_path, patience=7, verbose=False, delta=0): \"\"\" Args: save_path : 模型保存文件夹 patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 \"\"\" self.save_path = save_path self.patience = patience self.verbose = verbose self.counter = 0 self.best_score = None self.early_stop = False self.val_loss_min = np.Inf self.delta = delta def __call__(self, val_loss, model): score = -val_loss if self.best_score is None: self.best_score = score self.save_checkpoint(val_loss, model) elif score \u003c self.best_score + self.delta: self.counter += 1 print(f'EarlyStopping counter: {self.counter} out of {self.patience}') if self.counter \u003e= self.patience: self.early_stop = True else: self.best_score = score self.save_checkpoint(val_loss, model) self.counter = 0 def save_checkpoint(self, val_loss, model): '''Saves model when validation loss decrease.''' if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f} --\u003e {val_loss:.6f}). Saving model ...') path = os.path.join(self.save_path, 'best_network.pth') torch.save(model.state_dict(), path) # 这里会存储迄今最优模型的参数 self.val_loss_min = val_loss ","date":"2023-03-06","objectID":"/early-stopping/:2:0","tags":["Deep Learning","训练trick","early-stopping"],"title":"early-stopping","uri":"/early-stopping/"},{"categories":["Deep Learning","损失函数"],"content":"Focal Loss Focal Loss主要是为了解决类别不平衡的问题，Focal Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例： ","date":"2023-03-06","objectID":"/focal-loss/:0:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"原始Loss 原始的二分类： 其中 所以： 很容易理解，因为CE就是softmax在二分类的形式，实际运算中只关注对应标签的概率，对于二分类，如果是负样本的话，预测概率小于0.5则说明预测正确，则对应的实际的概率应该为1-p。最大化概率，就是最大化Log概率，也就是最小化-log概率。 ","date":"2023-03-06","objectID":"/focal-loss/:0:1","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"什么是易分类样本 对于正样本，如果预测的结果总是在0.5以上，就是易分类样本，如果总是在0.5以下，则说明是难分类样本。 对于负样本，如果预测的结果总是在0.5以下，就是易分类样本，如果总是在0.5以上，则说明是难分类样本。 对应\\(p_t\\)来说，就是\\(p_t\u003e0.5\\)为易分类，\\(p_t\u003c0.5\\)为难分类。 ","date":"2023-03-06","objectID":"/focal-loss/:0:2","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"gamma参数 在模型训练的时候，我们更希望关注难分类的样本，因此focal loss在原始loss上增加了一项，对整体进行了衰减： 对于公式中的参数\\(\\gamma\\)，一般会选择2，对于易分类的样本，即\\(p_t\u003e0.5\\)的样本，\\(1-p_t\\)则会小于0.5，则loss会衰减的更多，最终的损失就变的很小。而对于难分类的样本，loss会衰减的比较小，通过这种衰减的对比，则变相增加了模型对于难分类样本的权重。 ","date":"2023-03-06","objectID":"/focal-loss/:0:3","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"alpha参数 对于二分类任务，负样本的数量远远多于正样本，导致模型更多关注在负样本上，忽略正样本。因此在使用交叉熵损失的时候通常会增加一个平衡参数用来调节正负样本的比重。 所以会增加一个平衡参数来调节正负样本的比重。 其实这就是balanced cross entropy，可以将它引入focal loss 在式子中，\\(\\gamma\\)占据了主导地位，因此其实不用太在意\\(\\alpha\\)的数值。 ","date":"2023-03-06","objectID":"/focal-loss/:0:4","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"对于多分类 对于多分类任务，其实是一样的，因为如果一个类别的样本预测结果总是大于0.5，也说明它是易分类的，对于平衡因子，在实现的时候，可以提前设置好各类别的平衡因子，对于每一个类别都有一个对应的。 ","date":"2023-03-06","objectID":"/focal-loss/:1:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"为什么有效 focal loss从样本难易分类角度出发，解决样本非平衡带来的模型训练问题。 直觉上来讲样本非平衡造成的问题就是样本数少的类别分类难度较高。因此从样本难易分类角度出发，使得loss聚焦于难分样本，解决了样本少的类别分类准确率不高的问题，当然难分样本不限于样本少的类别，也就是focal loss不仅仅解决了样本非平衡的问题，同样有助于模型的整体性能提高。 ","date":"2023-03-06","objectID":"/focal-loss/:2:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"思考 难分类样本与易分类样本其实是一个动态概念，也就是说 p 会随着训练过程而变化。原先易分类样本即 p大的样本，可能随着训练过程变化为难训练样本即p小的样本。 上面讲到，由于Loss梯度中，难训练样本起主导作用，即参数的变化主要是朝着优化难训练样本的方向改变。当参数变化后，可能会使原先易训练的样本 p 发生变化，即可能变为难训练样本。当这种情况发生时，可能会造成模型收敛速度慢，正如苏剑林在他的文章中提到的那样。 为了防止难易样本的频繁变化，应当选取小的学习率。 ## 代码 ### 二分类 class Focal_Loss(): \"\"\" 二分类Focal Loss \"\"\" def __init__(self,alpha=0.25,gamma=2): super(Focal_Loss,self).__init__() self.alpha=alpha self.gamma=gamma def forward(self,preds,labels): \"\"\" preds:sigmoid的输出结果 labels：标签 \"\"\" eps=1e-7 loss_1=-1*self.alpha*torch.pow((1-preds),self.gamma)*torch.log(preds+eps)*labels loss_0=-1*(1-self.alpha)*torch.pow(preds,self.gamma)*torch.log(1-preds+eps)*(1-labels) loss=loss_0+loss_1 return torch.mean(loss) ","date":"2023-03-06","objectID":"/focal-loss/:3:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"多分类 class Focal_Loss(): def __init__(self,weight,gamma=2): super(Focal_Loss,self).__init__() self.gamma=gamma self.weight=weight def forward(self,preds,labels): \"\"\" preds:softmax输出结果 labels:真实值 \"\"\" eps=1e-7 y_pred =preds.view((preds.size()[0],preds.size()[1],-1)) #B*C*H*W-\u003eB*C*(H*W) target=labels.view(y_pred.size()) #B*C*H*W-\u003eB*C*(H*W) ce=-1*torch.log(y_pred+eps)*target floss=torch.pow((1-y_pred),self.gamma)*ce floss=torch.mul(floss,self.weight) floss=torch.sum(floss,dim=1) return torch.mean(floss) ","date":"2023-03-06","objectID":"/focal-loss/:3:1","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"参考 https://zhuanlan.zhihu.com/p/266023273 ","date":"2023-03-06","objectID":"/focal-loss/:4:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["面经"],"content":" 数据增强，即增加样本，也可以半监督如UDA。 正则化（Dropout等） Batch norm。本质是加快训练，让训练更稳定，但也可以缓解过拟合。配合relu也会缓解dead relu问题。 early-stop，在过拟合之前停下来。 降低模型复杂度，与第2点类似。 学习率衰减，按照固定的epoch后衰减学习率。 特征选择，选择主要的特征进行训练，本质也是降低模型复杂度。 ","date":"2023-03-06","objectID":"/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/:0:0","tags":["面经","过拟合的解决方法"],"title":"过拟合的解决方法","uri":"/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"},{"categories":["Deep Learning","数据增强"],"content":"NLP中的EDA 同义词替换，回译，近音字替换，随机插入，随机交换，随机删除 ","date":"2023-03-05","objectID":"/eda/:0:0","tags":["Deep Learning","数据增强","EDA"],"title":"EDA","uri":"/eda/"},{"categories":["Deep Learning","数据增强"],"content":"同义词替换 做法可以是维护一个同义词表，如哈工大的发布的同义词词典。在每次训练的时候，样本有一定的概率对里面的词语进行替换。如”自动驾驶、医疗、能源……一季度融资最多的人工智能公司” -\u003e “自动驾车、医术、能源……一季度融资最多的人工智能公司”。根据经验，有条件的话最好用项目领域的同义词词典，如做医疗的文本，就用医疗的同义词词典，做金融领域的就用金融的同义词词典，而不是用一个通用的字典。 还有种做法是用词向量进行替换，如上面的句子中，我们对”驾驶”一次进行同义词替换，发现在词向量表中，离”驾驶”余弦距离最近的一词是”行驶”，所以就把”驾驶”替换成”行驶”，当然这样做的话需要预先训练一个词向量表，还是那句话，最好用你目前手头任务领域的文本来训练词向量。 也可以用Bert等模型来进行替换，如把上面的句子随机MASK掉一些字，变成”自[MASK]驾驶、医疗、能源……一季[MASK]融资最多的人工智[MASK]公司”，再让Bert对MASK掉的字进行预测，这样又得到新的一个样本。 ","date":"2023-03-05","objectID":"/eda/:1:0","tags":["Deep Learning","数据增强","EDA"],"title":"EDA","uri":"/eda/"},{"categories":["Deep Learning","数据增强"],"content":"回译 做法很简单，把中文翻译成英文或法文或日本，再翻译回中文，如下面的句子，先翻译成英文，再翻译回中文，“自动驾驶、医疗、能源……一季度融资最多的人工智能公司” -\u003e “第一季度融资最多的自动驾驶仪、医疗保健、能源人工智能公司”，有时回译能得到语法结构不同的句子，如被动句变成主动句，有效增强了样本的多样性。不过个人觉得，长文本并不适用于回译，想想一个500多字的长文本，经过回译后，上下文是否还通顺是个问题，当然也可以随机对长文本中的单句进行回译，而不是把整个长文本进行回译。 ","date":"2023-03-05","objectID":"/eda/:2:0","tags":["Deep Learning","数据增强","EDA"],"title":"EDA","uri":"/eda/"},{"categories":["Deep Learning","训练trick"],"content":"在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning rate，然后随着训练的批次增加，逐渐增大学习率，直到达到原本想要设置的学习率。 关于warmup的好处，有： - 有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳 - 有助于保持模型深层的稳定性。 warmup有助于网络的收敛，当达到预期的学习率时，之后的步骤就可以每固定批次进行学习率衰减，防止过拟合，以慢慢达到收敛。 ","date":"2023-03-05","objectID":"/warmup/:0:0","tags":["Deep Learning","训练trick","warmup"],"title":"warmup","uri":"/warmup/"},{"categories":["Deep Learning","训练trick"],"content":"神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为onehot本身就是一个稀疏的向量，如果所有无关类别都为0的话，就可能会疏忽某些类别之间的联系。 具体的缺点有： - 真是标签与其它标签之间的关系被忽略了，很多有用的知识学不到了。 - 倾向于让模型更加武断，导致泛化性能差 - 面对有噪声的数据更容易收到影响。 label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 增加label smoothing后真实的概率分布有如下改变： ","date":"2023-03-05","objectID":"/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/:0:0","tags":["Deep Learning","训练trick","标签平滑"],"title":"标签平滑","uri":"/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/"},{"categories":["NLP"],"content":"参考 NLP新宠——浅谈Prompt的前世今生 - 知乎 (zhihu.com) ","date":"2023-03-02","objectID":"/prompt/:0:0","tags":["NLP","Prompt"],"title":"Prompt","uri":"/prompt/"},{"categories":["Deep Learning","训练trick"],"content":" 基本原则：快速试错。 小步试错，快速迭代 可以试试无脑的配置 实时打印一些结果 自动调参：网格搜索、random search、贝叶斯优化、 参数初始化 学习率warmup，慢慢增加，然后学习率衰减。 batch_size和lr 大的batchsize收敛到sharp minimum，而小的batchsize收敛到flat minimum，后者具有更好的泛化能力。两者的区别就在于变化的趋势，一个快一个慢，如下图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp minimum。 大的batchsize性能下降是因为训练时间不够长，本质上并不少batchsize的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。 如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。 尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。 ","date":"2023-03-02","objectID":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/:0:0","tags":["Deep Learning","训练trick","调参技巧"],"title":"调参技巧","uri":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"},{"categories":["Deep Learning","训练trick"],"content":"总结 学习率直接影响模型的收敛状态，batchsize则影响模型的泛化性能 ","date":"2023-03-02","objectID":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/:1:0","tags":["Deep Learning","训练trick","调参技巧"],"title":"调参技巧","uri":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"},{"categories":["Deep Learning","训练trick"],"content":"Min-Max公式 $$ {} {(x,y) }U[{r{adv}}L(,x+r_{adv},y)] $$ 内部max是为了找到worst-case的扰动，也就是攻击，其中， \\(L\\)为损失函数， \\(\\mathbb{S}\\) 为扰动的范围空间。 外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 \\(\\mathbb{D}\\) 是输入样本的分布。 简单理解就是在输入上进行梯度上升(增大loss)，在参数上进行梯度下降(减小loss) 加入扰动后的损失函数 $$ {} -P(y |x+r{adv};) \\[ 那扰动要如何计算呢？Goodfellow认为，**神经网络由于其线性的特点，很容易受到线性扰动的攻击。** # Fast Gradient Sign Method (FGSM) \\] r_{adv} = sgn(_{x}L(,x,y)) $$ # FGM ## 代码 代码来自[1]，注意的是一般扰动加在了embedding矩阵上，相当于x+r。 import torch class FGM(): def __init__(self, model): self.model = model self.backup = {} def attack(self, epsilon=1., emb_name='emb.'): # emb_name为embedding矩阵参数对应名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = epsilon * param.grad / norm param.data.add_(r_at) def restore(self, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {} 使用时： # 初始化 fgm = FGM(model) for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad # 对抗训练 fgm.attack() # 在embedding上添加对抗扰动 loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 fgm.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() 参考文献 【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现 - 知乎 (zhihu.com) ","date":"2023-03-02","objectID":"/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/:0:0","tags":["Deep","Learning","训练trick","对抗训练"],"title":"对抗训练","uri":"/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/"},{"categories":["Deep Learning","训练trick"],"content":"数据不均衡 所谓的不平衡指的是不同类别的样本量差异非常大，或者少数样本代表了业务的关键数据（少量样本更重要），需要对少量样本的模式有很好的学习。样本类别分布不平衡主要出现在分类相关的建模问题上。样本类别分布不平衡从数据规模上可以分为大数据分布不平衡和小数据分布不平衡两种。 大数据分布不均衡。这种情况下整体数据规模大，只是其中的少样本类的占比较少。但是从每个特征的分布来看，小样本也覆盖了大部分或全部的特征。例如拥有1000万条记录的数据集中，其中占比50万条的少数分类样本便于属于这种情况。 小数据分布不均衡。这种情况下整体数据规模小，并且占据少量样本比例的分类数量也少，这会导致特征分布的严重不平衡。例如拥有1000条数据样本的数据集中，其中占有10条样本的分类，其特征无论如何拟合也无法实现完整特征值的覆盖，此时属于严重的数据样本分布不均衡。 如果不同分类间的样本量差异达到超过10倍就需要引起警觉并考虑处理该问题，超过20倍就要一定要解决该问题。 主要有三种解决方法： - 欠采样：在少量样本数量不影响模型训练的情况下，可通过对多数类样本欠采样，实现少数样本和多数样本均衡。 - 过采样：在少量样本数量不支持模型训练的情况下，可以通过对少数类样本过采样，实现少数样本和多数样本的均衡。 - 模型算法：通过引入有权重的模型算法，针对少量样本着重拟合，以提升对少量样本特征的学习。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:1:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"欠采样 通过减少分类中多数类样本的样本数量来实现样本均衡。通过欠采样，在保留少数类样本的同时，会丢失多数样本中的一些信息。经过欠采样，样本数量在变少。因此我个人并不倾向于这种方法。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"随机法 随机的删除一些多数类样本，使少数类样本和多数类样本数量达到均衡。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"原型生成 PG 算法主要是在原有样本的基础上生成新的样本来实现样本均衡，对多数类样本生成新的样本去替代原样本，使得样本数目减少, 具体做法如下： ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"原型选择 原理：从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。 NearMiss 采用一些启发式的规则来选择样本，根据规则的不同可分为 3 类,通过设定 version 参数来确定： - NearMiss-1：选择到最近的 K 个少数类样本平均距离最近的多数类样本 - NearMiss-2：选择到最远的 K 个少数类样本平均距离最近的多数类样本 - 3: 对于每个少数类样本选择 K 个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:3","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"过采样 过采样（over-sampling）方法通过增加分类中少数的数量来实现样本均衡，最直接的方法是简单的复制少数类样本形成多条记录，这种方式可能导致样本特征少而可能出现过拟合的问题。经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或者通过一定规则产生新的合成样本。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"随机复制 就是随机选择少量样本进行复制。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"SMOTE 在随机过采样的基础上，通过样本构造一方面降低了直接复制样本代理的过拟合的风险，另一方法实现了样本的均衡。比如样本构造方法 SMOTE（Synthetic minority over-sampling technique）及其衍生算法。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"模型算法 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"cost sensitive算法 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"focal loss 可以查看本博客focal loss内容Focal Loss ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["面经"],"content":"面试中经常被问到的一个问题就是out of vocabulary，可能是因为当前数据集中出现了提前准备好的单词表中没有的word，也可能是因为test中出现了train中没有的word。 ## 解决办法： 1. 直接Ignore 2. 将token分配为[unk] 3. 增大词表 4. 检查拼写 5. BPE算法或word piece 面试时可以展开说一下具体的算法过程，不再赘述。 ","date":"2023-02-13","objectID":"/oov%E9%97%AE%E9%A2%98/:0:0","tags":["面经","OOV问题"],"title":"OOV问题","uri":"/oov%E9%97%AE%E9%A2%98/"},{"categories":["Deep Learning","GAN系列"],"content":"简介 生成对抗网络（Generative Adversarial Network，简称GAN）是无监督学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。 大白话： 说就是生成对抗网络有一个生成网络和一个判别网络，假设有一个真实数据和一个生成网络生成的假数据，那么判别网络就是识别出这两种数据，判别网络努力分类成功，生成网络努力生成和真实数据相似的数据使判别网络分类不出来。这就是所说的相互博弈的方式。 专业的话： 生成式对抗网络由生成器和判别器构成。生成对抗网络的核心目的是训练生成器。生成器的目的是生成与真实样本尽可能相似的“假样本”，判别器的目的是尽可能区分出给定样本是真实样本还是生成的“假样本”。二者目的相悖，在不断博弈的过程中相互提高，最终在判别器判别能力足够可靠的前提下仍无法区分给定样本是真实样本还是生成样本，从而我们说生成器能够生成“以假乱真”的样本。 ","date":"2022-12-22","objectID":"/gan/:1:0","tags":["Deep Learning","GAN系列","GAN"],"title":"GAN","uri":"/gan/"},{"categories":["Deep Learning","GAN系列"],"content":"优化 优化目标 - 价值函数 (Value Function) \\(\\min_G \\max_D V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\\) - 优化方式 - 生成器优化方向: 最小化价值函数 - 判别器优化方向: 最大化价值函数 简单理解，判别器就是要使正确的样本分为1，生成的样本分为0，那么就是最大化价值函数。 生成器就是要以假乱真，使判别器判生成的样本为1，即最大化\\(D(G(z))\\)，就是最小化价值函数。 这里的价值函数与交叉熵类似。真实样本为正例，生成样本为负例，一般都是最小化损失函数，因此需要做一下变形： 这个很好理解，与交叉熵一样。 上面是判别器的优化，下面是生成器的优化： 总的来说，生成器得损失函数就是 \\[ J^{(G)} = -\\frac{1}{2}E_z\\log D(G(z)) \\] 即“以假乱真”。 ","date":"2022-12-22","objectID":"/gan/:2:0","tags":["Deep Learning","GAN系列","GAN"],"title":"GAN","uri":"/gan/"},{"categories":["Machine Learning","降维算法"],"content":"线性判别分析LDA(Linear Discriminant Analysis) 线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别，一起学习一下吧。 ","date":"2022-12-21","objectID":"/lda/:0:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"主要思想 类内方差小 类间方差大 ","date":"2022-12-21","objectID":"/lda/:1:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"推导 这里以二类为例，即只有两个类别。 首先是投影，我们假定原来的数据是向量 \\(x\\)，那么顺着 $ w$ 方向的投影就是标量： \\[ z=w^T\\cdot x(=|w|\\cdot|x|\\cos\\theta) \\] 对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 \\(N_1\\)和 \\(N_2\\)，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 \\(S\\) 表示原数据的协方差： \\[ \\begin{aligned} C_1:Var_z[C_1]\u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_{c1}})(z_i-\\bar{z_{c1}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(x_i-\\bar{x_{c1}})(x_i-\\bar{x_{c1}})^Tw\\nonumber\\\\\\\\ =w^TS_1w\\\\\\\\\\\\\\\\ C_2:Var_z[C_2]\u0026=\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_{c2}})(z_i-\\bar{z_{c2}})^T\\nonumber\\\\\\\\ =w^TS_2w \\end{aligned} \\] 所以类内距离为： \\[ \\begin{align} Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w \\end{align} \\] 对于第二点，我们可以用两类的均值表示这个距离： \\[ \\begin{align} (\\bar{z_{c1}}-\\bar{z_{c2}})^2\u0026=(\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i-\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i)^2\\nonumber\\\\\\\\ \u0026=(w^T(\\bar{x_{c1}}-\\bar{x_{c2}}))^2\\nonumber\\\\\\\\ \u0026=w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw \\end{align} \\] 合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值： \\[ \\begin{align} \\hat{w}=\\mathop{argmax}\\limits_wJ(w)\u0026=\\mathop{argmax}\\limits_w\\frac{(\\bar{z_{c1}}-\\bar{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\\nonumber\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw}{w^T(S_1+S_2)w}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^TS_bw}{w^TS_ww} \\end{align} \\] 这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对w的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了： \\[ \\begin{aligned} \u0026\\frac{\\partial}{\\partial w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow w\\propto S_w^{-1}S_bw=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw\\propto S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}}) \\end{aligned} \\] 也就是说最后我们的结果就是\\(w=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})\\) 可以归一化求得单位的w值。 ","date":"2022-12-21","objectID":"/lda/:2:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"多类情况 前面的很容易类比二类的情况，现在的目标函数变成了： \\[ \\frac{W^TS_bW}{W^TS_wW} \\] 现在的问题就是这些都是矩阵，不能像上面那样直接优化，需要替换优化目标。 \\[ \\underbrace{arg\\;max}_W\\;\\;J(W) = \\frac{\\prod\\limits_{diag}W^TS_bW}{\\prod\\limits_{diag}W^TS_wW} \\] 其中 \\(\\prod_{diag}A\\)为A的主对角线元素的乘积,W为\\(n \\times d\\)的矩阵，n为原来的维度，d为映射到超平面的维度，则最终的目标就变成了： \\[ J(W) = \\frac{\\prod\\limits_{i=1}^dw_i^TS_bw_i}{\\prod\\limits_{i=1}^dw_i^TS_ww_i} = \\prod\\limits_{i=1}^d\\frac{w_i^TS_bw_i}{w_i^TS_ww_i} \\] 根据广式瑞利商，最大值是矩阵\\(S_w^{-1}S_b\\)的最大特征值,最大的d个值的乘积就是矩阵的\\(S_w^{-1}S_b\\)最大的d个特征值的乘积,此时对应的矩阵\\(W\\)为这最大的d个特征值对应的特征向量张成的矩阵。 ","date":"2022-12-21","objectID":"/lda/:3:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"总结 LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。 实际上LDA除了可以用于降维以外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。 我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 ","date":"2022-12-21","objectID":"/lda/:4:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"代码 mean_list = [] for i in range(2): mean_list.append(np.mean(X_train[y_train==i], axis=0)) mean_list = np.array(mean_list) S_W = np.zeros((X_train.shape[1], X_train.shape[1])) # 类内散度矩阵 for c, mv in zip(range(2), mean_list): class_scatter = np.zeros((X_train.shape[1], X_train.shape[1])) for row in X_train[y_train==c]: row, mv = row.reshape(X_train.shape[1], -1), mv.reshape(X_train.shape[1], -1) class_scatter += (row-mv).dot((row-mv).T) S_W += class_scatter over_all_mean = np.mean(X_train, axis=0) S_B = np.zeros((X_train.shape[1], X_train.shape[1])) # 类间散度矩阵 for i, mean_vec in enumerate(mean_list): n = X_train[y_train==i, :].shape[0] mean_list_temp = mean_list[i, :].reshape(1, -1) over_all_mean = over_all_mean.reshape(X_train.shape[1], 1) S_B += n*(mean_vec-over_all_mean).dot((mean_vec-over_all_mean).T) eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))] eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True) # eigv_sum = sum(eig_vals) # for i, j in enumerate(eig_pairs): # print('eigenvalue {0:}: {1:.2%}'.format(i + 1, (j[0] / eigv_sum).real)) # 根据百分比显示特征值，从而选取最大的n个特征值 W = np.hstack((eig_pairs[0][1].reshape(X_train.shape[1], 1), eig_pairs[1][1].reshape(X_train.shape[1], 1))) ","date":"2022-12-21","objectID":"/lda/:5:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["sklearn","feature_selection"],"content":"Sklearn.feature_selection.SelectFromModel class sklearn.feature_selection.SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None)[source] 参数 Parameters - estimator_：一个估算器 用来建立变压器的基本估计器。 只有当一个不适合的估计器传递给SelectFromModel时， 才会存储这个值，即当prefit为False时。 threshold_：float 用于特征选择的阈值。 方法: 'fit(self, X[, y])' 训练SelectFromModel元变压器。 'fit_transform(self, X[, y])' 训练元变压器，然后对X进行转换。 'get_params(self[, deep])' 获取此估计量的参数。 'get_support(self[, indices])' 获取所选特征的掩码或整数索引 'inverse_transform(self, X)' 反向转换操作 'partial_fit(self, X[, y])' 仅将SelectFromModel元变压器训练一次。 'set_params(self, \\*\\*params)' 设置此估算器的参数。 'transform(self, X)' 将X缩小为选定的特征。 ","date":"2022-12-10","objectID":"/selectfrommodel/:0:0","tags":["sklearn","feature_selection","SelectFromModel"],"title":"SelectFromModel","uri":"/selectfrommodel/"},{"categories":["Deep Learning","循环神经网络系列"],"content":" image.png 其中\\(r_{t}\\)为reset门，用于重置上一step的状态。\\(z_{t}\\)为update门，用于得到当前step的状态。 ","date":"2022-12-04","objectID":"/gru/:0:0","tags":["Deep Learning","循环神经网络系列","GRU"],"title":"GRU","uri":"/gru/"},{"categories":["pandas","api"],"content":" 很简单，就是统计x中的数出现次数，返回结果的最大长度就是x中的最大值+1，idx为对应的数，值为出现的次数，没有出现的为0。 x = np.array([7, 6, 2, 1, 4]) # 索引0出现了0次，索引1出现了1次......索引5出现了0次...... np.bincount(x) #输出结果为：array([0, 1, 1, 0, 1, 0, 1, 1]) weight这个参数也很好理解，x会被它加权，也就是说，如果值n发现在位置i，那么out[n] += weight[i]而不是out[n] += 1。所以weight必须和x等长。 w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # 我们可以看到x中最大的数为4，因此bin的数量为5，那么它的索引值为0-\u003e4 x = np.array([2, 1, 3, 4, 4, 3]) # 索引0 -\u003e 0 # 索引1 -\u003e w[1] = 0.5 # 索引2 -\u003e w[0] = 0.3 # 索引3 -\u003e w[2] + w[5] = 0.2 - 0.6 = -0.4 # 索引4 -\u003e w[3] + w[4] = 0.7 + 1 = 1.7 np.bincount(x, weights=w) # 因此，输出结果为：array([ 0. , 0.5, 0.3, -0.4, 1.7]) 没出现的还是0，出现的要按照出现的地方的weight计算。 ","date":"2022-11-29","objectID":"/bincount/:0:0","tags":["pandas","api","bincount"],"title":"bincount","uri":"/bincount/"},{"categories":["Mathematical Modeling"],"content":"优劣解距离法Topsis ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:0:0","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"步骤 ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:1:0","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"原始矩阵正向化 有四种指标： 极大型 极小型 中间型 区间型 正向化就是将其它的指标转化为极大型指标 极小型 -\u003e 极大型： 中间型 -\u003e 极大型： 区间型 -\u003e 极大型： ### 正向化矩阵标准化 \\[ z_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{i=1}^{n}x_{ij}^2}} \\] ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:1:1","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"计算得分并归一化 ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:1:2","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"如何计算得分 ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:2:0","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"只有一个指标时 构造评分的公式： \\[ \\frac{x-min}{max-min} \\] 本质上为 \\[ \\frac{x-min}{(max-x)+(x-min)} \\] 即 \\[ \\frac{\\text{x与最小值的距离}}{\\text{x与最大值的距离}+\\text{x与最小值的距离}} \\] ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:2:1","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"多个指标 这里就需要矩阵了 先把矩阵标准化 \\[ z_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{i=1}^{n}x_{ij}^2}} \\] 定义最大值 \\[ Z^+=(Z_1^+,Z_2^+, \\cdots, Z_m^+) \\\\\\\\ =(max\\\\{z_{11},z_{21},\\cdots,z_{n1}\\\\},max\\\\{z_{12},z_{22},\\cdots,z_{n2}\\\\},\\cdots,max\\\\{z_{1m},z_{2m},\\cdots,z_{nm}\\\\}) \\] 定义第\\(i(i=1,2,\\cdots,n)\\)个评价对象与最大值的距离 \\[ D_i^+=\\sqrt{\\sum_{j=1}^m(Z_j^+-z_{ij})^2} \\] 定义第\\(i(i=1,2,\\cdots,n)\\)个评价对象与最小值的距离 \\[ D_i^+=\\sqrt{\\sum_{j=1}^m(Z_j^--z_{ij})^2} \\] 这样就可以得到第\\(i(i=1,2,\\cdots,n)\\)个评价对象未归一化的得分\\(S_i = \\frac{D_i^-}{D_i^++D_i^-}\\) ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:2:2","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"权重怎么求？ 可以用层次分析法求权重，但使用层次分析法求出的权重主观性太强，因为判断矩阵依赖于专家。 较为客观的方法是熵权法。 熵权法要求标准化后的矩阵都是非负的，因此要使用max-min归一化方法。 对于某个指标来说，其概率值为\\(p_{ij} = \\frac{z_{ij}}{\\sum_{i=1}^{n}z_{ij}}\\) 对于第j个指标，其信息熵的计算公式为：\\(e_{j}=-\\frac{1}{\\ln n}\\sum_{i=1}^{n}p_{ij}\\ln (p_{ij})\\) 1.为什么除以\\(\\ln n\\)？ H(x)最大值就是取\\(\\ln n\\)，这里除以\\(\\ln n\\)就是使得信息熵能始终位于0-1区间内 信息效用值：\\(d_j=1-e_j\\) 信息效用值越大，其对应的信息越多 将信息效用值归一化，就能得到每个指标的熵权：\\(W_{j}=\\frac{d_j}{\\sum_{j=1}^{m}d_j}\\) 依据的原理： 指标的变异程度越小（方差小），所反映的信息量也越少（信息熵大，信息效用值小），其对应的权值也应该越低。（客观= 数据本身就可以告诉我们权重） ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:3:0","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["Mathematical Modeling"],"content":"附：python实现topsis代码。 import pandas as pd import numpy as np # 对极小型指标进行处理 def Min2Max(data): return data.max() - data # 对中间型指标进行处理 def Mid2Max(data, best): M = np.max(np.abs(data-best)) return 1 - np.abs(data-best)/M # 对区间型指标进行处理 def Inter2Max(data, a, b): row = data.shape[0] M = max([a - np.min(data), np.max(data) - b]) res = np.zeros(row) for i in range(row): if data[i] \u003c a: res[i] = 1 - (a - data[i])/M elif data[i] \u003e b: res[i] = 1 - (data[i] - b)/M else: res[i] = 1 return res data = pd.read_excel(\"20条河流的水质情况数据.xlsx\").values # 原始矩阵正向化 data[:, 3] = Min2Max(data[:, 3]) data[:, 2] = Mid2Max(data[:, 2], 7) data[:, 4] = Inter2Max(data[:, 4], 10 ,20) # 正向化矩阵标准化 data[:, 1:] = data[:, 1:] / np.sum(data[:, 1:]**2, axis=0)**0.5 # 计算得分并归一化 temp = data[:, 1:] Z_zheng = np.max(temp, axis=0) Z_fu = np.min(temp, axis=0) w = np.ones(temp.shape[1]) # 在这里修改权重 D_zheng = np.sum(w*(Z_zheng-temp)**2, axis=1)**0.5 D_fu = np.sum(w*(Z_fu-temp)**2, axis=1)**0.5 S = D_fu / (D_zheng + D_fu) Stand_S = S / np.sum(S) # 降序排列 inx = np.argsort(-Stand_S) Stand_S = Stand_S[inx] print(Stand_S) ","date":"2022-11-18","objectID":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/:4:0","tags":["Mathematical Modeling","优劣解距离法Topsis"],"title":"优劣解距离法Topsis","uri":"/%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95topsis/"},{"categories":["算法题"],"content":"把数字翻译成字符串 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ba-shu-zi-fan-yi-cheng-zi-fu-chuan-lcof/ ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:1:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： dp思想，不用管是什么字符，定义dp[i]为长度为i时 有多少个方法 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:2:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"代码: class Solution: def translateNum(self, num: int) -\u003e int: s = str(num) if len(s) \u003c 2: return 1 dp = [0] * len(s) dp[0] = 1 dp[1] = 2 if int(s[0] + s[1]) \u003c 26 else 1 for i in range(2,len(s)): dp[i] = dp[i-1] + dp[i-2] if int(s[i-1] + s[i]) \u003c 26 and s[i-1] != '0' else dp[i-1] return dp[-1] 注意如果长度小于等于1 则直接返回1 如果不是26个英文字母里面的 则dp[i] = dp[i-1] 说明方法次数并不改变 注意有首位为0的情况 所以要int一下 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:3:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["NLP"],"content":"Seq2Seq （本文只介绍最原始的seq2seq，带有注意力在attention文章中） ","date":"2022-11-09","objectID":"/seq2seq/:0:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"RNN 有关RNN Seq2Seq是典型的Encoder-decoder框架的模型，其中编码器和解码器都采用的RNN模型或者RNN模型的变体：GRU、LSTM等。 一般的RNN模型有几种形式，分别为一对一、一对多、多对一、多对多。 一对一： 就是一般的MLP，并不能称之为RNN模型 一对多: 典型的例子就是语音生成，比如输入某个值可以由程序生成一段音乐。 多对一： 最常见的文本分类或者情感分析就是这个模型架构 多对多： 序列标注、NER、分词，大多数标注任务就是用的这个模型架构。而Seq2Seq也属于多对多的任务，不过由于输入和输出的长度可能会不一样，因此采用encoder-decoder的框架，主要思想就是通过encoder将输入的信息编码，然后传入decoder再进行解码得到想要的结果。这常用于生成式的任务，比如说机器翻译、对话系统、文本摘要等等，都可以使用这个框架进行实现。著名的Transformer 就是使用的这个框架。 ","date":"2022-11-09","objectID":"/seq2seq/:1:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"encoder-decoder 框架 encoder-decoder框架可以看作一种深度学习领域的研究模式，应用场景十分广泛， 文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对\u003cSource,Target\u003e，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成。 \\[ Source = \u003cx_1, x_2, \\dots, x_m\u003e \\\\\\\\ Target = \u003cy_1, y_2, \\dots, y_n\u003e \\] Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： \\[ C = F(x_1, x_2, \\dots, x_m) \\] 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息\\(y_1,y_2,\\dots, y_{i-1}\\)来生成i时刻要生成的单词\\(y_i\\)： \\[ y_i = G(C, y_1,y_2,\\dots,y_{i-1}) \\] 每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。 Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。 ","date":"2022-11-09","objectID":"/seq2seq/:2:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"Seq2Seq模型 Seq2Seq模型是输出的长度不确定时采用的模型，这种情况一般是在机器翻译的任务中出现，将一句中文翻译成英文，那么这句英文的长度有可能会比中文短，也有可能会比中文长，所以输出的长度就不确定了。 ","date":"2022-11-09","objectID":"/seq2seq/:3:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"结构 seq2seq属于encoder-decoder结构的一种，这里看看常见的encoder-decoder结构，基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder。encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量C。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 而decoder则负责根据语义向量生成指定的序列，这个过程也称为解码，最简单的方式是将encoder得到的语义变量作为初始状态输入到decoder的RNN中，得到输出序列。可以看到上一时刻的输出会作为当前时刻的输入，而且其中语义向量C只作为初始状态参与运算，后面的运算都与语义向量C无关。 decoder处理方式还有另外一种，就是语义向量C参与了序列所有时刻的运算，如下图，上一时刻的输出仍然作为当前时刻的输入，但语义向量C会参与所有时刻的运算。 上面的这两种结构是我刚学的时候疑惑的一个地方，因为我在有的地方看到的代码是第一种结构的，而沐神的教程中的结构用的是第二种结构，其实这两种都是可以的。 ### 训练 最主要的思路就是语言模型，因为在RNN中，每一个step的输出层的大小就是单词表的大小，要预测最大概率出现的那个词汇，即最大化最有可能是当前输出的单词所在神经元的概率，实际上就是一个多分类问题，使用softmax归一化表示概率。 encoder就是简单的RNN系列模型，前文说的可以有多种方式计算语义向量c，并且c可以有两种方式参与到decoder的计算中。 decoder的主要训练方式就是将前一时刻的结果作为后一时刻的输入，也就是自回归。在训练中的体现是将语料进行错位训练，比如[“你好“ 世界”]，在训练中decoder中就是输入为[“S” ”你好“ “世界”]，而标签也就是实际值为[“你好” “世界” ”E“] ，按照这样的方式进行训练，训练过后，再进行测试，测试的过程就是严格按照前一时刻的输出作为后一时刻的输入，因为这时你也没有要输入的数据。也就是说训练时和测试时的decoder是不一样的。训练的时候我们有真实的数据，而预测的时候没有，只能自产自销，其实就是一个语言模型，叫做条件语言模型。这里引用两张图 既然是语言模型，可以用极大似然估计最大化输出序列的概率： \\[ \\begin{aligned} P(y_1,\\dots,y_{T} | x_1, \\dots x_T) = \\prod_{t=1}^TP(y_t|y_1 , \\dots y_{t-1}; c) \\end{aligned} \\] 在计算损失的时候，我们使用交叉熵作为损失函数，所以我们要找出这个V维向量中，正确预测对应的词的那一维的概率大小\\(\\hat{p}\\)，则这一步的损失就是它的负导数\\(-log(\\hat{p})\\)，将每一步的损失求和，即得到总体的损失函数： \\[ \\begin{aligned} J = -\\frac{1}{T}\\sum_{i}^Tlog(p(\\hat{y_i})) \\end{aligned} \\] 其中的\\(p(\\hat{y_i})\\)为时间t=i上的正确输出节点的概率值，即softmax值。 其中有三个特殊的标记，一个是S代表句子的开头，E代表句子的结尾，P代表Padding。 ## 束搜索(beam search) 一般来说，用前一步的结果作为下一步的输出，这种方式就是贪心策略，但也存在问题，也就是说每一步最优并不是全局最优，改进的办法就是束搜索。思想很简单，每一步就是多选几个作为候选，最后综合考虑，选出最优的组合。是不是和HMM中的维特比算法很像呢？ 以下为束搜索的步骤： - 首先需要设定一个候选集的大小beam size=k。 - 每一步的开始，我们从每个当前输入对应的所有可能输出，计算每一条路的序列得分 - 保留序列得分最大的k个作为下一步的输入 - 不断重复以上步骤，直至结束，选择序列得分最大的那个序列作为最终结果。 其中序列得分为： \\[ score(y_1,y_2, \\dots y_t) = \\sum_{i=1}^t \\log P(y_i|y_1,y_2,\\dots y_{i-1};x) \\] 过程如图所示： ","date":"2022-11-09","objectID":"/seq2seq/:3:1","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"评价标准 ","date":"2022-11-09","objectID":"/seq2seq/:4:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"BLUE指标 BLEU，全称是Bilingual Evaluation Understudy，它的主要思想是基于N-gram等特征来比较人工翻译和机器翻译结果的相似程度。 我们将BLEU定义为： \\[ \\exp \\left(\\min\\left(0,1-\\frac{len_{\\text {label }}}{len_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}} \\] 长的 \\(n\\) 元语法。另外, 用 \\(p_{n}\\) 表示 \\(n\\) 元语法的精确度, 它是两个数量的比值：第一个是预测序 列与标签序列中匹配的 \\(n\\) 元语法的数量, 第二个是预测序列中 \\(n\\) 元语法的数量的比率。具体 地说, 给定标签序列 \\(A 、 B 、 C 、 D 、 E 、 F\\) 和预测序列 \\(A 、 B 、 B 、 C 、 D\\), 我们有 \\(p_{1}=4 / 5 、 p_{2}=3 / 4 、 p_{3}=1 / 3\\) 和 \\(p_{4}=0\\) 。 根据 (9.7.4)中BLEU的定义，当预测序列与标签序列完全相同时, BLEU为 1 。 此外, 由于 \\(n\\) 元语法越长则匹配难度越大, 所以BLEU为更长的 \\(n\\) 元语法的精确度分配更大的权重。具体 来说, 当 \\(p_{n}\\) 固定时, \\(p_{n}^{1 / 2^{n}}\\) 会随着 \\(n\\) 的增长而增加（原始论文使用 \\(p_{n}^{1 / n}\\) )。而且, 由于预测 的序列越短获得的 \\(p_{n}\\) 值越高, 所以 (9.7.4)中乘法项之前的系数用于惩罚较短的预测序列。 例如, 当 \\(k=2\\) 时，给定标签序列 \\(A 、 B 、 C 、 D 、 E 、 F\\) 和预测序列 \\(A 、 B\\) ，尽管 \\(p_{1}=p_{2}=1\\) ， 惩罚因子 \\(\\exp (1-6 / 2) \\approx 0.14\\) 会降低BLEU。 ","date":"2022-11-09","objectID":"/seq2seq/:4:1","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"代码 # code by Tae Hwan Jung @graykode import numpy as np import torch import torch.nn as nn # S: Symbol that shows starting of decoding input # E: Symbol that shows starting of decoding output # P: Symbol that will fill in blank sequence if current batch data size is short than time steps def make_batch(): input_batch, output_batch, target_batch = [], [], [] for seq in seq_data: for i in range(2): seq[i] = seq[i] + 'P' * (n_step - len(seq[i])) input = [num_dic[n] for n in seq[0]] output = [num_dic[n] for n in ('S' + seq[1])] target = [num_dic[n] for n in (seq[1] + 'E')] input_batch.append(np.eye(n_class)[input]) output_batch.append(np.eye(n_class)[output]) target_batch.append(target) # not one-hot # make tensor return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch) # make test batch def make_testbatch(input_word): input_batch, output_batch = [], [] input_w = input_word + 'P' * (n_step - len(input_word)) input = [num_dic[n] for n in input_w] output = [num_dic[n] for n in 'S' + 'P' * n_step] input_batch = np.eye(n_class)[input] output_batch = np.eye(n_class)[output] return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0) # Model class Seq2Seq(nn.Module): def __init__(self): super(Seq2Seq, self).__init__() self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) self.fc = nn.Linear(n_hidden, n_class) def forward(self, enc_input, enc_hidden, dec_input): enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class] dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class] # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden] _, enc_states = self.enc_cell(enc_input, enc_hidden) # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)] outputs, _ = self.dec_cell(dec_input, enc_states) model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class] return model if __name__ == '__main__': n_step = 5 n_hidden = 128 char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz'] num_dic = {n: i for i, n in enumerate(char_arr)} seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']] n_class = len(num_dic) batch_size = len(seq_data) model = Seq2Seq() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) input_batch, output_batch, target_batch = make_batch() for epoch in range(5000): # make hidden shape [num_layers * num_directions, batch_size, n_hidden] hidden = torch.zeros(1, batch_size, n_hidden) optimizer.zero_grad() # input_batch : [batch_size, max_len(=n_step, time step), n_class] # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class] # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot output = model(input_batch, hidden, output_batch) # output : [max_len+1, batch_size, n_class] output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class] loss = 0 for i in range(0, len(target_batch)): # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1] loss += criterion(output[i], target_batch[i]) if (epoch + 1) % 1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Test def translate(word): input_batch, output_batch = make_testbatch(word) # make hidden shape [num_layers * num_directions, batch_size, n_hidden] hidden = torch.zeros(1, 1, n_hidden) output = model(input_batch, hidden, output_batch) # output : [max_len+1(=6), batch_size(=1), n_class] predict = output.data.max(2, keepdim=True)[1] # select n_class dimension decoded = [char_arr[i] for i in predict] end = decoded.index('E') translated = ''.join(decoded[:end]) return translated.replace('P', '') print('test') print('man -\u003e', translate('man')) prin","date":"2022-11-09","objectID":"/seq2seq/:5:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["Machine Learning","聚类算法"],"content":"DBSCAN属于密度聚类的一种。通常情形下，密度聚类算法从样 本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇 以获得最终的聚类结果。 DBSCAN基于一组“邻域”参数\\((\\epsilon, Minpts)\\)来刻画样本分布的紧密程度，给定数据集\\(D=\\\\{x_1,x_2, \\dots,x_m \\\\}\\)，定义几个概念： \\(\\epsilon\\)-邻域：对\\(x_j\\in D\\)，其\\(\\epsilon\\)-邻域包含样本集D中与\\(x_j\\)的距离不大于\\(\\epsilon\\)的样本，即\\(N_{\\epsilon}(x_j) = \\\\{dist(x_i, x_j) \\leq \\epsilon\\\\}\\)。 核心对象 (core object): 若 \\(x_j\\) 的 \\(\\epsilon\\)-邻域至少包含 MinPts 个样本, 即 \\(\\left|N_\\epsilon\\left(\\boldsymbol{x}_j\\right)\\right| \\geqslant \\operatorname{MinPts}\\), 则 \\(\\boldsymbol{x}_j\\) 是一个核心对象; 密度直达(directly density-reachable): 若 \\(\\boldsymbol{x}_j\\) 位于 \\(\\boldsymbol{x}_i\\) 的 \\(\\epsilon\\)-邻域中, 且 \\(\\boldsymbol{x}_i\\) 是 核心对象, 则称 \\(\\boldsymbol{x}_j\\) 由 \\(\\boldsymbol{x}_i\\) 密度直达; 密度可达(density-reachable): 对 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\), 若存在样本序列 \\(\\boldsymbol{p}_1, \\boldsymbol{p}_2, \\ldots, \\boldsymbol{p}_n\\), 其中 \\(\\boldsymbol{p}_1=\\boldsymbol{x}_i, \\boldsymbol{p}_n=\\boldsymbol{x}_j\\) 且 \\(\\boldsymbol{p}_{i+1}\\) 由 \\(\\boldsymbol{p}_i\\) 密度直达, 则称 \\(\\boldsymbol{x}_j\\) 由 \\(\\boldsymbol{x}_i\\) 密度可达; 密度相连 (density-connected): 对 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\), 若存在 \\(\\boldsymbol{x}_k\\) 使得 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\) 均由 \\(\\boldsymbol{x}_k\\) 密度可达, 则称 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\) 密度相连. 既然是聚类，那就要定义簇的概念 ","date":"2022-11-03","objectID":"/dbscan/:0:0","tags":["Machine Learning","聚类算法","DBSCAN"],"title":"DBSCAN","uri":"/dbscan/"},{"categories":["Machine Learning","关联规则算法"],"content":"参考：https://www.cnblogs.com/bill-h/p/14863262.html 大家可能听说过用于宣传数据挖掘的一个案例:啤酒和尿布；据说是沃尔玛超市在分析顾客的购买记录时，发现许多客户购买啤酒的同时也会购买婴儿尿布，于是超市调整了啤酒和尿布的货架摆放，让这两个品类摆放在一起；结果这两个品类的销量都有明显的增长；分析原因是很多刚生小孩的男士在购买的啤酒时，会顺手带一些婴幼儿用品。 不论这个案例是否是真实的，案例中分析顾客购买记录的方式就是关联规则分析法Association Rules。 关联规则分析也被称为购物篮分析，用于分析数据集各项之间的关联关系。 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:0:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"项集 item的集合，如集合{牛奶、麦片、糖}是一个3项集，可以认为是购买记录里物品的集合。 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:1:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"频繁项集 顾名思义就是频繁出现的item项的集合。如何定义频繁呢？用比例来判定，关联规则中采用支持度和置信度两个概念来计算比例值 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:2:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"支持度（support) 共同出现的项在整体项中的比例。以购买记录为例子，购买记录100条，如果商品A和B同时出现50条购买记录（即同时购买A和B的记录有50），那边A和B这个2项集的支持度为50% \\[ Support(A\\cap B) = \\frac{Freq(A\\cap B)}{N} \\] ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:3:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"置信度（Confidence） 购买A后再购买B的条件概率，根据贝叶斯公式，可如下表示： \\[ Confidence = \\frac{Freq(A\\cap B)}{Freq(A)} \\] ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:4:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"提升度 为了判断产生规则的实际价值，即使用规则后商品出现的次数是否高于商品单独出现的评率，提升度和衡量购买X对购买Y的概率的提升作用。如下公式可见，如果X和Y相互独立那么提升度为1，提升度越大，说明X-\u003eY的关联性越强 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:5:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["算法题"],"content":"字符串转换整数 (atoi) https://leetcode-cn.com/problems/string-to-integer-atoi/ #重点是正则表达式 class Solution: def myAtoi(s: str): import re ss = re.findall(\"^[\\+\\-]?\\d+\",s.strip()) res = int(*ss) if res \u003e (231-1): res = (231-1) if res \u003c -231: res = -231 return res WA了四次才整出来，太菜了，以为很简单，没有认真读题，要吸取教训。 ","date":"2022-10-26","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi/:0:0","tags":["算法题","字符串转换整数 (atoi)"],"title":"字符串转换整数 (atoi)","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi/"},{"categories":["Machine Learning","聚类算法"],"content":"基础就是高斯混合模型，假设我们熟知的高斯分布的概率密度函数为\\(p(x\\mid \\mu, \\Sigma)\\)。则高斯混合分布为： \\[ p_{\\mathcal{M}}(\\boldsymbol{x})=\\sum_{i=1}^k \\alpha_i \\cdot p\\left(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i\\right) \\] 分布共由 \\(k\\) 个混合成分组成, 每个混合成分对应一个高斯分布. 其中 \\(\\mu_i\\) 与 \\(\\Sigma_i\\) 是第 \\(i\\) 个高斯混合成分的参数, 而 \\(\\alpha_i\u003e0\\) 为相应的 “混合系数” (mixture coefficient), \\(\\sum_{i=1}^k \\alpha_i=1\\)。 假设样本的生成过程由高斯混合分布给出: 首先, 根据 \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k\\) 定义 的先验分布选择高斯混合成分, 其中 \\(\\alpha_i\\) 为选择第 \\(i\\) 个混合成分的概率; 然后, 根 据被选择的混合成分的概率密度函数进行采样, 从而生成相应的样本。 ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:0:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"聚类原理 如何利用高斯混合分布进行聚类？观察这个混合系数，思路就是有多少个混合的模型，就代表要聚多少类，对于给定数据集，可以定义 \\[ \\gamma_{j k}= \\begin{cases}1, \u0026 \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] 则样本j的簇标记\\(\\lambda_j= \\underbrace{\\arg \\max}_{i\\in {1,2, \\dots ,k}} \\gamma_{jk}\\) ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:1:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"EM算法 如何计算\\(\\gamma_{jk}\\)呢，如下式所示，其中\\(\\alpha_k\\)为混合系数，\\(\\theta_k\\)为第k个高斯分布的参数 \\[ \\begin{aligned} \\hat{\\gamma}_{j k} \u0026=E\\left(\\gamma_{j k} \\mid y, \\theta\\right)=P\\left(\\gamma_{j k}=1 \\mid y, \\theta\\right) \\\\\\\\ \u0026=\\frac{P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 式子里的y其实就是观测样本。 那么模型的参数要怎么估计呢，很显然可以使用EM算法，\\(\\gamma\\)为隐变量，其实我这里的叙述顺序是有问题的，其实是EM算法中求Q函数的过程中需要计算的一个值，详细的过程在本博客的EM算法里面。总之得到了\\(\\gamma_{jk}\\)后，就得到了Q函数: \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{k=1}^K\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\} \\] 极大似然估计Q函数就可以得到参数的下一轮估计值： \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 用 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 及 \\(\\hat{\\alpha}_k, k=1,2, \\cdots, K\\), 表示 \\(\\theta^{(i+1)}\\) 的各参数。求 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 只需分别对 \\(\\mu_k, \\sigma_k^2\\) 求偏导数并令其为 0 , 即可得到; 求 \\(\\hat{\\alpha}_k\\) 是在 \\(\\sum_{k=1}^K \\alpha_k=1\\) 条件 下求偏导数并令其为 0 得到的。结果如下: \\[ \\begin{gathered} \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left(y_j-\\mu_k\\right)^2}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\alpha}_k=\\frac{n_k}{N}=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}{N}, \\quad k=1,2, \\cdots, K \\end{gathered} \\] 得到参数后，再进行新的一轮迭代，计算\\(\\gamma\\)值，如此反复。 算法收敛后，就可以对样本进行聚类，根据\\(\\lambda_j= \\underbrace{\\arg \\max}_{i\\in {1,2, \\dots ,k}} \\gamma_{jk}\\)可以得到每个样本的簇标记。具体的流程如下： ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:2:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"总结 高斯混合分布的形式就注定了它可以用来进行聚类，并且还有EM算法如此强大的数学工具进行模型参数的学习，高斯混合聚类与Kmeans都属于原型聚类。 ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:3:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["NLP"],"content":"Tokenization技术 本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子中经常会有此类问题，并且都是外国学校的，说明外国学校还是比较注重这一块的基础的。 首先明确一个概念：token可以理解为一个符号，就代表一个语言单位，tokenize的意思就是把一个句子或语料分成token. ","date":"2022-10-17","objectID":"/tokenization/:0:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"word ","date":"2022-10-17","objectID":"/tokenization/:1:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"char ","date":"2022-10-17","objectID":"/tokenization/:2:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"子词(subword) ","date":"2022-10-17","objectID":"/tokenization/:3:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"BPE BPE 是一种简单的数据压缩算法，它在 1994 年发表的文章“A New Algorithm for Data Compression”中被首次提出。下面的示例将解释 BPE。老规矩，我们先用一句话概括它的核心思想： BPE每一步都将最常见的一对相邻数据单位替换为该数据中没有出现过的一个新单位，反复迭代直到满足停止条件。 BPE 确保最常见的词在token列表中表示为单个token，而罕见的词被分解为两个或多个subword tokens，因此BPE也是典型的基于subword的tokenization算法。 合并字符可以让你用最少的token来表示语料库，这也是 BPE 算法的主要目标，即数据的压缩。为了合并，BPE 寻找最常出现的字节对。在这里，我们将字符视为与字节等价。当然，这只是英语的用法，其他语言可能有所不同。现在我们将最常见的字节对合并成一个token，并将它们添加到token列表中，并重新计算每个token出现的频率。这意味着我们的频率计数将在每个合并步骤后发生变化。我们将继续执行此合并步骤，直到达到我们预先设置的token数限制或迭代限制。 ","date":"2022-10-17","objectID":"/tokenization/:4:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"算法过程 准备语料库，确定期望的 subword 词表大小等参数 通常在每个单词末尾添加后缀 ，统计每个单词出现的频率，例如，low 的频率为 5，那么我们将其改写为 “l o w ”：5 将语料库中所有单词拆分为单个字符，用所有单个字符建立最初的词典，并统计每个字符的频率，本阶段的 subword 的粒度是字符 挑出频次最高的符号对 ，比如说 t 和 h 组成的 th，将新字符加入词表，然后将语料中所有该字符对融合（merge），即所有 t 和 h 都变为 th。 重复上述操作，直到词表中单词数达到设定量 或下一个最高频数为 1 ，如果已经打到设定量，其余的词汇直接丢弃 ","date":"2022-10-17","objectID":"/tokenization/:4:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"例子 获取语料库，这样一段话为例：“ FloydHub is the fastest way to build, train and deploy deep learning models. Build deep learning models in the cloud. Train deep learning models. ” 拆分，加后缀\u003c/w\u003e ，统计词频 建立词表，统计字符频率（顺便排个序）： 以第一次迭代为例，将字符频率最高的 d 和 e 替换为 de，后面依次迭代： 更新词表 继续迭代直到达到预设的 subwords 词表大小或下一个最高频的字节对出现频率为 1。 ### 优点 BPE 的优点就在于，可以很有效地平衡词典大小和编码步骤数（将语料编码所需要的 token 数量）。 随着合并的次数增加，词表大小通常先增加后减小。迭代次数太小，大部分还是字母，没什么意义；迭代次数多，又重新变回了原来那几个词。所以词表大小要取一个中间值。 ","date":"2022-10-17","objectID":"/tokenization/:4:2","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"适用范围 BPE 一般适用在欧美语言拉丁语系中，因为欧美语言大多是字符形式，涉及前缀、后缀的单词比较多。而中文的汉字一般不用 BPE 进行编码，因为中文是字无法进行拆分。对中文的处理通常只有分词和分字两种。理论上分词效果更好，更好的区别语义。分字效率高、简洁，因为常用的字不过 3000 字，词表更加简短。 ","date":"2022-10-17","objectID":"/tokenization/:4:3","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"编码过程 BPE的总体思想就是利用替换字节对来逐步构造词汇表。在使用的过程有编码和解码两种。 在之前的算法中，我们已经得到了subword的词表，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。 我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如 编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。 ","date":"2022-10-17","objectID":"/tokenization/:4:4","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"代码 import re, collections def get_vocab(filename): vocab = collections.defaultdict(int) with open(filename, 'r', encoding='utf-8') as fhand: for line in fhand: words = line.strip().split() for word in words: vocab[' '.join(list(word)) + ' \u003c/w\u003e'] += 1 return vocab def get_stats(vocab): # 构建字符对频数字典 pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs def merge_vocab(pair, v_in): # 将频率最大的字符对替换 v_out = {} bigram = re.escape(' '.join(pair)) # 不转义 p = re.compile(r'(?\u003c!\\S)' + bigram + r'(?!\\S)') # 意思是bigram前面没有非空格字符，后面也没有非空格字符 for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_out def get_tokens(vocab): tokens = collections.defaultdict(int) for word, freq in vocab.items(): word_tokens = word.split() for token in word_tokens: tokens[token] += freq return tokens vocab = {'l o w \u003c/w\u003e': 5, 'l o w e r \u003c/w\u003e': 2, 'n e w e s t \u003c/w\u003e': 6, 'w i d e s t \u003c/w\u003e': 3} print('==========') print('Tokens Before BPE') tokens = get_tokens(vocab) print('Tokens: {}'.format(tokens)) print('Number of tokens: {}'.format(len(tokens))) print('==========') num_merges = 5 for i in range(num_merges): pairs = get_stats(vocab) if not pairs: break best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print('Iter: {}'.format(i)) print('Best pair: {}'.format(best)) print(vocab) # tokens = get_tokens(vocab) # print('Tokens: {}'.format(tokens)) ","date":"2022-10-17","objectID":"/tokenization/:4:5","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"wordpiece ","date":"2022-10-17","objectID":"/tokenization/:5:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"构造 wordpiece 词表的构造与BPE很相似，都是选择两个子词合并成新的子词。 最大的区别在于，BPE是选择频数最高的相邻子词合并，而wordpiece选择能够提升语言模型概率最大的相邻子词加入词表。 如何理解？ 假设各个子词之间是独立存在的，则句子S的语言模型似然值等价于所有子词概率的乘积 假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为： 从上面的公式，很容易发现，似然值的变化就是两个子词之间的互信息。简而言之，WordPiece每次选择合并的两个子词，他们具有最大的互信息值，也就是两子词在语言模型上具有较强的关联性，它们经常在语料中以相邻方式同时出现。 与BPE相似，通过以上方式构建词表。 ### 编码 1. 从第一个位置开始，由于是最长匹配，结束位置需要从最右端依次递减，所以遍历的第一个子词 是其本身 unaffable，该子词不在词汇表中 2. 结束位置左移一位得到子词 unaffabl，同样不在词汇表中 3. 重复这个操作，直到 un，该子词在词汇表中，将其加入 output_tokens，以第一个位置开始 的遍历结束 4. 跳过 un，从其后的 a 开始新一轮遍历，结束位置依然是从最右端依次递减，但此时需要在前 面加上 ## 标记，得到 ##affable 不在词汇表中 5. 结束位置左移一位得到子词 ##affabl，同样不在词汇表中 6. 重复这个操作，直到 ##aff，该字词在词汇表中， 将其加入 output_tokens，此轮遍历结束 7. 跳过 aff，从其后的 a 开始新一轮遍历，结束位置依然是从最右端依次递减。##able 在词汇 表中，将其加入 output_tokens 8. able 后没有字符了，整个遍历结束 ","date":"2022-10-17","objectID":"/tokenization/:5:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"代码 (来自https://github.com/google-research/bert/blob/master/tokenization.py) class WordpieceTokenizer(object): \"\"\"Runs WordPiece tokenziation.\"\"\" def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200): self.vocab = vocab self.unk_token = unk_token self.max_input_chars_per_word = max_input_chars_per_word def tokenize(self, text): \"\"\"Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary. For example: input = \"unaffable\" output = [\"un\", \"##aff\", \"##able\"] Args: text: A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer. Returns: A list of wordpiece tokens. \"\"\" text = convert_to_unicode(text) output_tokens = [] for token in whitespace_tokenize(text): chars = list(token) if len(chars) \u003e self.max_input_chars_per_word: output_tokens.append(self.unk_token) continue is_bad = False start = 0 sub_tokens = [] while start \u003c len(chars): end = len(chars) cur_substr = None while start \u003c end: substr = \"\".join(chars[start:end]) if start \u003e 0: substr = \"##\" + substr if substr in self.vocab: cur_substr = substr break end -= 1 if cur_substr is None: is_bad = True break sub_tokens.append(cur_substr) start = end if is_bad: output_tokens.append(self.unk_token) else: output_tokens.extend(sub_tokens) return output_tokens ","date":"2022-10-17","objectID":"/tokenization/:5:2","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"Unigram Language Model 与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。 初始时，建立一个足够大的词表(使用一些算法)。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。 针对当前词表，用EM算法求解每个子词在语料上的概率。 对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。 将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。 重复步骤2到4，直到词表大小减少到设定范围。 可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。 ### 代码 encode代码 def encode_word(word, model): # word初步分词，model中为-log值 best_segmentations = [{\"start\": 0, \"score\": 1}] + [ {\"start\": None, \"score\": None} for _ in range(len(word)) ] for start_idx in range(len(word)): # This should be properly filled by the previous steps of the loop best_score_at_start = best_segmentations[start_idx][\"score\"] for end_idx in range(start_idx + 1, len(word) + 1): token = word[start_idx:end_idx] if token in model and best_score_at_start is not None: score = model[token] + best_score_at_start # 加上start，即前缀对应的损失，log中相加等于原来概率相乘 # If we have found a better segmentation ending at end_idx, we update if ( best_segmentations[end_idx][\"score\"] is None or best_segmentations[end_idx][\"score\"] \u003e score # score即loss。越小越好 ): best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score} print(token) print(best_segmentations[end_idx][\"score\"], score) print(best_segmentations) segmentation = best_segmentations[-1] if segmentation[\"score\"] is None: # We did not find a tokenization of the word -\u003e unknown return [\"\u003cunk\u003e\"], None # 从后向前的最佳路径，即维特比算法 score = segmentation[\"score\"] start = segmentation[\"start\"] end = len(word) tokens = [] while start != 0: tokens.insert(0, word[start:end]) next_start = best_segmentations[start][\"start\"] end = start start = next_start tokens.insert(0, word[start:end]) return tokens, score 计算Loss def compute_loss(model): loss = 0 for word, freq in word_freqs.items(): _, word_loss = encode_word(word, model) loss += freq * word_loss return loss 计算score(用于删除token) import copy def compute_scores(model): scores = {} model_loss = compute_loss(model) for token, score in model.items(): # We always keep tokens of length 1 if len(token) == 1: continue model_without_token = copy.deepcopy(model) _ = model_without_token.pop(token) scores[token] = compute_loss(model_without_token) - model_loss return scores ","date":"2022-10-17","objectID":"/tokenization/:6:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"参考 Unigram tokenization - Hugging Face NLP 课程 ","date":"2022-10-17","objectID":"/tokenization/:6:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"CoVe Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器， ","date":"2022-10-09","objectID":"/cove/:0:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"模型训练 主要假设是，为了翻译一个句子，NMT编码器学会理解句子。 因此来自编码器的向量包含有关单词上下文的信息。 形式上，作者训练了一个带注意力的LSTM模型，比如Bahdanau Model，由于最终我们想使用经过训练的编码器来处理英文句子（不是因为我们只关心英文，而是因为下游任务的大多数数据集都是英文的），所以 NMT 系统必须从英文翻译成其他的语言（例如，德语）。 ","date":"2022-10-09","objectID":"/cove/:1:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"双向编码器 请注意，在这个 NMT 模型中，编码器是双向的：它连接前向和后向 LSTM 的输出。因此，编码器输出包含有关令牌左右上下文的信息。 ","date":"2022-10-09","objectID":"/cove/:2:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"获取表示(连接 GloVe 和 Cove 向量) 训练 NTM 模型后，我们只需要它的编码器。对于给定的文本，CoVe 向量是编码器的输出。对于下游任务，作者建议使用 Glove（代表单个令牌）和 CoVe（在上下文中编码的令牌）向量的串联。这个想法是这些向量编码不同类型的信息，它们的组合可能很有用。 ","date":"2022-10-09","objectID":"/cove/:3:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"总结 其实思想很简单，就是将已经训练好的机器翻译模型的编码器作为编码，最后再与GloVe进行拼接，可以达到很好的效果，因为使用的是机器翻译模型，因此可以视为有监督学习，有监督的预训练模型是很少的，CoVe就是其中之一，其中更具体的分类可以看本博客的预训练模型。 ","date":"2022-10-09","objectID":"/cove/:4:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["信息检索","文本匹配"],"content":"文本语义匹配是自然语言处理中一个重要的基础问题，NLP 领域的很多任务都可以抽象为文本匹配任务。例如，信息检索可以归结为查询项和文档的匹配，问答系统可以归结为问题和候选答案的匹配(基于文本的问答系统)，对话系统可以归结为对话和回复的匹配。语义匹配在搜索优化、推荐系统、快速检索排序、智能客服上都有广泛的应用。如何提升文本匹配的准确度，是自然语言处理领域的一个重要挑战。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:0:0","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"基于词的传统召回 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:1:0","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"传统文本匹配方法 传统的文本匹配技术有BoW、VSM、TF-IDF、 BM25、Jaccord、SimHash等算法，如BM25算法通过网络字段对查询字段的覆盖程度来计算两者间的匹配得分，得分越高的网页与查询的匹配度更好。主要解决词汇层面的匹配问题，或者说词汇层面的相似度问题。而实际上，基于词汇重合度的匹配算法有很大的局限性，原因包括： 语义局限，这些传统方法无法理解词义，比如在不同语境下单词有相同或者不同的意思。 结构局限，“机器学习”和“学习机器”虽然词汇完全重合，但表达的意思不同。 知识局限，“秦始皇打Dota”，这句话虽从词法和句法上看均没问题，但结合知识看这句话是不对的。 所以对于文本匹配不能只停留在字面匹配的层面，需要语义层面的匹配。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:1:1","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"主题模型 本博客有详细介绍。 将语句映射到等长的低维连续空间，在隐式的潜在语义空间上进行相似度的计算，这与协同过滤中的矩阵分解很类似。但是对于主题模型来讲对应的是LSA，即潜在语义分析，还有PLSA与LDA等高级的模型，但主题模型也是基于词袋模型来的，也是只停留在了字面匹配的层面，没有上升到语义层面的匹配。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:1:2","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"深度语义匹配模型 基于神经网络训练出的Word Embedding来进行文本匹配计算，训练方式简洁，所得的词语向量表示的语义可计算性进一步加强。但是只利用无标注数据训练得到的Word Embedding在匹配度计算的实用效果上和主题模型技术相差不大。他们本质都是基于共现信息的训练。另外，Word Embedding本身没有解决短语、句子的语义表示问题，也没有解决匹配的非对称性问题。 一般来说，深度文本匹配模型分为两种类型： 表示型和交互型。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:0","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"表示型(单语义) 表示型模型更侧重对表示层的构建，会在表示层将文本转换成唯一的一个整体表示向量。 representation-based类模型，思路是基于 Siamese (孪生神经网络)网络，提取文本整体语义再进行匹配 典型的Siamese结构，双塔共享参数，将两文本映射到统一空间，才具有匹配意义 表整层进行编码，使用MLP, CNN, RNN, Self-attention, Transformer encoder, BERT均可。 匹配层进行交互计算，采用点积、余弦、高斯距离、MLP、相似度矩阵均可 经典模型有：DSSM, CDSSM, MV-LSTM, ARC-I, CNTN, CA-RNN, MultiGranCNN等 优点 可以对文本预处理，构建索引，大幅度降低在线计算耗时 缺点 失去语义焦点，易语义漂移，难以衡量词的上下文重要性 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:1","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"交互型(多语义) 交互性模型摒弃后匹配的思路，假设全局的匹配度依赖于局部的匹配度，在输入层就进行词语间的先匹配，并将匹配的结果作为灰度图进行后续的建模。 交互层，由两文本词与词构成交互矩阵，交互运算类似于 attention，加性乘性都可以 表征层，负责对交互矩阵进行抽象表征，CNN、S-RNN均可 经典模型有：ARC-II、MatchPyramid、Match-SRNN、K-NRM、DRMM、DeepRank、DUET、IR-Transformer、DeepMatch、ESIM、ABCNN、BIMPM等 优点 更好的把握了语义焦点，能对上下文进行更好的建模 缺点 忽略了句法、句间对照等全局性信息，无法由局部匹配信息刻画全局匹配信息。 ### BERT及后辈 ## 应用(基于主题模型) ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:2","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"短文本-短文本语义匹配 由于主题模型在短文本上的效果不太理想（众所周知），在短文本-短文本匹配任务中 词向量的应用 比主题模型更为普遍。简单的任务可以使用Word2Vec这种浅层的神经网络模型训练出来的词向量。 如，计算两个Query的相似度， q1 = “推荐好看的电影”与 q2 = “2016年好看的电影”。 通过词向量按位累加的方式，计算这两个Query的向量表示 利用余弦相似度（Cosine Similarity）计算两个向量的相似度。 对于较难的短文本-短文本匹配任务，考虑引入有监督信号并利用“DSSM”或“CLSM”这些更复杂的神经网络进行语义相关性的计算。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:3","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"短文本-长文本语义匹配 在搜索引擎中，需要计算用户 Query 和一个网页正文（content）的语义相关度。由于 Query 通常较短，因此 Query 与 content 的匹配与上文提到的短文本-短文本不同，通常需要使用短文本-长文本语义匹配，以得到更好的匹配效果。 在计算相似度的时候，我们规避对短文本直接进行主题映射，而是根据长文本的 主题分布，计算该分布生成短文本的概率，作为他们之间的相似度。 这个学习过主题模型的很容易可以理解。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:4","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["信息检索","文本匹配"],"content":"长文本-长文本语义匹配 以新闻个性化推荐为例： 一图流。我们可以将用户近期阅读的新闻（或新闻标题）合并成一篇长“文档”，并将该“文档”的主题分布作为表达用户阅读兴趣的用户画像。 ","date":"2022-10-07","objectID":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/:2:5","tags":["信息检索","文本匹配","文本匹配概述"],"title":"文本匹配概述","uri":"/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning"],"content":"EM算法 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:0:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"引入 我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。（最大似然估计：利用已知的样本结果，反推最有可能导致这样结果的一组参数）但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。用EM算法可以解决。 EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。 EM算法的每次迭代由两步组成：E步，求期望；M步，求极大。所以被称为期望极大算法。 EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:1:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"三硬币模型 首先介绍一个使用 EM算法的例子。 (三硬币模型) 假设有 3 枚硬币, 分别记作 A, B, C。这些硬币正面出现 的概率分别是 \\(\\pi, p\\) 和 \\(q\\) 。进行如下郑硬币试验: 先掷硬币 \\(\\mathrm{A}\\), 根据其结果选出硬币 \\(\\mathrm{B}\\) 或硬币 \\(\\mathrm{C}\\), 正面选硬币 \\(\\mathrm{B}\\), 反面选硬币 \\(\\mathrm{C}\\); 然后郑选出的硬币, 掷硬币的结果, 出现正 面记作 1 , 出现反面记作 0 ; 独立地重复 \\(n\\) 次试验 (这里, \\(n=10\\) ), 观测结果如下: \\[ 1,1,0,1,0,0,1,0,1,1 \\] 假设只能观测到郑硬币的结果, 不能观测郑硬币的过程。问如何估计三硬币正面出现 的概率, 即三硬币模型的参数。 解 三硬币模型可以写作 \\[ \\begin{aligned} P(y \\mid \\theta) \u0026=\\sum_z P(y, z \\mid \\theta)=\\sum_z P(z \\mid \\theta) P(y \\mid z, \\theta) \\\\\\\\ \u0026=\\pi p^y(1-p)^{1-y}+(1-\\pi) q^y(1-q)^{1-y} \\end{aligned} \\] 这里, 随机变量 \\(y\\) 是观测变量, 表示一次试验观测的结果是 1 或 0 ; 随机变量 \\(z\\) 是隐 变量, 表示末观测到的掷硬币 \\(\\mathrm{A}\\) 的结果; \\(\\theta=(\\pi, p, q)\\) 是模型参数。这一模型是以上数 据的生成模型。注意, 随机变量 \\(y\\) 的数据可以观测, 随机变量 \\(z\\) 的数据不可观测。 将观测数据表示为 \\(Y=\\left(Y_1, Y_2, \\cdots, Y_n\\right)^{\\mathrm{T}}\\), 末观测数据表示为 \\(Z=\\left(Z_1, Z_2, \\cdots, Z_n\\right)^{\\mathrm{T}}\\) 则观测数据的似然函数为 \\[ P(Y \\mid \\theta)=\\sum_Z P(Z \\mid \\theta) P(Y \\mid Z, \\theta) \\] 即 \\[ P(Y \\mid \\theta)=\\prod_{j=1}^n\\left[\\pi p^{y_j}(1-p)^{1-y_j}+(1-\\pi) q^{y_j}(1-q)^{1-y_j}\\right] \\] 考虑求模型参数 \\(\\theta=(\\pi, p, q)\\) 的极大似然估计, 即 \\[ \\hat{\\theta}=\\arg \\max_\\theta \\log P(Y \\mid \\theta) \\] 这个问题没有解析解, 只有通过迭代的方法求解。EM算法就是可以用于求解这 个问题的一种迭代算法。下面给出针对以上问题的 EM算法, 其推导过程省略。 EM算法首先选取参数的初值, 记作 \\(\\theta^{(0)}=\\left(\\pi^{(0)}, p^{(0)}, q^{(0)}\\right)\\), 然后通过下面的 步骤迭代计算参数的估计值, 直至收敛为止。第 \\(i\\) 次迭代参数的估计值为 \\(\\theta^{(i)}=\\) \\(\\left(\\pi^{(i)}, p^{(i)}, q^{(i)}\\right)\\) 。EM算法的第 \\(i+1\\) 次迭代如下。 \\(\\mathrm{E}\\) 步：计算在模型参数 \\(\\pi^{(i)}, p^{(i)}, q^{(i)}\\) 下观测数据 \\(y_j\\) 来自郑硬币 \\(\\mathrm{B}\\) 的概率。这里就是使用的贝叶斯定理。 \\[ \\mu_j^{(i+1)}=\\frac{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_j}\\left(1-p^{(i)}\\right)^{1-y_j}}{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_j}\\left(1-p^{(i)}\\right)^{1-y_j}+\\left(1-\\pi^{(i)}\\right)\\left(q^{(i)}\\right)^{y_j}\\left(1-q^{(i)}\\right)^{1-y_j}} \\] \\(\\mathrm{M}\\) 步：计算模型参数的新估计值 \\[ \\pi^{(i+1)}=\\frac{1}{n} \\sum_{j=1}^n \\mu_j^{(i+1)} \\] \\[ \\begin{gathered} p^{(i+1)}=\\frac{\\sum_{j=1}^n \\mu_j^{(i+1)} y_j}{\\sum_{j=1}^n \\mu_j^{(i+1)}} \\\\\\\\ q^{(i+1)}=\\frac{\\sum_{j=1}^n\\left(1-\\mu_j^{(i+1)}\\right) y_j}{\\sum_{j=1}^n\\left(1-\\mu_j^{(i+1)}\\right)} \\end{gathered} \\] 进行数值计算。假设模型参数的初值取为 \\[ \\pi^{(0)}=0.5, \\quad p^{(0)}=0.5, \\quad q^{(0)}=0.5 \\] 对 \\(y_j=1\\) 与 \\(y_j=0\\) 均有 \\(\\mu_j^{(1)}=0.5\\) 。 利用迭代公式, 得到 \\[ \\pi^{(1)}=0.5, \\quad p^{(1)}=0.6, \\quad q^{(1)}=0.6 \\] \\[ \\mu_j^{(2)}=0.5, \\quad j=1,2, \\cdots, 10 \\] 继续迭代, 得 \\[ \\pi^{(2)}=0.5, \\quad p^{(2)}=0.6, \\quad q^{(2)}=0.6 \\] 于是得到模型参数 \\(\\theta\\) 的极大似然估计: \\[ \\hat{\\pi}=0.5, \\quad \\hat{p}=0.6, \\quad \\hat{q}=0.6 \\] \\(\\pi=0.5\\) 表示硬币 A 是均匀的, 这一结果容易理解。 如果取初值 \\(\\pi^{(0)}=0.4, p^{(0)}=0.6, q^{(0)}=0.7\\), 那么得到的模型参数的极大似然 估计是 \\(\\hat{\\pi}=0.4064, \\hat{p}=0.5368, \\hat{q}=0.6432\\) 。这就是说, EM算法与初值的选择有关, 选择不同的初值可能得到不同的参数估计值。 一般地, 用 \\(Y\\) 表示观测随机变量的数据, \\(Z\\) 表示隐随机变量的数据。 \\(Y\\) 和 \\(Z\\) 连 在一起称为完全数据 (complete-data), 观测数据 \\(Y\\) 又称为不完全数据 (incompletedata）。假设给定观测数据 \\(Y\\), 其概率分布是 \\(P(Y \\mid \\theta)\\), 其中 \\(\\theta\\) 是需要估计的模型参数, 那么不完全数据 \\(Y\\) 的似然函数是 \\(P(Y \\mid \\theta)\\), 对数似然函数 \\(L(\\theta)=\\log P(Y \\mid \\theta)\\); 假设 \\(Y\\) 和 \\(Z\\) 的联合概率分布是 \\(P(Y, Z \\mid \\theta)\\), 那么完全数据的对数似然函数是 \\(\\log P(Y, Z \\mid \\theta)\\) 。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:2:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"算法步骤 输入: 观测变量数据 \\(Y\\), 隐变量数据 \\(Z\\), 联合分布 \\(P(Y, Z \\mid \\theta)\\), 条件分布 \\(P(Z \\mid Y, \\theta)\\); 输出：模型参数 \\(\\theta\\) 。 （1）选择参数的初值 \\(\\theta^{(0)}\\), 开始迭代; (2) \\(\\mathrm{E}\\) 步: 记 \\(\\theta^{(i)}\\) 为第 \\(i\\) 次迭代参数 \\(\\theta\\) 的估计值, 在第 \\(i+1\\) 次迭代的 \\(\\mathrm{E}\\) 步, 计算 \\[ \\begin{aligned} Q\\left(\\theta, \\theta^{(i)}\\right) \u0026=E_Z\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\\\\ \u0026=\\sum_Z \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\end{aligned} \\] 这里, \\(P\\left(Z \\mid Y, \\theta^{(i)}\\right)\\) 是在给定观测数据 \\(Y\\) 和当前的参数估计 \\(\\theta^{(i)}\\) 下隐变量数据 \\(Z\\) 的条 件概率分布; (3) \\(\\mathrm{M}\\) 步：求使 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 极大化的 \\(\\theta\\), 确定第 \\(i+1\\) 次迭代的参数的估计值 \\(\\theta^{(i+1)}\\) \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 重复第 (2) 步和第 (3) 步, 直到收敛。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:3:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"Q函数 函数 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 是 EM算法的核心, 称为 \\(Q\\) 函数 ( \\(Q\\) function)。 \\({Q}\\) 函数 : 完全数据 的对数似然函数 \\(\\log P(Y, Z \\mid \\theta)\\) 关于在给定观测数 据 \\(Y\\) 和当前参数 \\(\\theta^{(i)}\\) 下对未观测数据 \\(Z\\) 的条件概率分布 \\(P\\left(Z \\mid Y, \\theta^{(i)}\\right)\\) 的期望称为 \\(Q\\) 函数, 即 \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=E_Z\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] = \\sum_Z \\log P(Y,Z\\mid \\theta) P\\left (Z\\mid Y, \\theta^{(i)}\\right) \\] ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:4:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"Jensen不等式 如果f是凸函数，X是随机变量，那么有 \\[ E[f(X)] \\geq f[E(X)] \\] 如果f是凹函数则相反 这个图可以比较清晰的看出这个结论。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:5:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"EM算法的导出 为什么 EM算法能近似实现对观测数据的极大似然估计 呢? 下面通过近似求解观测数据的对数似然函数的极大化问题来导出 EM算法, 由此 可以清楚地看出 EM算法的作用。 我们面对一个含有隐变量的概率模型, 目标是极大化观测数据 (不完全数据) \\(Y\\) 关于参数 \\(\\theta\\) 的对数似然函数, 即极大化 \\[ \\begin{aligned} L(\\theta) \u0026=\\log P(Y \\mid \\theta)=\\log \\sum_Z P(Y, Z \\mid \\theta) \\\\\\\\ \u0026=\\log \\left(\\sum_Z P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right) \\end{aligned} \\] 注意到这一极大化的主要困难是式中有末观测数据并有包含和 (或积分) 的 对数。 事实上, EM算法是通过迭代逐步近似极大化 \\(L(\\theta)\\) 的。假设在第 \\(i\\) 次迭代后 \\(\\theta\\) 的 估计值是 \\(\\theta^{(i)}\\) 。我们希望新估计值 \\(\\theta\\) 能使 \\(L(\\theta)\\) 增加, 即 \\(L(\\theta)\u003eL\\left(\\theta^{(i)}\\right)\\), 并逐步达到极 大值。为此, 考虑两者的差: \\[ L(\\theta)-L\\left(\\theta^{(i)}\\right)=\\log \\left(\\sum_Z P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\] 利用 Jensen 不等式 (Jensen inequality)得到其下界，这里的f即为log函数，是凹函数，则结论与凸函数时的结论是相反的。: \\[ \\begin{aligned} L(\\theta)-L\\left(\\theta^{(i)}\\right) \u0026=\\log \\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\\\\ \u0026 \\geqslant \\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\\\\ \u0026=\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)} \\end{aligned} \\] 令 \\[ B\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)} \\] 则要求： \\[ L(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right) \\] 即函数 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 是 \\(L(\\theta)\\) 的一个下界, 可知, \\[ L\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right) \\] 因此, 任何可以使 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 增大的 \\(\\theta\\), 也可以使 \\(L(\\theta)\\) 增大。这里回顾一下我们最原始的目标，就是为了最大化\\(L(\\theta)\\)，为了使 \\(L(\\theta)\\) 有尽可能大 的增长, 选择 \\(\\theta^{(i+1)}\\) 使 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 达到极大, 即 \\[ \\theta^{(i+1)}=\\arg \\max_\\theta B\\left(\\theta, \\theta^{(i)}\\right) \\] 现在求 \\(\\theta^{(i+1)}\\) 的表达式。省去对 \\(\\theta\\) 的极大化而言是常数的项 \\[ \\begin{aligned} \\theta^{(i+1)} \u0026=\\arg \\max_\\theta\\left(L\\left(\\theta^{(i)}\\right)+\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta\\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta\\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\end{aligned} \\] 这等价于 EM算法的一次迭代, 即求 \\(Q\\) 函数及其极大化。EM算法是通过 不断求解下界的极大化逼近求解对数似然函数极大化的算法。 下图给出 EM算法的直观解释。注意两个曲线的交点就是在\\(\\theta^{(i)}\\) 这里其实就是相当于推导为什么最大化Q函数对应的参数就是当前迭代的最佳参数。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:6:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"高斯混合模型 高斯混合模型是指有如下形式的概率分布模型： \\[ P(y\\mid \\theta ) = \\sum_{k=1}^K\\alpha_k \\Phi(y\\mid \\theta_k) \\] 其中\\(\\alpha_k\\)为系数，\\(\\alpha_k \\geq 0, \\sum_{k=1}^K\\alpha_k=1\\); \\(\\Phi(y\\mid \\theta_k)\\)为高斯密度函数，\\(\\theta_k=(\\mu_k,\\sigma_k^2)\\) \\[ \\Phi(y\\mid \\theta_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp \\left(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2} \\right) \\] 为第k个模型。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"EM算法的应用 明确隐变量, 写出完全数据的对数似然函数 可以设想观测数据 \\(y_j, j=1,2, \\cdots, N\\), 是这样产生的: 首先依概率 \\(\\alpha_k\\) 选择第 \\(k\\) 个高斯分布分模型 \\(\\phi\\left(y \\mid \\theta_k\\right)\\), 然后依第 \\(k\\) 个分模型的概率分布 \\(\\phi\\left(y \\mid \\theta_k\\right)\\) 生成观测数据 \\(y_j\\) 。这时观测数据 \\(y_j, j=1,2, \\cdots, N\\), 是已知的; 反映观测数据 \\(y_j\\) 来自第 \\(k\\) 个分模 型的数据是末知的, \\(k=1,2, \\cdots, K\\), 以隐变量 \\(\\gamma_{j k}\\) 表示, 其定义如下: \\[ \\gamma_{j k}= \\begin{cases}1, \u0026 \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] \\[ j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\] \\(\\gamma_{j k}\\) 是 0-1 随机变量。 有了观测数据 \\(y_j\\) 及末观测数据 \\(\\gamma_{j k}\\), 那么完全数据是 \\[ \\left(y_j, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K}\\right), \\quad j=1,2, \\cdots, N \\] 于是, 可以写出完全数据的似然函数: \\[ \\begin{aligned} P(y, \\gamma \\mid \\theta) \u0026=\\prod_{j=1}^N P\\left(y_j, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K} \\mid \\theta\\right) \\\\\\\\ \u0026=\\prod_{k=1}^K \\prod_{j=1}^N\\left[\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)\\right]^{\\gamma_{j k}} \\\\\\\\ \u0026=\\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N\\left[\\phi\\left(y_j \\mid \\theta_k\\right)\\right]^{\\gamma_{j k}} \\\\\\\\ \u0026=\\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma_k} \\exp \\left(-\\frac{\\left(y_j-\\mu_k\\right)^2}{2 \\sigma_k^2}\\right)\\right]^{\\gamma_{j k}} \\end{aligned} \\] 式中, \\(n_k=\\sum_{j=1}^N \\gamma_{j k}, \\sum_{k=1}^K n_k=N\\) 。 那么, 完全数据的对数似然函数为 \\[ \\log P(y, \\gamma \\mid \\theta)=\\sum_{k=1}^K\\left\\\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\gamma_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right\\} \\] EM 算法的 \\(\\mathrm{E}\\) 步: 确定 \\(Q\\) 函数 \\[ \\begin{aligned} Q\\left(\\theta, \\theta^{(i)}\\right) \u0026=E\\left[\\log P(y, \\gamma \\mid \\theta) \\mid y, \\theta^{(i)}\\right] \\\\\\\\ \u0026=E\\left{\\sum_{k=1}^K\\left{n_k \\log \\alpha_k+\\sum_{j=1}^N \\gamma_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right}\\right} \\\\\\\\ \u0026=\\sum_{k=1}^K\\left\\\\{\\sum_{j=1}^N\\left(E \\gamma_{j k}\\right) \\log \\alpha_k+\\sum_{j=1}^N\\left(E \\gamma_{j k}\\right)\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right} \\end{aligned} \\] 这里需要计算 \\(E\\left(\\gamma_{j k} \\mid y, \\theta\\right)\\), 记为 \\(\\hat{\\gamma}_{j k}\\) 。 \\[ \\begin{aligned} \\hat{\\gamma}_{j k} \u0026=E\\left(\\gamma_{j k} \\mid y, \\theta\\right)=P\\left(\\gamma_{j k}=1 \\mid y, \\theta\\right) \\\\\\\\ \u0026=\\frac{P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] \\(\\hat{\\gamma}_{j k}\\) 是在当前模型参数下第 \\(j\\) 个观测数据来自第 \\(k\\) 个分模型的概率, 称为分模型 \\(k\\) 对 观测数据 \\(y_j\\) 的响应度。 将 \\(\\hat{\\gamma}_{j k}=E \\gamma_{j k}\\) 及 \\(n_k=\\sum_{j=1}^N E \\gamma_{j k}\\) 代入, 即得 \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{k=1}^K\\left\\\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right\\\\\\} \\] 确定 EM 算法的 \\(M\\) 步 迭代的 \\(\\mathrm{M}\\) 步是求函数 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 对 \\(\\theta\\) 的极大值, 即求新一轮迭代的模型参数: \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 用 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 及 \\(\\hat{\\alpha}_k, k=1,2, \\cdots, K\\), 表示 \\(\\theta^{(i+1)}\\) 的各参数。求 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 只需分别对 \\(\\mu_k, \\sigma_k^2\\) 求偏导数并令其为 0 , 即可得到; 求 \\(\\hat{\\alpha}_k\\) 是在 \\(\\sum_{k=1}^K \\alpha_k=1\\) 条件 下求偏导数并令其为 0 得到的。结果如下: \\[ \\begin{gathered} \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left(y_j-\\mu_k\\right)^2}{\\sum_{j=1}^N \\ha","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:1","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"算法应用总结 (高斯混合模型参数估计的EM算法) 输入: 观测数据 \\(y_1, y_2, \\cdots, y_N\\), 高斯混合模型; 输出：高斯混合模型参数。 （1）取参数的初始值开始迭代; (2) \\(\\mathrm{E}\\) 步: 依据当前模型参数, 计算分模型 \\(k\\) 对观测数据 \\(y_j\\) 的响应度 \\[ \\hat{\\gamma}_{j k}=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\] \\(\\mathrm{M}\\) 步：计算新一轮迭代的模型参数 \\[ \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\] \\[ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{jk}(y_j-\\mu_k)^2}{\\sum_{j=1}^N \\hat{\\gamma}_{jk}}, \\quad k= 1,2,\\dots, K \\] \\[ \\hat{\\alpha}_k = \\frac{\\sum_{j=1}^N \\hat{\\gamma}_{jk}}{N} ,\\quad k=1,2,\\dots, K \\] 重复直到收敛。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:2","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"总结 总之来说em算法作为数据挖掘十大算法之一，应用范围十分广泛，它不能看作是一个具体的模型，常常用于模型的求解，比如HMM的学习参数问题等等，是必须要学会的算法之一。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:8:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["NLP"],"content":"这是一种句向量的表示方式，即sentence2vec，实际上是对skip thought的改进， ","date":"2022-10-03","objectID":"/quick-thought/:0:0","tags":["NLP","Quick-Thought"],"title":"Quick-Thought","uri":"/quick-thought/"},{"categories":["算法题"],"content":"分发饼干 https://leetcode-cn.com/problems/assign-cookies/ class Solution: def findContentChildren(g, s) -\u003e int: g = sorted(g) s = sorted(s) n = 0 for i in range(len(s)): if g[n] \u003c= s[i]: n += 1 if n == len(g): return n return n 贪心算法的题目，考虑局部最优 ","date":"2022-10-03","objectID":"/%E5%88%86%E5%8F%91%E9%A5%BC%E5%B9%B2/:0:0","tags":["算法题","分发饼干"],"title":"分发饼干","uri":"/%E5%88%86%E5%8F%91%E9%A5%BC%E5%B9%B2/"},{"categories":["others"],"content":"正则表达式 [abcd]匹配中括号里的所有字符 [^abcd]匹配除了括号里的所有字符 [A-Za-z]匹配所有字母 [\\s\\S]是匹配所有空白符，包括换行，非空白符，包括换行 [\\w] 匹配字母、数字、下划线。等价于 [A-Za-z0-9_] 匹配有特殊含义的，比如* ^ 等 记得要加上反斜杠进行转义 ？匹配前面的表达式0次或1次 **.匹配除* +匹配前面的表达式1次或多次 *匹配前面的表达式0次或多次 {n}匹配n次，{n,m}最少匹配n次，最多m次，{n,}至少匹配n次 定位符：^ $ 即限定在哪里匹配 （）灵活应用 [.]等价于\\. ","date":"2022-09-28","objectID":"/regex/:0:0","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["others"],"content":"运算优先级 image.png ","date":"2022-09-28","objectID":"/regex/:1:0","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["others"],"content":"零宽度断言（前后预查） ","date":"2022-09-28","objectID":"/regex/:2:0","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["others"],"content":"?=…正先行断言 ?=...表示正先行断言，表示第一部分表达式之后必须跟着?=...定义的表达式。 返回结果只包含满足匹配条件的第一部分表达式。 定义一个正先行断言要使用 ()。在括号内部使用一个问号和等号： (?=...)。 正先行断言的内容写在括号中的等号后面。 例如，表达式 (T|t)he(?=\\sfat) 匹配 The 和 the，在括号中我们又定义了正先行断言 (?=\\sfat) ，即 The 和 the 后面紧跟着 (空格)fat。 ","date":"2022-09-28","objectID":"/regex/:2:1","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["others"],"content":"?!… 负先行断言 负先行断言 ?! 用于筛选所有匹配结果，筛选条件为 其后不跟随着断言中定义的格式。 正先行断言 定义和 负先行断言 正好相反。 表达式 (T|t)he(?!\\sfat) 匹配 The 和 the，且其后不跟着 (空格)fat。 ### ?\u003c=… 正后发断言 正后发断言 记作(?\u003c=...) 用于筛选所有匹配结果，筛选条件为 其前跟随着断言中定义的格式。 例如，表达式 (?\u003c=(T|t)he\\s)(fat|mat) 匹配 fat 和 mat，且其前跟着 The 或 the。 ### ?\u003c!… 负后发断言 负后发断言 记作 (?\u003c!...) 用于筛选所有匹配结果，筛选条件为 其前不跟随着断言中定义的格式。 例如，表达式 (?\u003c!(T|t)he\\s)(cat) 匹配 cat，且其前不跟着 The 或 the。 此外还有一种是?:，用于括号匹配中，即non-capturing group， # 实战 ","date":"2022-09-28","objectID":"/regex/:2:2","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["others"],"content":"Matching a decimal numbers 来源:https://regexone.com/problem/matching_decimal_numbers 意思就是要匹配Match的还要避开Skip的 具体实现： ^-?\\d+(,\\d+)*(\\.\\d+(e\\d+)?)?$ 解释： ​ 开头匹配负号，其实这里可以变成[\\\\-\\\\+]? 以防出现正号 ​ 然后匹配带逗号的数字，匹配0次或多次 ​ 然后匹配带小数点的数字，匹配0次或1次，因为小数点最多出现一次 ​ 显然后面跟着匹配指数，显然指数也最多出现一次。 ","date":"2022-09-28","objectID":"/regex/:3:0","tags":["others","regex"],"title":"regex","uri":"/regex/"},{"categories":["pandas"],"content":"pandas实践1 在读取数据之前，我修改了表格里面的表头，以便程序的编写。 先从 excel 读取数据,然后看看 shape 了解行数列数,然后调用 info 方法， 看看有没有缺失值，发现并没有缺失值，但题目里说了可能有重复或者格式 不对的数据，因为最主要的是学号,一般学号的长度都是 12 个数字，所以筛 选出不是 12 位数的 data[data['studentid'].apply(lambda x:len(x)!=12)] 考虑到可能出现中文的情况，先尝试转化为整数试试 data[‘studentid’] = data[‘studentid’].astype(“int64”) 发现报错了，然后就看见了那个学号是’忘记了’的 最后修改成了 data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')] 将这些数据删除 data = data.drop(data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')].index) 考虑到有重复，重复的两个因素就是姓名和学号，因此进行去重处理 data.drop_duplicates(subset=['name','studentid'],keep='first',inplace=Tru e) 此外，对专业的处理，将无用的 xx-x 去掉即可，这里考虑到了正则表达式 data['class'] = data['class'].apply(lambda s:re.sub(r\"[\\s*\\d*\\-*\\—*\\ － *\\–*\\/*]?\",'',s)) 因为各种各样的-负号千奇百怪，我只能一次次修改后然后统计一下即调用 data[‘class’].value_counts() 有没有没有处理到的，然后把那个-符号加进去 还发现了有/号。 最后就成了那样，写到这里我有了更好的想法，和下面的 某两个个例有关系。 然后就是那个 maps 表，都简化为简称，对称呼进行统一，用了 apply 方 法 再统计一下，发现了两个专业后面带名字的学长学姐，因为就两个，就把他 们加到 maps 里面了，其实也可以判断名字是否在专业里面，如果在就替换 为空吧。 之后就差不多可以了，数据预处理完毕，按照要求保存即可。 #数据预处理文件 import pandas as pd import re data = pd.read_excel(\"附件1.xlsx\") #去除错误数据 data = data.drop(data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')].index) #去重 data.drop_duplicates(subset=['name','studentid'], keep='first', inplace=True) data['class'] = data['class'].apply(lambda s:re.sub(r\"[\\s*\\d*\\-*\\—*\\－*\\–*\\/*]?\", '', s)) maps = { '智能科学':'智科', '云计算':'云计', '应用统计学':'统计', '信息与计算科学':'信计', '智能科学与技术':'智科', '应用统计':'统计', '软件工程':'软工', '信息与计算科学（云计算）':'信计', '光电信息与科学':'光电', '信计（云计算）':'信计', '光电信息科学与工程':'光电', '数据科学':'大数据', '智科科学':'智科', '信计学长':'信计', '信计学姐':'信计', '统计学':'统计', '信息计算与科学':'信计', '信计与计算科学':'信计' } def replaces(clas): if clas in maps.keys(): return maps[clas] else: return clas data['class'] = data['class'].apply(replaces) res = pd.DataFrame() res['账号'] = '21aidc' + data['studentid'] res['姓名'] = data['name'] res['密码'] = res['账号'] res['专业'] = data['class'] res.to_excel(\"result.xlsx\", index=False,encoding='utf-8') ","date":"2022-09-20","objectID":"/aidc%E6%B5%8B%E8%AF%95/:0:0","tags":["pandas","task1"],"title":"aidc测试","uri":"/aidc%E6%B5%8B%E8%AF%95/"},{"categories":["Deep Learning","优化算法"],"content":"Adam算法 ","date":"2022-09-11","objectID":"/adam/:0:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"背景 作为机器学习的初学者必然会接触梯度下降算法以及SGD，基本上形式如下： \\[ \\theta_t = \\theta_{t-1} - \\alpha \\;g(\\theta) \\] 其中\\(\\alpha\\)为学习率，\\(g(\\theta)\\)为梯度。 简单来说，Adam = Momentum + Adaptive Learning Rate Momentum实际上就用过去梯度的moving average来更新参数。 ","date":"2022-09-11","objectID":"/adam/:1:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"moment(矩) 矩在数学中的定义，一阶矩(first moment)就是样本的均值(mean), 二阶矩就是方差（variance）。 ## 滑动平均 滑动平均(exponential moving average)，或者叫做指数加权平均(exponentially weighted moving average)，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。在时间序列预测中也常用。 变量 \\(v\\) 在 \\(t\\) 时刻记为 \\(v_{t} ，\\text{可以理解为0到t时刻的平均值} 。\\quad \\theta_{t}\\) 为变量 \\(v\\) 在 \\(t\\) 时刻的取值，即在不使用滑动平均模型时 \\(v_{t}=\\theta_{t}\\) ，在使用滑动平均模型后， \\(v_{t}\\) 的更新公式如下: \\[ v_{t}=\\beta \\cdot v_{t-1}+(1-\\beta) \\cdot \\theta_{t} \\] 上式中， \\(\\beta \\in[0,1) ， \\beta=0\\) 相当于没有使用滑动平均。 这也是RMSProp和Adam等算法里使用的最重要的思想。通过滑动平均来降低梯度的波动值。 ","date":"2022-09-11","objectID":"/adam/:2:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"SGD-Momentum 带动量的随机梯度下降方法 它的思路就是计算前面梯度的该变量，每次迭代会考虑前面的计算结果。这样如果在某个维度上波动厉害的特征，会由于“momentum”的影响，而抵消波动的方向（因为波动剧烈的维度每次更新的方向是相反的，momentum能抵消这种波动）。使得梯度下降更加的平滑，得到更快的收敛效率。而后续提出的Adagrad，RMSProp以及结合两者优点的Adam算法都考虑了这种“momentum”的思想。 前面求梯度的过程省略了，后面可以这样写： \\[ \\begin{align} \u0026 v_t = \\beta v_{t-1} + (1-\\beta)g_t \\\\\\\\ \u0026 \\theta = \\theta - \\alpha v_t \\end{align} \\] 其中\\(\\alpha\\)为学习率，一般的\\(\\beta\\)为0.9。v就是动量。 所以，SGD + Momentum可以理解为，利用历史权重梯度矩阵 \\(W_{i} l(i\u003ct)\\) 和当前权重梯度矩 阵 \\(W_{t} l\\) 的加权平均和，来更新权重矩阵 \\(W\\) 。由于 \\(\\beta \\in(0,1)\\) ，所以随着 \\(t\\) 的增大和 \\(i\\) 的减 小， \\(\\beta^{t-i}\\) 会减小，历史权重梯度矩阵 \\(W_{i} l(i\u003ct)\\) 会逐渐减小。通俗来讲，会逐渐遗忘越旧的权重梯度矩阵。 ","date":"2022-09-11","objectID":"/adam/:3:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"AdaGrad算法 AdaGrad直接暴力累加平方梯度，这种做法的缺点就是累加的和会持续增长，会导致学习率变小最终变得无穷小，最后将无法获得额外信息。 ","date":"2022-09-11","objectID":"/adam/:4:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"RMSProp算法 RMSProp和Adagrad算法的最大区别就是在于更新累积梯度值 r 的时候RMSProp考虑加入了一个权重系数 ρ 。 它使用了一个梯度平方的滑动平均。其主要思路就是考虑历史的梯度，对于离得近的梯度重点考虑，而距离比较远的梯度则逐渐忽略。注意图中的是内积。 ","date":"2022-09-11","objectID":"/adam/:5:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"Adam 下面看最经典的伪代码： adam算法比起adagrad和RMSProp，不仅加入了一阶和二阶moment的计算。而且加入了bias-correction term。以下将展开分析： ","date":"2022-09-11","objectID":"/adam/:6:0","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"adam的更新率（stepsize) adam算法中最重要的就是每次迭代的迭代率（step size），他决定了adam算法的效率。根据上 文的算法， step size等于: \\(\\Delta_{t}=\\alpha \\cdot \\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\) 1) 当 \\(\\left(1-\\beta_{1}\\right)\u003e\\sqrt{1-\\beta_{2}}\\) 的时候，它的上界满足不等式: \\(\\left|\\Delta_{t}\\right| \\leq \\alpha \\cdot\\left(1-\\beta_{1}\\right) / \\sqrt{1-\\beta_{2}}\\) 2) 否则 \\(\\left|\\Delta_{t}\\right| \\leq \\alpha\\) 1）通常发生在数据很稀疏的时候。当数据密集的时候， stepsize会更小。 3) 当 \\(\\left(1-\\beta_{1}\\right)=\\sqrt{1-\\beta_{2}}\\) 的时候，因为 \\(\\left|\\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\right|\u003c1\\) 所以，也满足条件 2 的 \\(\\left|\\Delta_{t}\\right| \\leq \\alpha\\) 总结以上3个条件，可以近似得出stepsize 满足 \\(\\left|\\Delta_{t}\\right| \\cong \\alpha\\) 这里的 \\(\\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\) 通常也成为信噪比（Signal-to-noise ratio SNR)，并且满足SND越小， stepsize也越小。 ","date":"2022-09-11","objectID":"/adam/:6:1","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法"],"content":"初始化偏差矫正项 原算法中的这两行 \\[ \\begin{aligned} \u0026\\widehat{m}_{t} \\leftarrow m_{t} /\\left(1-\\beta_{1}^{t}\\right) \\\\\\\\ \u0026\\hat{v}_{t} \\leftarrow v_{t} /\\left(1-\\beta_{2}^{t}\\right) \\end{aligned} \\] 称为偏差校正项(bias-correction term),他使用了滑动平均值(EMA: exponential moving average)的思想，例如计算二次moment的 \\(v_{t}=\\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}\\) 可以写成如下的形 式： \\[ v_{t}=\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2} \\] 我们的目的是求得 \\(\\mathbb{E}\\left[v_{t}\\right]\\) (EMA) 和二阶moment \\(\\mathbb{E}\\left[g_{t}^{2}\\right]\\) 之间的关系，推导如下: \\[ \\begin{aligned} \\mathbb{E}\\left[v_{t}\\right] \u0026=\\mathbb{E}\\left[\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2}\\right] \\\\\\\\ \u0026=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i}+\\zeta \\\\\\\\ \u0026=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}^{t}\\right)+\\zeta \\end{aligned} \\] 最后得出 \\(\\mathbb{E}\\left[g_{t}^{2}\\right]=\\frac{\\mathbb{E}\\left[v_{t}\\right]-\\zeta}{\\left(1-\\beta_{2}^{t}\\right)}\\) 通常可以忽略常数 \\(\\zeta\\) 。得出 \\[ \\bar{v_t} = \\frac{v_t}{1-\\beta_2^t} \\] 综上所述，Adam 优化器可以根据历史梯度的震荡情况和过滤震荡后的真实历史梯度对变量进行更新 ","date":"2022-09-11","objectID":"/adam/:6:2","tags":["Deep Learning","优化算法","Adam算法"],"title":"Adam算法","uri":"/adam/"},{"categories":["sklearn"],"content":"导入包 import numpy as np import pandas as pd ","date":"2022-09-06","objectID":"/logistic-regression/:1:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["sklearn"],"content":"导入数据 data = pd.read_csv(\"./datasets/Social_Network_Ads.csv\") data.head() User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 X = data.iloc[:,[2,3]].values Y = data.iloc[:,4].values ","date":"2022-09-06","objectID":"/logistic-regression/:2:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["sklearn"],"content":"交叉验证 from sklearn.model_selection import train_test_split X_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=1/4,random_state=0) ","date":"2022-09-06","objectID":"/logistic-regression/:3:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["sklearn"],"content":"标准化 from sklearn.preprocessing import StandardScaler standardscaler = StandardScaler() X_train = standardscaler.fit_transform(X_train) X_test = standardscaler.transform(X_test) ","date":"2022-09-06","objectID":"/logistic-regression/:4:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["sklearn"],"content":"训练模型 from sklearn.linear_model import LogisticRegression model = LogisticRegression() model.fit(X_train,Y_train) LogisticRegression() ","date":"2022-09-06","objectID":"/logistic-regression/:5:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["sklearn"],"content":"模型得分 model.score(X_test,Y_test) 0.7933333333333333 ","date":"2022-09-06","objectID":"/logistic-regression/:6:0","tags":["sklearn","Logistic Regression"],"title":"Logistic Regression","uri":"/logistic-regression/"},{"categories":["Mathematical Modeling"],"content":"皮尔逊相关系数 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:0:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"假设检验 步骤与概率论中假设检验的步骤一样。主要是统计量的构造 \\[ t = r\\sqrt{\\frac{n-2}{1-r^2}} \\] t为服从自由度为n-2的t分布 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:1:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"假设检验的条件 实验数据通常假设是成对的来自于正态分布的总体 实验数据之间差距不能太大 每组样本之间是独立抽样的。 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:2:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"对数据进行正态分布检验 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:3:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"JB检验 斯皮尔曼相关系数 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:3:1","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"公式 \\[ r_s = 1 - \\frac{6\\sum_{i=1}^nd_i^2}{n(n^2-1)} \\] 其中\\(d_i\\)为X,Y之间的等级差 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:4:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"假设检验 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:5:0","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Mathematical Modeling"],"content":"小样本 直接查表就可以 ### 大样本 构造统计量 \\[ r_s\\sqrt{n-1} \\] 属于标准正态分布 两种相关系数的选择 连续数据，正态分布，线性关系，就用皮尔逊相关系数 三个条件有一个不满足就用斯皮尔曼相关系数 两个定序数据之间也用斯皮尔曼相关系数。 ","date":"2022-09-02","objectID":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/:5:1","tags":["Mathematical Modeling","相关系数"],"title":"相关系数","uri":"/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"},{"categories":["Machine Learning","性能指标"],"content":"精确率和召回率 ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"混淆矩阵 True Positive(真正, TP)：将正类预测为正类数. True Negative(真负 , TN)：将负类预测为负类数. False Positive(假正, FP)：将负类预测为正类数 False Negative(假负 , FN)：将正类预测为负类数 ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:1","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"精确率 \\[ P = \\frac{TP}{TP+FP} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:2","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"准确率 \\[ ACC = \\frac{TP+TN}{TP+TN+FP+FN} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:2:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"召回率 \\[ R = \\frac{TP}{TP+FN} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:3:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"F1 \\[ \\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:4:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"区别 精确率（查准率）：在所有预测为正的样本中（分母），真正为正的有多少（分子）。 召回率（查全率）：在所有实际为正的样本中（分母），成功预测出来的有多少（分子） img img ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:5:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","分类算法"],"content":"参考：https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html 《机器学习》周志华 决策树 决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。这样，结点说法就与模块说法对应上了，理解就好。 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"步骤 1.特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。 特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。 (1).香农熵 在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)，这个名字来源于信息论之父克劳德·香农。 如果看不明白什么是信息增益和熵，请不要着急，因为他们自诞生的那一天起，就注定会令世人十分费解。克劳德·香农写完信息论之后，约翰·冯·诺依曼建议使用”熵”这个术语，因为大家都不知道它是什么意思。 熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为 ： \\[ l(x_i) = -log_{2}p(x_i) \\] 其中p(xi)是选择该分类的概率。有人可能会问，信息为啥这样定义啊？答曰：前辈得出的结论。这就跟1+1等于2一样，记住并且会用即可。上述式中的对数以2为底，也可以e为底(自然对数)。 通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到： \\[ H = -\\sum_{i=1}^np(x_i)log_2p(x_i) \\] 期中n是分类的数目。熵越大，随机变量的不确定性就越大。 当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, = 1,2,3,.. .,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为 ： \\[ H(D) = -\\sum_{k=1}^K \\frac{|c_k|}{|D|}log_2\\frac{|c_k|}{|D|} \\] 根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为： \\[ H(D) = -\\frac{9}{15}log_2\\frac{9}{15} - \\frac{6}{15}log_2\\frac{6}{15} = 0.971 \\] (2)信息增益 在上面，我们已经说过，如何选择特征，需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。 在讲解信息增益定义之前，我们还需要明确一个概念，条件熵。 熵我们知道是什么，条件熵又是个什么鬼？条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望： \\[ H(Y|X) = \\sum_{i=1}^np_iH(Y|X = x_i), \\\\\\\\ p_i = P(X = x_i) \\] 明确了条件熵和经验条件熵的概念。接下来，让我们说说信息增益。前面也提到了，信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即： \\[ g(D,A) = H(D) - H(D|A) \\] 一般地，熵H(D)与条件熵H(D|A)之差称为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 设特征A有n个不同的取值{a1,a2,···,an}，根据特征A的取值将D划分为n个子集{D1,D2，···,Dn}，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik = Di ∩ Ck，|Dik|为Dik的样本个数。于是经验条件熵的公式可以些为 \\[ H(D|A) = \\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|} \\] 说了这么多概念性的东西，没有听懂也没有关系，举几个例子，再回来看一下概念，就懂了。 以贷款申请样本数据表为例进行说明。看下年龄这一列的数据，也就是特征A1，一共有三个类别，分别是：青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集出现的概率是十五分之五，也就是三分之一。同理，年龄是中年和老年的数据在训练数据集出现的概率也都是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在五个数据中，只有两个数据显示拿到了最终的贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别为五分之三、五分之四。所以计算年龄的信息增益，过程如下： 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 同理，计算其余特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为： 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 最后，比较特征的信息增益，由于特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征。 由于特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。它将训练集D划分为两个子集D1(A3取值为”是”)和D2(A3取值为”否”)。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。 对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益： 机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜 根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。由于A2有两个可能取值，从这一结点引出两个子结点：一个对应”是”(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为”是”；另一个是对应”否”(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为”否”。 这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示。 机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:1:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"总结 我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。 构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。 决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 决策树的一些优点： 易于理解和解释。决策树可以可视化。 几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。 使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。 可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。 可以处理多值输出变量问题。 使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。 即使对真实模型来说，假设无效的情况下，也可以较好的适用。 决策树的一些缺点： 决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。 概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems。 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:2:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"代码 import numpy as np import math def equalNums(label_list, label): \"\"\" 函数说明： 计算标记集中某个标记的数量 Parameters： label_list - 标记集 label - 某个标记 Returns： num - 某个标记的数量 \"\"\" return np.sum(label_list == label) def calcShannonEnt(label_list): \"\"\" 函数说明： 计算信息熵 对应公式 Ent(D) = -∑ Pk*log2(Pk) k=1..len(label_set) Parameters： label_list - 标记集 Returns： shannonEnt - 当前标记集的信息熵 \"\"\" label_set = set(label_list) len_label_list = label_list.size shannonEnt = 0.0 for label in label_set: prob = equalNums(label_list, label)/len_label_list shannonEnt -= prob * math.log2(prob) return shannonEnt def conditionnalEntropy(feature_list, label_list): \"\"\" 函数说明： 计算条件信息熵，对应信息增益公式中的被减项 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： entropy - 条件信息熵 \"\"\" feature_list = np.asarray(feature_list) label_list = np.asarray(label_list) feature_set = set(feature_list) entropy = 0.0 for feat in feature_set: pro = equalNums(feature_list, feat)/feature_list.size entropy += pro * calcShannonEnt(label_list[feature_list == feat]) return entropy def calcInfoGain(feature_list, label_list): \"\"\" 函数说明： 计算信息增益 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： 当前属性的信息增益 \"\"\" return calcShannonEnt(label_list) - conditionnalEntropy(feature_list, label_list) def splitDataSet(sample_list, label_list, axis, value): \"\"\" 函数说明： 决策树在选好当前最优划分属性之后划分样本集 依据value选择对应样例，并去除第axis维属性 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： return_sample_list, return_label_list \"\"\" # sample_list[sample_list[...,axis] == value] 利用了numpy数组的布尔索引 filtered_sample_list = sample_list[sample_list[...,axis] == value] return_label_list = label_list[sample_list[...,axis] == value] # np.hstack 将数组横向拼接，也就是去除第axis维属性 return_sample_list = np.hstack((filtered_sample_list[...,:axis], filtered_sample_list[...,axis+1:])) return return_sample_list, return_label_list def chooseBestFeatureToSplit(sample_list, label_list): \"\"\" 函数说明： 选取最优划分属性 Parameters： sample_list - 样本集 label_list - 标记集 Returns： bestFeat_index - 最优划分属性的索引值 \"\"\" numFeatures = sample_list.shape[1] bestInfoGain = 0 bestFeat_index = -1 for i in range(numFeatures): infoGain = calcInfoGain(sample_list[..., i], label_list) if infoGain \u003e bestInfoGain: bestInfoGain = infoGain bestFeat_index = i return bestFeat_index def createTree(sample_list, label_list, attr_list_copy): \"\"\" 函数说明： 生成决策树 Parameters： sample_list - 样本集 label_list - 标记集 attr_list_copy - 属性集（之所以加copy是为了删属性的时候是在副本上，防止递归出错） Returns： myTree - 最终的决策树 \"\"\" # attr_list 有del操作，不用副本的话递归会出错 attr_list = attr_list_copy.copy() if len(set(label_list)) == 1: # 如果只有一种标记，直接返回标记 return label_list[0] elif sample_list.size == 0: # 如果所有属性都被遍历，返回最多的标记 return voteLabel(label_list) # bestFeat_index 最优划分属性的索引值 # bestAttr 最优划分属性对应的名字 bestFeat_index = chooseBestFeatureToSplit(sample_list, label_list) bestAttr = attr_list[bestFeat_index] myTree = {bestAttr: {}} del(attr_list[bestFeat_index]) feat_set = set(sample_list[..., bestFeat_index]) # 依据最优划分属性进行划分，并向下递归 for feat in feat_set: return_sample_list, return_label_list = splitDataSet(sample_list, label_list, bestFeat_index, feat) myTree[bestAttr][feat] = createTree(return_sample_list, return_label_list, attr_list) return myTree def voteLabel(label_list): \"\"\" 函数说明： 这个函数是用在遍历完所有特征时，返回最多的类别 Parameters： label_list: 标记列表 Returns： 数量最多的标记 \"\"\" # unique_label_list 是label_list中标记种类列表 # label_num 是unique_label_list对应的数量列表 unique_label_list = list(set(label_list)) label_num_list = [] for label in unique_label_list: label_num_list.append(equalNums(label_list, label)) # label_num.index(max(label_num))是label_num数组中最大值的下标 return unique_label_list[label_num_list.index(max(label_num_list))] def classify(decisionTree, testVec, attr_list): \"\"\" 函数说明： 对tesVec进行分类 Parameters： decisionTree - 决策树 attr_list - 属性名列表 testVec - 测试向量 Returns： label - 预测的标记 \"\"\" feature = list(decisionTree.keys())[0] # feature为决策树的根节点 feature_dict = decisionTree[feature] # feature_dict为根节点下的子树 feature_index = attr_list.index(feature) # feature_index为featur","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:3:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"分类算法 主要区分一下生成模型和判别模型，首先要知道生成模型和判别模型都属于监督学习，即样本有其对应的标签的。还有一个概念就是硬分类和软分类，简单理解就是硬分类是直接分出类别，比如线性判别分析、感知机。而软分类是计算出概率，根据概率来得到类别，生成模型和判别模型都是软分类。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:0:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"生成模型 学习得到联合概率分布\\(P(x,y)\\)，即特征x与标签y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。通俗的说就是如果有k类，就学习k个概率密度分布，对于样本，算出在每个概率密度分布下的概率，哪个概率大就属于哪一类。 生成模型要求的数据量比较大，能够更好地估计概率密度。 ## 判别模型 学习得到条件概率分布\\(P(y|x)\\)，即在特征x出现的情况下标记y出现的概率。 判别模型对样本的要求没有那么多。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:1:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"理解 无论是生成还是判别模型都是来求有监督模型的，目的是通过分类函数或者条件概率函数进行数据分类。 算出属于正负样本的概率再互相对比的就是生成模型，直接得到结果概率的就是判别模型，生成模型得到分布，判别模型得到最优划分。 生成模型可以得到判别模型，反之不成立。 生成模型是求联合概率分布，判别模型是求条件概率分布。 生成方法的学习收敛速度更快，当样本容量增加的时候，学到的模型可以更快的收敛于真实模型。 判别学习不能反映训练数据本身的特性，但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异，直接面对预测，往往学习的准确率高于生成模型。 简单的说，生成模型是从大量的数据中找规律，属于统计学习；而判别模型只关心不同类型的数据的差别，利用差别来分类。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"生成式模型： 朴素贝叶斯 混合高斯模型 隐马尔科夫模型(HMM) 贝叶斯网络 Sigmoid Belief Networks 马尔科夫随机场(Markov Random Fields) 深度信念网络(DBN) ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:1","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"判别式模型 K近邻(KNN) 线性回归(Linear Regression) 逻辑回归(Logistic Regression) 神经网络(NN) 支持向量机(SVM) 高斯过程(Gaussian Process) 条件随机场(CRF) CART(Classification and Regression Tree) ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:2","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Mathematical Modeling"],"content":"灰色关联分析进行系统分析 灰色关联分析主要用于数据量较小，即样本较少的情况，别的情况使用回归分析等常规方法就可以。 ","date":"2022-08-09","objectID":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/:0:0","tags":["Mathematical Modeling","灰色关联分析"],"title":"灰色关联分析","uri":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"步骤 1.画统计图 画图后配上简单的分析。 2.确定分析数列 (1)母序列（又叫参考数列、母指标）：能反映系统行为特征的数据序列。类似于因变量Y (2)子序列（又叫比较数列、子指标）：影响系统行为的因素组成的数据序列。类似于自变量X 3.对变量进行预处理（去量纲、缩小变量范围简化计算） 对母序列、子序列中的每个指标进行预处理，先求出每个指标的均值，然后用每个元素除以这个均值 4.计算子序列中各个指标与母序列的关联系数 记\\(a=min min\\mid x_0(k) - x_i(k) \\mid\\)为两极最小差 \\(b=maxmax\\mid x_0(k) - x_i(k) \\mid\\)为两极最大差 定义 \\[ y(x_0(k),x_i(k)) = \\frac{a+\\rho b}{\\mid x_0(k)-x_i(k) \\mid+\\rho b} \\\\\\\\ \\rho为分辨系数，一般取0.5 \\] 5.定义\\(y(x_0,x_i)=\\frac{1}{n}\\sum_{k=1}^ny(x_0(k),x_i(k))\\)为 \\(x_0\\)与\\(x_i\\)的灰色关联度 6.比较灰色关联度观察母序列受哪个指标的影响最大 如果有多个母序列的话则分别计算灰色关联度。 灰色关联分析进行综合评价 ","date":"2022-08-09","objectID":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/:1:0","tags":["Mathematical Modeling","灰色关联分析"],"title":"灰色关联分析","uri":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"步骤 1.对指标进行正向化处理 2.对正向化后的矩阵进行预处理 3.将预处理后的矩阵每一行取出最大值构成母序列（虚构的） 4.计算各个指标与母序列的灰色关联度：\\(r_1,r_2,\\cdots,r_m\\) 5.计算各个指标的权重：\\(w_1=\\frac{r_1}{r_1+r_2+\\cdots+r_m},w_2=\\frac{r_2}{r_1+r_2+\\cdots+r_m},\\cdots,w_m=\\frac{r_m}{r_1+r_2+\\cdots+r_m}\\) 6.第k个评价对象的得分：\\(S_k=\\sum_{i=1}^{m}z_{ki} \\sdot w_i\\) 7.对得分进行归一化：\\(S_1^{'}=\\frac{S_1}{S_1+S_2+\\cdots+S_n},S_2^{'}=\\frac{S_2}{S_1+S_2+\\cdots+S_n},\\cdots,S_n^{'}=\\frac{S_n}{S_1+S_2+\\cdots+S_n}\\) ","date":"2022-08-09","objectID":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/:2:0","tags":["Mathematical Modeling","灰色关联分析"],"title":"灰色关联分析","uri":"/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"},{"categories":["others"],"content":"一些我常用的学习的网站和书籍 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:0:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"编程前的准备 在编程之前需要哪些准备，这个讲座讲了我们学习编程缺失的课程 大学里的计算机课程通常专注于讲授从操作系统到机器学习这些学院派的课程或主题，而对于如何精通工具这一主题则往往会留给学生自行探索。在这个系列课程中，我们讲授命令行、强大的文本编辑器的使用、使用版本控制系统提供的多种特性等等。学生在他们受教育阶段就会和这些工具朝夕相处（在他们的职业生涯中更是这样）。 因此，花时间打磨使用这些工具的能力并能够最终熟练地、流畅地使用它们是非常有必要的。 https://missing-semester-cn.github.io/ ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:1:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"GIT 编程也需要团队合作，如何高效的进行团队开发，这都是我们要面临的问题。目前最主流的工具就是git。这个教程先学习在本地对仓库进行操作，最后学习如何进行远程仓库操作。而且演示非常直观，学习乐趣很高。 https://learngitbranching.js.org/?locale=zh_CN ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:2:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"SQL 学习SQL最快的方式就是多练习，这个教程就是用练习的方式来学习SQL，难度逐渐增大，循序渐进，很容易沉浸其中，效果很好。 https://sqlzoo.net/ ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:3:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"REGEX(正则表达式) 正则表达式是很好的工具，熟练运行可以简化代码，让程序效率更高，当然也是一种爬虫的方法。这个网站前面是教程，后面就是练习，让你自己解决问题，但是感觉练习比较简单，实际问题要比这复杂，这里就需要正则测试网站来验证自己的正则表达式是否匹配。 https://regexone.com/ ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:4:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"SPIDER(爬虫) 简单的爬虫已经可以满足大部分的需求了，学习起来也比较简单。这个网站感觉覆盖了基础和进阶，可以来入门爬虫 https://python3webspider.cuiqingcai.com/ ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:5:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"MARKDOWN https://www.zybuluo.com/codeep/note/163962 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:6:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"PYTHON基础 PYTHON网上的教程很多，优秀的博客也有很多，像廖雪峰的博客，不过覆盖面比较广，深度比较浅，可以提前了解一些进阶技术。这里推荐一本书，叫《python编程：从入门到实践》，浅显易懂，例子也很有趣，后面实践的例子涉及到了游戏开发、数据可视化、网站开发等。 https://github.com/vllbc/My_Learn/blob/master/%E7%94%B5%E5%AD%90%E4%B9%A6/Python%E7%BC%96%E7%A8%8B%EF%BC%9A%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5_%E8%B6%85%E6%B8%85%E7%89%88.pdf 此外多说一句，学习阶段不要屯太多网站或者书籍，或者是光收藏不看，不管是什么教程，坚持看下去学下去都可以很好的入门，所以我建议不管学习什么都只需要一种学习资料，因此上面的我只推荐了一个网站。 当然官方的教程也是很好的，不过更推荐看官方的标准库教程 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:7:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"PYTHON进阶 很多人觉得PYTHON很简单，当然入门很简单，没有特别难的语法，也没有特别难理解的概念。但仅仅入门是没有办法进行项目开发的，不了解设计模式，就没办法写出可维护性高的代码，不学习函数式，写出的代码可读性很低，另外多线程、协程、装饰器、魔法方法等不仅仅可以解决问题，还可以写出高效并且优美的代码。这里推荐一些书籍和网站。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"一、PYTHON工匠 我一直觉得编程某种意义上是一门『手艺』，因为优雅而高效的代码，就如同完美的手工艺品一样让人赏心悦目。 在雕琢代码的过程中，有大工程：比如应该用什么架构、哪种设计模式。也有更多的小细节，比如何时使用异常（Exceptions）、或怎么给变量起名。那些真正优秀的代码，正是由无数优秀的细节造就的。 『Python 工匠』这个系列文章，是我的一次小小尝试。它专注于分享 Python 编程中的一些偏 『小』 的东西。希望能够帮到每一位编程路上的匠人。 https://github.com/piglei/one-python-craftsman 我喜欢的教程之一，很多实用的技巧，还可以了解一些设计模式。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:1","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"二、PYTHON COOKBOOK https://python3-cookbook.readthedocs.io/zh_CN/latest/index.html 看完大为震惊的书，跟着练习了一段时间后学会了很多奇技淫巧，我感觉内容覆盖了一个高级python工程师必须具备的技能。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:2","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"三、FLUENT PYTHON 现在还未完全搞懂的书，内容很丰富，难度也很大，可以当作参考书使用，完全理解难度很大。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:3","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"四、PYTHON最佳实践指南 https://pythonguidecn.readthedocs.io/zh/latest/ 就像它的名字，当你学完了基础不知道该怎么做的时候不妨来看看这个网站。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:4","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"五、PYTHON设计模式 https://github.com/faif/python-patterns 介绍了PYTHON的几种设计模式的具体实现，可以很大的提高编程水平 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:5","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"六、WTF PYTHON 就像它的名字一样，就是总结了学习PYTHON过程中的各种坑，看完之后有个大致了解就好，不然以后遇到了其中的问题忙的焦头烂额也找不到解决的办法 Python, 是一个设计优美的解释型高级语言, 它提供了很多能让程序员感到舒适的功能特性. 但有的时候, Python 的一些输出结果对于初学者来说似乎并不是那么一目了然. 这个有趣的项目意在收集 Python 中那些难以理解和反人类直觉的例子以及鲜为人知的功能特性, 并尝试讨论这些现象背后真正的原理! 虽然下面的有些例子并不一定会让你觉得 WTFs, 但它们依然有可能会告诉你一些你所不知道的 Python 有趣特性. 我觉得这是一种学习编程语言内部原理的好办法, 而且我相信你也会从中获得乐趣! 如果您是一位经验比较丰富的 Python 程序员, 你可以尝试挑战看是否能一次就找到例子的正确答案. 你可能对其中的一些例子已经比较熟悉了, 那这也许能唤起你当年踩这些坑时的甜蜜回忆 😅 https://github.com/leisurelicht/wtfpython-cn ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:8:6","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"FLASK 这只是简单的入门，可以简单自己实现一个网站，看着自己跟着实现了个网站很有成就感，当然也有一些拓展功能让你自己实现，如果想更加深入学习的话可以买本作者的实体书，书很厚，不过已经在我这里吃灰了。 https://read.helloflask.com/ ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:9:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"VIM 如果你也喜欢敲键盘而不喜欢时不时去操作鼠标，那么VIM就是你写代码的最佳选择，使用它你可以完全摆脱鼠标的束缚，不管是光标移动，复制粘贴，甚至是宏 都可以摆脱鼠标的束缚。 其中作者提的”AHA moment”我觉得也很有趣，简单的理解就是用刚学到的知识自主的完成较为复杂的操作。以VIM而言 当你学会了gU是大写操作 c是修改 ciw是修改光标所在的单词，“AHA moment”就是通过gUiw大写某个单词，而且完全是自主去实现的。 https://github.com/wsdjeg/Learn-Vim_zh_cn ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:10:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["others"],"content":"其它有趣的网站推荐 http://www.pythonchallenge.com/ ：PYTHON挑战关卡，考验知识面和观察力，看看你可以闯到第几关吧！ https://pytorch.org/docs/stable/index.html ：PYTORCH的官方教程，还可以用作参考和查询函数。 https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets ：SKLEARN的API查询网站。 https://vllbc.top/ ：我的个人网站。 ","date":"2022-08-08","objectID":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/:11:0","tags":["others","others"],"title":"一些学习资料","uri":"/%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"},{"categories":["NLP"],"content":"BM25算法 BM25算法，通常用来作搜索相关性平分。一句话概况其主要思想：对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，计算每个语素qi与D的相关性得分，最后，将qi相对于D的相关性得分进行加权求和，从而得到Query与D的相关性得分。 ","date":"2022-08-07","objectID":"/bm25/:0:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"原理 BM25一般公式如下： \\[ score(Q,d) = \\sum_{i}^nW_iR(q_i, d) \\] 其中Q表示为Query，\\(q_i\\)表示Q解析后的一个语素(对中文而言，我们可以把对Query的分词作为语素分析，每个词看成语素)。d表示一个搜索结果文档，\\(W_i\\)表示语素\\(q_i\\)的权重，\\(R(q_i, d)\\)表示语素\\(q_i\\)与文档d的相关性得分。 ","date":"2022-08-07","objectID":"/bm25/:1:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"权重 下面我们来看如何定义Wi。判断一个词与一个文档的相关性的权重，方法有多种，较常用的是IDF。这里以IDF为例，公式如下： \\[ IDF(q_i) = \\log \\frac{N-n(q_i)+0.5}{n(q_i)+0.5} \\] 其中，N为索引中的全部文档数，n(qi)为包含了qi的文档数。 根据IDF的定义可以看出，对于给定的文档集合，包含了qi的文档数越多，qi的权重则越低。也就是说，当很多文档都包含了qi时，qi的区分度就不高，因此使用qi来判断相关性时的重要度就较低。 ","date":"2022-08-07","objectID":"/bm25/:2:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"相关性得分 下面定义相关性得分\\(R(q_i,d)\\) 首先来看BM25中相关性得分的一般形式： 其中\\(k_1, k_2, b\\)为调节因子，根据经验设置，一般\\(k_1=2,b=0.75\\) ，\\(f_i\\)为\\(q_i\\)在文档d中出现的频率，\\(qf_i\\)为\\(q_i\\)在Query中出现的频率，dl为文档d的长度，avgdl为所有文档的平均长度，绝大部分情况下\\(q_i\\)在Query中只会出现一次，因此 \\(qf_i=1\\)，所以可以简化为： 从K的定义中可以看到，参数b的作用是调整文档长度对相关性影响的大小。b越大，文档长度的对相关性得分的影响越大，反之越小。而文档的相对长度越长，K值将越大，则相关性得分会越小。这可以理解为，当文档较长时，包含qi的机会越大，因此，同等fi的情况下，长文档与qi的相关性应该比短文档与qi的相关性弱。 综上，BM25算法的相关性得分公式可总结为： 从BM25的公式可以看到，通过使用不同的语素分析方法、语素权重判定方法，以及语素与文档的相关性判定方法，我们可以衍生出不同的搜索相关性得分计算方法，这就为我们设计算法提供了较大的灵活性。 ","date":"2022-08-07","objectID":"/bm25/:3:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"代码 import math import jieba import re text = ''' 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。 因此，这一领域的研究将涉及自然语言，即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。 自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的计算机系统， 特别是其中的软件系统。因而它是计算机科学的一部分。 ''' class BM25(object): def __init__(self, docs): self.D = len(docs) # doc个数 self.avgdl = sum([len(doc)+0.0 for doc in docs]) / self.D # 每篇平均长度 self.docs = docs self.f = [] # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数 self.df = {} # 存储每个词及出现了该词的文档数量(count) self.idf = {} # 存储每个词的idf值，当作权重 self.k1 = 1.5 self.b = 0.75 self.init() def init(self): for doc in self.docs: tmp = {} for word in doc: tmp[word] = tmp.get(word, 0) + 1 # 存储每个文档中每个词的出现次数（也可以用defaultdict) self.f.append(tmp) # idx为索引，f[idx]为一个dict，dict存储着第idx+1个文档中每个词的出现次数,idx代表第几个文档。 for k in tmp.keys(): # 如果词k出现在了当前文档中，则df[k]即文档数量加1 self.df[k] = self.df.get(k, 0) + 1 for k, v in self.df.items(): self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5) # 计算idf def sim(self, query, index): # 与单个文档的相似度 score = 0 for word in query: if word not in self.f[index]: continue d = len(self.docs[index]) # 当前文档的长度 score += (self.idf[word]*(self.f[index][word]/d)*(self.k1+1) / ((self.f[index][word]/d)+self.k1*(1-self.b+self.b*d / self.avgdl))) return score def simall(self, query): scores = [] for index in range(self.D): score = self.sim(query, index) scores.append(score) return scores def get_sentences(doc): line_break = re.compile('[\\r\\n]') # 以换行符分割 delimiter = re.compile('[，。？！；]') # 以中文标点符号分割 sentences = [] for line in line_break.split(doc): line = line.strip() if not line: continue for sent in delimiter.split(line): sent = sent.strip() if not sent: continue sentences.append(sent) return sentences if __name__ == '__main__': sents = get_sentences(text) print(sents) doc = [] for sent in sents: words = list(jieba.cut(sent)) doc.append(words) # print(doc) s = BM25(doc) # print(s.f) # print(s.df) # print(s.idf) print(s.simall(['自然语言', '计算机科学', '领域', '人工智能', '领域'])) ","date":"2022-08-07","objectID":"/bm25/:4:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["信息检索","IR导论笔记"],"content":"信息检索就是从大规模非结构化数据（通常是文本）的集合（通常保存在计算机上）中找出满足用户信息需求的资料（通常是文档）的过程。 布尔索引顾名思义就是利用布尔操作对文档进行索引，对于检索任务来说，要求的就是对于一个query，检索出与其相关的文档，首先肯定的是文档中要出现query中的词项才有可能有关，易得的一个想法就是建立词项-文档关联矩阵，行向量表示词项在各文档中是否出现，列向量表示某文档中各词项是否出现，得到了这个矩阵，对于query，就可以根据布尔检索得到对应的doc。这是一个很容易想到的方法。 但很明显的一个缺点就是矩阵太太太稀疏了，太占内存了，所以要用别的索引，一个很闻名的索引就是倒排索引，就是对词项进行索引，对词项进行索引，值就是出现该词项的docid，看例子就很好理解了。 构建倒排索引的步骤： 收集文档 文档词条化（将每篇文档转换成词条的列表） 语言学预处理，产生归一化的词条作为词项 构建索引（输入二元组（词项，文档id）的列表，排序、合并、构建索引） ","date":"2022-08-05","objectID":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/:0:0","tags":["信息检索","IR导论笔记","布尔检索"],"title":"布尔检索","uri":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/"},{"categories":["信息检索","IR导论笔记"],"content":"布尔查询 由于倒排记录表是按照id升序排列的，与运算可通过合并实现：使用双指针，若两个指针指向id相同，则输出到结果表中、并同时将两个指针后移一位，否则将较小的id对应指针后移。 可行的查询优化包括： 优先将文档频率（即倒排记录表）小的文档合并（哈夫曼树思想）。 在纯与运算情况下，使用中间结果和后续倒排记录表合并，而不是每次输入两个表输出一个表。 ","date":"2022-08-05","objectID":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/:1:0","tags":["信息检索","IR导论笔记","布尔检索"],"title":"布尔检索","uri":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/"},{"categories":["信息检索","IR导论笔记"],"content":"排序检索模型 而现在大部分的搜索引擎都是使用的排序检索模型，基本很少使用布尔检索了，排序检索模型不关心逻辑运算，而是采用若干词进行自由文本查询，计算查询与各个文档之间的得分，根据得分进行排序，返回得分最高的几个结果，因此在排序检索系统中，计算相关性（打分）和排序算法的重要性就很高。 布尔检索系统通过加入邻近操作符实现更精细的检索，但布尔检索存在的普遍问题是使用AND操作符导致正确率高召回率低、使用OR操作符导致召回率高正确率低，难以找到折中方案。 ","date":"2022-08-05","objectID":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/:2:0","tags":["信息检索","IR导论笔记","布尔检索"],"title":"布尔检索","uri":"/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/"},{"categories":["NLP"],"content":"GPT ","date":"2022-07-27","objectID":"/gpt/:0:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["NLP"],"content":"预训练(从左到右的 Transformer 语言模型) GPT 是一种基于 Transformer 的从左到右的语言模型。该架构是一个 12 层的 Transformer 解码器（没有解码器-编码器）。 ## 模型架构 就是12层的transformer-decoder。其中只使用了transformer模型中的decoder部分，并且把decoder里面的encoder-decoder attention部分去掉了，只保留了masked self-attention，再加上feed-forward部分。再提一句，masked self-attention保证了GPT模型是一个单向的语言模型。 另外，作者在position encoding上做了调整，使用了可学习的位置编码，不同于transformer的三角函数位置编码。 ","date":"2022-07-27","objectID":"/gpt/:1:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["NLP"],"content":"微调：将 GPT 用于下游任务 微调损失包括特定于任务的损失以及语言建模损失： \\[ L = L_{xent} + \\lambda \\cdot L_{task}. \\] ","date":"2022-07-27","objectID":"/gpt/:2:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["Deep Learning","网络正则化"],"content":"Dropout 在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值h以保留概率概率p由随机变量替换(即drop经过神经元后的值代替drop神经元) \\[ h^{'}= \\begin{cases} 0, \\quad 概率为1-p \\\\\\\\ \\frac{h}{p}, \\quad 概率为p \\end{cases} \\] 注意期望不要变，即 \\[ E[h^{'}] = (1-p)*0 + p *\\frac{h}{p} = h \\] 也可以训练时非丢弃单元不除以概率p，而是测试时模型参数乘以p，这样可以保证训练集和测试集的期望相同。和上面的效果相同。 注意：正则项（Dropout）只在训练过程中使用，因为其会影响模型参数的更新 所以在推理过程中，丢弃法直接返回输入。 ","date":"2022-07-21","objectID":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/:0:0","tags":["Deep Learning","网络正则化","Dropout正则化"],"title":"Dropout正则化","uri":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning","网络正则化"],"content":"代码 import torch from torch import nn from d2l import torch as d2l import torchvision from torchvision import transforms from torch.utils import data def dropout_layer(X,dropout): assert 0\u003c=dropout \u003c= 1 # 如果keep_prob设置为1，全部元素被保留 if dropout == 1: return X # 如果keep_prob设置为0，全部元素被丢弃 if dropout == 0: return torch.zeros_like(X) mask = (torch.rand(X.shape) \u003c dropout).float() #使用mask而不是直接置零是为了提高计算效率 return mask * X/(dropout) ","date":"2022-07-21","objectID":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/:1:0","tags":["Deep Learning","网络正则化","Dropout正则化"],"title":"Dropout正则化","uri":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["算法题"],"content":"最长回文子串 ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:0:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/longest-palindromic-substring/ ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:1:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： ​ 一开始暴力解法，比较好想，结果超时了哎，后来看见了标签是动态规划，才知道不能暴力 class Solution: def longestPalindrome(self, s: str) -\u003e str: if len(s) \u003c= 1: return s maxs = -float(\"inf\") res = collections.defaultdict(list) left,right = 0,len(s)-1 while left \u003c right: for i in range(left,right+2): if s[left:i] == s[left:i][::-1]: maxs = max(maxs,len(s[left:i])) res[maxs].append(s[left:i]) left += 1 return max(res[max(res.keys())],key=len) 也用到了双指针，超时在情理之中。 后来用到了动态规划 class Solution: def longestPalindrome(self, s: str) -\u003e str: if len(s) \u003c= 1: return s length = len(s) dp = [[False for _ in range(length)] for _ in range(length)] for i in range(length): dp[i][i] = True start = 0 max_len = 1 for j in range(1, length): for i in range(0, j): if s[i] == s[j]: if j - i \u003c 3: dp[i][j] = True else: dp[i][j] = dp[i + 1][j - 1] else: dp[i][j] = False if dp[i][j]: cur_len = j - i + 1 if cur_len \u003e max_len: max_len = cur_len start = i return s[start:start + max_len] ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:2:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"有效的数独 https://leetcode-cn.com/problems/valid-sudoku/ #有效的数独 难点在将3*3里的数取出来 class Solution: def isValidSudoku(board) -\u003e bool: for line1,line2 in zip(board,zip(*board)): #行列 for n1,n2 in zip(line1,line2): if (n1 != '.' and line1.count(n1) \u003e 1) or (n2!='.' and line2.count(n2) \u003e1): return False pal = [[board[i+m][j+n] for m in range(3) for n in range(3) if board[i+m][j+n] != '.'] for i in (0, 3, 6) for j in (0, 3, 6)] for line in pal: if len(set(line)) != len(line): return False return True ","date":"2022-07-20","objectID":"/%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E7%8B%AC/:0:0","tags":["算法题","有效的数独"],"title":"有效的数独","uri":"/%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E7%8B%AC/"},{"categories":["算法题"],"content":"使括号有效的最少添加 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:0:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-add-to-make-parentheses-valid/ ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:1:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"思路： 通过一个值来判断是否匹配 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:2:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"代码： class Solution: def minAddToMakeValid(self, S: str) -\u003e int: res,temp = 0,0 for i in S: if i == '(': temp += 1 if i == ')': temp -= 1 if temp == -1: temp = 0 res += 1 return res + temp 如果右括号过多的话，就在左边补一个左括号。这时结果+1 如果一直是左括号的话，res 为0 temp就是应该补的个数 如果都相匹配的话，temp = 0 相应 res也为0 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:3:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["Machine Learning","集成学习"],"content":"集成学习 在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。 集成学习在各个规模的数据集上都有很好的策略。 数据集大：划分成多个小数据集，学习多个模型进行组合 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合 集成学习主要有两类： 1. Bagging Boosting Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。 Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。 ","date":"2022-07-09","objectID":"/ensemble-learning/:0:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"Bagging bagging的名称来源于 （ Bootstrap Aggregating ），意思是自助抽样集成，这种方法将训练集分成m个新的训练集，然后在每个新训练集上构建一个模型，各自不相干，最后预测时我们将这个m个模型的结果进行整合，得到最终结果。整合方式就是：分类问题用majority voting，回归用均值。 因此Bagging使用的抽样方法是Bootstrap方法，即自助法，本质上就是一个有放回的随机抽样问题。 每一个样本在每一次抽的时候有同样的概率\\(\\frac{1}{N}\\)被抽中。没被抽中的概率为\\(1-\\frac{1}{N}\\)，一共抽了N次，即\\(1-(\\frac{1}{N})^N\\)当N趋于无穷时，由高等数学学的极限的求解可以算出来是\\(\\frac{1}{e}\\)，大概为36.8%，这些留下来的1/3的样本可以作为验证集，这样的方式叫做包外估计(out of bag estimate) ","date":"2022-07-09","objectID":"/ensemble-learning/:1:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"Boosting Boosting与Bagging的区别就是取样方式不同，Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。很好理解吧。 ","date":"2022-07-09","objectID":"/ensemble-learning/:2:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"一个故事 一个故事用于理解，来源：https://www.joinquant.com/view/community/detail/adfb5ce37f0b39e348aae32e8412c68c 有一个医学专（砖）家，他看过很多很多病人，还记了小本本来归类这些病人的特征和病情来方便以后诊断。有一天来了个病人，这个专家就问病人了，“大爷您贵姓？多少岁，哪不舒服，病情怎么样？”大爷说 “我姓李，48，最近老吐痰，老咳嗽，还发烧…”。这个专家拿出他的小本本一查“”属性：“姓李”，“年龄48”，“吐痰”，“咳嗽”，“发烧”…,根据我的小本本以前这样的病例有80个，有76个是感冒，成，就诊断他是感冒了！”这个专家就是棵决策树 镜头一转，来到医院A，同样的病人来医院A治病。医院有很大的病例数据库，有100个医学专家通过学习数据库的一部分知识形成了自己的诊断方案。医院想：“我财大气粗，为了提供更好的医疗服务，我让100个专家做诊断，然后他们投票决出最后的判断”。 随机森林 完成 医院B就不爽了，你这治疗方案太受欢迎，把客户都抢走了，我要用科学的治疗方案来击败你。医院B想了想，大家投票不一定针对到用户情况，我先从我的100个专家里找一个最好的先给病人做一个诊疗方案，再根据第一个专家的不足找第二个，根据第二个再找第三个…… 最后不同专家再根据他们诊断的表现以不同权重投票。这样岂不是更针对病人痛点？ Boosting 方法就被这群人建立起来了 到了医院C想搞差异化，你医院B根据上一个专家的全部不足找新专家，那我就根据上一个专家判断最偏颇的方向找专家，虽然听起来差不多，但我的差异化说不定就能更好。 Gradient Boosting 产生了 更加财大气粗的医院D来了，他觉得虽然整个市场的大格调基本确定，但我可以通过提升整个诊疗的流程大大小小的细节来取胜啊！于是医院D在C的基础上改进了很多，于是找偏颇的方向更快更准，纠结专家，诊疗的速度也大大加快，整个医院的硬件设施也前所未有的提高。这差不多就是 XGBoost 了 突然有股叫大数据的潮流吹来，本来医院D已经在医院C一分钟治疗10000人的基础上提升了10多倍速度，但新的要求是：一分钟不行，最多给你3秒，10万人也不行，我现在有全世界的数据，你得分秒内召集几百几千个专家，这些专家每一个的知识得相当于以前一个医院那没多，还得分秒内服务数10倍的病人，最后治疗的精度不能下降。 LightGBM 出场了，虽然精度提高不多，但速度大大加快了。 ","date":"2022-07-09","objectID":"/ensemble-learning/:3:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["算法题"],"content":"旋转图像 https://leetcode-cn.com/problems/rotate-image/ 没难度的中等题，这方法很python class Solution: def rotate(self, matrix: List[List[int]]) -\u003e None: \"\"\" Do not return anything, modify matrix in-place instead. \"\"\" n = len(matrix) for i in list(map(list,map(reversed,zip(*matrix)))): matrix.append(i) del matrix[:n] ","date":"2022-07-07","objectID":"/%E6%97%8B%E8%BD%AC%E5%9B%BE%E5%83%8F/:0:0","tags":["算法题","旋转图像"],"title":"旋转图像","uri":"/%E6%97%8B%E8%BD%AC%E5%9B%BE%E5%83%8F/"},{"categories":["sklearn"],"content":"导入必要的包 import pandas as pd import numpy as np 读入数据 data = pd.read_csv(\"./datasets/Data.csv\") data.head() Country Age Salary Purchased 0 France 44.0 72000.0 No 1 Spain 27.0 48000.0 Yes 2 Germany 30.0 54000.0 No 3 Spain 38.0 61000.0 No 4 Germany 40.0 NaN Yes ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:1:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["sklearn"],"content":"确定X,Y变量 X = data.iloc[:,:-1].values Y = data.iloc[:,3].values ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:2:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["sklearn"],"content":"数据空值处理 from sklearn.impute import SimpleImputer imputer = SimpleImputer() X[:,1:3] = imputer.fit_transform(X[:,1:3]) X array([['France', 44.0, 72000.0], ['Spain', 27.0, 48000.0], ['Germany', 30.0, 54000.0], ['Spain', 38.0, 61000.0], ['Germany', 40.0, 63777.77777777778], ['France', 35.0, 58000.0], ['Spain', 38.77777777777778, 52000.0], ['France', 48.0, 79000.0], ['Germany', 50.0, 83000.0], ['France', 37.0, 67000.0]], dtype=object) ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:3:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["sklearn"],"content":"编码处理 from sklearn.preprocessing import LabelEncoder,OneHotEncoder labelencoder = LabelEncoder() X[:,0] = labelencoder.fit_transform(X[:,0]) onehotencoder = OneHotEncoder() X = onehotencoder.fit_transform(X).toarray() labelencoder_Y = LabelEncoder() Y = labelencoder_Y.fit_transform(Y) ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:4:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["sklearn"],"content":"分割数据 from sklearn.model_selection import train_test_split X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2, random_state=0) ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:5:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["sklearn"],"content":"标准化 from sklearn.preprocessing import StandardScaler sc_X = StandardScaler() X_train = sc_X.fit_transform(X_train) X_test = sc_X.fit_transform(X_test) ","date":"2022-07-02","objectID":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/:6:0","tags":["sklearn","数据预处理"],"title":"数据预处理","uri":"/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},{"categories":["Machine Learning","分类算法"],"content":"KNN 参考：https://cuijiahua.com/blog/2017/11/ml_1_knn.html 《统计学习方法》李航（kd树） ","date":"2022-06-25","objectID":"/knn/:0:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"简介 k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 ","date":"2022-06-25","objectID":"/knn/:1:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"步骤 k-近邻算法步骤如下： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。 ","date":"2022-06-25","objectID":"/knn/:2:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"总结 优点 简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归； 可用于数值型数据和离散型数据； 训练时间复杂度为O(n)；无数据输入假定； 对异常值不敏感 缺点 计算复杂性高；空间复杂性高； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）； 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。 最大的缺点是无法给出数据的内在含义。 关于algorithm参数kd_tree的原理，可以查看《统计学方法 李航》书中的讲解； 关于距离度量的方法还有切比雪夫距离、马氏距离、巴氏距离等； ","date":"2022-06-25","objectID":"/knn/:3:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["NLP"],"content":"主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"词袋模型 将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的 例子： 句子1：我 爱 北 京 天 安 门 转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0] 句子2：我 喜 欢 上 海 转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] from sklearn.feature_extraction.text import CountVectorizer corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ] vectorizer = CountVectorizer() vectorizer.fit_transform(corpus).toarray() 结果： [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LSA LSA就是潜在语义分析。特点是通过矩阵分解发现文本与单词之间基于主题（话题）的语义关系。 首先要清楚几个概念： ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"单词-文本矩阵 \\[ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\\\\\\\ x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] \\] 这是一个 \\(m \\times n\\) 矩阵, 元素 \\(x_{i j}\\) 表示单词 \\(w_i\\) 在文本 \\(d_j\\) 中出现的频数或权值。由于单 词的种类很多, 而每个文本中出现单词的种类通常较少, 所以单词-文本矩阵是一个稀 疏矩阵。 权值通常用单词频率-逆文本频率 (term frequency-inverse document frequency, TF-IDF）表示，其定义是 \\[ \\operatorname{TFIDF}_{i j}=\\frac{\\mathrm{tf}_{i j}}{\\mathrm{tf}_{\\bullet j}} \\log \\frac{\\mathrm{df}}{\\mathrm{df}_i}, \\quad i=1,2, \\cdots, m ; \\quad j=1,2, \\cdots, n \\] 直观上讲，可以直接用每一列作为文本语义表达， 因此可以通过余弦相似度等计算文本之间的相似性，并且矩阵稀疏，计算量较少。但其并不关心文本中词语出现的顺序等信息，因此需要改进。 ### 单词-主题矩阵 假设所有文本共含有 \\(k\\) 个话题。假设每个话题由一个定义在单词集合 \\(W\\) 上的 \\(m\\) 维向量表示, 称为话题向量, 即 \\[ t_l=\\left[\\begin{array}{c} t_{1 l} \\\\\\\\ t_{2 l} \\\\\\\\ \\vdots \\\\\\\\ t_{m l} \\end{array}\\right], \\quad l=1,2, \\cdots, k \\] 其中 \\(t_{i l}\\) 是单词 \\(w_i\\) 在话题 \\(t_l\\) 的权值, \\(i=1,2, \\cdots, m\\), 权值越大, 该单词在该话题中 的重要度就越高。这 \\(k\\) 个话题向量 \\(t_1, t_2, \\cdots, t_k\\) 张成一个话题向量空间 (topic vector 话题向量空间 \\(T\\) 也可以表示为一个矩阵, 称为单词-主题矩阵 (word-topic matrix）, 记作 \\[ T=\\left[\\begin{array}{cccc} t_{11} \u0026 t_{12} \u0026 \\cdots \u0026 t_{1 k} \\\\\\\\ t_{21} \u0026 t_{22} \u0026 \\cdots \u0026 t_{2 k} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ t_{m 1} \u0026 t_{m 2} \u0026 \\cdots \u0026 t_{m k} \\end{array}\\right] \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"主题-文本矩阵 将单词-文本矩阵中的文本\\(x_j\\)投影到主题向量空间\\(J\\)中，得到在主题空间中的一个向量\\(y_j\\)。 \\[ Y=\\left[\\begin{array}{cccc} y_{11} \u0026 y_{12} \u0026 \\cdots \u0026 y_{1 n} \\\\\\\\ y_{21} \u0026 y_{22} \u0026 \\cdots \u0026 y_{2 n} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ y_{k 1} \u0026 y_{k 2} \u0026 \\cdots \u0026 y_{k n} \\end{array}\\right] \\] 从单词向量空间到主题向量空间的线性变换 单词-文本矩阵\\(X\\)可以近似表示为单词-主题矩阵\\(T\\)与主题-文本矩阵\\(Y\\)的乘积，这就是潜在语义分析： \\[ X\\approx TY \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"潜在语义分析 给定单词-文本矩阵\\(X\\)，每一行代表一个单词，每一列代表一个文本。其中的元素代表单词在文本中的权重或者频数（词袋模型）。 截断奇异值分析 \\[ X \\approx U_k \\Sigma_k V_k^{\\mathrm{T}}=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 \u0026 0 \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 \\sigma_2 \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 \\ddots \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 0 \u0026 \\sigma_k \\end{array}\\right]\\left[\\begin{array}{c} v_1^{\\mathrm{T}} \\\\\\\\ v_2^{\\mathrm{T}} \\\\\\\\ \\vdots \\\\\\\\ v_k^{\\mathrm{T}} \\end{array}\\right] \\] 接下来考虑文本在主题空间中的表示。 \\[ \\begin{aligned} X \u0026=\\left[\\begin{array}{llll} x_1 \u0026 x_2 \u0026 \\cdots \u0026 x_n \\end{array}\\right] \\approx U_k \\Sigma_k V_k^{\\mathrm{T}} \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 \u0026 \u0026 \u0026 \\\\\\\\ \u0026 \\sigma_2 \u0026 0 \u0026 \\\\\\\\ 0 \u0026 \\ddots \u0026 \\\\\\\\ \u0026 \u0026 \\sigma_k \\end{array}\\right]\\left[\\begin{array}{cccc} v_{11} \u0026 v_{21} \u0026 \\cdots \u0026 v_{n 1} \\\\\\\\ v_{12} \u0026 v_{22} \u0026 \\cdots \u0026 v_{n 2} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ v_{1 k} \u0026 v_{2 k} \u0026 \\cdots \u0026 v_{n k} \\end{array}\\right] \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 v_{11} \u0026 \\sigma_1 v_{21} \u0026 \\cdots \u0026 \\sigma_1 v_{n 1} \\\\\\\\ \\sigma_2 v_{12} \u0026 \\sigma_2 v_{22} \u0026 \\cdots \u0026 \\sigma_2 v_{n 2} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ \\sigma_k v_{1 k} \u0026 \\sigma_k v_{2 k} \u0026 \\cdots \u0026 \\sigma_k v_{n k} \\end{array}\\right] \\end{aligned} \\] 其中: \\[ u_l = \\begin{bmatrix}u_{1l} \\\\\\\\u_{2l} \\\\\\\\ \\vdots \\\\\\\\u_{ml} \\end{bmatrix}, \\quad l= 1, 2, \\dots, k \\] 代表单词对主题的权重。 由式知, 矩阵 \\(X\\) 的第 \\(j\\) 列向量 \\(x_j\\) 满足 \\[ \\begin{aligned} x_j \u0026 \\approx U_k\\left(\\Sigma_k V_k^{\\mathrm{T}}\\right)\\_j \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{c} \\sigma_1 v_{j 1} \\\\\\\\ \\sigma_2 v_{j 2} \\\\\\\\ \\vdots \\\\\\\\ \\sigma_k v_{j k} \\end{array}\\right] \\\\\\\\ \u0026=\\sum_{l=1}^k \\sigma_l v_{j l} u_l, \\quad j=1,2, \\cdots, n \\end{aligned} \\] 则\\(\\Sigma_kV_k^T\\)每一个列向量是一个文本在主题向量空间中的表示。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"PLSA ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"生成模型 假设有单词集合 \\(W={w_1, w_2, \\cdots, w_M}\\), 其中 \\(M\\) 是单词个数; 文本 (指标) 集 合 \\(D={d_1, d_2, \\cdots, d_N}\\), 其中 \\(N\\) 是文本个数; 话题集合 \\(Z={z_1, z_2, \\cdots, z_K}\\), 其中 \\(K\\) 是预先设定的话题个数。随机变量 \\(w\\) 取值于单词集合; 随机变量 \\(d\\) 取值于文本集 合, 随机变量 \\(z\\) 取值于话题集合。概率分布 \\(P(d)\\) 、条件概率分布 \\(P(z \\mid d)\\) 、条件概率分 布 \\(P(w \\mid z)\\) 皆属于多项分布, 其中 \\(P(d)\\) 表示生成文本 \\(d\\) 的概率, \\(P(z \\mid d)\\) 表示文本 \\(d\\) 生 成话题 \\(z\\) 的概率, \\(P(w \\mid z)\\) 表示话题 \\(z\\) 生成单词 \\(w\\) 的概率。 每个文本 \\(d\\) 拥有自己的话题概率分布 \\(P(z \\mid d)\\), 每个话题 \\(z\\) 拥有自己的单词概率分 布 \\(P(w \\mid z)\\); 也就是说一个文本的内容由其相关话题决定, 一个话题的内容由其相关单词决定。 生成模型通过以下步骤生成文本-单词共现数据: (1) 依据概率分布 \\(P(d)\\), 从文本 (指标) 集合中随机选取一个文本 \\(d\\), 共生成 \\(N\\) 个文本; 针对每个文本, 执行以下操作; (2) 在文本 \\(d\\) 给定条件下, 依据条件概率分布 \\(P(z \\mid d)\\), 从话题集合随机选取一个 话题 \\(z\\), 共生成 \\(L\\) 个话题, 这里 \\(L\\) 是文本长度; (3) 在话题 \\(z\\) 给定条件下, 依据条件概率分布 \\(P(w \\mid z)\\), 从单词集合中随机选取一 个单词 \\(w\\) 。 生成模型中, 单词变量 \\(w\\) 与文本变量 \\(d\\) 是观测变量, 话题变量 \\(z\\) 是隐变量。也就 是说模型生成的是单词-话题-文本三元组 \\((w, z, d)\\) 的集合, 但观测到的是单词-文本二 元组 \\((w, d)\\) 的集合, 观测数据表示为单词-文本矩阵 \\(T\\) 的形式, 矩阵 \\(T\\) 的行表示单词, 列表示文本, 元素表示单词-文本对 \\((w, d)\\) 的出现次数。 从数据的生成过程可以推出, 文本-单词共现数据 \\(T\\) 的生成概率为所有单词-文本 对 \\((w, d)\\) 的生成概率的乘积, \\[ P(T)=\\prod_{(w, d)} P(w, d)^{n(w, d)} \\] 这里 \\(n(w, d)\\) 表示 \\((w, d)\\) 的出现次数, 单词-文本对出现的总次数是 \\(N \\times L\\) 。每个单 词-文本对 \\((w, d)\\) 的生成概率由以下公式决定: \\[ \\begin{aligned} P(w, d) \u0026=P(d) P(w \\mid d) \\\\\\\\ \u0026=P(d) \\sum_z P(w, z \\mid d) \\\\\\\\ \u0026=P(d) \\sum_z P(z \\mid d) P(w \\mid z) \\end{aligned} \\] 即生成模型的定义。 生成模型假设在话题 \\(z\\) 给定条件下, 单词 \\(w\\) 与文本 \\(d\\) 条件独立, 即 \\[ P(w, z \\mid d)=P(z \\mid d) P(w \\mid z) \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"共现模型 \\[ P(T)=\\prod_{(w, d)} P(w, d)^{n(w, d)} \\] 每个单词-文本对 \\((w, d)\\) 的概率由以下公式决定: \\[ P(w, d)=\\sum_{z \\in Z} P(z) P(w \\mid z) P(d \\mid z) \\] 式 (18.5) 即共现模型的定义。容易验证, 生成模型 (18.2) 和共现模型 (18.5) 是等价的。 共现模型假设在话题 \\(z\\) 给定条件下, 单词 \\(w\\) 与文本 \\(d\\) 是条件独立的, 即 \\[ P(w, d \\mid z)=P(w \\mid z) P(d \\mid z) \\] 直观解释： ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"与潜在语义分析的关系 共现模型也可以表示为三个矩阵乘积的形式。这样, 概率潜在语义分析与 潜在语义分析的对应关系可以从中看得很清楚。下面是共现模型的矩阵乘积形式: \\[ \\begin{aligned} X^{\\prime} \u0026=U^{\\prime} \\Sigma^{\\prime} V^{\\prime \\mathrm{T}} \\\\\\\\ X^{\\prime} \u0026=[P(w, d)]\\_{M \\times N} \\\\\\\\ U^{\\prime} \u0026=[P(w \\mid z)]\\_{M \\times K} \\\\\\\\ \\Sigma^{\\prime} \u0026=[P(z)]\\_{K \\times K} \\\\\\\\ V^{\\prime} \u0026=[P(d \\mid z)]\\_{N \\times K} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概率潜在语义分析的算法 Plsa是含有隐变量的模型，其学习通常使用EM算法。 E步是计算Q函数，M步是极大化Q函数。 设单词集合为 \\(W={w_1, w_2, \\cdots, w_M}\\), 文本集合为 \\(D={d_1, d_2, \\cdots, d_N}\\), 话 题集合为 \\(Z={z_1, z_2, \\cdots, z_K}\\) 。给定单词-文本共现数据 \\(T={n\\left(w_i, d_j\\right)}, i=\\) \\(1,2, \\cdots, M, j=1,2, \\cdots, N\\), 目标是估计概率潜在语义分析模型（生成模型）的 参数。如果使用极大似然估计, 对数似然函数是 \\[ \\begin{aligned} L \u0026=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\log P\\left(w_i, d_j\\right) \\\\\\\\ \u0026=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\log \\left[\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right] \\end{aligned} \\] 但是模型含有隐变量, 对数似然函数的优化无法用解析方法求解, 这时使用 EM算法。 应用 EM算法的核心是定义 \\(Q\\) 函数。 \\(\\mathrm{E}\\) 步：计算 \\(Q\\) 函数 \\(Q\\) 函数为完全数据的对数似然函数对不完全数据的条件分布的期望。针对概率潜 在语义分析的生成模型, \\(Q\\) 函数是 \\[ Q=\\sum_{k=1}^K{\\sum_{j=1}^N n\\left(d_j\\right)\\left[\\log P\\left(d_j\\right)+\\sum_{i=1}^M \\frac{n\\left(w_i, d_j\\right)}{n\\left(d_j\\right)} \\log P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right]} P\\left(z_k \\mid w_i, d_j\\right) \\] 式中 \\(n\\left(d_j\\right)=\\sum_{i=1}^M n\\left(w_i, d_j\\right)\\) 表示文本 \\(d_j\\) 中的单词个数, \\(n\\left(w_i, d_j\\right)\\) 表示单词 \\(w_i\\) 在文本 \\(d_j\\) 中出现的次数。条件概率分布 \\(P\\left(z_k \\mid w_i, d_j\\right)\\) 代表不完全数据, 是已知变量。条件概 率分布 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 的乘积代表完全数据, 是末知变量。 由于可以从数据中直接统计得出 \\(P\\left(d_j\\right)\\) 的估计, 这里只考虑 \\(P\\left(w_i \\mid z_k\\right), P\\left(z_k \\mid d_j\\right)\\) 的估计, 可将 \\(Q\\) 函数简化为函数 \\(Q^{\\prime}\\) \\[ Q^{\\prime}=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\sum_{k=1}^K P\\left(z_k \\mid w_i, d_j\\right) \\log \\left[P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right] \\] \\(Q^{\\prime}\\) 函数中的 \\(P\\left(z_k \\mid w_i, d_j\\right)\\) 可以根据贝叶斯公式计算 \\[ P\\left(z_k \\mid w_i, d_j\\right)=\\frac{P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)}{\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)} \\] 其中 \\(P\\left(z_k \\mid d_j\\right)\\) 和 \\(P\\left(w_i \\mid z_k\\right)\\) 由上一步迭代得到。 \\(\\mathrm{M}\\) 步: 极大化 \\(Q\\) 函数。 通过约束最优化求解 \\(Q\\) 函数的极大值, 这时 \\(P\\left(z_k \\mid d_j\\right)\\) 和 \\(P\\left(w_i \\mid z_k\\right)\\) 是变量。因为 变量 \\(P\\left(w_i \\mid z_k\\right), P\\left(z_k \\mid d_j\\right)\\) 形成概率分布, 满足约束条件 \\[ \\begin{aligned} \u0026\\sum_{i=1}^M P\\left(w_i \\mid z_k\\right)=1, \\quad k=1,2, \\cdots, K \\\\\\\\ \u0026\\sum_{k=1}^K P\\left(z_k \\mid d_j\\right)=1, \\quad j=1,2, \\cdots, N \\end{aligned} \\] 应用拉格朗日法, 引入拉格朗日乘子 \\(\\tau_k\\) 和 \\(\\rho_j\\), 定义拉格朗日函数 \\(A\\) \\[ \\Lambda=Q^{\\prime}+\\sum_{k=1}^K \\tau_k\\left(1-\\sum_{i=1}^M P\\left(w_i \\mid z_k\\right)\\right)+\\sum_{j=1}^N \\rho_j\\left(1-\\sum_{k=1}^K P\\left(z_k \\mid d_j\\right)\\right) \\] 将拉格朗日函数 \\(\\Lambda\\) 分别对 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 求偏导数, 并令其等于 0 , 得到下面 的方程组 \\[ \\begin{aligned} \u0026\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)-\\tau_k P\\left(w_i \\mid z_k\\right)=0, \\quad i=1,2, \\cdots, M ; \\quad k=1,2, \\cdots, K \\\\\\\\ \u0026\\sum_{i=1}^M n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)-\\rho_j P\\left(z_k \\mid d_j\\right)=0, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 解方程组得到 \\(M\\) 步的参数估计公式: \\[ P\\left(w_i \\mid z_k\\right)=\\frac{\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{\\sum_{m=1}^M \\sum_{j=1}^N n\\left(w_m, d_j\\right) P\\left(z_k \\mid w_m, d_j\\right)} \\] \\[ P(z_k\\mid d_j) = \\frac{\\sum_{i=1}^Mn(w_i, d_j)P(z_k\\mid w_i,d_j)}{n(d_j)} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:4","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"总结算法 输入: 设单词集合为 \\(W={w_1, w_2, \\cdots, w_M}\\), 文本集合为 \\(D={d_1, d_2, \\cdots, d_N}\\), 话题集合为 \\(Z={z_1, z_2, \\cdots, z_K}\\), 共现数据 \\({n\\left(w_i, d_j\\right)}, i=1,2, \\cdots, M, j=1\\), \\(2, \\cdots, N\\); 输出: \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 。 (1) 设置参数 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 的初始值。 (2) 迭代执行以下 \\(\\mathrm{E}\\) 步, \\(\\mathrm{M}\\) 步, 直到收敛为止。 \\(\\mathrm{E}\\) 步: \\[ P\\left(z_k \\mid w_i, d_j\\right)=\\frac{P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)}{\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)} \\] M 步: \\[ \\begin{aligned} P\\left(w_i \\mid z_k\\right) \u0026=\\frac{\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{\\sum_{m=1}^M \\sum_{j=1}^N n\\left(w_m, d_j\\right) P\\left(z_k \\mid w_m, d_j\\right)} \\\\\\\\ P\\left(z_k \\mid d_j\\right) \u0026=\\frac{\\sum_{i=1}^M n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{n\\left(d_j\\right)} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:5","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"用法 与LSA类似，可以把文档对各个主题的概率看作是文档的表示，最后用到的就是\\(P(z_k\\mid d_j)\\)。 k就是我们自己设定的主题数，一般来说K远远小于文档个数和词汇表大小，这样也达到了降维的目的。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:6","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"优点与不足 优点 pLSA是在一套比较完整的思想的基础上提出来的，模型中各项参数有明确的物理含义，可解释性比较强。相比LSA，pLSA对人类生成文本机制的刻画更加细致、更加符合我们的常识，比如，pLSA基于条件概率，引入了一个“隐含变量”（相对于可以看到的文档和词语，是不可观测变的），即主题，来描述文本生成的过程。 #### 不足 pLSA的理论与我们的实践不是那么的统一: (1) 我们说话的时候，根本不会考虑” 我说这段话的概率大小”，即 \\(p\\left(d_t\\right)\\) (2) pLSA认为，我们说话时面向的主题分布，取决于 “文档” （实际上是文档ID)。这个假设显然是不合理的，小说家不会因为自己写到第666回而调整 主题。 (3) 类似 (2)，随着上下文的变化，我们围绕一个主题说话的内容和方式也 会发生改变。在主题模型中，这种改变的体现，就是一个主题下的词语概率分 布会发生改变。而pLSA忽略了这样的事实。 从计算复杂度的角度看pLSA有两个比较大的缺陷: (1) pLSA中，对文档 出现的概率估计，来自对训练语料的学习。而对于一个 末知文档，我们是无法估计它出现的概率的一一因此pLSA无法对训练语料之 外的文档进行处理。pLSA的这个特点决定了，在在线(online) 场景中(数据是 持续增加的)，那么文档处理系统就需要定时使用pLSA对整个语料库进行计 算。因此，pLSA比较适合允许一定时滞的离线计算。 (2) pLSA认为一个文档对各个主题的隶属度是一定的——而一个主题对各个词语的隶属度也是一定的，因此pLSA在生成一个文档的各个词语时、使用了相同的词语概率分布。这样，pLSA需要为每一个文档记录一个专门的随着语料数据集规模的增加，pLSA的参数规模也会增加，导致模型训练越来越困难。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:7","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LDA LDA模型是文本集合的生成概率模型。 LDA 的文本集合的生成过程如下: 首先随机生成一个文本的话题分布, 之后在该 文本的每个位置, 依据该文本的话题分布随机生成一个话题, 然后在该位置依据该话 题的单词分布随机生成一个单词, 直至文本的最后一个位置, 生成整个文本。重复以 上过程生成所有文本。 LDA 模型是含有隐变量的概率图模型。模型中, 每个话题的单词分布, 每个文 本的话题分布, 文本的每个位置的话题是隐变量; 文本的每个位置的单词是观测变 量。LDA 模型的学习与推理无法直接求解, 通常使用吉布斯抽样 (Gibbs sampling) 和 变分 EM算法 (variational EM algorithm), 前者是蒙特卡罗法, 而后者是近似算法。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"多项分布 (多项分布) 若多元离散随机变量 \\(X=\\left(X_1, X_2, \\cdots, X_k\\right)\\) 的概率质 量函数为 \\[ \\begin{aligned} P\\left(X_1=n_1, X_2=n_2, \\cdots, X_k=n_k\\right) \u0026=\\frac{n !}{n_{1} ! n_{2} ! \\cdots n_{k} !} p_1^{n_1} p_2^{n_2} \\cdots p_k^{n_k} \\\\\\\\ \u0026=\\frac{n !}{\\prod_{i=1}^k n_{i} !} \\prod_{i=1}^k p_i^{n_i} \\end{aligned} \\] 其中 \\(p=\\left(p_1, p_2, \\cdots, p_k\\right), p_i \\geqslant 0, i=1,2, \\cdots, k, \\sum_{i=1}^k p_i=1, \\sum_{i=1}^k n_i=n\\), 则称随机变 量 \\(X\\) 服从参数为 \\((n, p)\\) 的多项分布, 记作 \\(X \\sim \\operatorname{Mult}(n, p)\\) 。 当试验的次数 \\(n\\) 为 1 时, 多项分布变成类别分布 (categorical distribution)。类 别分布表示试验可能出现的 \\(k\\) 种结果的概率。显然多项分布包含类别分布。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"狄利克雷分布 狄利克雷分布 (Dirichlet distribution) 是一种多元连续随机变量的概率分布, 是 贝塔分布 (beta distribution) 的扩展。在贝叶斯学习中, 狄利克雷分布常作为多项分 布的先验分布使用。 (狄利克雷分布) 若多元连续随机变量 \\(\\theta=\\left(\\theta_1, \\theta_2, \\cdots, \\theta_k\\right)\\) 的概率密 度函数为 \\[ p(\\theta \\mid \\alpha)=\\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\] 其中 \\(\\sum_{i=1}^k \\theta_i=1, \\theta_i \\geqslant 0, \\alpha=\\left(\\alpha_1, \\alpha_2, \\cdots, \\alpha_k\\right), \\alpha_i\u003e0, i=1,2, \\cdots, k\\), 则称随机变量 \\(\\theta\\) 服从参数为 \\(\\alpha\\) 的狄利克雷分布, 记作 \\(\\theta \\sim \\operatorname{Dir}(\\alpha)\\) 。 式中 \\(\\Gamma(s)\\) 是伽马函数, 定义为 \\[ \\Gamma(s)=\\int_0^{\\infty} x^{s-1} \\mathrm{e}^{-x} \\mathrm{~d} x, \\quad s\u003e0 \\] 具有性质： \\[ \\Gamma(s+1) = s\\Gamma(s) \\] 当s为自然数时，有： \\[ \\Gamma(s+1) = s! \\] 令 \\[ \\mathrm{B}(\\alpha)=\\frac{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)}{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)} \\] 则狄利克雷分布的密度函数可以写成 \\[ p(\\theta \\mid \\alpha)=\\frac{1}{\\mathrm{~B}(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\] \\(\\mathrm{B}(\\alpha)\\) 是规范化因子, 称为多元贝塔函数 (或扩展的贝塔函数)。由密度函数的性质 \\[ \\int \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\prod_{i=1}^{\\alpha_i-1} \\mathrm{~d} \\theta=\\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\int \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\mathrm{~d} \\theta=1 \\] 得 \\[ \\mathrm{B}(\\alpha)=\\int \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\mathrm{~d} \\theta \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"二项分布与贝塔分布 二项分布是多项分布的特殊情况, 贝塔分布是狄利克雷分布的特殊情况。 二项分布是指如下概率分布。 \\(X\\) 为离散随机变量, 取值为 \\(m\\), 其概率质量函数为 \\[ P(X=m)=\\left(\\begin{array}{c} n \\\\\\\\ m \\end{array}\\right) p^m(1-p)^{n-m}, \\quad m=0,1,2, \\cdots, n \\] 其中 \\(n\\) 和 \\(p(0 \\leqslant p \\leqslant 1)\\) 是参数。 贝塔分布是指如下概率分布, \\(X\\) 为连续随机变量, 取值范围为 \\([0,1]\\), 其概率密度 函数为 \\[ p(x)= \\begin{cases}\\frac{1}{\\mathrm{~B}(s, t)} x^{s-1}(1-x)^{t-1}, \u0026 0 \\leqslant x \\leqslant 1 \\\\\\\\ 0, \u0026 \\text { 其他 }\\end{cases} \\] 其中 \\(s\u003e0\\) 和 \\(t\u003e0\\) 是参数, \\(\\mathrm{B}(s, t)=\\frac{\\Gamma(s) \\Gamma(t)}{\\Gamma(s+t)}\\) 是贝塔函数, 定义为 \\[ \\mathrm{B}(s, t)=\\int_0^1 x^{s-1}(1-x)^{t-1} \\mathrm{~d} x = \\frac{\\Gamma(s)\\Gamma(t)}{\\Gamma(s+t)} \\] 当 \\(s, t\\) 是自然数时(\\(\\Gamma(s+1) = s!\\)), \\[ \\mathrm{B}(s, t)=\\frac{(s-1) !(t-1) !}{(s+t-1) !} \\] 当 \\(n\\) 为 1 时, 二项分布变成伯努利分布（Bernoulli distribution）或 0-1 分布。 伯努利分布表示试验可能出现的 2 种结果的概率。显然二项分布包含伯努利分布。给出几种概率分布的关系。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"基本想法 在LDA主题模型下，一篇文章由词语的序列组成。首先以一定概率选择一个主题，其次以一定概率在这个主题中选择一个词。如果一篇文章由1000个词组成，那么就把上述方式重复1000遍，就能组成这篇文章。那么值得注意的是，以一定概率选择一个主题是服从多项式分布的，而多项式分布的参数是服从Dirichlet分布的。以一定概率在特定主题中选择一个词也是服从多项式分布的，多项式分布的参数是服从Dirichlet分布的。为什么呢？因为Dirichlet分布是多项式分布的共轭分布，也就是说由贝叶斯估计得到的后验分布仍然是Dirichlet分布。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:4","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LDA与PLSA的关系 二者都是概率模型，都是利用概率生成模型对文本集合进行主题分析的无监督学习方法。 PLSA是用了频率派的方法，利用极大似然进行学习，而LDA使用了贝叶斯派的方法，进行贝叶斯推断。 二者都假设存在两个分布：话题是单词的多项分布，文本是话题的多项分布，不同的在于LDA认为多项分布的参数也服从一个分布，而不是固定不变的，使用狄利克雷分布作为多项分布的先验分布，也就是多项分布的参数服从狄利克雷分布。 引入先验概率的作用可以防止过拟合。为啥选择狄利克雷分布呢？因为它是多项分布的共轭先验分布，先验分布与后验分布形式相同，便于由先验分布得到后验分布。 LDA是在Plsa的基础上，为单词分布和主题分布增加了两个狄利克雷先验。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:5","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"模型定义 模型要素 潜在狄利克雷分配 (LDA) 使用三个集合: 一是单词集合 \\(W={w_1, \\cdots, w_v, \\cdots}\\), , 其中 \\(w_v\\) 是第 \\(v\\) 个单词, \\(v=1,2, \\cdots, V, V\\) 是单词的个数。二是文本集合 \\(D={\\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\cdots, \\mathbf{w}_M}\\), 其中 \\(\\mathbf{w}_m\\) 是第 \\(m\\) 个文本, \\(m=1,2, \\cdots, M, M\\) 是文本 的个数。文本 \\(\\mathbf{w}_m\\) 是一个单词序列 \\(\\mathbf{w}_m=\\left(w_{m 1}, \\cdots, w_{m n}, \\cdots, w_{m N_m}\\right)\\), 其中 \\(w_{m n}\\) 是 文本 \\(\\mathbf{w}_m\\) 的第 \\(n\\) 个单词, \\(n=1,2, \\cdots, N_m, N_m\\) 是文本 \\(\\mathbf{w}_m\\) 中单词的个数。三是主题集合集合 \\(Z={z_1, \\cdots, z_k, \\cdots, z_K}\\), 其中 \\(z_k\\) 是第 \\(k\\) 个话题, \\(k=1,2, \\cdots, K, K\\) 是话题的个数。 每一个话题 \\(z_k\\) 由一个单词的条件概率分布 \\(p\\left(w \\mid z_k\\right)\\) 决定, \\(w \\in W\\) 。分布 \\(p\\left(w \\mid z_k\\right)\\) 服从多项分布 (严格意义上类别分布), 其参数为 \\(\\varphi_k\\) 。参数 \\(\\varphi_k\\) 服从狄利克雷分布 (先验分布), 其超参数为 \\(\\beta\\) 。参数 \\(\\varphi_k\\) 是一个 \\(V\\) 维向量 \\(\\varphi_k=\\left(\\varphi_{k 1}, \\varphi_{k 2}, \\cdots, \\varphi_{k V}\\right)\\), 其中 \\(\\varphi_{k v}\\) 表示话题 \\(z_k\\) 生成单词 \\(w_v\\) 的概率。所有话题的参数向量构成一个 \\(K \\times V\\) 矩阵 \\(\\varphi=\\{\\varphi_k\\}_{k=1}^K\\) 。超参数 \\(\\beta\\) 也是一个 \\(V\\) 维向量 \\(\\beta=\\left(\\beta_1, \\beta_2, \\cdots, \\beta_V\\right)\\_{\\text {。 }}\\)(对于话题\\(z_k\\)其生成单词\\(w_v\\)先验服从狄利克雷分布，因此是一个V维向量) 每一个文本 \\(\\mathbf{w}_m\\) 由一个话题的条件概率分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 决定, \\(z \\in Z_{\\text {。 }}\\) 分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 服从多项分布 (严格意义上类别分布), 其参数为 \\(\\theta_m\\) 。参数 \\(\\theta_m\\) 服从狄利克雷分布 (先验分布), 其超参数为 \\(\\alpha\\) , 参数 \\(\\theta_m\\) 是一个 \\(K\\) 维向量 \\(\\theta_m=\\left(\\theta_{m 1}, \\theta_{m 2}, \\cdots, \\theta_{m K}\\right)\\), 其中 \\(\\theta_{m k}\\) 表示文本 \\(\\mathrm{w}_m\\) 生成话题 \\(z_k\\) 的概率。所有文本的参数向量构成一个 \\(M \\times K\\) 矩阵 \\(\\theta=\\{\\theta_m\\}_{m=1}^M\\) 。超参数 \\(\\alpha\\) 也是一个 \\(K\\) 维向量 \\(\\alpha=\\left(\\alpha_1, \\alpha_2, \\cdots, \\alpha_K\\right)\\) 。 每一个文本 \\(\\mathbf{w}_m\\) 中的每一个单词 \\(w_{m n}\\) 由该文本的话题分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 以及所有话 题的单词分布 \\(p\\left(w \\mid z_k\\right)\\) 决定。 生成过程 LDA 文本集合的生成过程如下: 给定单词集合 \\(W\\), 文本集合 \\(D\\), 话题集合 \\(Z\\), 狄利克雷分布的超参数 \\(\\alpha\\) 和 \\(\\beta\\) 。 1.生成单词分布 随机生成 \\(K\\) 个话题的单词分布。具体过程如下, 按照狄利克雷分布 \\(\\operatorname{Dir}(\\beta)\\) 随机 生成一个参数向量 \\(\\varphi_k, \\varphi_k \\sim \\operatorname{Dir}(\\beta)\\), 作为话题 \\(z_k\\) 的单词分布 \\(p\\left(w \\mid z_k\\right), w \\in W, k=\\) \\(1,2, \\cdots, K\\) 。 2.生成主题分布 随机生成 \\(M\\) 个文本的主题分布。具体过程如下: 按照狄利克雷分布 \\(\\operatorname{Dir}(\\alpha)\\) 随 机生成一个参数向量 \\(\\theta_m, \\theta_m \\sim \\operatorname{Dir}(\\alpha)\\), 作为文本 \\(\\mathbf{w}_m\\) 的主题分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right), m=\\) \\(1,2, \\cdots, M_{}\\) 。 3.生成文本的单词序列 随机生成 \\(M\\) 个文本的 \\(N_m\\) 个单词。文本 \\(\\mathbf{w}_m(m=1,2, \\cdots, M)\\) 的单词 \\(w_{m n}(n=\\) \\(\\left.1,2, \\cdots, N_m\\right)\\) 的生成过程如下: 3.1 首先按照多项分布 \\(\\operatorname{Mult}\\left(\\theta_m\\right)\\) 随机生成一个话题 \\(z_{m n}, z_{m n} \\sim \\operatorname{Mult}\\left(\\theta_m\\right)\\) 3.2 然后按照多项分布 \\(\\operatorname{Mult}\\left(\\varphi_{z_{m n}}\\right)\\) 随机生成一个单词 \\(w_{m n}, w_{m n} \\sim \\operatorname{Mult}\\left(\\varphi_{z_{m n}}\\right)\\_{\\text {。 }}\\) 文本 \\(\\mathbf{w}_m\\) 本身是单词序列 \\(\\mathbf{w}_m=\\left(w_{m 1}, w_{m 2}, \\cdots, w_{m N_m}\\right)\\), 对应着隐式的话题序列 \\(\\mathbf{z}_m=\\left(z_{m 1}, z_{m 2}, \\cdots, z_{m N_m}\\right) 。\\) 引用一下LDA数学八卦的图： \\(\\vec{\\alpha} \\rightarrow \\vec{\\theta}_m \\rightarrow z_{m, n}\\), 这个过程表示在生成第 \\(m\\) 篇文档的时候，先从第一个坛子中抽了一个doc-topic 骰子 \\(\\vec{\\theta}_m\\),然后投这个骰子生成了文档\\(m\\)中第 \\(n\\) 个词的topic编号 \\(z_{m, n}\\) ； \\(\\vec{\\beta} \\rightarrow \\vec{\\varphi}_k \\rightarrow w_{m, n} \\mid k=z_{m, n}\\), 这个过程表示用如下动作生成语料中第 \\(m\\) 篇文档的第 \\(n\\) 个词: 在上帝手头的 \\(K\\) 个topic-word 骰子 \\(\\vec{\\varphi}_k\\) 中，挑选编号为 \\(k=z_{m, n}\\) 的那个骰子进行投掷，然后生成 word \\(w_{m, n}\\) ; 理解 LDA最重要的就是理解这两个物理过程。LDA 模型在基于 \\(K\\) 个 topic 生成语料中的 \\(M\\) 篇文档的过程中， 由于是 bag-of-words 模型，有一些物理过程是相互独立可交换的。由此，LDA生成模型中， \\(M\\) 篇文档会对应 于 \\(M\\) 个独立的 Dirichlet-Multinomial 共轭结构；K个 个 topic 会对应于 \\(K\\) 个独立的 Dirichlet-Multinomial 共轭结 构。所以理解 LDA 所需要的所有数学就是理解 Dirichlet-Multiomail 共轭，其它都就是理解物理过程。 总结 对于话题 \\(z_k(k=1,2, \\cdots, K)\\) : 生成多项分布参数 \\(\\varphi_k \\sim \\operatorname{Dir}(\\beta)\\), 作为话题的单词分布 \\(p\\left(w \\mid z_k\\right)\\); 对于文本 \\(\\mathbf{w}_m(m=1,2, \\cdots, M)\\); 生成多项分布参数 \\(\\theta_m \\sim \\operatorname{Dir}(\\alpha)\\), 作为文本的话题分布 \\(p\\left(z \\mid \\mathb","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:6","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概率计算 LDA 模型整体是由观测变量和隐变量组成的联合概率分布, 可以表为 \\[ p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)=\\prod_{k=1}^K p\\left(\\varphi_k \\mid \\beta\\right) \\prod_{m=1}^M p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m} p\\left(z_{m n} \\mid \\theta_m\\right) p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right) \\] (其中M为文本数，\\(N_m\\)为文档m的长度，K为主题数) 其中观测变量 \\(\\mathrm{w}\\) 表示所有文本中的单词序列, 隐变量 \\(\\mathrm{z}\\) 表示所有文本中的话题序列, 隐变量 \\(\\theta\\) 表示所有文本的话题分布的参数, 隐变量 \\(\\varphi\\) 表示所有话题的单词分布的参 数, \\(\\alpha\\) 和 \\(\\beta\\) 是超参数。 \\(p\\left(\\varphi_k \\mid \\beta\\right)\\) 表示超参数 \\(\\beta\\) 给定条件下第 \\(k\\) 个话题的单词分布的参数 \\(\\varphi_k\\) 的生成概率; \\(p\\left(\\theta_m \\mid \\alpha\\right)\\) 表示超参数 \\(\\alpha\\) 给定条件下第 \\(m\\) 个文本的话题分布的 参数 \\(\\theta_m\\) 的生成概率; \\(p\\left(z_{m n} \\mid \\theta_m\\right)\\) 表示第 \\(m\\) 个文本的话题分布 \\(\\theta_m\\) 给定条件下文本的 第 \\(n\\) 个位置的话题 \\(z_{m n}\\) 的生成概率; \\(p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right)\\) 表示在第 \\(m\\) 个文本的第 \\(n\\) 个位 置的话题 \\(z_{m n}\\) 及所有话题的单词分布的参数 \\(\\varphi\\) 给定条件下第 \\(m\\) 个文本的第 \\(n\\) 个位 置的单词 \\(w_{m n}\\) 的生成概率。 第 \\(m\\) 个文本的联合概率分布可以表为 \\[ p\\left(\\mathbf{w}_m, \\mathbf{z}_m, \\theta_m, \\varphi \\mid \\alpha, \\beta\\right)=\\prod_{k=1}^K p\\left(\\varphi_k \\mid \\beta\\right) p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m} p\\left(z_{m n} \\mid \\theta_m\\right) p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right) \\] 其中 \\(\\mathbf{w}_m\\) 表示该文本中的单词序列, \\(\\mathbf{z}_m\\) 表示该文本的话题序列, \\(\\theta_m\\) 表示该文本的话 题分布参数。 LDA 模型的联合分布含有隐变量, 对隐变量进行积分得到边缘分布。 参数 \\(\\theta_m\\) 和 \\(\\varphi\\) 给定条件下第 \\(m\\) 个文本的生成概率是 \\[ p\\left(\\mathbf{w}_m \\mid \\theta_m, \\varphi\\right)=\\prod_{n=1}^{N_m}\\left[\\sum_{k=1}^K p\\left(z_{m n}=k \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_k\\right)\\right] \\] 超参数 \\(\\alpha\\) 和 \\(\\beta\\) 给定条件下第 \\(m\\) 个文本的生成概率是 \\[ p\\left(\\mathbf{w}_m \\mid \\alpha, \\beta\\right)=\\prod_{k=1}^K \\int p\\left(\\varphi_k \\mid \\beta\\right)\\left[\\int p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m}\\left[\\sum_{l=1}^K p\\left(z_{m n}=l \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_l\\right)\\right] \\mathrm{d} \\theta_m\\right] \\mathrm{d} \\varphi_k \\] 超参数 \\(\\alpha\\) 和 \\(\\beta\\) 给定条件下所有文本的生成概率是 \\[ p(\\mathbf{w} \\mid \\alpha, \\beta)=\\prod_{k=1}^K \\int p\\left(\\varphi_k \\mid \\beta\\right)\\left[\\prod_{m=1}^M \\int p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m}\\left[\\sum_{l=1}^K p\\left(z_{m n}=l \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_l\\right)\\right] \\mathrm{d} \\theta_m\\right] \\mathrm{d} \\varphi_k \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:7","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"吉布斯抽样 基本思想 有三个主要目标： - 话题序列的集合\\(z=(z_1, z_2, \\cdots, z_M)\\)的后验概率分布，其中\\(z_m\\)是第m个文本的主题序列，\\(z_m=(z_{m1}, \\cdots, z_{mN_{m}})\\); - 参数\\(\\theta=(\\theta_1, \\cdots, \\theta_{M})\\)，其中\\(\\theta_m\\)是第m个文本的主题分布的参数； - 参数\\(\\varphi=(\\varphi_1, \\cdots, \\varphi_K)\\)，其中\\(\\varphi_k\\)是第k个主题的单词分布的参数。 对\\(p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)\\)进行估计 吉布斯抽样, 这是一种常用的马尔可夫链蒙特卡罗法。为了估计 多元随机变量 \\(x\\) 的联合分布 \\(p(x)\\), 吉布斯抽样法选择 \\(x\\) 的一个分量, 固定其他分量, 按照其条件概率分布进行随机抽样, 依次循环对每一个分量执行这个操作, 得到联合 分布 \\(p(x)\\) 的一个随机样本, 重复这个过程, 在燃烧期之后, 得到联合概率分布 \\(p(x)\\) 的 样本集合。 LDA 模型的学习通常采用收缩的吉布斯抽样 (collapsed Gibbs sampling) , 基本想法是, 通过对隐变量 \\(\\theta\\) 和 \\(\\varphi\\) 积分, 得到边缘概率分布 \\(p(\\mathbf{w}, \\mathbf{z} \\mid \\alpha, \\beta)\\) (也是联合分 布), 其中变量 \\(\\mathbf{w}\\) 是可观测的, 变量 \\(\\mathbf{z}\\) 是不可观测的; 对后验概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 进 行吉布斯抽样, 得到分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本集合; 再利用这个样本集合对参数 \\(\\theta\\) 和 \\(\\varphi\\) 进行估计, 最终得到 LDA 模型 \\(p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)\\) 的所有参数估计。 #### 算法流程 输入: 文本的单词序列 \\(\\mathbf{w}=\\{\\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\cdots, \\mathbf{w}_M\\}, \\mathbf{w}_m=\\left(w_{m 1}, \\cdots, w_{m n}, \\cdots\\right.\\), \\(\\left.w_{m_{N_m}}\\right)\\); 输出: 文本的话题序列 \\(\\mathrm{z}=\\{\\mathbf{z}_1, \\cdots, \\mathbf{z}_m, \\cdots, \\mathbf{z}_M\\}, \\mathbf{z}_m=\\left(z_{m 1}, \\cdots, z_{m n}, \\cdots, z_{m_{N_m}}\\right)\\) 的后验概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本计数, 模型的参数 \\(\\varphi\\) 和 \\(\\theta\\) 的估计值; 参数: 超参数 \\(\\alpha\\) 和 \\(\\beta\\), 话题个数 \\(K\\) 。 设所有计数矩阵的元素 \\(n_{m k}, n_{k v}\\), 计数向量的元素 \\(n_m, n_k\\) 初值为 0 ; 对所有文本 \\(\\mathbf{w}_m, m=1,2, \\cdots, M\\) 对第 \\(m\\) 个文本中的所有单词 \\(w_{m n}, n=1,2, \\cdots, N_m\\) 抽样话题 \\(z_{m n}=z_k \\sim \\operatorname{Mult}\\left(\\frac{1}{K}\\right)\\);(对于文本m，其多项分布的参数为\\(\\frac{1}{K}\\)，由\\(\\alpha\\)生成，即\\(\\theta_m \\sim Dir(\\alpha)\\)，\\(\\theta_m\\)为长度为K的向量。) 增加文本-话题计数 \\(n_{m k}=n_{m k}+1\\), 增加文本-话题和计数 \\(n_m=n_m+1\\), 增加话题-单词计数 \\(n_{k v}=n_{k v}+1\\), 增加话题-单词和计数 \\(n_k=n_k+1\\); （3）循环执行以下操作, 直到进入燃烧期 对所有文本 \\(\\mathbf{w}_m, m=1,2, \\cdots, M\\) 对第 \\(m\\) 个文本中的所有单词 \\(w_{m n}, n=1,2, \\cdots, N_m\\) 当前的单词 \\(w_{m n}\\) 是第 \\(v\\) 个单词, 话题指派 \\(z_{m n}\\) 是第 \\(k\\) 个话题; 减少计数 \\(n_{m k}=n_{m k}-1, n_m=n_m-1, n_{k v}=n_{k v}-1, n_k=n_k-1\\); 按照满条件分布进行抽样 \\[ p\\left(z_i \\mid \\mathbf{z}_{-i}, \\mathbf{w}, \\alpha, \\beta\\right) \\propto \\frac{n_{k v}+\\beta_v}{\\sum_{v=1}^V\\left(n_{k v}+\\beta_v\\right)} \\cdot \\frac{n_{m k}+\\alpha_k}{\\sum_{k=1}^K\\left(n_{m k}+\\alpha_k\\right)} \\] 得到新的第 \\(k^{\\prime}\\) 个话题, 分配给 \\(z_{m n}\\); 增加计数 \\(n_{m k^{\\prime}}=n_{m k^{\\prime}}+1, n_m=n_m+1, n_{k^{\\prime} v}=n_{k^{\\prime} v}+1, n_{k^{\\prime}}=n_{k^{\\prime}}+1\\); 得到更新的两个计数矩阵 \\(N_{K \\times V}=\\left[n_{k v}\\right]\\) 和 \\(N_{M \\times K}=\\left[n_{m k}\\right]\\), 表示后验 概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本计数; 利用得到的样本计数, 计算模型参数 \\[ \\begin{aligned} \\theta_{m k} \u0026=\\frac{n_{m k}+\\alpha_k}{\\sum_{k=1}^K\\left(n_{m k}+\\alpha_k\\right)} \\\\\\\\ \\varphi_{k v} \u0026=\\frac{n_{k v}+\\beta_v}{\\sum_{v=1}^V\\left(n_{k v}+\\beta_v\\right)} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:8","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"训练与推断 有了LDA模型，我们的目标有两个： 估计模型中的参数\\(\\varphi_1, \\cdots, \\varphi_K\\)和\\(\\theta_1, \\cdots, \\theta_M\\); 对于新来的一篇doc，我们能够计算这篇文档的topic分布\\(\\theta_{new}\\)。 有了吉布斯采样公式就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic语义分析，训练的过程就是通过Gibbs Samping获取语料中的（z,w）样本，而模型中的所有参数可以基于采样的样本进行估计。 训练流程如下： 随机初始化：对语料中的每篇文档的每个词w，随机赋一个topic编号z。 重新扫描语料库，对每个词按照吉布斯采样公式重新采样它的topic，在语料中进行更新。 重复以上语料库的重新采样过程直到吉布斯采样收敛。 统计语料库的topic-word共现频率矩阵，就是LDA的模型 由这个矩阵我们可以计算每一个\\(p(word\\mid topic)\\)概率，从而计算出模型参数\\(\\varphi_1, \\cdots, \\varphi_K\\)，也可以计算另一个参数\\(\\theta_1, \\cdots, \\theta_M\\)，只要在吉布斯抽样收敛后统计每篇文章的topic频率分布，就可以计算每一个\\(p(topic\\mid doc)\\)概率，由于它是和训练语料的每篇文章相关的，对于我们理解新的文档毫无用处，所以一般没有必要保留这个概率。 如何对新的文档进行推断呢？其实和训练过程完全相似，对于新的文档，认为\\(\\varphi_{kt}\\)是稳定不变的，是由训练语料得到的模型提供的。采样过程只估计该文档的topic分布\\(\\theta_{new}\\)就好了。 推断过程如下： 随机初始化：对当前文档的每个词w，随机的赋一个topic编号z； 重新扫描当前文档，按照吉布斯抽样公式，对每个词w，重新采样它的topic； 重复以上过程直到吉布斯采样收敛 统计文档中的topic分布，该分布就是\\(\\theta_{new}\\) ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:9","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"代码 实现了吉布斯推断的python代码： \"\"\" LDA implementation in Python @author: Michael Zhang \"\"\" import matplotlib.pyplot as plt import numpy as np import scipy class LDA(object): def __init__(self, tdm, T, alpha = 1., beta=1., iteration=100): \"\"\" tdm: the copus, of (D, Num_words_in_corpus), the value of each entry is the counts of corresponding words in this the corresponding document. e.g. tdm[d, w] = number of word w appears in document d. T: the number of topics \"\"\" self.tdm = tdm self.D, self.W = self.tdm.shape self.alpha= alpha # count for expected value for hyper parameter alpha of theta, i.e. document-topic distribution. self.beta = beta # count for expected value for hyper parameter beta topic-word distribution. self.T = T self.iteration = iteration # z must take in (d,w,i) as input, corresponding to # topic indicator for i-th obserevation of word w in doc d self.z = {} self.topic_word_matrix = np.zeros((self.T, self.W)) # initialize the topic-word matrix. self.doc_topic_matrix = np.zeros((self.D, self.T)) # initialize the documnet-topic matrix. self.topic_counts = np.zeros(self.T) # initialize the topic counter for after sampling process, should be sum of value in self.topic_word_matrix self.doc_counts = np.zeros(self.D) # initialize the doc counter for after sampling process, should be sum of value in self.doc_topic_matrix self.log_likelihood = np.zeros(self.iteration) # store the value of log likelihood at each iteration self._init_matrix() # @pysnooper.snoop('init.log') def _init_matrix(self): \"\"\" for all words 1. sample a topic randomly from T topics for each word 2. increment topic word count, self.topic_word_matrix 3. increment document topic count, self.doc_topic_matrix 4. update the topic indicator z. \"\"\" for d in range(self.D): doc = scipy.sparse.coo_matrix(self.tdm[d]) word_freq_topic = zip(doc.col, doc.data) for w, frequency in word_freq_topic: # (word, freq) for i in range(frequency): ############ Finish the following initialization steps ############# # 1. sample a topic randomly from T topics for each word topic = np.random.randint(self.T) # 2. increment topic word count, self.topic_word_matrix self.topic_word_matrix[topic, w] += 1 # 3. increment document topic count, self.doc_topic_matrix self.doc_topic_matrix[d, topic] += 1 # 4. update the topic indicator z. self.z[(d, w, i)] = topic # d: document ID; w: word ID: i: instance ID，即在d中第几个w self.topic_counts = self.topic_word_matrix.sum(axis=1) self.doc_counts = self.doc_topic_matrix.sum(axis=1) # @pysnooper.snoop('fit.log') def fit(self): for it in range(self.iteration): # iterate over all the documents for d in range(self.D): # iterate over all the words in d for w in self.tdm[d].indices: # iterate over number of times observed word w in doc d for i in range(self.tdm[d, w]): # we apply the hidden-varible method of Gibbs sampler, the hidden variable is z[(d,w,i)] self.doc_topic_matrix[d,self.z[(d,w,i)]] -= 1 self.doc_counts[d] -= 1 self.topic_word_matrix[self.z[(d,w,i)],w] -= 1 self.topic_counts[self.z[(d,w,i)]] -= 1 # estimation of phi and theta for the current corpus phi_hat = (self.topic_word_matrix[:,w] + self.beta) / (self.topic_counts + self.beta * self.W) theta_hat = (self.doc_topic_matrix[d,:] + self.alpha) / (self.doc_counts[d] + self.alpha * self.T) # calculate the full conditional distribution full_conditional = phi_hat * theta_hat # normalize full_conditional such that it summation equals to 1. full_conditional = full_conditional / full_conditional.sum() # sample a topic for i-th obserevation of word w in doc d based on full_conditional new_topic = np.random.multinomial(1, full_conditional).argmax() # update z, doc_topic_matrix, doc_counts, topic_word_matrix, topic_counts here. self.z[(d,w,i)] = new_topic self.doc_topic_matrix[d,self.z[(d,w,i)]] += 1 self.topic_word_matrix[self.z[(d,w,i)],w] += 1 self.doc_counts[d] += 1 self.topic_counts[self.z[(d,w,i)]] += 1 ############################################################ # Equation 2 log P(w|z) for each itera","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:10","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"本质与使用条件 本质上说，主题模型根本上是实现文本数据的结构化，结构化的文档可以彼此比较和查询，实现传统的任务。 LDA主题模型本质上解决了两类问题： - 文档聚类 - 词汇聚类 主要价值在于： 1）文档的结构化，相比于传统的词袋模型达到了降维的效果 2）完成了文档的聚类和词汇的聚类，实现文本信息的抽象化分析，帮助分析者探索隐含的语义内容。 实践中数据要有以下性质才会有较好的结果： 文档足够多 文档足够长 词汇特征够多 词频足够大 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:11","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"总结 历时好几周，终于完结了主题模型，主要是概率论没有学好，跟着推导的过程过于痛苦，不过也算是稍微理解了一点LDA，复述一下： LDA理解可以类比于PLSA，大体的思想都是根据文档生成主题分布，再根据主题分布和单词分布得到文档中的各个单词。不同的是LDA是贝叶斯派的思想，对于两种分布加入了狄利克雷先验概率。LDA的生成过程可以看成上帝掷骰子，从M个骰子中选取一个作为文本m的主题分布，从K个骰子中选取一个作为主题k的单词分布，（注意这里的多项分布的参数就是多项分布中的概率p，其服从狄利克雷分布，比如对于\\(\\theta_m\\)，它其实就是文本m生成不同主题k的概率\\(p(z\\mid d_m)\\)，是个K维的向量。对于\\(\\varphi_k\\)，是由主题k生成不同单词v的概率\\(p(w\\mid z_k)\\)，是个V维的向量。也就是根据狄利克雷分布采样得到的是一些概率，这些概率也是我们最终要求的参数，这些概率作为多项分布的参数再采样生成主题或者单词，还有就是\\(p(z_k\\mid d_m)\\)与\\(z_{mn}\\)的理解，前者就是相当于\\(\\theta_{mk}\\)，后者肯定是主题集合中的一个，不过是根据参数为\\(\\theta_m\\)的多项分布在位置n采样得到的。这就是LDA的整个的理解，当然模型的求解是使用吉布斯抽样的方法，与上面写的步骤不同。写这些是便于理解）。 由主题分布可以对文本的每个位置赋值一个主题，再根据主题-单词分布可以生成整个文本。一切的一切都是和PLSA一样，求两个分布，以至于可以生成我们的文档。LDA也可以得到文档的主题分布，得到了主题分布和单词分布可以应用于各种任务当中。具体可以参考《LDA漫游指南》。 现在知道了LDA是怎么一回事了，但还是感觉模模糊糊的，感觉如“通俗理解LDA主题模型”这篇文章开头所说的那样陷入了LDA的细枝末节中，所以写了一些主题，加深自己的印象与理解，经过代码的洗礼，又理解深入了一些，但感觉还没有掌握的很好，可能需要消化消化，那就先告一段落了。以后常看看就行。 ## 参考 https://zhuanlan.zhihu.com/p/374924140 https://www.cnblogs.com/gasongjian/p/7631978.html ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:5:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"Transformer \\[ -\\log \\frac{\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_i^{+}\\right) / \\tau})}{\\sum_{j=1}^N\\left(\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_j^{+}\\right) / \\tau})+\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_j^{-}\\right) / \\tau}\\right))} \\] ","date":"2022-06-08","objectID":"/transformer/:0:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。 而ELMo就解决了这个问题，它使用了双向的LSTM，具体的可以看ELMo,总之使用RNN作为特征提取器，解决了多义词的问题，但现在来看，RNN的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了RNN和CNN的地位，成为了最流行的特征提取器，大火的GPT和BERT都与Transformer离不开关系。拿bank为例，RNN在读取整个句子之前不会理解bank的含义，也就是RNN的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer 的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-attention），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6 个）中。 在每个解码器层中，前缀标记也通过自注意力机制相互交互。 下面就详细介绍一下。 ","date":"2022-06-08","objectID":"/transformer/:1:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"self-attention 首先介绍一下最主要的self-attention，可以说是self-attention实现了上述的token之间交互的功能。 自注意力是模型的关键组成部分之一。注意 和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。 形式上，这种直觉是通过查询键值注意来实现的。self-attention 中的每个输入标记都会收到三种表示，对应于它可以扮演的角色： query key value 进入正题： 作为我们想要翻译的输入语句“The animal didn’t cross the street because it was too tired”。句子中”it”指的是什么呢？“it”指的是”street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。 当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。当模型处理每个位置的词时，self-attention允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。如果你对RNN熟悉，就能想到RNN的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer使用self-attention来将相关词的理解编码到当前词中。 下面看一下self-attention是如何计算的： ","date":"2022-06-08","objectID":"/transformer/:2:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"向量计算 第一步，根据编码器的输入向量，生成三个向量，比如，对每个词向量，生成query-vec, key-vec, value-vec，生成方法为分别乘以三个矩阵，这些矩阵在训练过程中需要学习。【注意：不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；权重矩阵是基于输入位置的转换矩阵；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】 注意到这些新向量的维度比输入词向量的维度要小（512–\u003e64），并不是必须要小的，是为了让多头attention的计算更稳定。 第二步，计算attention就是计算一个分值。对“Thinking Matchines”这句话，对“Thinking”（pos#1）计算attention 分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。 这个分，通过“Thing”对应query-vector与所有词的key-vec依次做点积得到。所以当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积。这也就是所谓的注意力得分. 第三步和第四步，除以8(\\(=\\sqrt{dim_{key}}\\))，这样梯度会更稳定。然后加上softmax操作，归一化分值使得全为正数且加和为1。 softmax分值决定着在这个位置，每个词的表达程度（关注度）。很明显，这个位置的词应该有最高的归一化分数，但大部分时候总是有助于关注该词的相关的词。 第五步，将softmax分值与value-vec按位相乘。保留关注词的value值，削弱非相关词的value值。 第六步，将所有加权向量加和，产生该位置的self-attention的输出结果。 上述就是self-attention的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。 ","date":"2022-06-08","objectID":"/transformer/:2:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"矩阵计算 第一步，计算query/key/value matrix，将所有输入词向量合并成输入矩阵\\(X\\)，并且将其分别乘以权重矩阵\\(W^q, W^k,W^v\\) 最后，鉴于我们使用矩阵处理，将步骤2~6合并成一个计算self-attention层输出的公式。 ","date":"2022-06-08","objectID":"/transformer/:2:2","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"多头注意力机制 论文进一步增加了multi-headed的机制到self-attention上，在如下两个方面提高了attention层的效果： 多头机制扩展了模型集中于不同位置的能力。在上面的例子中，z1只包含了其他词的很少信息，仅由实际自己词决定。在其他情况下，比如翻译 “The animal didn’t cross the street because it was too tired”时，我们想知道单词”it”指的是什么。 多头机制赋予attention多种子表达方式。像下面的例子所示，在多头下有多组query/key/value-matrix，而非仅仅一组（论文中使用8-heads）。每一组都是随机初始化，经过训练之后，输入向量可以被映射到不同的子表达空间中。 如果我们计算multi-headed self-attention的，分别有八组不同的Q/K/V matrix，我们得到八个不同的矩阵。 这会带来点麻烦，前向网络并不能接收八个矩阵，而是希望输入是一个矩阵，所以要有种方式处理下八个矩阵合并成一个矩阵。 上述就是多头自注意机制的内容，我认为还仅是一部分矩阵，下面尝试着将它们放到一个图上可视化如下。 #### 代码 下面实现一下多头注意力机制，在原论文中，实现的方法如下： 也就是对每个W进行多头的设置，即为原维度/head，然后拼接后，再经过\\(hd_v\\times d_{model}\\)的转换又得到原来的维度，代码的实现不太一样，代码是W还是\\(d_{model}\\times d_{model}\\)的矩阵然后得到q,k,v之后再进行截断，实现如下。 class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1) -\u003e None: # h为head，这里为8，d_model为embedding的维度，这里为512 super().__init__() assert d_model % h == 0 self.d_k = d_model // h # 64 self.h = h self.Q_Linear = nn.Linear(d_model, d_model) self.K_Linear = nn.Linear(d_model, d_model) self.V_Linear = nn.Linear(d_model, d_model) self.res_Linear = nn.Linear(d_model, d_model) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) batch_size = query.size(0) query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64) query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64) key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k) # attn为(batch_size, h, seq_len1, seq_len2) x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (batch_size, h, seq_len, d_k) -\u003e (batch_size, seq_len, h, d_k) -\u003e (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512) return self.res_Linear(x) ","date":"2022-06-08","objectID":"/transformer/:2:3","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"Masked self-attention 在训练的时候，主要是消除后面的信息对预测的影响，因为decoder输入的是整个句子，也就是我们所谓的参考答案，而实际预测的时候就是预测后面的token，用不到后面的token，如果不mask掉，当前的token将看到“未来”，这不是我们想要的，因此必须要mask掉。 其实decoder里的sequence mask与encoder里的padding mask异曲同工，padding mask其实很简单，就是为了使句子长度一致进行了padding，而为了避免关注padding的位置，进行了mask，具体的做法就是将这些位置的值变成负无穷，这样softmax之后就接近于0了。 而sequence mask思想也差不多： 假设现在解码器的输入”\u003c s \u003e who am i \u003c e \u003e“在分别乘上一个矩阵进行线性变换后得到了Q、K、V，且Q与K作用后得到了注意力权重矩阵（此时还未进行softmax操作），如图17所示。 此时已经计算得到了注意力权重矩阵。由第1行的权重向量可知，在解码第1个时刻时应该将20%（严格来说应该是经过softmax后的值）的注意力放到’\u003c s \u003e’上，30%的注意力放到’who’上等等。不过此时有一个问题就是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer中的Decoder通过加入注意力掩码机制来解决了这一问题。 当然还要进行softmax等计算。 在网上查了很多资料，说法都很不一样，不过我更倾向于这样的看法。而在预测的时候是用前面的输出结果作为输入的。 几张图帮助理解： 后面还有padding mask，所有的self attention都要用这个，因为pad的位置没有任何意义。 实践一下加深理解： 首先我们来定义模型： # 词典数为10， 词向量维度为8 embedding = nn.Embedding(10, 8) # 定义Transformer，注意一定要改成eval模型，否则每次输出结果不一样 transformer = nn.Transformer(d_model=8, batch_first=True).eval() 接下来定义我们的src和tgt： # Encoder的输入 src = torch.LongTensor([[0, 1, 2, 3, 4]]) # Decoder的输入 tgt = torch.LongTensor([[4, 3, 2, 1, 0]]) 然后我们将[4]送给Transformer进行预测，模拟推理时的第一步： transformer(embedding(src), embedding(tgt[:, :1]), # 这个就是用来生成阶梯式的mask的 tgt_mask=nn.Transformer.generate_square_subsequent_mask(1)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 然后我们将[4, 3]送给Transformer，模拟推理时的第二步： transformer(embedding(src), embedding(tgt[:, :2]), tgt_mask=nn.Transformer.generate_square_subsequent_mask(2)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 出的第一个向量和上面那个一模一样。 最后我们再将tgt一次性送给transformer，模拟训练过程： transformer(embedding(src), embedding(tgt), tgt_mask=nn.Transformer.generate_square_subsequent_mask(5)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396], [ 1.4799, -0.3575, 0.8310, 0.1642, 0.8811, -1.3140, -1.5643, -0.1204], [ 1.4359, -0.6524, 0.8377, 0.1742, 1.0521, -1.3222, -1.3799, -0.1454], [ 1.3465, -0.3771, 0.9107, 0.1636, 0.8627, -1.5061, -1.4732, 0.0729]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 可以看到使用mask后就可以保证前面的结果都是不变的，不然如果没有mask则计算attention时因为计算注意力变化所以结果都会变化，这就是Mask self-attention的意义。 到这里self-attention就介绍完了 ","date":"2022-06-08","objectID":"/transformer/:2:4","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"代码 def attention(query, key, value, mask=None, dropout=None): d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # 最后两个维度相乘，即为scores，再scale一下。 if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 将mask的位置的scores置为-1e9 # 实际上pad mask的时候，pad也会作为key与其它token对应的k,v计算score，pad mask只是消除pad作为k,v时候的影响。但在最后softmax的时候，将pad的损失值全部置为0 p_attn = F.softmax(scores, dim=-1) # 将scores进行softmax，得到p_attn，这里是在最后一个维度上softmax，因为对每个query的所有key进行softmax if dropout: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1) -\u003e None: # h为head，这里为8，d_model为embedding的维度，这里为512 super().__init__() assert d_model % h == 0 self.d_k = d_model // h # 64 self.h = h self.Q_Linear = nn.Linear(d_model, d_model) self.K_Linear = nn.Linear(d_model, d_model) self.V_Linear = nn.Linear(d_model, d_model) self.res_Linear = nn.Linear(d_model, d_model) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) batch_size = query.size(0) query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64) query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64) key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k) # attn为(batch_size, h, seq_len1, seq_len2) x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (batch_size, h, seq_len, d_k) -\u003e (batch_size, seq_len, h, d_k) -\u003e (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512) return self.res_Linear(x) ","date":"2022-06-08","objectID":"/transformer/:2:5","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"模型架构 下面是原始论文中的架构： self-attention上面已经讲的比较详细了，下面说一下其余的部分。 ","date":"2022-06-08","objectID":"/transformer/:3:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"FFN(前馈网络) 除了注意力以外，每一层都有一个前馈网络：两个线性层之间具有ReLU非线性： \\[ FFN(x) = max(0, xW_1+b_1)W_2+b_2 \\] 在通过注意力机制查看其他令牌之后，模型使用 FFN 块来处理这些新信息。 class PositionwiseFeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout=0.1): super().__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(p=dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) ","date":"2022-06-08","objectID":"/transformer/:3:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"残差连接 残差连接非常简单（将块的输入添加到其输出），但同时也非常有用：它们缓解了通过网络的梯度流并允许堆叠很多层。解决了网络退化的问题。 在 Transformer 中，在每个注意力和 FFN 块之后使用残差连接。在上图中，残差显示为围绕一个块到黄色 “Add \u0026 Norm”层的箭头。在“Add \u0026 Norm”部分， “Add”部分代表残差连接。 ","date":"2022-06-08","objectID":"/transformer/:3:2","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"Layer Norm “Add \u0026 Norm”层中的“Norm”部分 表示 Layer Normalization。它批量独立地标准化每个示例的向量表示 - 这样做是为了控制“流”到下一层。层归一化提高了收敛稳定性，有时甚至提高了质量。 这里的scale和bias都是可以训练的参数。 注意Layer Norm与Batch Norm是不同的，这里引用一下沐神的视频： 这是Batch Norm的切法，即对每个特征进行norm。 这是Layer norm的切法，即对每个样本进行norm。 为什么用layer norm而不用Batch norm呢？ 当你的样本长度变化比较大的时候，使用batch norm计算的均值和方差波动比较大，而且batch norm需要记录全局的均值和方差，当遇到新的测试样本的时候，由于长度的原因，之前的均值方差可能就效果不太好了。 但是如果使用layer norm 的话就没有那么多的问题，因为它是每个样本自己计算均值方差，不需要存在一个全局的均值方差，所以会稳定一点。 class LayerNorm(nn.Module): def __init__(self, features, eps=1e-6) -\u003e None: super().__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 class SublayerConnection(nn.Module): def __init__(self, size, dropout) -\u003e None: super().__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(p=dropout) def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x))) # 这里和论文不同，先norm再扔给sublayer（比如多头注意力、ffd）,理论上是self.norm(x+self.dropout(sublayer(x))) ","date":"2022-06-08","objectID":"/transformer/:3:3","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"位置编码(position encoding) (Position Embedding是学习式，而Position Encoding为固定式) 请注意，由于 Transformer 不包含递归或卷积，它不知道输入标记(token)的顺序。因此，我们必须让模型明确地知道标记的位置。为此，我们有两组嵌入：用于标记（我们总是这样做）和用于位置（该模型所需的新嵌入）。那么令牌的输入表示是两个嵌入的总和：令牌和位置。 位置嵌入是可以学习的，但作者发现固定的嵌入不会影响质量。Transformer 中使用的固定位置编码是： \\[ PE_{pos,2i} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\] \\[ PE_{pos,2i+1} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\] 可以看到，每个词的维度都是512维，假设句子长度为10，则位置编码的计算如上图所示。 得到位置编码后，将位置编码与词嵌入简单相加即可。 #### 代码 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000) -\u003e None: super().__init__() self.dropout = nn.Dropout(p=dropout) position_embedding = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) position_embedding[:, 0::2] = torch.sin(position * div_term) position_embedding[:, 1::2] = torch.cos(position * div_term) position_embedding = position_embedding.unsqueeze(0) # 增加一维预留batch size的位置，所以后面forward要在第二维上选取序列长度 self.register_buffer('PositionalEncoding', position_embedding) def forward(self, x): return self.dropout(x + Variable(self.PositionalEncoding[:, :x.size(1)], requires_grad=False)) 这里为了计算做了转换。 ### Padding Mask 对于输入序列一般我们都要进行padding补齐，也就是说设定一个统一长度N，在较短的序列后面填充0到长度为N。对于那些补零的数据来说，我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样经过softmax后，这些位置的权重就会接近0。Transformer的padding mask实际上是一个张量，每个值都是一个Boolean，值为false的地方就是要进行处理的地方。 ","date":"2022-06-08","objectID":"/transformer/:3:4","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"label smoothing(标签平滑) 神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。 label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 增加label smoothing后真实的概率分布有如下改变： 代码 class LabelSmoothing(nn.Module): # [标签平滑](../Deep%20Learning/训练trick/标签平滑.md)损失函数 def __init__(self, size, padding_idx, smoothing=0.0) -\u003e None: super().__init__() self.criterion = nn.KLDivLoss(size_average=False) self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): # x的shape为(batch.size * seq.len, target.vocab.size) # y的shape是(batch.size * seq.len) # x=logits，(seq.len, target.vocab.size) # 每一行，代表一个位置的词 # 类似于：假设seq.len=3, target.vocab.size=5 # x中保存的是log(prob) #x = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) # target 类似于： # target = tensor([2, 1, 0])，torch.size=(3) assert x.size(1) == self.size true_dist = x.data.clone() # true_dist = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) true_dist.fill_(self.smoothing / (self.size - 2)) # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333], #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333], #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]]) # 注意，这里分母target.vocab.size-2是因为 # (1) 最优值 0.6要占一个位置； # (2) 填充词 \u003cblank\u003e 要被排除在外 # 所以被激活的目标语言词表大小就是self.size-2 true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) # target.data.unsqueeze(1) -\u003e # tensor([[2], #[1], #[0]]); shape=torch.Size([3, 1]) # self.confidence = 0.6 # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值 # 填入true_dist: 因为target.data是2,1,0的内容， # 所以，0.6填入第0行的第2列（列号，行号都是0开始） # 0.6填入第1行的第1列 # 0.6填入第2行的第0列： # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333], #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333], #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]]) true_dist[:, self.padding_idx] = 0 # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333], #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]]) # 设置true_dist这个tensor的第一列的值全为0 # 因为这个是填充词'\u003cblank\u003e'所在的id位置，不应该计入 # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表 #中的一个词的id mask = torch.nonzero(target.data == self.padding_idx) # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0， # 因为是'\u003cblank\u003e'的id，所以通过上面的一步，把他们找出来 # 如果不加上nonzero，那么mask的shape就是torch.Size([3]) if mask.dim() \u003e 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) # 当target reference序列中有0这个'\u003cblank\u003e'的时候，则需要把 # 这一行的值都清空。 # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要 # pad '\u003cblank\u003e'来填充，所以会出现类似于(2,1,0)这样的情况 # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333], # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) self.true_dist = true_dist return self.criterion(x, Variable(true_dist, requires_grad=False)) # 这一步就是调用KL loss来计算 # x = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333], # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) ","date":"2022-06-08","objectID":"/transformer/:3:5","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"预测 预测过程与一般seq2seq不同的是，t时刻是将1到t-1时刻所有的预测结果作为序列进行预测，而seq2seq只是使用前一时刻的输出作为当前时刻的输入，这里困扰了我很久，实现了transformer代码后对比李沐老师的代码才理解。 其中seq2seq: transformer: 注意ys最后与之前的ys使用cat函数合并在一起。 ## 总结 Transformer还有很多的模型细节，以后遇到了再记录一下，在面试中很容易问到这些细节，因此可以参考面经边学习边记录，可以查缺补漏也可以学到新的东西。接下来把代码复现一下可以加深理解，并且提高自己的代码水平和实践能力。 ","date":"2022-06-08","objectID":"/transformer/:4:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"一些问题 Transformer在哪里做了权重共享，为什么可以做权重共享？ Transformer在两个地方进行了权重共享： （1）Encoder和Decoder间的Embedding层权重共享； （2）Decoder中Embedding层和FC层权重共享。 对于（1），《Attention is all you need》中Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，嵌入时都只有对应语言的embedding会被激活，因此是可以共用一张词表做权重共享的。 论文中，Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。 但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。 对于（2），Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。 那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和softmax概率会是最大的。 因此，Embedding层和FC层权重共享，Embedding层中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的Embedding层和FC层有点像互为逆过程。 通过这样的权重共享可以减少参数的数量，加快收敛。 ","date":"2022-06-08","objectID":"/transformer/:5:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"为什么除以根号d 论文中的解释是：向量的点积结果会很大，将 softmax 函数 push 到梯度很小的区域，scaled 会缓解这种现象。 \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}=\\mathrm{diag}(\\mathbf{y})-\\mathbf{y}\\mathbf{y}^T\\] 当\\(\\mathbf{y} =\\)softmax\\(( \\mathbf{x} )\\)时，\\(\\mathbf{y}\\)对\\(\\mathbf{x}\\)的梯度为： 这是一个jacobi矩阵\\(^{+}\\),表示y的每一个元素对x每一个元素的导数是什么。 展开： \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}=\\begin{bmatrix}y_1\u00260\u0026\\cdots\u00260\\\\0\u0026y_2\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u0026y_d\\end{bmatrix}-\\begin{bmatrix}y_1^2\u0026y_1y_2\u0026\\cdots\u0026y_1y_d\\\\y_2y_1\u0026y_2^2\u0026\\cdots\u0026y_2y_d\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\y_dy_1\u0026y_dy_2\u0026\\cdots\u0026y_d^2\\end{bmatrix}\\] 根据前面的讨论，当输入 \\(\\mathbf{x}\\) 的某一个元素较大时，softmax 会把大部分概率分布\\(^{+}\\)分配给最大的元 素，假设我们的输入数量级很大，那么就将产生一个接近 one-hot 的向量 \\[\\mathbf{y}\\approx[1,0,\\cdots,0]^\\top \\] 此时上面的矩阵变为如下形式 \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\\approx\\begin{bmatrix}1\u00260\u0026\\cdots\u00260\\\\0\u00260\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u00260\\end{bmatrix}-\\begin{bmatrix}1\u00260\u0026\\cdots\u00260\\\\0\u00260\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u00260\\end{bmatrix}=\\mathbf{0}\\] 也就是所有的梯度都接近0 除以\\(\\sqrt{ d }\\)后就使x的分布更加平缓，从而防止梯度消失。 from scipy.special import softmax import numpy as np def test_gradient(dim, time_steps=50, scale=1.0): # Assume components of the query and keys are drawn from N(0, 1) independently q = np.random.randn(dim) ks = np.random.randn(time_steps, dim) x = np.sum(q * ks, axis=1) / scale # x.shape = (time_steps,) y = softmax(x) grad = np.diag(y) - np.outer(y, y)# softmax gradient(dy/dx) return np.max(np.abs(grad)) # the maximum component of gradients NUMBER_OF_EXPERIMENTS = 5 # results of 5 random runs without scaling print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)]) # results of 5 random runs with scaling print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)]) 输出可看到下面的梯度比上面的梯度更大。 这时又有一个问题，为什么多分类的softmax+交叉熵不需要除以东西呢？这是因为交叉熵中有一个log，log_softmax的梯度和刚才算出来的不同，就算输入的某一个x过大也不会梯度消失。所以就又可以推断出softmax+MSE会导致梯度消失，因为MSE中没有Log，这是为什么分类任务不使用MSE损失函数的原因之一。 ### 为什么 Transformer 需要进行 Multi-head Attention 实验证明多头是必要的，8/16个头都可以取得更好的效果，但是超过16个反而效果不好。每个头关注的信息不同，但是头之间的差异随着层数增加而减少。并且不是所有头都有用，有工作尝试剪枝，可以得到更好的表现。 论文中提到模型分为多个头，形成多个子空间，每个头关注不同方面的信息。 那为什么每个头的维度要降呢? 一言蔽之的话，大概是：在不增加时间复杂度的情况下，同时，借鉴CNN多核的思想，在更低的维度，在多个独立的特征空间，更容易学习到更丰富的特征信息。 ### 为什么 Transformer 的 Embedding 最后要乘dmodel 具体的原因是，如果使用 Xavier 初始化，Embedding 的方差为 1/d_model，当d_model非常大时，矩阵中的每一个值都会减小。通过乘一个 dmodel 可以将方差恢复到1。 因为Position Encoding是通过三角函数算出来的，值域为[-1, 1]。所以当加上 Position Encoding 时，需要放大 embedding 的数值，否则规模不一致相加后会丢失信息。 因为 Bert 使用的是学习式的Embedding，所以 Bert 这里就不需要放大。 # 参考 Bert/Transformer 被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com) ","date":"2022-06-08","objectID":"/transformer/:5:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["算法题"],"content":"最大子序和 https://leetcode-cn.com/problems/maximum-subarray/ 一开始直接暴力，结果tle了最后 class Solution: def maxSubArray(nums): res = -float('inf') for i in range(len(nums)): for j in range(i,len(nums)): res = max(res,sum(nums[i:j+1])) return res 这说明在leetcode尽量不要嵌套循环，大概率Tle class Solution: def maxSubArray(nums): for i in range(1,len(nums)): maxs = max(nums[i-1]+nums[i],nums[i]) nums[i] = maxs return max(nums) 最后巧妙地利用了替换的思想，将每次相加的值和当前比较，并将当前替换为较大的那个值，最后求整个列表的最大值。 ","date":"2022-06-08","objectID":"/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/:0:0","tags":["算法题","最大子序和"],"title":"最大子序和","uri":"/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/"},{"categories":["算法题"],"content":"使用最小花费爬楼梯 每日一题刷到的。 动态规划类型的题目，重点就是找状态转移方程，因为我不太熟练，对动态规划的题目做的比较少，所以WA了好几次。 class Solution: def minCostClimbingStairs(cost): res = [] #res[i]就是到第i阶梯时最小的花费 res.append(cost[0]) #到第一阶梯最小就是0+cost[0] res.append(cost[1]) #第二阶梯最小就是0+cost[1] #状态转移方程:res[i] = min(res[i-1],res[i-2])+cost[i] for i in range(2,len(cost)): res.append(min(res[i-1],res[i-2])+cost[i]) # return min(res[-1],res[-2]) 踏上第i级台阶有两种方法： 先踏上第i-2级台阶（最小总花费dp[i-2]），再直接迈两步踏上第i级台阶（花费cost[i]），最小总花费dp[i-2] + cost[i]； 先踏上第i-1级台阶（最小总花费dp[i-1]），再迈一步踏上第i级台阶（花费cost[i]），最小总花费dp[i-1] + cost[i]； 上述为引用的题解的说明，更加深了对动态规划的理解 ","date":"2022-06-01","objectID":"/%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E8%8A%B1%E8%B4%B9%E7%88%AC%E6%A5%BC%E6%A2%AF/:0:0","tags":["算法题","使用最小花费爬楼梯"],"title":"使用最小花费爬楼梯","uri":"/%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E8%8A%B1%E8%B4%B9%E7%88%AC%E6%A5%BC%E6%A2%AF/"},{"categories":["Mathematical Modeling","插值算法"],"content":"Hermite插值法 ","date":"2022-05-15","objectID":"/hermite/:0:0","tags":["Mathematical Modeling","插值算法","Hermite"],"title":"Hermite","uri":"/hermite/"},{"categories":["Mathematical Modeling","插值算法"],"content":"公式 ","date":"2022-05-15","objectID":"/hermite/:1:0","tags":["Mathematical Modeling","插值算法","Hermite"],"title":"Hermite","uri":"/hermite/"},{"categories":["Mathematical Modeling","插值算法"],"content":"代码 import matplotlib.pyplot as plt import numpy as np #计算基函数的导数值 def dl(i, xi): result = 0.0 for j in range(0,len(xi)): if j!=i: result += 1/(xi[i]-xi[j]) result *= 2 return result #计算基函数值 def l(i, xi, x): deno = 1.0 nu = 1.0 for j in range(0, len(xi)): if j!= i: deno *= (xi[i]-xi[j]) nu *= (x-xi[j]) return nu/deno #Hermite插值函数 def get_Hermite(xi, yi, dyi): def he(x): result = 0.0 for i in range(0, len(xi)): result += (yi[i]+(x-xi[i])*(dyi[i]-2*yi[i]*dl(i, xi))) * ((l(i,xi,x))**2) return result return he import math sr_x = [(i * math.pi) + (math.pi / 2) for i in range(-3, 3)] sr_fx = [math.sin(i) for i in sr_x] deriv = [0 for i in sr_x] # 导数都为 0 Hx = get_Hermite(sr_x, sr_fx, deriv) # 获得插值函数 tmp_x = [i * 0.1 * math.pi for i in range(-20, 20)] # 测试用例 tmp_y = [Hx(i) for i in tmp_x] # 根据插值函数获得测试用例的纵坐标 #画图 plt.plot(sr_x, sr_fx, 'ro') plt.plot(tmp_x, tmp_y, 'b-') plt.title('Hermite Interpolation') plt.show() ","date":"2022-05-15","objectID":"/hermite/:2:0","tags":["Mathematical Modeling","插值算法","Hermite"],"title":"Hermite","uri":"/hermite/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"CRF ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:0:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"概率图模型与无向图 图是由结点和连接结点的边组成的集合。结点和边分别记作v和e，结点和边的集合分别记作V和E，图记作\\(G=(V, E)\\)。 无向图是指没有方向的图。 概率图模型是由图表示的概率分布。设有联合概率分布P(Y), Y是一组随机变量，由无向图\\(G=(V,E)\\)表示概率分布P(Y)，即在图G中，结点\\(v\\in V\\)表示一个随机变量\\(Y_v\\)，\\(Y=(Y_v)\\_{v\\in V}\\)，边e表示随机变量之间的依赖关系。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:1:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"概率无向图模型 设有联合概率分布P(Y)，由无向图\\(G=(V,E)\\)表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布满足成对、局部或全局马尔科夫性，就称此联合概率分布称为概率无向图模型，或马尔科夫随机场。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:2:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"因子分解 首先给出无向图的团和最大团的定义： 无向图G中任何两个结点均有边连接的结点子集称为团。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为更大的团，则称此C为最大团。 将无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。 给定概率无向图模型, 设其无向图为 \\(G, C\\) 为 \\(G\\) 上的最大团, \\(Y_C\\) 表示 \\(C\\) 对应的 随机变量。那么概率无向图模型的联合概率分布 \\(P(Y)\\) 可写作图中所有最大团 \\(C\\) 上的 函数 \\(\\Psi_C\\left(Y_C\\right)\\) 的乘积形式, 即 \\[ P(Y)=\\frac{1}{Z} \\prod_C \\Psi_C\\left(Y_C\\right) \\] 其中, \\(Z\\) 是规范化因子 (normalization factor), 由式 \\[ Z=\\sum_Y \\prod_C \\Psi_C\\left(Y_C\\right) \\] 给出。规范化因子保证 \\(P(Y)\\) 构成一个概率分布。函数 \\(\\Psi_C\\left(Y_C\\right)\\) 称为势函数 (potential function)。这里要求势函数 \\(\\Psi_C\\left(Y_C\\right)\\) 是严格正的, 通常定义为指数函数: \\[ \\Psi_C\\left(Y_C\\right)=\\exp \\\\{-E\\left(Y_C\\right)\\\\\\} \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:3:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"条件随机场 条件随机场是指给定随机变量X的条件下，随机变量Y的马尔科夫随机场。一般的条件随机场主要是指线性链条件随机场，可以用于标注等问题。这里的\\(P(Y|X)\\)中，Y是输出变量，表示标注序列，X是输入变量，表示需要标注的观察序列。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"一般的条件随机场 (条件随机场) 设 \\(X\\) 与 \\(Y\\) 是随机变量, \\(P(Y \\mid X)\\) 是在给定 \\(X\\) 的条件 下 \\(Y\\) 的条件概率分布。若随机变量 \\(Y\\) 构成一个由无向图 \\(G=(V, E)\\) 表示的马尔可夫 随机场, 即 \\[ P\\left(Y_v \\mid X, Y_w, w \\neq v\\right)=P\\left(Y_v \\mid X, Y_w, w \\sim v\\right) \\] 对任意结点 \\(v\\) 成立, 则称条件概率分布 \\(P(Y \\mid X)\\) 为条件随机场。式中 \\(w \\sim v\\) 表示在 图 \\(G=(V, E)\\) 中与结点 \\(v\\) 有边连接的所有结点 \\(w, w \\neq v\\) 表示结点 \\(v\\) 以外的所有结 点, \\(Y_v, Y_u\\) 与 \\(Y_w\\) 为结点 \\(v, u\\) 与 \\(w\\) 对应的随机变量。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"线性链条件随机场 设\\(X=(X_1,X_2, \\dots, X_n), \\quad Y=(Y_1, Y_2, \\dots , Y_n)\\)均为线性链表示的随机变量序列，若在给定随机变量序列X的条件下，随机变量Y的条件概率分布\\(P(Y|X)\\)构成条件随机场，即满足马尔科夫性 \\[ P\\left(Y_i \\mid X, Y_1, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_n\\right)=P\\left(Y_i \\mid X, Y_{i-1}, Y_{i+1}\\right) \\] \\(i=1,2, \\cdots, n\\) (在 \\(i=1\\) 和 \\(n\\) 时只考虑单边) 则称 \\(P(Y \\mid X)\\) 为线性链条件随机场。在标注问题中, \\(X\\) 表示输入观测序列, \\(Y\\) 表示对 应的输出标记序列或状态序列。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"线性链条件随机场参数化形式 根据因子分解, 可以给出线性链条件随机场 \\(P(Y \\mid X)\\) 的因子分解式, 各因子是定 义在相邻两个结点 (最大团) 上的势函数。 (线性链条件随机场的参数化形式) 设 \\(P(Y \\mid X)\\) 为线性链条件随机 场, 则在随机变量 \\(X\\) 取值为 \\(x\\) 的条件下, 随机变量 \\(Y\\) 取值为 \\(y\\) 的条件概率具有如下 形式: \\[ P(y \\mid x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_k t_k\\left(y_{i-1}, y_i, x, i\\right)+\\sum_{i, l} \\mu_l s_l\\left(y_i, x, i\\right)\\right) \\] 其中, \\[ Z(x)=\\sum_y \\exp \\left(\\sum_{i, k} \\lambda_k t_k\\left(y_{i-1}, y_i, x, i\\right)+\\sum_{i, l} \\mu_l s_l\\left(y_i, x, i\\right)\\right) \\] 式中, \\(t_k\\) 和 \\(s_l\\) 是特征函数, \\(\\lambda_k\\) 和 \\(\\mu_l\\) 是对应的权值。 \\(Z(x)\\) 是规范化因子, 求和是在所 有可能的输出序列上进行的。 这两个式子是线性链条件随机场模型的基本形式, 表示给定输入序列 \\(x\\), 对输出序列 \\(y\\) 预测的条件概率。\\(t_k\\) 是定义在边上的特 征函数, 称为转移特征, 依赖于当前和前一个位置; \\(s_l\\) 是定义在结点上的特征函数, 称为状态特征, 依赖于当前位置。 \\(t_k\\) 和 \\(s_l\\) 都依赖于位置, 是局部特征函数。通常, 特 征函数 \\(t_k\\) 和 \\(s_l\\) 取值为 1 或 0 ; 当满足特征条件时取值为 1 , 否则为 0 。条件随机场完 全由特征函数 \\(t_k, s_l\\) 和对应的权值 \\(\\lambda_k, \\mu_l\\) 确定。 线性链条件随机场也是对数线性模型 (log linear model)。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:3","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"条件随机场的简化形式 为简便起见, 首先将转移特征和状态特征及其权值用统一的符号表示。设有 \\(K_1\\) 个转移特征, \\(K_2\\) 个状态特征, \\(K=K_1+K_2\\), 记 \\[ f_k\\left(y_{i-1}, y_i, x, i\\right)= \\begin{cases}t_k\\left(y_{i-1}, y_i, x, i\\right), \u0026 k=1,2, \\cdots, K_1 \\\\\\\\ s_l\\left(y_i, x, i\\right), \u0026 k=K_1+l ; l=1,2, \\cdots, K_2\\end{cases} \\] 然后, 对转移与状态特征在各个位置 \\(i\\) 求和, 记作 \\[ f_k(y, x)=\\sum_{i=1}^n f_k\\left(y_{i-1}, y_i, x, i\\right), \\quad k=1,2, \\cdots, K \\] 用 \\(w_k\\) 表示特征 \\(f_k(y, x)\\) 的权值, 即 \\[ w_k= \\begin{cases}\\lambda_k, \u0026 k=1,2, \\cdots, K_1 \\\\\\\\ \\mu_l, \u0026 k=K_1+l ; l=1,2, \\cdots, K_2\\end{cases} \\] 于是, 条件随机场可表示为 \\[ \\begin{aligned} P(y \\mid x) \u0026=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^K w_k f_k(y, x) \\\\\\\\ Z(x) \u0026=\\sum_y \\exp \\sum_{k=1}^K w_k f_k(y, x) \\end{aligned} \\] 若以 \\(w\\) 表示权值向量, 即 \\[ w=\\left(w_1, w_2, \\cdots, w_K\\right)^{\\mathrm{T}} \\] 以 \\(F(y, x)\\) 表示全局特征向量, 即 \\[ F(y, x)=\\left(f_1(y, x), f_2(y, x), \\cdots, f_K(y, x)\\right)^{\\mathrm{T}} \\] 则条件随机场可以写成向量 \\(w\\) 与 \\(F(y, x)\\) 的内积的形式: \\[ P_w(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\] 其中, \\[ Z_w(x)=\\sum_y \\exp (w \\cdot F(y, x)) \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:4","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"矩阵形式 对每个 标记序列引进特殊的起点和终点状态标记 \\(y_0=\\) start 和 \\(y_{n+1}=s t o p\\), 这时标注序列 的概率 \\(P_w(y \\mid x)\\) 可以通过矩阵形式表示并有效计算。 对观测序列 \\(x\\) 的每一个位置 \\(i=1,2, \\cdots, n+1\\), 由于 \\(y_{i-1}\\) 和 \\(y_i\\) 在 \\(m\\) 个标记中 取值, 可以定义一个 \\(m\\) 阶矩阵随机变量 \\[ M_i(x)=\\left[M_i\\left(y_{i-1}, y_i \\mid x\\right)\\right] \\] 矩阵随机变量的元素为 \\[ \\begin{aligned} \u0026M_i\\left(y_{i-1}, y_i \\mid x\\right)=\\exp \\left(W_i\\left(y_{i-1}, y_i \\mid x\\right)\\right) \\\\\\\\ \u0026W_i\\left(y_{i-1}, y_i \\mid x\\right)=\\sum_{k=1}^K w_k f_k\\left(y_{i-1}, y_i, x, i\\right) \\end{aligned} \\] 这里 \\(w_k\\) 和 \\(f_k\\) 分别由前面的式子给出, \\(y_{i-1}\\) 和 \\(y_i\\) 是标记随机变量 \\(Y_{i-1}\\) 和 \\(Y_i\\) 的取值。 这样, 给定观测序列 \\(x\\), 相应标记序列 \\(y\\) 的非规范化概率可以通过该序列 \\(n+1\\) 个矩阵的适当元素的乘积 \\(\\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right)\\) 表示。于是, 条件概率 \\(P_w(y \\mid x)\\) 是 \\[ P_w(y \\mid x)=\\frac{1}{Z_w(x)} \\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right) \\] 其中, \\(Z_w(x)\\) 为规范化因子, 是 \\(n+1\\) 个矩阵的乘积的 (start, stop) 元素, 即 \\[ Z_w(x)=\\left[M_1(x) M_2(x) \\cdots M_{n+1}(x)\\right]_{\\text {start,stop }} \\] 注意, \\(y_0=\\) start 与 \\(y_{n+1}=\\) stop 表示开始状态与终止状态, 规范化因子 \\(Z_w(x)\\) 是以 start 为起点 stop为终点通过状态的所有路径 \\(y_1 y_2 \\cdots y_n\\) 的非规范化概率 \\(\\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right)\\) 之和。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:5","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"概率计算问题 与HMM类似，引入前向和后向变量，递归的计算概率和一些期望值。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"前向-后向算法 对每个指标 \\(i=0,1, \\cdots, n+1\\), 定义前向向量 \\(\\alpha_i(x)\\) : \\[ \\alpha_0(y \\mid x)= \\begin{cases}1, \u0026 y=\\text { start } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] 递推公式为 \\[ \\alpha_i^{\\mathrm{T}}\\left(y_i \\mid x\\right)=\\alpha_{i-1}^{\\mathrm{T}}\\left(y_{i-1} \\mid x\\right)\\left[M_i\\left(y_{i-1}, y_i \\mid x\\right)\\right], \\quad i=1,2, \\cdots, n+1 \\] 又可表示为 \\[ \\alpha_i^{\\mathrm{T}}(x)=\\alpha_{i-1}^{\\mathrm{T}}(x) M_i(x) \\] \\(\\alpha_i\\left(y_i \\mid x\\right)\\) 表示在位置 \\(i\\) 的标记是 \\(y_i\\) 并且从 1 到 \\(i\\) 的前部分标记序列的非规范化概 率, \\(y_i\\) 可取的值有 \\(m\\) 个, 所以 \\(\\alpha_i(x)\\) 是 \\(m\\) 维列向量。 同样, 对每个指标 \\(i=0,1, \\cdots, n+1\\), 定义后向向量 \\(\\beta_i(x)\\) : \\[ \\begin{aligned} \\beta_{n+1}\\left(y_{n+1} \\mid x\\right) \u0026= \\begin{cases}1, \u0026 y_{n+1}=\\text { stop } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\\\\\\\ \\beta_i\\left(y_i \\mid x\\right) \u0026=\\left[M_{i+1}\\left(y_i, y_{i+1} \\mid x\\right)\\right] \\beta_{i+1}\\left(y_{i+1} \\mid x\\right) \\end{aligned} \\] 又可表示为 \\[ \\beta_i(x)=M_{i+1}(x) \\beta_{i+1}(x) \\] \\(\\beta_i\\left(y_i \\mid x\\right)\\) 表示在位置 \\(i\\) 的标记为 \\(y_i\\) 并且从 \\(i+1\\) 到 \\(n\\) 的后部分标记序列的非规范化 概率。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"概率计算 按照前向-后向向量的定义, 很容易计算标记序列在位置 \\(i\\) 是标记 \\(y_i\\) 的条件概率 和在位置 \\(i-1\\) 与 \\(i\\) 是标记 \\(y_{i-1}\\) 和 \\(y_i\\) 的条件概率: \\[ P\\left(Y_i=y_i \\mid x\\right)=\\frac{\\alpha_i^{\\mathrm{T}}\\left(y_i \\mid x\\right) \\beta_i\\left(y_i \\mid x\\right)}{Z(x)} \\] \\[ P\\left(Y_{i-1}=y_{i-1}, Y_i=y_i \\mid x\\right)=\\frac{\\alpha_{i-1}^{\\mathrm{T}}\\left(y_{i-1} \\mid x\\right) M_i\\left(y_{i-1}, y_i \\mid x\\right) \\beta_i\\left(y_i \\mid x\\right)}{Z(x)} \\] 其中, \\[ Z(x)=\\alpha_n^{\\mathrm{T}}(x) \\mathbf{1}=1 \\beta_1(x) \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"预测问题 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"维特比算法 还是使用维特比算法。 \\[ \\begin{aligned} y^* \u0026=\\arg \\max_y P_w(y \\mid x) \\\\\\\\ \u0026=\\arg \\max_y \\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\\\\\\\ \u0026=\\arg \\max_y \\exp (w \\cdot F(y, x)) \\\\\\\\ \u0026=\\arg \\max_y(w \\cdot F(y, x)) \\end{aligned} \\] 于是, 条件随机场的预测问题成为求非规范化概率最大的最优路径问题 \\[ \\max_y(w \\cdot F(y, x)) \\] 这里, 路径表示标记序列。其中, \\[ \\begin{aligned} w \u0026=\\left(w_1, w_2, \\cdots, w_K\\right)^{\\mathrm{T}} \\\\\\\\ F(y, x) \u0026=\\left(f_1(y, x), f_2(y, x), \\cdots, f_K(y, x)\\right)^{\\mathrm{T}} \\\\\\\\ f_k(y, x) \u0026=\\sum_{i=1}^n f_k\\left(y_{i-1}, y_i, x, i\\right), \\quad k=1,2, \\cdots, K \\end{aligned} \\] 注意, 这时只需计算非规范化概率, 而不必计算概率, 可以大大提高效率。为了求解最 优路径, 写成如下形式: \\[ \\max_y \\sum_{i=1}^n w \\cdot F_i\\left(y_{i-1}, y_i, x\\right) \\] 其中, \\[ F_i\\left(y_{i-1}, y_i, x\\right)=\\left(f_1\\left(y_{i-1}, y_i, x, i\\right), f_2\\left(y_{i-1}, y_i, x, i\\right), \\cdots, f_K\\left(y_{i-1}, y_i, x, i\\right)\\right)^{\\mathrm{T}} \\] 是局部特征向量。 下面叙述维特比算法。首先求出位置 1 的各个标记 \\(j=1,2, \\cdots, m\\) 的非规范化概率: \\[ \\delta_1(j)=w \\cdot F_1\\left(y_0=\\text { start, } y_1=j, x\\right), \\quad j=1,2, \\cdots, m \\] 一般地, 由递推公式, 求出到位置 \\(i\\) 的各个标记 \\(l=1,2, \\cdots, m\\) 的非规范化概率的最 大值, 同时记录非规范化概率最大值的路径 \\[ \\begin{gathered} \\delta_i(l)=\\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\\\\\\\ \\Psi_i(l)=\\arg \\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\end{gathered} \\] 直到 \\(i=n\\) 时终止。这时求得非规范化概率的最大值为 \\[ \\operatorname{max}_y(w \\cdot F(y, x))=\\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\] 及最优路径的终点 \\[ y_n^* =\\arg \\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\] 由此最优路径终点返回, \\[ y_i^* =\\Psi_{i+1}\\left(y_{i+1}^* \\right), \\quad i=n-1, n-2, \\cdots, 1 \\] 求得最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)^{\\mathrm{T}}\\) 。 综上所述, 得到条件随机场预测的维特比算法。 (条件随机场预测的维特比算法) 输入: 模型特征向量 \\(F(y, x)\\) 和权值向量 \\(w\\), 观测序列 \\(x=\\left(x_1, x_2, \\cdots, x_n\\right)\\); 输出: 最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)\\) 。 (1) 初始化 \\[ \\delta_1(j)=w \\cdot F_1\\left(y_0=\\operatorname{start}, y_1=j, x\\right), \\quad j=1,2, \\cdots, m \\] 递推。对 \\(i=2,3, \\cdots, n\\) \\[ \\begin{gathered} \\delta_i(l)=\\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\\\\\\\ \\Psi_i(l)=\\arg \\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\end{gathered} \\] （3）终止 \\[ \\begin{gathered} \\max_y(w \\cdot F(y, x))=\\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\\\\\\\ y_n^* =\\arg \\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\end{gathered} \\] 返回路径 \\[ y_i^* =\\Psi_{i+1}\\left(y_{i+1}^* \\right), \\quad i=n-1, n-2, \\cdots, 1 \\] 求得最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)\\_{\\text {。 }}\\) 自己的理解就是非规范化概率每个i代表时间步i，要对所有的\\(\\lambda_k t_k\\)和 \\(\\mu_ks_k\\) 进行筛选，找出符合条件的相加，这里要注意下标的理解。括号里的和HMM的类似，代表y的取值。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"实例 这里使用维特比算法求解 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"参数学习问题 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:7:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型","马尔科夫网络"],"content":"总结 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:8:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["Mathematical Modeling","插值算法"],"content":"分段线性插值 利用线性函数作插值 每一段的线性函数： \\[ F1 = \\frac{x-x_{i+1}}{x_i-x_{i+1}}f(x_i)+\\frac{x-x_i}{x_{i+1}-x_i}f(x_{i+1}) \\] ","date":"2022-05-10","objectID":"/%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/:0:0","tags":["Mathematical Modeling","插值算法","分段线性插值"],"title":"分段线性插值","uri":"/%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"代码 import numpy as np import matplotlib.pyplot as plt #分段线性插值闭包 def get_line(xn, yn): def line(x): index = -1 #找出x所在的区间 for i in range(1, len(xn)): if x \u003c= xn[i]: index = i-1 break else: i += 1 if index == -1: return -100 #插值 result = (x-xn[index+1])*yn[index]/float((xn[index]-xn[index+1])) + (x-xn[index])*yn[index+1]/float((xn[index+1]-xn[index])) return result return line xn = [i for i in range(-50,50,10)] yn = [i**2 for i in xn] #分段线性插值函数 lin = get_line(xn, yn) x = [i for i in range(-50, 40)] y = [lin(i) for i in x] #画图 plt.plot(xn, yn, 'ro') plt.plot(x, y, 'b-') plt.show() ","date":"2022-05-10","objectID":"/%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/:1:0","tags":["Mathematical Modeling","插值算法","分段线性插值"],"title":"分段线性插值","uri":"/%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling"],"content":"层次分析法 ","date":"2022-04-30","objectID":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/:0:0","tags":["Mathematical Modeling","层次分析法"],"title":"层次分析法","uri":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"categories":["Mathematical Modeling"],"content":"一、基本步骤 1、建立层次结构模型。在深入分析实际问题的基础上，将有关的各个因素按照不同属性自上而下地分解成若干层次，同一层的诸因素从属于上一层的因素或对上层因素有影响，同时又支配下一层的因素或受到下层因素的作用。最上层为目标层，通常只有1个因素，最下层通常为方案或对象层，中间可以有一个或几个层次，通常为准则或指标层。当准则过多时(譬如多于9个)应进一步分解出子准则层。 2、构造成对比较阵。从层次结构模型的第2层开始，对于从属于(或影响)上一层每个因素的同一层诸因素，用成对比较法和1—9比较尺度构造成对比较阵，直到最下层。 3、计算权向量并做一致性检验。对于每一个成对比较阵计算最大特征根及对应特征向量，利用一致性指标、随机一致性指标和一致性比率做一致性检验。若检验通过，特征向量(归一化后)即为权向量：若不通过，需重新构造成对比较阵。 4、计算组合权向量并做组合一致性检验。计算最下层对目标的组合权向量，并根据公式做组合一致性检验，若检验通过，则可按照组合权向量表示的结果进行决策，否则需要重新考虑模型或重新构造那些一致性比率较大的成对比较阵。 ## 二、建立模型 ","date":"2022-04-30","objectID":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/:1:0","tags":["Mathematical Modeling","层次分析法"],"title":"层次分析法","uri":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"categories":["Mathematical Modeling"],"content":"三、构建比较矩阵 比较第i个元素与第j个元素相对上一层某个因素的重要性时，使用数量化的相对权重a i j来描述。设共有n个元素参与比较，则\\(A=(a_{ij})\\_{n\\times n}\\)称为成对比较矩阵。 成对比较矩阵中a i j的取值可参考Satty的提议，按下述标度进行赋值。a i j在1-9及其倒数中间取值。 a i j = 1，元素i与元素j对上一层次因素的重要性相同； a i j = 3，元素i比元素j略重要； a i j = 5，元素i比元素j重要； a i j = 7，元素i比元素j重要得多； a i j = 9，元素i比元素j的极其重要； a i j = 2 n，n=1,2,3,4，元素i与j的重要性介于\\(a_{ij} = 2 n − 1\\)与 \\(a_{ij} = 2 n + 1\\)之间； \\(a_{ij}=\\frac{1}{n}\\)，n=1,2,…,9，当且仅当a j i = n。 成对比较矩阵的特点：\\(a_{ij}\u003e0,a_{ij}=1,a_{ij}=\\frac{1}{a_{ji}}\\)。（备注：当i=j时候，a i j = 1） ","date":"2022-04-30","objectID":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/:2:0","tags":["Mathematical Modeling","层次分析法"],"title":"层次分析法","uri":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"categories":["Mathematical Modeling"],"content":"四、一致性检验 从理论上分析得到：如果A是完全一致的成对比较矩阵，应该有 \\[ a_{ij}a_{jk} = a_{ik}, 1\u003ci,j,k\\leq n \\] 但实际上在构造成对比较矩阵时要求满足上述众多等式是不可能的。因此退而要求成对比较矩阵有一定的一致性，即可以允许成对比较矩阵存在一定程度的不一致性。 由分析可知，对完全一致的成对比较矩阵，其绝对值最大的特征值等于该矩阵的维数。对成对比较矩阵的一致性要求，转化为要求： 的绝对值最大的特征值和该矩阵的维数相差不大。 检验成对比较矩阵A一致性的步骤如下： 计算衡量一个成对比较矩阵A （n\u003e1 阶方阵）不一致程度的指标CI： \\[ CI = \\frac{\\lambda_{max}(A)-n}{n-1} \\] 判断方法如下： 当CR\u003c0.1时，判定成对比较阵A 具有满意的一致性，或其不一致程度是可以接受的；否则就调整成对比较矩阵A，直到达到满意的一致性为止。 ## 五、求权重 ## 六、计算结果 ","date":"2022-04-30","objectID":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/:3:0","tags":["Mathematical Modeling","层次分析法"],"title":"层次分析法","uri":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"categories":["Mathematical Modeling"],"content":"附：求权重的python代码 # 层次分析法 import numpy as np RI_dict = {1: 0, 2: 0, 3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45} A = [[1, 1/2, 4, 3, 3], [2, 1, 7, 5, 5], [1/4, 1/7, 1, 1/2, 1/3], [1/3, 1/5, 2, 1, 1], [1/3, 1/5, 3, 1, 1]] n = len(A) # 算数平均法求权重 Sum_A = np.sum(A, axis=0, keepdims=True) print(\"算数平均值法：\") print(np.sum(A / Sum_A, axis=1) / n) print(\"\") # 几何平均法求权重 mul = np.prod(A, axis=1) temp = mul ** (1/n) print(temp / np.sum(temp)) # 特征值法求权重 d,v = np.linalg.eig(A) # d为特征值，v为特征向量 Max_eig = np.max(d) c = np.where(d == Max_eig)[0] print(\"特征值法：\") res = v[:, c] / sum(v[:, c]) print(np.squeeze(res)) CI = (Max_eig - n) / (n - 1) RI = RI_dict[n] CR = CI / RI if CR \u003c 0.1: print(f\"CR的值为{CR}，一致性可以接受\") else: print(f\"CR的值为{CR}，一致性不可以接受\") ","date":"2022-04-30","objectID":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/:4:0","tags":["Mathematical Modeling","层次分析法"],"title":"层次分析法","uri":"/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"categories":["sklearn","feature_selection"],"content":"sklearn.feature_selection.RFE RFE（Recursive feature elimination）：递归特征消除，用来对特征进行重要性评级。主要用于特征选择 RFE阶段 1 初始的特征集为所有可用的特征。 2 使用当前特征集进行建模，然后计算每个特征的重要性。 3 删除最不重要的一个（或多个）特征，更新特征集。 4 跳转到步骤2，直到完成所有特征的重要性评级。 用法如下 model = LogisticRegression() rfe = RFE(model,8) rfe = rfe.fit(X,y) print(f\"Selected features {list(X.columns[rfe.support_])}\") sklearn.feature_selection.RFECV 相对RFE加了CV即交叉验证的阶段 CV阶段 1 根据RFE阶段确定的特征重要性，依次选择不同数量的特征。 2 对选定的特征集进行交叉验证。 3 确定平均分最高的特征数量，完成特征选择。 用法如下： rfecv = RFECV(estimator=LogisticRegression(),step=1,cv=10,scoring='accuracy') rfecv.fit(X,y) print(\"Optimal number of features:\",rfecv.n_features_) print(rfecv.support_) print(\"Selecting features:\",list(X.columns[rfecv.support_])) ","date":"2022-04-29","objectID":"/rfe/:0:0","tags":["sklearn","feature_selection","RFE"],"title":"RFE","uri":"/rfe/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。 这段叙述里面有很多需要解决的东西，比如如何计算误差率，如何得到弱学习器权重系数，如何更新样本权重，如何结合。 ","date":"2022-04-27","objectID":"/adaboost/:0:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"Adaboost 先看一下adaboost的算法流程 将初始化样本的权重为\\(\\frac{1}{m}\\)，那么这个样本权重到底怎么用呢，主要是用于在构建弱分类器时计算弱学习器在训练集上的误差时使用，也就是计算误差： \\[ \\epsilon_t = \\sum_{i=1}^mP(h_t(x_i) \\neq y_i) =\\sum_{i=1}^m D_{ti}I(h_t(x_i)\\neq y_i) \\] 如果弱分类器为决策树的话，肯定要选择误差最小的划分作为最佳划分，最小的误差最为本轮弱分类器的误差。实际上样本权重不参与训练，只是通过样本权重动态调节模型的关注重点，即通过计算误差，影响决策树的划分，来调节模型的关注重点，这就是样本权重的作用。 然后计算弱学习器权重\\(\\alpha_t\\)，更新样本权重，对于分类错误的样本权重上升，对于分类正确的样本权重下调，再除以规范化因子，确保仍然是一个分布，一般就是\\(D_t\\)的和作为规范化因子，下一轮的模型就更注意划分错误的这些样本点。这样每一轮迭代都会得到弱分类器系数和弱分类器，这里具体的计算过程可以参考李航的《统计学习方法》，迭代完成以后，按照公式将权重与弱分类器相乘后相加，最后经过信号函数得到最终的分类结果。 ## 优点 Adaboost作为分类器时，分类精度很高 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。 作为简单的二元分类器时，构造简单，结果可理解。 不容易发生过拟合 ","date":"2022-04-27","objectID":"/adaboost/:1:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"缺点 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。 ","date":"2022-04-27","objectID":"/adaboost/:2:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"另一种解释 上面叙述的是一般的Adaboost算法的解释，其实还有另外一种解释，就是adaboost为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。 那么什么是前向分步算法？ 输入: 训练数据集\\(T=\\{(x_1, y_1 ),(x_2, y_2), \\cdots,\\left(x_N, y_N\\right)\\}\\); 损失函数 \\(L(y, f(x))\\); 基函数集 \\(\\{b(x ; \\gamma)\\}\\); 输出: 加法模型 \\(f(x)\\) 。 初始化 \\(f_0(x)=0\\); 对 \\(m=1,2, \\cdots, M\\) 极小化损失函数 \\[ \\left(\\beta_m, \\gamma_m\\right)=\\arg \\min_{\\beta, \\gamma} \\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i ; \\gamma\\right)\\right) \\] 得到参数 \\(\\beta_m, \\gamma_m\\) 。 更新 \\[ f_m(x)=f_{m-1}(x)+\\beta_m b\\left(x ; \\gamma_m\\right) \\] 得到加法模型 \\[ f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b\\left(x ; \\gamma_m\\right) \\] 学过GBDT的话就感觉会很熟悉。实际上提升树和GBDT也都是使用的前向分步算法。 直接上结论吧，推导的感觉没有必要，毕竟都是boosting方法。具体证明的过程可以看李航老师的《统计学习方法》。 Adaboost算法是前向分步加法算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。 ","date":"2022-04-27","objectID":"/adaboost/:3:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"总结 总是要写个总结的，adaboost一种boosting方法，就是不断迭代调整权重，最后将每一步的结果加权求和，可以说是一种典型的boosting的方法。 ","date":"2022-04-27","objectID":"/adaboost/:4:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Deep Learning","优化算法"],"content":"反向传播算法遵循两个法则：梯度下降法则和链式求导法则。 梯度下降法则不用多说，记住一切的目的就是为了减小损失，即朝着局部最小值点移动。链式求导也就是高数中学的那一套。具体地看一个推导就可以了，反向传播需要一步一步来，要搞清楚每一步在做什么。 这里以输出层为sigmoid激活函数的神经网络为例子。 参数\\(w\\)和\\(b\\)梯度的求解： \\[ \\frac{\\partial J}{\\partial w}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{\\partial J}{\\partial a^{(i)}}\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial w} \\] \\[ \\frac{\\partial J}{\\partial a^{(i)}}= -\\frac{y}{a^{(i)}}+\\frac{1-y}{1-a^{(i)}} \\] \\[ \\frac{\\partial g(z)}{\\partial z}=-\\frac{1}{(1+e^{-z})^2}(-e^{-z})=\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}\\times(1-\\frac{1}{1+e^{-z}})=g(z)(1-g(z)) \\] 所以 \\[ \\frac{\\partial a^{(i)}}{\\partial z^{(i)}}=a^{(i)}(1-a^{(i)}) \\] \\[ \\frac{\\partial z^{(i)}}{\\partial w}=x^{(i)} \\] 可得， \\[ \\frac{\\partial J}{\\partial w}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(a^{(i)}-y)x^{(i)} \\] 求和可以使用numpy的dot函数通过内积计算来实现。 同样地，推导可得， \\[ \\frac{\\partial J}{\\partial b}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(a^{(i)}-y) \\] 多隐藏层也一样，不过是当前层的激活值作为下一层的输入。慢慢推导即可。 实现的过程不必这么复杂和小心，只需要定义不同层的前向传播和反向传播即可，然后按照积木一样搭建，比如定义Linear层、sigmoid层等，定义前向传播和反向传播，注意输入输出就好，然后搭建起来就是神经网络了。 ","date":"2022-04-09","objectID":"/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/:0:0","tags":["Deep Learning","优化算法","反向传播算法"],"title":"反向传播算法","uri":"/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"},{"categories":["sklearn"],"content":" 通过验证曲线判定过拟合于欠拟合。 验证曲线是一种通过定位过拟合于欠拟合等诸多问题的方法，帮助提高模型性能的有效工具。 验证曲线绘制的是准确率与模型参数之间的关系。 from sklearn.model_selection import train_test_split,validation_curve import numpy as np from sklearn.svm import SVC from sklearn.datasets import load_digits import matplotlib.pyplot as plt digits = load_digits() X = digits.data Y = digits.target param_range = np.logspace(-6,-2.3,5) train_loss,test_loss = validation_curve(SVC(),X,Y,param_name=\"gamma\", param_range=param_range, cv=10, scoring='neg_mean_squared_error', ) param_range array([1.00000000e-06, 8.41395142e-06, 7.07945784e-05, 5.95662144e-04, 5.01187234e-03]) train_mean = -np.mean(train_loss,axis=1) test_mean = -np.mean(test_loss,axis=1) plt.plot(param_range,train_mean,label=\"Training\") plt.plot(param_range,test_mean,label=\"Cross-validation\") plt.legend() plt.show() ​ ​ 验证曲线（validation_curve）和学习曲线（sklearn.model_selection.learning_curve()）的区别是，验证曲线的横轴为某个超参数，如一些树形集成学习算法中的max_depth、min_sample_leaf等等。 从验证曲线上可以看到随着超参数设置的改变，模型可能从欠拟合到合适，再到过拟合的过程，进而选择一个合适的位置，来提高模型的性能。 ","date":"2022-04-04","objectID":"/%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF/:0:0","tags":["sklearn","验证曲线"],"title":"验证曲线","uri":"/%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF/"},{"categories":["算法题"],"content":"分割等和子集 ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:0:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/partition-equal-subset-sum/?utm_source=LCUS\u0026utm_medium=ip_redirect\u0026utm_campaign=transfer2china ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:1:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"思路： 典型的01背包问题，利用套路框架做即可 注意做了优化，把原本的二维dp降低了一维 ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:2:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"代码： class Solution: def canPartition(self, nums: List[int]) -\u003e bool: if sum(nums) % 2: return False s = sum(nums) // 2 dp = [False for _ in range(s+1)] dp[0] = True for i in range(1,len(nums)+1): for j in range(s,nums[i-1]-1,-1): # 容量 dp[j] = dp[j] or dp[j-nums[i-1]] # 用了or操作符 return dp[s] 更一般的套路，定义二维数组，然后二维dp # i代表前i个物品,j代表背包容量。 class Solution: def canPartition(self, nums: List[int]) -\u003e bool: if len(nums) \u003c= 1: return False if sum(nums) % 2: return False s = sum(nums) // 2 dp = [[False for _ in range(s+1)] for _ in range(len(nums)+1)] for i in range(len(nums)+1): dp[i][0] = True # 背包容量为0时 永远都是满的 所以为true for i in range(1,len(nums)+1): # 物品个数 for j in range(1,s+1): # 背包容量，最大为总和的一半，也就是需要求的 if j - nums[i-1] \u003c 0: # 如果容量小于当前物品的重量 dp[i][j] = dp[i-1][j] else: dp[i][j] = dp[i-1][j] or dp[i-1][j-nums[i-1]] if dp[i][s]: # 剪枝 return True return dp[len(nums)][s] '''首先，由于i是从 1 开始的，而数组索引是从 0 开始的，所以第i个物品的重量应该是nums[i-1]，这一点不要搞混。 dp[i - 1][j-nums[i-1]]也很好理解：你如果装了第i个物品，就要看背包的剩余重量j - nums[i-1]限制下是否能够被恰好装满。 换句话说，如果j - nums[i-1]的重量可以被恰好装满，那么只要把第i个物品装进去，也可恰好装满j的重量；否则的话，重量j肯定是装不满的。''' ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:3:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["面经"],"content":"kd树 knn算法就是用kd树实现的 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:0:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"二分查找 很简单 就不说了 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:1:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"BST 很简单 就不说了 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:2:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"多维数组 假设数组B为\\([[6, 2], [6, 3], [3, 5], [5, 0], [1, 2], [4, 9], [8, 1]]\\)，有一个元素x，我们要找到数组B中距离x最近的元素，应该如何实现呢？比较直接的想法是用数组B中的每一个元素与x求距离，距离最小的那个元素就是我们要找的元素。假设x = [1, 1]，那么用数组B中的所有元素与x求距离得到[5.0, 5.4, 4.5, 4.1, 1.0, 8.5, 7.0]，其中距离最小的是1，对应的元素是数组B中的[1, 2]，所以[1, 2]就是我们的查找结果。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:3:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"kd-tree ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:4:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"如何建立 你 1. 建立根节点； 选取方差最大的特征作为分割特征(或者根据深度选择) 选择该特征的中位数作为分割点； 将数据集中该特征小于中位数的传递给根节点的左儿子，大于中位数的传递给根节点的右儿子； 递归执行步骤2-4，直到所有数据都被建立到KD Tree的节点上为止。 不难看出，KD Tree的建立步骤跟BST是非常相似的，可以认为BST是KD Tree在一维数据上的特例。KD Tree的算法复杂度介于O(Log2(N))和O(N)之间。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:5:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"为什么选择方差最大的维度 数据分割后分散的比较开，主要是为了减少回溯时间，减少子树的访问。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:6:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"为什么选择中位数作为分割点 因为借鉴了BST，选取中位数，让左子树和右子树的数据数量一致，便于二分查找。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:7:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"查找元素 从根节点出发进行查找，根据当前深度计算比较的特征维度，若目标节点的特征值小于当前节点的特征值则遍历左子树，否则遍历右子树 找到叶子结点后，将其暂时标记为当前最邻近的点 递归地向上回退，在回退时需要做： 如果当前节点与目标节点的距离更近，则更新最邻近节点为当前节点 如果当前节点对应特征与目标节点对应特征的值距离小于当前最小值时，进入当前节点的另一个子节点（因为刚刚从一个子节点遍历回来）进行查找（如果存在子节点的话），有可能存在更近的节点。否则的话继续向上回退。 回退到根节点结束。得到最邻近点。 class Node: def __init__(self, data, left=None, right=None): self.val = data self.left = left self.right = right class KDTree: def __init__(self, k): self.k = k def create_Tree(self, dataset, depth): if not dataset: return None mid_index = len(dataset) // 2 # 中位数索引 axis = depth % self.k # 选择的维度 sort_dataset = sorted(dataset, key=(lambda x: x[axis])) # 按照维度排序 mid_data = sort_dataset[mid_index] # 中位数索引对应的数据 cur_node = Node(mid_data) # 创建节点 left_data = sort_dataset[:mid_index] # 左子树数据 right_data = sort_dataset[mid_index+1:] # 右子树数据 cur_node.left = self.create_Tree(left_data, depth+1) # 递归创建左子树 cur_node.right = self.create_Tree(right_data, depth+1) # 递归创建右子树 # print(cur_node.val) return cur_node def search(self, tree, new_data): # kd树的搜索 self.near_node = None # 最近的节点 self.near_val = None # 最近的节点的值 def dfs(node, depth): if not node: return axis = depth % self.k # 当前深度对应选择的维度 if new_data[axis] \u003c node.val[axis]: # 如果新数据的维度值小于当前节点的维度值 dfs(node.left, depth+1) # 递归搜索左子树 else: dfs(node.right, depth+1) # 递归搜索右子树 # 到这就相当于到达了叶子节点 dist = self.distance(new_data, node.val) # 计算新数据与当前节点的距离 if not self.near_val or dist \u003c self.near_val: # 如果当前节点的距离小于最近的节点的距离 self.near_val = dist # 更新最近的节点的距离 self.near_point = node.val # 更新最近的节点的值 #判断是否要进入兄弟节点寻找 if abs(new_data[axis] - node.val[axis]) \u003c self.near_val: # 如果新数据的维度值与当前节点的维度值的差值小于最近的节点的距离，说明兄弟节点区域有可能存在更接近的值。 if new_data[axis] \u003c node.val[axis]: # 控制去兄弟节点而不是刚刚回溯来的节点。 dfs(node.right, depth+1) else: dfs(node.left, depth+1) dfs(tree, 0) return self.near_point def distance(self, point_1, point_2): res = 0 for i in range(self.k): res += (point_1[i] - point_2[i]) ** 2 return res ** 0.5 if __name__ == '__main__': data_set = [[2,3],[5,4],[9,6],[4,7],[8,1],[7,2]] new_data = [1,5] k = len(data_set[0]) kd_tree = KDTree(k) our_tree = kd_tree.create_Tree(data_set, 0) predict = kd_tree.search(our_tree, new_data) print('Nearest Point of {}: {}'.format(new_data,predict)) Nearest Point of [1, 5]: [2, 3] 借用一下别人画的解题过程 ## 参考 https://zhuanlan.zhihu.com/p/499241064#:~:text=kd%E6%A0%91%E7%94%A8%E4%BA%8E%E5%AF%B9k%E7%BB%B4,%E7%9A%84%E6%97%B6%E5%80%99%E9%9D%9E%E5%B8%B8%E8%80%97%E6%97%B6%E3%80%82 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:8:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["算法题"],"content":"最小公众前缀 leetcode上的简单题，最小公众前缀 有三种解法，一种常规，两种巧妙解法 # 最小公共前缀 #解1：常规解法 思路就是一个一个判断 先判断所有字符串第一个是否相同，不相同就返回，否则然后依次往后判断 def longestCommonPrefix1(strs): if len(strs) == 0: return '' if len(strs) == 1: return strs[0] minl=min([len(x) for x in strs]) #求最小长度 end = 0 while end \u003c minl: #判断是否到最小长度 for i in range(1,len(strs)): #以第一个字符串为基准 if strs[i][end] != strs[i-1][end]: #如果到end这里不再相等 则返回到end这里的字符串即最小公共前缀 return strs[0][:end] end+=1 return strs[0][:end] #常规方法容易想到 但是缺点是运行速度慢，从每次判断都要遍历所有字符串就可以看出 #解2: 通过ascii码来判断 #Python里字符串是可以比较的，按照ascII值排 def longestCommonPrefix2(strs): if not strs: return 0 s1 = max(strs) s2 = min(strs) #找出s1 s2的最小公共前缀即为整个列表的最小公共前缀 for i,s in enumerate(s2): if s1[i] != s: return s1[:i] return s2 #通过max 和 min 函数来找到列表里面最大最小的两个字符串 然后找到这两个字符串的最小公共前缀。 #解3：通过python语法糖 将每个字符串的每个对应字符串存为一组，用zip函数，比如说所有的字符串第一个存在一起，然后用set去重，如果留下了一个，则说明都重复了，则就是相同的 def longestCommonPrefix3(strs): if not strs: return 0 cc = list(map(set,zip(*strs))) #为什么用map呢 因为要对zip压缩后的每一个序列去重 res = '' #结果 for i,s in enumerate(cc): x = list(s) if len(x) \u003e 1: #如果长度大于1 说明有不一样的 则直接退出 break res += x[0] return res 如上！ ","date":"2022-03-20","objectID":"/%E6%9C%80%E5%B0%8F%E5%85%AC%E4%BC%97%E5%89%8D%E7%BC%80/:0:0","tags":["算法题","最小公众前缀"],"title":"最小公众前缀","uri":"/%E6%9C%80%E5%B0%8F%E5%85%AC%E4%BC%97%E5%89%8D%E7%BC%80/"},{"categories":["Machine Learning","回归算法"],"content":"参考：https://cuijiahua.com/blog/2017/12/ml_13_regtree_1.html ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"1、ID3算法的弊端 回忆一下，决策树的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。 除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征离散化，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在特性。 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"2、CART算法 与ID3算法相反，CART算法正好适用于连续型特征。CART算法使用二元切分法来处理连续型变量。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。 CART算法有两步： 决策树生成：递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义”最好”： 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。 决策树剪枝我们先不管，我们看下决策树生成。 在决策树的文章中，我们先根据信息熵的计算找到最佳特征切分数据集构建决策树。CART算法的决策树生成也是如此，实现过程如下： 使用CART算法选择特征 根据特征切分数据集合 构建树 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"推导 选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小 值时的(j,s)对。其中Rm是被划分的输入空间， \\(\\mathrm{cm}\\) 是空间Rm对应的固定输出值。 \\[ \\min_{j, s}\\left[\\min_{c_{1}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min_{c_{2}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}\\right] \\] 用选定的(j,s)对，划分区域并决定相应的输出值 \\[ \\begin{gathered} R_{1}(j, s)={x \\mid x^{(j)} \\leq s}, R_{2}(j, s)={x \\mid x^{(j)}\u003es} \\\\\\\\ \\hat{c}_{m}=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}(j, s)} y_{i} \\\\\\\\ x \\in R_{m}, m=1,2 \\end{gathered} \\] 继续对两个子区域调用上述步骤，将输入空间划分为 \\(M\\) 个区域R1,R2,..,Rm，生成决策树。 \\[ f(x)=\\sum_{m=1}^{M} \\hat{c}_{m} I\\left(x \\epsilon R_{m}\\right) \\] 当输入空间划分确定时，可以用平方误差来表示回归树对于训练数据的预测方法，用平方误差最小 的准则求解每个单元上的最优输出值。 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:2:1","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"实例 【机器学习】回归决策树-CSDN博客 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd.read_csv(\"./datasets/Social_Network_Ads.csv\") X = data.iloc[:, [2, 3]].values y = data.iloc[:, 4].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.fit_transform(X_test) from sklearn.svm import SVC classifier = SVC(kernel = 'linear', random_state = 0) classifier.fit(X_train, y_train) SVC(kernel='linear', random_state=0) y_pred = classifier.predict(X_test) classifier.score(X_test,y_test) 0.88 from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_test, y_pred) from matplotlib.colors import ListedColormap X_set, y_set = X_train, y_train X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('SVM (Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() from matplotlib.colors import ListedColormap X_set, y_set = X_test, y_test X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01), np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j) plt.title('SVM (Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() ","date":"2022-03-13","objectID":"/svm_sklearn/:0:0","tags":["sklearn","SVM_sklearn"],"title":"SVM_sklearn","uri":"/svm_sklearn/"},{"categories":["Mathematical Modeling","插值算法"],"content":"牛顿插值 ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:0:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"差商 定义：设 \\(f(x)\\) 在互异节点\\(x_i\\)处的函数值为\\(f_i, i=0,1,\\dots,n\\)，称\\(f[x_i,x_j]=\\frac{f_i-f_j}{x_i-x_j}\\)为\\(f(x)\\)关于节点\\(x_i,x_j\\)的一阶差商，\\(f[x_i,x_j,x_k]=\\frac{f[x_i,x_j]-f[x_j,x_k]}{x_i-x_k}\\)为\\(f(x)\\)关于\\(x_i,x_j,x_k\\)的二阶差商，以此类推k阶差商： \\[ f[x_0,x_1,\\dots ,x_k-1,x_k] = \\frac{f[x_0,x_1,\\dots ,x_{k-1}]-f[x_1,\\dots ,x_k]}{x_0-x_k} \\] ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:1:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"牛顿基本插值 \\[ \\begin{aligned} \u0026N_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\\dots +a_n(x-x_0)(x-x_1)\\dots(x-x_{n-1})\\\\\\\\ \u0026= f_0 + \\sum_{k=1}^{k-1}f[x_0,x_1,\\dots,x_k]\\omega_k(x) \\\\\\\\ \u0026其中\\omega_k(x)=\\prod_{j=0}^{k-1}(x-x_j) \\end{aligned} \\] ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:2:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"差分 ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:3:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"优缺点 优点：计算简单 缺点：和拉格朗日插值方法相同，插值曲线在节点处有尖点，不光滑，节点处不可导 ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:4:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"代码 # 牛顿插值法 import numpy as np import matplotlib.pyplot as plt #递归求差商 def get_diff_quo(xi, fi): if len(xi) \u003e 2 and len(fi) \u003e 2: return (get_diff_quo(xi[:len(xi)-1], fi[:len(fi)-1]) - get_diff_quo(xi[1:len(xi)], fi[1:len(fi)])) / float(xi[0] - xi[-1]) return (fi[0]-fi[1]) / float(xi[0]-xi[1]) #求w，使用闭包函数 def get_w(i, xi): def wi(x): result = 1.0 for j in range(i): result *= (x - xi[j]) return result return wi #做插值 def get_Newton(xi, fi): def Newton(x): result = fi[0] for i in range(2, len(xi)): result += (get_diff_quo(xi[:i], fi[:i]) * get_w(i-1, xi)(x)) return result return Newton #已知结点 xn = [i for i in range(-50, 50, 10)] fn = [i**2 for i in xn] #插值函数 Nx = get_Newton(xn, fn) #测试用例 tmp_x = [i for i in range(-50, 51)] tmp_y = [Nx(i) for i in tmp_x] #作图 plt.plot(xn, fn, 'r*') plt.plot(tmp_x, tmp_y, 'b-') plt.title('Newton Interpolation') plt.xlabel('x') plt.ylabel('y') plt.show() ","date":"2022-03-13","objectID":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/:5:0","tags":["Mathematical Modeling","插值算法","牛顿插值"],"title":"牛顿插值","uri":"/%E7%89%9B%E9%A1%BF%E6%8F%92%E5%80%BC/"},{"categories":["算法题"],"content":"搜索旋转排序数组 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:0:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/search-in-rotated-sorted-array/ ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:1:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"思路： 明显的二分查找，不过不是有序数组了，而是部分有序，所以需要有判断 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:2:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"代码： class Solution(object): def search(self, nums, target): left, right = 0, len(nums) - 1 while left \u003c= right: mid = left + (right - left) // 2 if nums[mid] == target: return mid if nums[mid] \u003c nums[right]:#右边为升序 if nums[mid] \u003c target \u003c= nums[right]: left = mid + 1 else: right = mid if nums[left] \u003c= nums[mid]:#左边为升序 if nums[left] \u003c= target \u003c nums[mid]: right = mid else: left = mid + 1 return -1 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:3:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["sklearn"],"content":"导入包 import pandas as pd import numpy as np import matplotlib.pyplot as plt ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:1:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"导入数据 data = pd.read_csv(\"./datasets/studentscores.csv\") data.head() Hours Scores 0 2.5 21 1 5.1 47 2 3.2 27 3 8.5 75 4 3.5 30 ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:2:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"数据处理 X = data.iloc[:,:1].values Y = data.iloc[:,1].values from sklearn.model_selection import train_test_split X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=1/4,random_state=0) ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:3:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"训练模型 from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor = regressor.fit(X_train,Y_train) ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:4:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"预测 Y_pred = regressor.predict(X_test) ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:5:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"画图 plt.scatter(X_train,Y_train,color='red') plt.plot(X_train,regressor.predict(X_train),color='blue') plt.scatter(X_test , Y_test, color = 'red') plt.plot(X_test , regressor.predict(X_test), color ='blue') ","date":"2022-03-12","objectID":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:6:0","tags":["sklearn","简单的线性回归"],"title":"简单的线性回归","uri":"/%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"import pandas as pd import numpy as np ","date":"2022-03-04","objectID":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:0","tags":["sklearn","复杂的线性回归"],"title":"复杂的线性回归","uri":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"读入数据 data = pd.read_csv(\"./datasets/50_Startups.csv\") data.head() R\u0026D Spend Administration Marketing Spend State Profit 0 165349.20 136897.80 471784.10 New York 192261.83 1 162597.70 151377.59 443898.53 California 191792.06 2 153441.51 101145.55 407934.54 Florida 191050.39 3 144372.41 118671.85 383199.62 New York 182901.99 4 142107.34 91391.77 366168.42 Florida 166187.94 ","date":"2022-03-04","objectID":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:1:0","tags":["sklearn","复杂的线性回归"],"title":"复杂的线性回归","uri":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"分开xy X = data.iloc[:,:-1].values Y = data.iloc[:,-1].values ","date":"2022-03-04","objectID":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:2:0","tags":["sklearn","复杂的线性回归"],"title":"复杂的线性回归","uri":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"编码 from sklearn.preprocessing import LabelEncoder,OneHotEncoder labelEncoder = LabelEncoder() X[:,3] = labelEncoder.fit_transform(X[:,3]) onehotencoder = OneHotEncoder() X = onehotencoder.fit_transform(X).toarray() X = X[:,1:] from sklearn.model_selection import train_test_split X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train,Y_train) LinearRegression() ","date":"2022-03-04","objectID":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:3:0","tags":["sklearn","复杂的线性回归"],"title":"复杂的线性回归","uri":"/%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["sklearn"],"content":"学习曲线能判定偏差和方差问题 from sklearn.model_selection import train_test_split,learning_curve import numpy as np from sklearn.svm import SVC from sklearn.datasets import load_digits import matplotlib.pyplot as plt digits = load_digits() X = digits.data Y = digits.target train_sizes,train_loss,test_loss = learning_curve(SVC(gamma=0.001),X,Y,cv=10, scoring='neg_mean_squared_error', train_sizes=[0.1,0.25,0.5,0.75,1]) train_sizes array([ 161, 404, 808, 1212, 1617]) train_loss array([[-0. , -0.09937888, -0.09937888, -0.09937888, -0.09937888, -0.09937888, -0.09937888, -0.09937888, -0.09937888, -0.09937888], [-0. , -0.03960396, -0.03960396, -0.03960396, -0.03960396, -0.03960396, -0.03960396, -0.03960396, -0.03960396, -0.03960396], [-0. , -0.01980198, -0.01980198, -0.06435644, -0.01980198, -0.01980198, -0.01980198, -0.01980198, -0.01980198, -0.01980198], [-0. , -0.01650165, -0.01320132, -0.01320132, -0.01320132, -0.01320132, -0.01320132, -0.01320132, -0.01320132, -0.01320132], [-0.02226345, -0.03215832, -0.00989487, -0.03215832, -0.03215832, -0.03215832, -0.03215832, -0.03215832, -0.03215832, -0.00989487]]) test_loss array([[-1.26666667e+00, -1.43333333e+00, -3.96666667e+00, -9.73888889e+00, -6.95000000e+00, -5.24444444e+00, -3.02777778e+00, -5.25139665e+00, -3.48044693e+00, -4.85474860e+00], [-1.81111111e+00, -1.13333333e+00, -1.35555556e+00, -3.06666667e+00, -2.08333333e+00, -2.85000000e+00, -8.38888889e-01, -1.94413408e+00, -5.41899441e-01, -1.35195531e+00], [-1.71111111e+00, -3.61111111e-01, -5.11111111e-01, -9.61111111e-01, -6.16666667e-01, -5.88888889e-01, -1.22222222e-01, -9.16201117e-01, -7.76536313e-01, -1.14525140e+00], [-1.22222222e+00, -3.61111111e-01, -4.44444444e-01, -7.00000000e-01, -5.55555556e-01, -2.66666667e-01, -8.88888889e-02, -1.11731844e-02, -9.21787709e-01, -8.43575419e-01], [-9.33333333e-01, -0.00000000e+00, -2.66666667e-01, -2.83333333e-01, -2.77777778e-01, -3.61111111e-01, -8.88888889e-02, -5.58659218e-03, -9.21787709e-01, -4.18994413e-01]]) train_mean = -np.mean(train_loss,axis=1) test_mean = -np.mean(test_loss,axis=1) train_mean array([0.08944099, 0.03564356, 0.02227723, 0.01221122, 0.02671614]) plt.plot(train_sizes,train_mean,label=\"Training\") plt.plot(train_sizes,test_mean,label=\"Cross-validation\") plt.legend() plt.show() ​ ​ ","date":"2022-02-24","objectID":"/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/:0:0","tags":["sklearn","学习曲线"],"title":"学习曲线","uri":"/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/"},{"categories":["Mathematical Modeling"],"content":"灰色预测模型 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:0:0","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"介绍 灰色预测模型（Gray Forecast Model）是通过少量的、不完全的信息，建立数学模型并做出预测的一种预测方法。是处理小样本（4个就可以）预测问题的有效工具，而对于小样本预测问题回归和神经网络的效果都不太理想。 灰色系统理论认为，尽管客观表象复杂，但总是有整体功能的，因此必然蕴含某种内在规律。关键在于如何选择适当的方式去挖掘和利用它。灰色系统时通过对原始数据的整理来寻求其变化规律的，这是一种就数据寻求数据的现实规律的途径，也就是灰色序列的生产。一切灰色序列都能通过某种生成弱化其随机性，显现其规律性。数据生成的常用方式有累加生成、累减生成和加权累加生成。常用的是累加生成。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:1:0","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"建模 原始数据为\\(x^0 = (x^0(1),x^0(2),\\dots \\dots,x^0(n))\\) ### 累加生成 \\[ x^1(n) = x^0(1) + x^0(2) + \\dots \\dots + x^0(n) \\] ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:0","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"加权临值生成 \\[ z^1(n) = \\alpha x^1(n) + (1-\\alpha)x^1(n-1) \\] 由此得到的数列称为邻值生成数，权\\(\\alpha\\)也称为生成系数。 特别地，当生成系数\\(\\alpha=0.5\\)时，则称该数列为均值生成数，也称为等权邻值生成数。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:1","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"灰色模型GM(1,1) GM代表grey model（灰色模型），GM(1,1)是一阶微分方程模型。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:2","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"1.数据检验 数据要先进行准指数规律检验，看看能不能进行灰色预测建模，检验不通过的数据无法进行建模。 首先计算级比 \\[ \\lambda(k) = \\frac{x^0(k-1)}{x^0(k)}, k = 2,3,\\dots,n \\] 如果所有的级比都落在可容覆盖区间 \\(X=(e^{\\frac{-2}{n+1}}, e^{\\frac{2}{n+1}})\\)内，则数列\\(x^{(0)}\\)可以建立GM(1,1)模型进行灰色预测。否则就需要对数据做适当的变换处理，如平移等。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:3","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"2.构建灰色模型 定义灰微分方程： \\[ x^0(k) = -az^1(k) + b \\] 其中-a为发展系数，b为灰作用度。 可以近似的看成线性回归方程\\(y = kx+b\\)，从而求出a和b 先矩阵化。 \\[ u = (a,b)^T , Y = \\begin{bmatrix} x^0(2)\\\\\\\\ x^0(3)\\\\\\\\ \\vdots \\\\\\\\ x^0(n) \\end{bmatrix}, B = \\begin{bmatrix} -z^1(2) \u0026 1 \\\\\\\\ -z^1(3) \u0026 1 \\\\\\\\ \\vdots \u0026 \\vdots \\\\\\\\ -z^1(n) \u0026 1 \\end{bmatrix} \\] 用最小二乘法的到u的估计值即a,b的估计值 \\[ \\hat{u} = (B^TB)^{-1}B^TY \\] 即正规方程。具体的推导看下图 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:4","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"3.预测 相应的白化模型为 \\[ \\frac{dx^1(t)}{dt} = -\\hat{a}x^1(t) + \\hat{b} \\] 通过高数中学的微分方程的求解可以得到 \\[ \\hat{x}^1(t) = \\left(x^0(1) - \\frac{\\hat{b}}{\\hat{a}} \\right)e^{-\\hat{a}(t-1)} + \\frac{\\hat{b}}{\\hat{a}} \\] 由\\(x^0\\)与\\(x^1\\)的关系可以得到 \\[ \\hat{x}^0(m+1) = (1-e^{\\hat{a}})\\left(x^0(1)-\\frac{\\hat{b}}{\\hat{a}} \\right)e^{-\\hat{a}m},其中m=1,2,\\dots,n-1 \\] 这样就可以进行预测。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:5","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"4.检验 (1)残差检验 相对残差：\\(\\epsilon_r(k)=\\frac{\\mid x^0(k) - \\hat{x}^0(k) \\mid}{x^0(k)}\\) 平均相对残差：\\(\\bar{\\epsilon_r} = \\frac{1}{n-1}\\sum_{k=2}^n \\mid \\epsilon_r(k) \\mid\\) \\(\\bar{\\epsilon_r}\\)越小，说明拟合效果越好。 (2)后验差检验法 计算残差\\(e(k) = x^0(k) - \\hat{x}^0(k)\\) 计算原始序列\\(x^0\\)的方差和残差e的方差 \\[ S_1 = \\frac{1}{n} \\sum_{k=1}^n(x^0(k) - \\bar{x})^2 \\\\\\\\ S_2 = \\frac{1}{n} \\sum_{k=1}^n(e(k)-\\bar{e})^2 \\] 观察后验差比 \\(C = \\frac{S_2}{S_1}\\) 然后通过查表观察效果。 ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:2:6","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["Mathematical Modeling"],"content":"代码 import numpy as np import pandas as pd import matplotlib.pyplot as plt from itertools import accumulate data = np.array([71.1, 72.4, 72.4, 72.1, 71.4, 72, 71.6]) lens = len(data) # 数据检验 ## 计算级比 lambds = [] for i in range(1, lens): lambds.append(data[i-1]/data[i]) ## 计算区间 X_min = np.e**(-2/(lens+1)) X_max = np.e**(2/(lens+1)) ## 检验 is_ok = True for lambd in lambds: if (lambd \u003c X_min or lambd \u003e X_max): is_ok = False if (is_ok == False): print('该数据未通过检验') else: print('该数据通过检验') # 构建灰色模型GM(1,1) ## 累加数列 data_1 = list(accumulate(data)) ## 灰导数及临值生成数列 ds = [] zs = [] for i in range(1, lens): ds.append(data[i]) zs.append(-1/2*(data_1[i-1]+data_1[i])) ## 求a、b B = np.array(zs).reshape(lens-1,1) one = np.ones(lens-1) B = np.c_[B, one] # 加上一列1 Y = np.array(ds).reshape(lens-1,1) a, b = np.dot(np.dot(np.linalg.inv(np.dot(B.T, B)), B.T), Y) # 正规方程 print('a='+str(a)) print('b='+str(b)) # 预测 def func(k): c = b/a return (data[0]-c)*(np.e**(-a*k))+c data_1_hat = [] # 累加预测值 data_0_hat = [] # 原始预测值 data_1_hat.append(func(0)) data_0_hat.append(data_1_hat[0]) for i in range(1, lens+5): # 多预测5次 data_1_hat.append(func(i)) data_0_hat.append(data_1_hat[i]-data_1_hat[i-1]) print('预测值为：') for i in data_0_hat: print(i) # 模型检验 ## 预测结果方差 data_h = np.array(data_0_hat[0:7]).T sum_h = data_h.sum() mean_h = sum_h/lens S1 = np.sum((data_h-mean_h)**2)/lens ## 残差方差 e = data - data_h sum_e = e.sum() mean_e = sum_e/lens S2 = np.sum((e-mean_e)**2)/lens ## 后验差比 C = S2/S1 ## 结果 if (C \u003c= 0.35): print('1级，效果好') elif (C \u003c= 0.5 and C \u003e= 0.35): print('2级，效果合格') elif (C \u003c= 0.65 and C \u003e= 0.5): print('3级，效果勉强') else: print('4级，效果不合格') # 画图 plt.figure(figsize=(9, 4), dpi=100) x1 = np.linspace(1, 7, 7) x2 = np.linspace(1, 12, 12) plt.subplot(121) plt.title('x^0') plt.plot(x2, data_0_hat, 'r--', marker='*') plt.scatter(x1, data, marker='^') plt.subplot(122) plt.title('x^1') plt.plot(x2, data_1_hat, 'r--', marker='*') plt.scatter(x1, data_1, marker='^') plt.show() ","date":"2022-02-23","objectID":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/:3:0","tags":["Mathematical Modeling","灰色预测模型"],"title":"灰色预测模型","uri":"/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"},{"categories":["pandas"],"content":"pandas补充学习 推荐网站：http://joyfulpandas.datawhale.club/Content/Preface.html pandas核心操作手册：https://mp.weixin.qq.com/s/l1V5e726XixI0W3EDHx0Nw ","date":"2022-02-22","objectID":"/some_api/:0:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.join和pd.merge 可以说merge包含了join操作，merge支持两个df间行方向或列方向的拼接操作，默认列拼接，取交集，而join只是简化了merge的行拼接的操作 pandas的merge方法提供了一种类似于SQL的内存链接操作，官网文档提到它的性能会比其他开源语言的数据操作（例如R）要高效。 如果对于sql比较熟悉的话，merge也比较好理解。 merge的参数 on：列名，join用来对齐的那一列的名字，用到这个参数的时候一定要保证左表和右表用来对齐的那一列都有相同的列名。 left_on：左表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 right_on：右表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 left_index/ right_index: 如果是True的haunted以index作为对齐的key how：数据融合的方法。 sort：根据dataframe合并的keys按字典顺序排序，默认是，如果置false可以提高表现。 简单举一个刚刚在比赛中里面用到的例子 data = pd.merge(data1, data2, on=\"carid\", how=\"inner\") # 根据carid合并两个数据集 可以用pd.merge，也可以用dataframe.merge，更多的信息可以查阅官方API。 ## pd.concat 也是合并dataframe 用法： pd.concat([df1, df2]) # 纵向合并 pd.concat([df1, df2], axis=1) # 横向合并 参数 - ignore_index=True，重新设置合并后的dataframe对象的index值 - sort=False，列的顺序保持原样 - join : {“inner”, “outer”}，默认为outer。 ## pd.append # 语法结构 df.append(self, other, ignore_index=False, verify_integrity=False, sort=False) other 是它要追加的其他 DataFrame 或者类似序列内容 ignore_index 如果为 True 则重新进行自然索引 verify_integrity 如果为 True 则遇到重复索引内容时报错 sort 进行排序 ","date":"2022-02-22","objectID":"/some_api/:1:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"同结构 将同结构的数据追加在原数据后面 ### 不同结构 没有的列会增加，没有的相应内容为空。 ### 可以合并追加多个 result = df1.append([df2, df3]) ","date":"2022-02-22","objectID":"/some_api/:1:1","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"追加序列 s2 = pd.Series(['X0', 'X1', 'X2', 'X3'], index=['A', 'B', 'C', 'D']) result = df1.append(s2, ignore_index=True) ","date":"2022-02-22","objectID":"/some_api/:1:2","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"追加字典列表 dicts = [{'A': 1, 'B': 2, 'C': 3, 'X': 4}, {'A': 5, 'B': 6, 'C': 7, 'Y': 8}] result = df1.append(dicts, ignore_index=True, sort=False) ","date":"2022-02-22","objectID":"/some_api/:1:3","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.rename DataFrame.rename(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors=‘ignore’) 作用就是修改index或者columns的名字。使用时可以指定mapper，然后指定axis，默认axis=0，即修改index 也可以使用index=xxx,columns=xxx，同时进行修改。 ","date":"2022-02-22","objectID":"/some_api/:2:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.get_dummies 用于构造离散数据的独热编码 用法 training = pd.get_dummies(train_data, columns=[\"xxx\", \"xx\", \"x\"]) # 对xxx、xx、x这三列进行onehot编码 ","date":"2022-02-22","objectID":"/some_api/:3:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.melt 直观的看就是将宽数据转化为长数据。转化为variable-value这样的形式。 pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) 参数解释： frame:要处理的数据集。 id_vars:不需要被转换的列名。 value_vars:需要转换的列名，如果剩下的列全部都要转换，就不用写了。 var_name和value_name是自定义设置对应的列名。 col_level :如果列是MultiIndex，则使用此级别。 常常与seaborn的FacetGrid一起进行操作，示例： f = pd.melt(train_data, value_vars=numeric_features) g = sns.FacetGrid(f, col=\"variable\", col_wrap=2, sharex=False, sharey=False) # col_warp限制一行只有两个，sharx、sharey默认为True,与matplotlib.pyplot.subplots相反。 g = g.map(sns.distplot, \"value\") # 第一个参数可以自定义函数。 ","date":"2022-02-22","objectID":"/some_api/:4:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"sns.pairplot seaborn一般与pandas的数据结合，因此不分开做笔记了。 根据官方文档，sns.pairplot是在数据集中绘制成对关系。用来展现变量两两之间的关系，比如线性、非线性、相关等等。 hue参数可以指定分类。与其它plot的hue参数一致。 一般都会使用参数diag_kind=\"kde\"，因为对角线上的变量x与y都是一样的，探究关系没有意义，因此展示核密度分布。 seaborn.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='hist', markers=None, size=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None)¶ 数据指定： \u003e vars : 与data使用，否则使用data的全部变量。参数类型：numeric类型的变量list。 {x, y}_vars : 与data使用，否则使用data的全部变量。参数类型：numeric类型的变量list。 dropna : 是否剔除缺失值。参数类型：boolean, optional 特殊参数： \u003e kind : {‘scatter’, ‘reg’}, optional Kind of plot for the non-identity relationships. diag_kind : {‘hist’, ‘kde’}, optional。Kind of plot for the diagonal subplots. 基本参数： \u003e size : 默认 6，图的尺度大小（正方形）。参数类型：numeric hue : 使用指定变量为分类变量画图。参数类型：string (变量名) hue_order : list of strings Order for the levels of the hue variable in the palette palette : 调色板颜色 markers : 使用不同的形状。参数类型：list aspect : scalar, optional。Aspect * size gives the width (in inches) of each facet. {plot, diag, grid}_kws : 指定其他参数。参数类型：dicts ","date":"2022-02-22","objectID":"/some_api/:4:1","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["Machine Learning","分类算法"],"content":"线性判别分析(LDA) 线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别 ## 主要思想 1. 类内方差小 2. 类间方差大 ## 推导 这里以二类为例，即只有两个类别。 首先是投影，我们假定原来的数据是向量 \\(x\\)，那么顺着 \\(w\\) 方向的投影就是标量： \\[ z=w^T\\cdot x(=|w|\\cdot|x|\\cos\\theta) \\] 对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 \\(N_1\\)和 \\(N_2\\)，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 \\(S\\) 表示原数据的协方差： \\[ \\begin{align} C_1:Var_z[C_1]\u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_{c1}})(z_i-\\bar{z_{c1}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(x_i-\\bar{x_{c1}})(x_i-\\bar{x_{c1}})^Tw\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^TS_1w\\\\\\\\\\\\\\\\ C_2:Var_z[C_2]\u0026=\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_{c2}})(z_i-\\bar{z_{c2}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^TS_2w \\end{align} \\] 所以类内距离为： \\[ \\begin{align} Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w \\end{align} \\] 对于第二点类间距离，我们可以用两类的均值表示这个距离： \\[ \\begin{align} (\\bar{z_{c1}}-\\bar{z_{c2}})^2\u0026=(\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i-\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i)^2\\nonumber\\\\\\\\\\\\\\\\ \u0026=(w^T(\\bar{x_{c1}}-\\bar{x_{c2}}))^2\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw \\end{align} \\] 合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值： \\[ \\begin{align} \\hat{w}=\\mathop{argmax}\\limits_wJ(w)\u0026=\\mathop{argmax}\\limits_w\\frac{(\\bar{z_{c1}}-\\bar{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw}{w^T(S_1+S_2)w}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^TS_bw}{w^TS_ww} \\end{align} \\] 这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对w的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了： \\[ \\begin{aligned} \u0026\\frac{\\partial}{\\partial w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow w\\propto S_w^{-1}S_bw=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw\\propto S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}}) \\end{aligned} \\] 也就是说最后我们的结果就是\\(w=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})\\) 可以归一化求得单位的w值。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:0:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"多类情况 前面的很容易类比二类的情况，现在的目标函数变成了： \\[ \\frac{W^TS_bW}{W^TS_wW} \\] 现在的问题就是这些都是矩阵，不能像上面那样直接优化，需要替换优化目标。 \\[ \\underbrace{arg\\;max}_W\\;\\;J(W) = \\frac{\\prod\\limits_{diag}W^TS_bW}{\\prod\\limits_{diag}W^TS_wW} \\] 其中 \\(\\prod_{diag}A\\)为A的主对角线元素的乘积,W为\\(n \\times d\\)的矩阵，n为原来的维度，d为映射到超平面的维度，则最终的目标就变成了： \\[ J(W) = \\frac{\\prod\\limits_{i=1}^dw_i^TS_bw_i}{\\prod\\limits_{i=1}^dw_i^TS_ww_i} = \\prod\\limits_{i=1}^d\\frac{w_i^TS_bw_i}{w_i^TS_ww_i} \\] 根据广式瑞利商，最大值是矩阵\\(S_w^{-1}S_b\\)的最大特征值,最大的d个值的乘积就是矩阵的 \\(S_w^{-1}S_b\\) 最大的d个特征值的乘积,此时对应的矩阵\\(W\\)为这最大的d个特征值对应的特征向量张成的矩阵。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:1:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"总结 LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。 实际上LDA除了可以用于降维以外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。 我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:2:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"代码 mean_list = [] for i in range(2): mean_list.append(np.mean(X_train[y_train==i], axis=0)) mean_list = np.array(mean_list) S_W = np.zeros((X_train.shape[1], X_train.shape[1])) # 类内散度矩阵 for c, mv in zip(range(2), mean_list): class_scatter = np.zeros((X_train.shape[1], X_train.shape[1])) for row in X_train[y_train==c]: row, mv = row.reshape(X_train.shape[1], -1), mv.reshape(X_train.shape[1], -1) class_scatter += (row-mv).dot((row-mv).T) S_W += class_scatter over_all_mean = np.mean(X_train, axis=0) S_B = np.zeros((X_train.shape[1], X_train.shape[1])) # 类间散度矩阵 for i, mean_vec in enumerate(mean_list): n = X_train[y_train==i, :].shape[0] mean_list_temp = mean_list[i, :].reshape(1, -1) over_all_mean = over_all_mean.reshape(X_train.shape[1], 1) S_B += n*(mean_vec-over_all_mean).dot((mean_vec-over_all_mean).T) eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))] eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True) # eigv_sum = sum(eig_vals) # for i, j in enumerate(eig_pairs): # print('eigenvalue {0:}: {1:.2%}'.format(i + 1, (j[0] / eigv_sum).real)) # 根据百分比显示特征值，从而选取最大的n个特征值 W = np.hstack((eig_pairs[0][1].reshape(X_train.shape[1], 1), eig_pairs[1][1].reshape(X_train.shape[1], 1))) ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:3:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"条件概率 \\(P(B|A) = \\frac{P(AB)}{P(A)}\\) ","date":"2022-02-16","objectID":"/bayes/:1:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"乘法法则 如果P(A) \u003e 0 \\(P(AB) = P(A)P(B|A)\\) 如果\\(P(A_1 \\dots A_{n-1})\\) \u003e 0 则 \\[ \\begin{aligned} P(A_1A_2\\dots A_n) = P(A_1A_2\\dots A_{n-1})P(A_n | A_1A_2\\dots A_{n-1}) \\\\\\\\ = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\\dots P(A_n|A_1A_2\\dots A_{n-1}) \\end{aligned} \\] 其中第一步使用了乘法公式，然后再对前者继续使用乘法公式，以此类推，就可以得到最后的结果。 ","date":"2022-02-16","objectID":"/bayes/:2:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"全概率公式(加法法则) \\[ P(A) = \\sum_{i=1}^n P(B_i)P(A\\lvert B_i) = \\sum_{i=1}^n P(AB_i) \\] 如果是连续变量则为 \\[ P(A) = \\int P(A,B) \\, dB \\] （加法规则与乘法规则结合是一些推导的基础，注意连续中的积分等同于离散中的连加） 特例为: \\[ P(A)=P(A\\lvert B)P(B) + P(A\\lvert \\bar{B})P(\\bar{B}) \\] 全概率公式的意义： 将复杂的事件A划分为较为简单的事件 \\[ AB_1,AB_2,\\ldots,AB_n \\] 再结合加法公式和乘法公式计算出A的概率 ## 贝叶斯公式 先引入一个小例子。 \\[ P(X=玩LOL)=0.6;\\\\\\\\ P(X=不玩LOL)=0.4 \\] 这个概率是根据统计得到或者根据自身经验给出的一个概率值，我们称之为先验概率(prior probability) 此外 \\[ P(Y=男性\\lvert X=玩LOL)=0.8,\\quad P(Y=小姐姐\\vert X=玩LOL)=0.2\\\\\\\\ P(Y=男性\\lvert X=不玩LOL)=0.2，\\quad P(Y=小姐姐\\vert X=不玩LOL)=0.8 \\] 求在已知玩家为男性的情况下，他是LOL玩家的概率是多少： 根据贝叶斯准则 \\[ P(X=玩LOL\\lvert Y=男性)=P(Y=男性\\lvert X=玩LOL)\\frac{P(X=玩LOL)}{[P(Y=男性\\lvert X=玩LOL)P(X=玩LOL)+P(Y=男性\\lvert X=不玩LOL)]P(X=不玩LOL)} \\] 分母为全概率公式 下面是贝叶斯公式的推导。 \\[ P(B\\lvert A)=\\frac{P(AB)}{P(A)}=\\frac{P(BA)}{P(A)}\\iff \\frac{P(B)P(A\\lvert B)}{\\displaystyle \\sum_{j=1}^n P(B_j)P(A\\lvert B_j)} \\] 贝叶斯公式的意义： 在事件A已经发生的条件下，贝叶斯公式可用来寻找导致A发生各种“原因”Bi的概率。 对于先验概率和后验概率来说， \\[ \\begin{aligned} P(B\\lvert A)为后验概率 \\\\\\\\ P(B)和P(A)为先验概率 \\\\\\\\ P(A\\vert B)为可能性 \\end{aligned} \\] ","date":"2022-02-16","objectID":"/bayes/:3:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"介绍 朴素贝叶斯属于生成式模型， 其主要用于分类，属于是最简单的概率图模型，主要用到概率论中学到的贝叶斯公式，其中需要对模型进行假设，即贝叶斯假设。 ","date":"2022-02-16","objectID":"/bayes/:4:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"贝叶斯假设 条件独立性假设(最简单的概率图模型(有向图))，目的是简化计算 ","date":"2022-02-16","objectID":"/bayes/:5:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"推导 对于数据集\\(\\\\{(x_i, y_i)\\\\}^N_{i=1}\\)，\\(x_i \\in R^p , \\quad y_i \\in \\\\{ 0, 1\\\\}\\) \\[ \\begin{aligned} \\hat{y} \u0026= \\arg \\max(y|X) \\\\\\\\ \u0026 = \\arg \\max\\frac{P(X,y)}{P(X)} \\\\\\\\ \u0026 = \\arg \\max\\frac{P(y)P(X|y)}{P(X)} \\\\\\\\ \u0026 = \\arg \\max(y) P(X|y) \\\\\\\\ \u0026 = \\arg \\max(y)P(x_1,x_2,\\dots x_p| y) \\end{aligned} \\] 其中由于我们的条件独立性假设，因此\\(P(X|y)\\)可以写为\\(\\prod_{j=1}^pP(x_j|y)\\) 即最终的式子就是 \\[ \\hat{y} = \\arg \\max(y)\\prod_{j=1}^p P(x_j|y) \\] 这就是朴素贝叶斯的主要推导。 注意术语： \\(P(y)\\)为先验概率 \\(P(y|X)\\)为后验概率 \\(P(X,y)\\)为联合概率 MAP，即最大后验估计，选择有最高后验概率的类。 ","date":"2022-02-16","objectID":"/bayes/:6:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"后验概率最大化的含义 （来自李航《统计学习方法》,比西瓜书上的更容易理解） ","date":"2022-02-16","objectID":"/bayes/:7:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"极大似然估计 在朴素贝叶斯法中, 学习意味着估计 \\(P\\left(Y=c_{k}\\right)\\) 和 \\(P\\left(X^{(j)}=x^{(j)} \\mid Y=c_{k}\\right)\\) 。可以 应用极大似然估计法估计相应的概率。先验概率 \\(P\\left(Y=c_{k}\\right)\\) 的极大似然估计是 \\[ P\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)}{N}, \\quad k=1,2, \\cdots, K \\] 设第 \\(j\\) 个特征 \\(x^{(j)}\\) 可能取值的集合为 \\(\\left\\{a_{j 1}, a_{j 2}, \\cdots, a_{j S_{j}}\\right\\}\\), 条件概率 \\(P\\left(X^{(j)}=a_{j l} \\mid Y=\\right.\\) \\(c_{k}\\) ) 的极大似然估计是 \\[ \\begin{aligned} \u0026P\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\\right)}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)} \\\\\\\\ \u0026j=1,2, \\cdots, n ; \\quad l=1,2, \\cdots, S_{j} ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 式中, \\(x_{i}^{(j)}\\) 是第 \\(i\\) 个样本的第 \\(j\\) 个特征; \\(a_{j l}\\) 是第 \\(j\\) 个特征可能取的第 \\(l\\) 个值; \\(I\\) 为指 示函数。 \\(S_j\\)为\\(x^{(j)}\\)的可能取值数，\\(K\\)为类别数。 ","date":"2022-02-16","objectID":"/bayes/:8:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"拉普拉斯平滑 用极大似然估计可能会出现所要估计的概率值为 0 的情况。这时会影响到后验概 率的计算结果, 使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地, 条 件概率的贝叶斯估计是 \\[ P_{\\lambda}\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\\right)+\\lambda}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+S_{j} \\lambda} \\] 式中 \\(\\lambda \\geqslant 0\\) 。等价于在随机变量各个取值的频数上赋予一个正数 \\(\\lambda\u003e0\\) 。当 \\(\\lambda=0\\) 时就 是极大似然估计。常取 \\(\\lambda=1\\), 这时称为拉普拉斯平滑 (Laplacian smoothing)。显然, 对任何 \\(l=1,2, \\cdots, S_{j}, k=1,2, \\cdots, K\\), 有 \\[ \\begin{aligned} \u0026P_{\\lambda}\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)\u003e0 \\\\\\\\ \u0026\\sum_{l=1}^{S_{j}} P\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=1 \\end{aligned} \\] 同样, 先验概率的贝叶斯估计是 \\[ P_{\\lambda}\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+\\lambda}{N+K \\lambda} \\] ","date":"2022-02-16","objectID":"/bayes/:9:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"文本分类 接下来是朴素贝叶斯在文本分类中的运用，这里以简单的二分类问题，情感分析为例。 ","date":"2022-02-16","objectID":"/bayes/:10:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"如何定义几个概率？ \\(P(y=k)\\)很容易得到，可以只评估带有标签k的文档比例，即 \\[ P(y=k) = \\frac{N(y=k)}{\\sum_iN(y=i)} \\] \\(P(x|y=k)= P(x_1,x_2,\\dots, x_n | y=k)\\) 这里假设文档x被表示为一组特征，例如一组它的词\\((x_1,x_2,\\dots, x_n)\\) 这里需要两个假设，其中一个是上面提到的贝叶斯假设，即： - 条件独立假设：特征在给定类的情况下是独立的 - Bag of Words假设：词序无关紧要 直观地说，假设 每个单词出现在类别为k的文档中的概率不依赖上下文，因此得到： \\[ P(x|y=k) = P(x_1,x_2,\\dots,x_n|y=k) = \\prod_{t=1}^nP(x_t|y=k) \\] 概率\\(P(x_i|y=k)\\)为单词\\(x_i\\)出现在标签为k的文档中的频率，即 \\[ P(x_i|y=k) = \\frac{N(x_i, y=k)}{\\sum_{t=1}^{|V|}N(x_t,y=k)} \\] 但是有个问题就是有可能会出现\\(N(x_i, y=k)=0\\)的情况 这时就需要拉普拉斯平滑，即在所有的计数中都加入一个新的参数\\(\\delta\\)， \\[ P(x_i|y=k)=\\frac{ {\\delta} + N(x_i, y=k) }{\\sum\\limits_{t=1}^{|V|}( {\\delta} + N(x_t, y=k))} = \\frac{ {\\delta} + N(x_i, y=k) }{ {\\delta\\cdot |V|} + \\sum\\limits_{t=1}^{|V|} N(x_t, y=k)} , \\] 直观地说，朴素贝叶斯期望某些词作为类指示符。例如，对于情感分类标记 awesome、 brilliant、 great 将有更高的概率给定正面类别然后负面类别。 类似地，给定负类比正类 ，标记awful, boring, bad的概率更高。 在实践中，一般都是取log，单调性不变，变为\\(\\log(x, y=k) = \\log P(y=k) + \\sum \\log P(x_i|y=k)\\) ","date":"2022-02-16","objectID":"/bayes/:10:1","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"补充：贝叶斯估计 易知\\(P(\\theta \\mid D)\\)称为后验概率，有三种估计\\(\\theta\\)的方法： 使用后验分布的密度函数最大值点作为\\(\\theta\\)的点估计的最大后验估计（MAP）。 使用后验分布的中位数作为\\(\\theta\\)的点估计的后验中位数估计（不常用）。 使用后验分布的均值作为\\(\\theta\\)的点估计的后验期望估计。 其中后验期望估计也就是贝叶斯估计。 贝叶斯估计是在MAP上做进一步拓展，不直接估计参数的值，而是允许参数服从一定的概率密度分布，先求出\\(\\theta\\)的后验分布\\(p(\\theta \\mid x)\\)，然后求出\\(\\theta\\)的期望值。 ","date":"2022-02-16","objectID":"/bayes/:11:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["pandas","api"],"content":"正为逆时针转，负为顺时针转。 import numpy as np mat = np.array([[1,3,5], [2,4,6], [7,8,9] ]) print mat, \"# orignal\" mat90 = np.rot90(mat, 1) print mat90, \"# rorate 90 \u003cleft\u003e anti-clockwise\" mat90 = np.rot90(mat, -1) print mat90, \"# rorate 90 \u003cright\u003e clockwise\" mat180 = np.rot90(mat, 2) print mat180, \"# rorate 180 \u003cleft\u003e anti-clockwise\" mat270 = np.rot90(mat, 3) print mat270, \"# rorate 270 \u003cleft\u003e anti-clockwise\" 直接复制的代码，python2，能看懂就行。 ","date":"2022-02-12","objectID":"/rot90/:0:0","tags":["pandas","api","rot90"],"title":"rot90","uri":"/rot90/"},{"categories":["Mathematical Modeling"],"content":"内生性和外生性 假设模型为： \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots +\\beta_kx_k+\\mu_i\\) ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:0:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"内生性定义 \\[ cov(x_j,\\mu_i)\\neq0, j\\neq i \\] ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:1:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"内生性的坏处 影响回归系数 ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:2:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"解释 \\(\\mu_i\\)为无法观测的且满足一定关系的扰动项 如果满足误差项 \\(\\mu_i\\)与所有的自变量\\(x\\)均不相关，则称该回归模型具有外生性 （如果相关，则存在内生性，内生性会导致回归系数估计的不准确，不满足无偏性与一致性） 那么，\\(\\mu_i\\) 包括什么？ 包含了所有与y相关，但未添加到回归模型中的变量。 如果这些变量和我们已经添加的自变量相关，则存在内生性。 无内生性（no endogeneity）要求所有解释变量均与扰动项不相关。 这个假定通常太强，因为解释变量一般很多（比如，5‐15个解释变量），且需要保证它们全部外生。 是否可能弱化此条件？答案是肯定的，如果你的解释变量可以区分为核心 解释变量与控制变量两类。 核心解释变量：我们最感兴趣的变量，因此我们特别希望得到对其系数的一致估计（当样本容量无限增大时，收敛于待估计参数的真值）。 控制变量：我们可能对于这些变量本身并无太大兴趣；而之所以把它们也放入回归方程，主要是为了“控制住” 那些对被解释变量有影响的遗漏因素。 在实际应用中，我们只要保证核心解释变量与𝝁不相关即可。 异方差 误差项的方差应为常数，不满足这个要求则说明模型具有异方差 在之前的回归分析里我们都默认扰动项是球形扰动项，即满足 同方差 和 无自相关 两个条件。 ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:3:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"如果扰动项存在异方差 （1）OLS估计出来的回归系数是无偏、一致的。 （2）假设检验无法使用（构造的统计量失效了）。 （3）OLS估计量不再是最优线性无偏估计量（BLUE）。 ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:4:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"检验异方差 一般使用怀特检验 ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:5:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"怎么解决异方差 （1）使用OLS + 稳健的标准误 如果发现存在异方差，一种处理方法是，仍然进行OLS 回归，但使用稳健标 准误。这是最简单，也是目前通用的方法。只要样本容量较大，即使在异方差的 情况下，若使用稳健标准误，则所有参数估计、假设检验均可照常进行。换言之， 只要使用了稳健标准误，就可以与异方差“和平共处”了。 （2）广义最小二乘估计法GLS 原理：方差较大的数据包含的信息较少，我们可以给予信息量大的数据（即方差 较小的数据更大的权重） 缺点：我们不知道扰动项真实的协方差矩阵，因此我们只能用样本数据来估计， 这样得到的结果不稳健，存在偶然性。 常用的是第一种方法。 多重共线性 回归模型中，两个或者两个以上的自变量彼此相关时，称回归模型中存在多重共线性。 为什么多重共线性会导致一系列问题呢？试想一下，假如两个变量完全共线性，设两个变量为A,B.那么A=xB，x是常数。如果把这两个变量带入回归方程，由于一个变量完全可以用另外一个变量乘以一个常数来表示，带入两个变量，就需要给他们分配系数，怎么分配呢，显然有很多种可能，而计算机并不知道哪一种是最好的，但是在输出结果时，它会给你一种，管它是不是你想要的呢，它只关心跑完了自己的程序。 多重共线性导致的问题： 1）线性关系显著（F检验显著，或者回归关系显著），大部分回归系数却不显著； 2）回归系数的符号与理论或者预期不符合。 多重共线性的识别： 1）各自变量之间显著相关（使用散点图矩阵和相关系数矩阵） 2）线性关系检验显著（F检验显著），各自变量系数却大多数不显著 3）回归系数正负号与预期相反 4）容忍度（tolerance）小于0.1或者方差扩大因子（VIF）大于10，认为存在严重共线性 多重共线性问题的处理 1）删除相关性很强的两个自变量中的一个，或者删除多个相关性很强的自变量中的几个变量； 2）降维。 3）增加样本数。 回归分析的五个基本假设 1.线性和可加性 2.误差项之间相互独立 若不满足，我们称模型之间具有自相关性 3.自变量之间相互独立 \u003e 若不满足，则称模型具有多重共线性 如果我们发现本应相互独立的自变量们出现了一定程度（甚至高度）的相关性，那我们就很难得知自变量与因变量之间真正的关系了。 当多重共线性性出现的时候，变量之间的联动关系会导致我们测得的标准差偏大，置信区间变宽。 采用岭回归，Lasso回归可以一定程度上减少方差，解决多重共线性性问题。因为这些方法，在最小二乘法的基础上，加入了一个与回归系数的模有关的惩罚项，可以收缩模型的系数。也可以称为线性回归模型的正则化。 4.误差项的方差应为常数 若满足则具有同方差性，否则具有异方差性 异方差性的出现意味着误差项的方差不恒定，这常常出现在有异常值（Outlier）的数据集上，如果使用标准的回归模型，这些异常值的重要性往往被高估。在这种情况下，标准差和置信区间不一定会变大还是变小。 5.误差项应呈正太分布 还有一点就是误差项与自变量之间是独立的，要保证严格的外生性。 ","date":"2022-01-30","objectID":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/:6:0","tags":["Mathematical Modeling","回归分析"],"title":"回归分析","uri":"/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"categories":["比赛相关"],"content":"数据挖掘比赛 ","date":"2022-01-26","objectID":"/eda/:0:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"对赛题进行理解 ","date":"2022-01-26","objectID":"/eda/:1:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"数据分析 ","date":"2022-01-26","objectID":"/eda/:2:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"EDA目标 EDA的价值在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用。 当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系。 引导数据科学从业者进行数据处理以及特征工程的步骤,使数据集的结构和特征集让接下来的预测问题更加可靠。 完成对于数据的探索性分析，并对于数据进行一些图表或者文字总结并打卡。 ","date":"2022-01-26","objectID":"/eda/:2:1","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要操作 载入各种数据科学以及可视化库: 数据科学库 pandas、numpy、scipy； 可视化库 matplotlib、seabon； 其他； 载入数据： 载入训练集和测试集； 简略观察数据(head()+shape)； 数据总览: 通过describe()来熟悉数据的相关统计量 通过info()来熟悉数据类型 判断数据缺失和异常 查看每列的存在nan情况 异常值检测 了解预测值的分布 总体分布概况（无界约翰逊分布等） 查看skewness and kurtosis 查看预测值的具体频数 特征分为类别特征和数字特征，并对类别特征查看unique分布 数字特征分析 相关性分析 查看几个特征得 偏度和峰值 每个数字特征得分布可视化 数字特征相互之间的关系可视化 多变量互相回归关系可视化 类型特征分析 unique分布 类别特征箱形图可视化 类别特征的小提琴图可视化 类别特征的柱形图可视化类别 特征的每个类别频数可视化(count_plot) 用pandas_profiling生成数据报告 ","date":"2022-01-26","objectID":"/eda/:2:2","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要步骤 对于数据的初步分析（直接查看数据，或.sum(), .mean()，.descirbe()等统计函数）可以从：样本数量，训练集数量，是否有时间特征，是否是时许问题，特征所表示的含义（非匿名特征），特征类型（字符类似，int，float，time），特征的缺失情况（注意缺失的在数据中的表现形式，有些是空的有些是”NAN”符号等），特征的均值方差情况。 分析记录某些特征值缺失占比30%以上样本的缺失处理，有助于后续的模型验证和调节，分析特征应该是填充（填充方式是什么，均值填充，0填充，众数填充等），还是舍去，还是先做样本分类用不同的特征模型去预测。 对于异常值做专门的分析，分析特征异常的label是否为异常值（或者偏离均值较远或者事特殊符号）,异常值是否应该剔除，还是用正常值填充，是记录异常，还是机器本身异常等。 对于Label做专门的分析，分析标签的分布情况等。 进步分析可以通过对特征作图，特征和label联合做图（统计图，离散图），直观了解特征的分布情况，通过这一步也可以发现数据之中的一些异常值等，通过箱型图分析一些特征值的偏离情况，对于特征和特征联合作图，对于特征和label联合作图，分析其中的一些关联性。 记录自己之前没用到的东西 ### 数据的偏度和峰度 - 数据的偏度(skewness)：dataframe.skew() - 数据的峰度(kurtosis)：dataframe.kurt() ### log变换 一般要求预测值需要符合正态分布，因此需要先log变换一下 ### sns.pairplot 用来展现变量两两之间的关系，比如线性、非线性、相关 hue参数可以指定分类。 ","date":"2022-01-26","objectID":"/eda/:2:3","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"sns.heatmap 热度图，可以直观感受变量之间的相关程度。 更多的处理方法可以看pandas笔记 ","date":"2022-01-26","objectID":"/eda/:2:4","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"特征工程 ","date":"2022-01-26","objectID":"/eda/:3:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要步骤 异常处理： 通过箱线图（或 3-Sigma）分析删除异常值； BOX-COX 转换（处理有偏分布）； 长尾截断； 特征归一化/标准化： 标准化（转换为标准正态分布）； 归一化（抓换到 [0,1] 区间）； 针对幂律分布，可以采用公式：\\(log(\\frac{1+x}{1+median})\\) 数据分桶： 等频分桶； 等距分桶； Best-KS 分桶（类似利用基尼指数进行二分类）； 卡方分桶； 缺失值处理： 不处理（针对类似 XGBoost 等树模型）； 删除（缺失数据太多）； 插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等； 分箱，缺失值一个箱； 特征构造： 构造统计量特征，报告计数、求和、比例、标准差等； 时间特征，包括相对时间和绝对时间，节假日，双休日等； 地理信息，包括分箱，分布编码等方法； 非线性变换，包括 log/ 平方/ 根号等； 特征组合，特征交叉； 仁者见仁，智者见智。 特征筛选： 过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择发/相关系数法/卡方检验法/互信息法； 包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ； 嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归； 降维： PCA/LDA/LCA;本博客都有介绍 特征选择也是降维 ## 建模调参 ## 模型融合 ","date":"2022-01-26","objectID":"/eda/:3:1","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["面经"],"content":"什么是前缀树？ 前缀树是N叉树的一种特殊形式。通常来说，一个前缀树是用来存储字符串的。前缀树的每一个节点代表一个字符串（前缀）。每一个节点会有多个子节点，通往不同子节点的路径上有着不同的字符。子节点代表的字符串是由节点本身的原始字符串，以及通往该子节点路径上所有的字符组成的。 在上图示例中，我们在节点中标记的值是该节点对应表示的字符串。例如，我们从根节点开始，选择第二条路径 ‘b’，然后选择它的第一个子节点 ‘a’，接下来继续选择子节点 ‘d’，我们最终会到达叶节点 “bad”。节点的值是由从根节点开始，与其经过的路径中的字符按顺序形成的。 值得注意的是，根节点表示空字符串。 前缀树的一个重要的特性是，节点所有的后代都与该节点相关的字符串有着共同的前缀。这就是前缀树名称的由来。 我们再来看这个例子。例如，以节点 “b” 为根的子树中的节点表示的字符串，都具有共同的前缀 “b”。反之亦然，具有公共前缀 “b” 的字符串，全部位于以 “b” 为根的子树中，并且具有不同前缀的字符串来自不同的分支。 前缀树有着广泛的应用，例如自动补全，拼写检查等等。 ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:1:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["面经"],"content":"代码 import collections class TrieNode(object): # 定义节点 # Initialize your data structure here. def __init__(self): self.node = collections.defaultdict(TrieNode) self.char = \"\" self.is_word = False @property def data(self): return self.node def __getitem__(self, key): return self.node[key] def __str__(self) -\u003e str: return self.char __repr__ = __str__ class Trie(object): def __init__(self): \"\"\" Initialize your data structure here. \"\"\" self.root = TrieNode() def insert(self, word): \"\"\" Inserts a word into the trie. :type word: str :rtype: void \"\"\" node = self.root for chars in word: temp = node.char node = node[chars] # 因为defaultdict，如果不存在则自动生成键 node.char = temp + chars # 键可以视为路径上的字符，节点可以视为从根到当前节点表示的前缀或单词。 node.is_word = True # 这里很重要，用于search判断是否为完整的单词 def search(self, word): \"\"\" Returns if the word is in the trie. :type word: str :rtype: bool \"\"\" node = self.root for chars in word: if chars not in node.data.keys(): # 因为defaultdict所以不能判断value为None，要判断键是否存在 return False node = node[chars] # 判断单词是否是完整的存在在trie树中 return node.is_word def startsWith(self, prefix): \"\"\" Returns if there is any word in the trie that starts with the given prefix. :type prefix: str :rtype: bool \"\"\" node = self.root for chars in prefix: if chars not in node.data.keys(): # 和上面同理 return False node = node[chars] return True def get_all_words(self): # 获取所有的单词 q = [self.root] while q: node = q.pop(0) # 相当于一个队列 for child in node.data.values(): if child.is_word: yield child.char q.append(child) ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:2:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["面经"],"content":"应用 前缀树在中文分词的应用，前缀树的实现和上面的可能不太一样，不过功能都是一样的，主要就是找句子中的词有没有出现在前缀树中。 from trie import Trie import time class TrieTokenizer(Trie): \"\"\" 基于字典树(Trie Tree)的中文分词算法 \"\"\" def __init__(self, dict_path): \"\"\" :param dict_path:字典文件路径 \"\"\" super(TrieTokenizer, self).__init__() self.dict_path = dict_path self.create_trie_tree() self.punctuations = \"\"\"！？｡＂＃＄％＆＇：（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\"\" def load_dict(self): \"\"\" 加载字典文件 词典文件内容如下，每行是一个词： AA制 ABC ABS AB制 AB角 :return: \"\"\" words = [] with open(self.dict_path, mode=\"r\", encoding=\"utf-8\") as file: for line in file: words.append(line.strip().encode('utf-8').decode('utf-8-sig')) return words def create_trie_tree(self): \"\"\" 遍历词典，创建字典树 :return: \"\"\" words = self.load_dict() for word in words: self.insert(word) def mine_tree(self, tree, sentence, trace_index): \"\"\" 从句子第trace_index个字符开始遍历查找词语，返回词语占位个数 :param tree: :param sentence: :param trace_index: :return: \"\"\" if trace_index \u003c= (len(sentence) - 1): if sentence[trace_index] in tree.data: trace_index = trace_index + 1 trace_index = self.mine_tree(tree.data[sentence[trace_index - 1]], sentence, trace_index) return trace_index def tokenize(self, sentence): tokens = [] sentence_len = len(sentence) while sentence_len != 0: trace_index = 0 # 从句子第一个字符开始遍历 trace_index = self.mine_tree(self.root, sentence, trace_index) if trace_index == 0: # 在字典树中没有找到以sentence[0]开头的词语 tokens.append(sentence[0:1]) # 当前字符作为分词结果 sentence = sentence[1:len(sentence)] # 重新遍历sentence sentence_len = len(sentence) else: # 在字典树中找到了以sentence[0]开头的词语，并且trace_index为词语的结束索引 tokens.append(sentence[0:trace_index]) # 命中词语作为分词结果 sentence = sentence[trace_index:len(sentence)] # sentence_len = len(sentence) return tokens def combine(self, token_list): \"\"\" TODO:对结果后处理：标点符号/空格/停用词 :param token_list: :return: \"\"\" flag = 0 output = [] temp = [] for i in token_list: if len(i) != 1: # 当前词语长度不为1 if flag == 0: output.append(i[::]) else: # ['该', '方法'] # temp=['该'] output.append(\"\".join(temp)) output.append(i[::]) temp = [] flag = 0 else: if flag == 0: temp.append(i) flag = 1 else: temp.append(i) return output if __name__ == '__main__': now = lambda: time.time() trie_cws = TrieTokenizer('data/32w_dic.txt') start = now() print(f\"Build Token Tree Time : {now() - start}\") sentence = '该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。' '可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，' '便可以认为此字组可能构成了一个词。该方法又称为无字典分词。' tokens = trie_cws.tokenize(sentence) combine_tokens = trie_cws.combine(tokens) end = now() print(tokens) print(combine_tokens) print(f\"tokenize Token Tree Time : {end - start}\") ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:3:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"Logistic回归 ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"线性回归 线性回归表达式： \\[ y = w^Tx+b \\] 广义回归模型： \\[ y = g^{-1}(w^Tx+b) \\] ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"Sigmoid函数 在分类任务中，需要找到一个联系函数，即g，将线性回归的输出值与实际的标签值联系起来。因此可以使用Sigmoid函数 即： \\[ \\delta(z) = \\frac{1}{1+e^{-z}} \\] 对数几率其实是一种“sigmoid”函数，它将z值转化为一个接近 0 或 1 的 \\(y\\) 值: \\[ y=\\frac{1}{1+e^{-\\left(w^{T} x+b\\right)}} \\rightarrow \\operatorname{In} \\frac{y}{1-y}=w^{T} x+b \\] 若将y视为样本 \\(x\\) 作为正例的可能性，则1-y是其反例的可能性，两者的比值 \\(\\frac{y}{1-y}\\) 称为“几率”，反映了x作为正例的相对可能性，对几率取对 数则得到 \\(\\operatorname{In} \\frac{y}{1-y}\\) ，可以看出，上式其实是在用线性回归模型的预测结果去逼近真实标记的对数几率。所以该模型也被称作“对数几率回 归”。 ## 损失函数 \\[ J = -\\frac{1}{m}\\sum_{i=1}^my_i\\log(\\hat{y_i})+(1-y_i)\\log(1-\\hat{y}) \\] 实际上可以看作下面交叉熵损失函数形式在二分类问题上的形式： \\[ J = -\\frac{1}{m}\\sum_{i=1}^my_i\\log(\\hat{y_i}) \\] 这里的\\(y_i\\)与\\(\\hat{y_i}\\)都是向量，其长度就是类别的数量。其中\\(y_i\\)代表实际分布，形式上为onehot向量。\\(\\hat{y_i}\\)是概率分布，为预测的值。 其实这里可以想一下神经网络，对于sigmoid来说，输出层的神经元可以是一个，也可以是两个，如果是一个的话就可以用上面的形式，如果是两个的话可以用下面的这种形式。 也可以这样理解，对于softmax的这种形式，对于二分类我们可以拆分成这样 \\[ \\begin{cases} \\log \\hat{y_i}, \\quad y_i=1 \\\\\\\\ \\log (1-\\hat{y_i}), \\quad y_i=0 \\end{cases} \\] 再结合起来，这样就可以得到逻辑回归的损失函数的结果。 ## 与极大似然估计的关系 \\[ h(x;\\theta) = p(y=1|x;\\theta) = \\frac{1}{1+e^{-\\theta x+b}} \\] \\[ p(y=0|x;\\theta) = 1-p(y=1|x;\\theta) \\] 则对于单个样本： \\[ p(y|x;\\theta) = h(x;\\theta)^y(1-h(x;\\theta))^{(1-y)} \\] 接下来用极大似然估计估计出参数\\(\\theta\\) \\[ \\begin{aligned} L(\\theta) = \\prod_{i=1}^mp(y_i|x_i;\\theta)\\\\\\\\ =\\prod_{i=1}^mh(x_i;\\theta)^{y_i}(1-h(x_i;\\theta)) ^{1-y_i} \\end{aligned} \\] 则： \\[ l(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^my_i\\ln (h(x_i;\\theta ))+(1-y_i)ln(1-h(x_i|\\theta)) \\] 极大这个函数，也就是最小化这个函数的负数，也就是上面的损失函数。 ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"python实现 class LogisticRegression: def __init__(self): pass def sigmoid(self,a): res = [] for x in a: if x \u003e= 0: res.append(1/(1+np.exp(-x))) else: res.append(np.exp(x) / (np.exp(x) + 1)) return np.array(res) def train(self, X, y_true, n_iters=100, learning_rate=1): \"\"\" 根据给定的训练集X和y来训练逻辑回归 \"\"\" # 第零步：初始化参数 n_samples, n_features = X.shape #👆样本数m和特征量数n分别赋值为X的行数和列数 self.weights = np.zeros((n_features,1)) self.bias = 0 costs = [] for i in range(n_iters): # 第一步和第二步：计算输入的特征量和权值的线性组合，使用sigmoid函数 y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias) # 第三步：计算代价值，用于之后计算代价函数值 cost = (-1/n_samples)*np.sum(y_true*np.log(y_predict+1e-5)+(1-y_true)*(np.log(1-y_predict+1e-5))) # 第四步：计算梯度 dw = (1/n_samples)*np.dot(X.T,(y_predict - y_true)) db = (1/n_samples)*np.sum(y_predict-y_true) # 第五步；更新参数 self.weights = self.weights - learning_rate * dw self.bias = self.bias - learning_rate * db costs.append(cost) if i%10 == 0: print(f\"Cost after iteration {i}:{cost}\") # return self.weights,self.bias,costs def predict(self,X): \"\"\" 对于测试集X，预测二元分类标签 \"\"\" y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias) return np.array(y_predict) ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["pandas","api"],"content":"pd.melt ","date":"2022-01-12","objectID":"/melt/:0:0","tags":["pandas","api","melt"],"title":"melt","uri":"/melt/"},{"categories":["pandas","api"],"content":"用法 直观的看就是将宽数据转化为长数据。转化为variable-value这样的形式。 pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) 参数解释： frame:要处理的数据集。 id_vars:不需要被转换的列名。 value_vars:需要转换的列名，如果剩下的列全部都要转换，就不用写了。 var_name和value_name是自定义设置对应的列名。 col_level :如果列是MultiIndex，则使用此级别。 ## 实例 import pandas as pd df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, 'B': {0: 1, 1: 3, 2: 5}, 'C': {0: 2, 1: 4, 2: 6} }) df ''' A B C 0 a 1 2 1 b 3 4 2 c 5 6 ''' pd.melt(df, id_vars=['A'], value_vars=['B']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 ''' pd.melt(df, id_vars=['A'], value_vars=['B', 'C']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 ''' pd.melt(df, id_vars=['A'], value_vars=['B'], var_name='myVarName', value_name='myValueName') ''' A myVarName myValueName 0 a B 1 1 b B 3 2 c B 5 ''' pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 ''' # 多重索引 df.columns = [list('ABC'), list('DEF')] df ''' A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 ''' # 选择最外层索引 pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 ''' # 选择内层索引 pd.melt(df, col_level=1, id_vars=['D'], value_vars=['E']) # 选择复合索引 pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')]) ''' (A, D) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 ''' ","date":"2022-01-12","objectID":"/melt/:1:0","tags":["pandas","api","melt"],"title":"melt","uri":"/melt/"},{"categories":["Deep Learning","优化算法"],"content":"梯度下降法 ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:0:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["Deep Learning","优化算法"],"content":"简介 ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:1:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["Deep Learning","优化算法"],"content":"批度梯度下降 其实就是一次将整个数据集进行梯度下降的迭代 ## 随机梯度下降 就是对样本进行循环，每循环一个样本就更新一次参数，但是不容易收敛 ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:2:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["Deep Learning","优化算法"],"content":"小批量梯度下降 大多数用于深度学习的梯度下降算法介于以上两者之间，使用一个以上而又不是全部的训练样本。传统上，这些会被称为小批量(mini-batch)或小批量随机(mini-batch stochastic)方法，现在通常将它们简单地成为随机(stochastic)方法。对于深度学习模型而言，人们所说的“随机梯度下降, SGD”，其实就是基于小批量（mini-batch）的随机梯度下降。 ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:3:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["Deep Learning","优化算法"],"content":"代码 以线性回归为例 import numpy as np import matplotlib.pyplot as plt np.random.seed(42) w = np.array([2, 1, 4, 5, 3]) d = len(w) X = [] Y = [] for _ in range(1000000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X.append(x) Y.append(y) X = np.array(X) Y = np.array(Y) def mse(y_true, y_test): return ((y_true - y_test) ** 2) / len(y_true) def gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent(w, alpha, x, y): y_pred = x.dot(w) error = mse(y, y_pred).mean() grad = np.dot(x.T, gradient(y, y_pred)) w = w - alpha * grad return w, error def stochastic_gradient_descent(w, alpha, x, y, epoch): alpha_update = alpha for i in range(len(x)): y_pred = x[i].dot(w) grad = np.dot(x[i].T, (y_pred - y[i])) * 2 / len(x) w = w- alpha_update * grad alpha_update = alpha_update / (epoch+1) error = mse(y, x.dot(w)).mean() return w, error X_test = [] Y_test = [] for _ in range(10000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X_test.append(x) Y_test.append(y) X_test = np.array(X_test) Y_test = np.array(Y_test) def l2_mse(y_true, y_test, l, w): return ((y_true - y_test) ** 2) / len(y_true) + l * np.sum(w ** 2) def l2_gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent_with_l2(w, alpha, x, y, l): y_pred = x.dot(w) error = l2_mse(y, y_pred, l, w).mean() grad = np.dot(x.T, l2_gradient(y, y_pred)) w = w - alpha * grad - alpha * l * w *2 return w, error if __name__ == \"__main__\": train_loss = [] test_loss = [] print(\"Batch Gradient Descent\") for epoch in range(1000): w, error = batch_gradient_descent(w, 0.01, X, Y) # train y_pred = X_test.dot(w) error_test = mse(Y_test, y_pred).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-No-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-No-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-No-L2\") plt.plot(test_loss, label=\"Test-No-L2\") plt.legend() plt.show() # ============================================ train_loss = [] test_loss = [] print(\"Batch Gradient Descent with L2\") l = 0.0001 # lambda for epoch in range(1000): w, error = batch_gradient_descent_with_l2(w, 0.01, X, Y, l) # train y_pred = X_test.dot(w) error_test = l2_mse(Y_test, y_pred, l, w).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-L2\") plt.plot(test_loss, label=\"Test-L2\") plt.legend() plt.show() ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:4:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["Deep Learning","优化算法"],"content":"参考 https://www.cnblogs.com/shenxiaolin/p/8648804.html#:~:text=Python%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%201%20%E6%A2%AF%E5%BA%A6%20%3A%20%E8%A1%A8%E7%A4%BA%E6%9F%90%E4%B8%80%E5%87%BD%E6%95%B0%E5%9C%A8%E4%B8%80%E7%82%B9%E5%A4%84%E5%8F%98%E5%8C%96%E7%8E%87%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91%E5%90%91%E9%87%8F%20%28%E5%8F%AF%E7%90%86%E8%A7%A3%20…%202,%E5%85%B6%E4%B8%AD%20X%20%E4%B8%BA%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%8C%20%E4%B8%BA%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%8C%20%E6%98%AF%E7%89%B9%20…%207%20%E7%BB%8F%E5%85%B8%E7%9A%84%E5%B9%B3%E6%96%B9%E5%B7%AE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%A6%82%E4%B8%8B%EF%BC%9A ","date":"2022-01-04","objectID":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/:5:0","tags":["Deep Learning","优化算法","梯度下降法"],"title":"梯度下降法","uri":"/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"},{"categories":["NLP"],"content":"预训练模型 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概述 预训练模型，则是使自然语言处理由原来的手工调参、依靠 ML 专家的阶段，进入到可以大规模、可复制的大工业施展的阶段。而且预训练模型从单语言、扩展到多语言、多模态任务。一路锐气正盛，所向披靡。 预训练通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。体现某一个词在一个特定上下文中的语义表征。第二个步骤是微调，针对具体的任务修正网络。训练数据可以是文本、文本-图像对、文本-视频对。预训练模型的训练方法可使用自监督学习技术（如自回归的语言模型和自编码技术）。可训练单语言、多语言和多模态的模型。此类模型可经过微调之后，用于支持分类、序列标记、结构预测和序列生成等各项技术，并构建文摘、机器翻译、图片检索、视频注释等应用。 为什么我们要做预训练模型？首先，预训练模型是一种迁移学习的应用，利用几乎无限的文本，学习输入句子的每一个成员的上下文相关的表示，它隐式地学习到了通用的语法语义知识。第二，它可以将从开放领域学到的知识迁移到下游任务，以改善低资源任务，对低资源语言处理也非常有利。第三，预训练模型在几乎所有 NLP 任务中都取得了目前最佳的成果。最后，这个预训练模型+微调机制具备很好的可扩展性，在支持一个新任务时，只需要利用该任务的标注数据进行微调即可，一般工程师就可以实现。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"发展趋势 首先，第一个关键技术是 Transformer。它在 NLP 各个任务中都取得了优异的性能，它是预训练语言模型的核心网络。给定一句话或是一个段落作为输入，首先将输入序列中各个词转换为其对应的词向量，同时加上每一个词的位置向量，体现词在序列的位置。然后将这些词向量输入到多层 Transformer 网络中，通过自注意力（self-attention）机制来学习词与词之间的关系，编码其上下文信息，再通过一个前馈网络经过非线性变化，输出综合了上下文特征的各个词的向量表示。每一层 Transformer 网络主要由 Multi-head self-attention 层（多头自注意力机制）和前馈网络层两个子层构成。Multi-head self-attention 会并行地执行多个不同参数的 self-attention，并将各个 self-attention 的结果拼接作为后续网络的输入，self-attention 机制会在后面中做详细介绍。此后，我们得到了蕴含当前上下文信息的各个词的表示，然后网络会将其输入到前馈网络层以计算非线性层次的特征。 在每一层 Transformer 网络中，会将残差连接（residual connection）把自注意力机制前或者前馈神经网络之前的向量引入进来，以增强自注意力机制或者前馈网络的输出结果向量。并且还做一个 layer normalization，也就是通过归一化把同层的各个节点的多维向量映射到一个区间里面，这样各层节点的向量在一个区间里面。这两个操作加入在每个子层后，可更加平滑地训练深层次网络。 Transformer 可以用于编码，也可以用于解码。所谓解码就是根据一个句子的输入得到一个预想的结果，比如机器翻译（输入源语言句子，输出目标语言句子），或者阅读理解（输入文档和问题，输出答案）。解码时，已经解码出来的词要做一个自注意力机制，之后和编码得到的隐状态的序列再做一个注意力机制。这样可以做 N 层，然后通过一个线性层映射到词表的大小的一个向量。每个向量代表一个词表词的输出可能性，经过一个softmax 层得到每个词的输出概率。 接下来介绍一下 self-attention 机制，以一个 head 作为示例。假定当前输入包含三个词，给定其输入词向量或是其上一层 Transformer 网络的输出，将其通过三组线性变换，转换得到三组 queries、keys 和 values 向量。Query 和 key 向量用来计算两两词之间的得分，也就是其依赖关系，这个得分会同其对应的 value 向量做加权和，以得到每个词综合上下文信息的表示。给定当前第一个词的 query 向量，其首先同各个词的 key 向量通过点积操作得到这两个词的得分，这些得分用来表示这两个词的依赖或是相关程度。这些得分之后会根据 query 等向量的维度做一定比例的缩放，并将这些得分通过 softmax 操作做归一化。之后，各个得分会同其相对应的 value 向量相乘得到针对第一个词加权的各个 value 向量，这些加权的 value 向量最终相加以得到当前第一个词的上下文表示。 在得到第一个词的上下文表示后，给定第二个词的 query 向量，我们会重复之前的操作，计算当前 query 向量同各个词 key 向量的得分，对这些得分做 softmax 归一化处理，并将这些得分同其对应的 value 向量做加权和，以得到其编码上下文信息的表示。 第二个关键技术是自监督学习。在预训练的模型中，AR（自回归）LM 和 AE（自动编码器）是最常用的自监督学习方法，其中，自回归 LM 旨在利用前面的词序列预测下个词的出现概率（语言模型）。自动编码器旨在对损坏的输入句子，比如遮掩了句子某个词、或者打乱了词序等，重建原始数据。通过这些自监督学习手段来学习单词的上下文相关表示。 第三个关键技术就是微调。在做具体任务时，微调旨在利用其标注样本对预训练网络的参数进行调整。以我们使用基于 BERT（一种流行的预训练模型）为例来判断两个句子是否语义相同。输入是两个句子，经过 BERT 得到每个句子的对应编码表示，我们可以简单地用预训练模型的第一个隐节点预测分类标记判断两个句子是同义句子的概率，同时需要额外加一个线性层和 softmax 计算得到分类标签的分布。预测损失可以反传给 BERT 再对网络进行微调。当然也可以针对具体任务设计一个新网络，把预训练的结果作为其输入。 总体来讲，预训练模型发展趋势：第一，模型越来越大。比如 Transformer 的层数变化，从12层的 Base 模型到24层的 Large 模型。导致模型的参数越来越大，比如 GPT 110 M，到 GPT-2 是1.5 Billion，图灵是 17 Billion，而 GPT-3 达到了惊人的 175 Billion。一般而言模型大了，其能力也会越来越强，但是训练代价确实非常大。第二，预训练方法也在不断增加，从自回归 LM，到自动编码的各种方法，以及各种多任务训练等。第三，还有从语言、多语言到多模态不断演进。最后就是模型压缩，使之能在实际应用中经济的使用，比如在手机端。这就涉及到知识蒸馏和 teacher-student models，把大模型作为 teacher，让一个小模型作为 student 来学习，接近大模型的能力，但是模型的参数减少很多。 每个观点都可以看一下参考文章。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"深层表征阶段 在word embedding这篇文章里面，介绍了传统的词向量，也就是固定的词向量。本文将介绍deep contextualized词向量模型。也就是深层表征阶段。 两个伟大的想法： 编码的内容：从单词到上下文中的单词 （从 Word2Vec/GloVe/etc. 到 Cove/ELMo 的过渡） 用于下游任务：从仅替换特定任务模型中的词嵌入到替换整个特定任务模型 （从 Cove/ELMo 到 GPT/BERT 的过渡）。 具体的模型可以看本博客其余内容。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:3:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"参考 https://www.zhihu.com/question/327642286 https://zhuanlan.zhihu.com/p/115014536 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:4:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["pandas","api"],"content":"矩阵的反转，可以按照各个维度很好理解。 例子： cs_matrix = np.array([[ 4, 3, 2, 1, 0], [ 8, 7, 6, 5, 1], [11, 10, 9, 6, 2], [13, 12, 10, 7, 3], [14, 13, 11, 8, 4]]) np.flip(cs_matrix, 0) 变成了： np.flip(cs_matrix, 1) 变成了： ","date":"2022-01-01","objectID":"/flip/:0:0","tags":["pandas","api","flip"],"title":"flip","uri":"/flip/"},{"categories":["sklearn","feature_extraction"],"content":"将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的 句子1：我 爱 北 京 天 安 门 转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0] 句子2：我 喜 欢 上 海 转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] from sklearn.feature_extraction.text import CountVectorizer corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ] vectorizer = CountVectorizer() vectorizer.fit_transform(corpus).toarray() result: [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] ","date":"2021-12-14","objectID":"/countvectorizer/:0:0","tags":["sklearn","feature_extraction","CountVectorizer"],"title":"CountVectorizer","uri":"/countvectorizer/"},{"categories":["NLP"],"content":"ELMo 在Transformer中提到了ELMo解决了word2vec中存在的多义词问题，其使用双向的LSTM作为特征提取器，考虑了上下文的语义，所以可以解决多义词问题。这篇文章就详细介绍一下ELMo。 ElMo与CoVe很类似，不过它不是基于机器翻译模型，而是语言模型。仅仅通过用来自 LM 的嵌入替换词嵌入 (GloVe)，他们就在问答、共指解析、情感分析、命名实体识别等多项任务上获得了巨大的改进。 ","date":"2021-12-12","objectID":"/elmo/:0:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"模型训练(char-CNN 之上的前向和后向 LSTM-LMs) 该模型非常简单，它由两层 LSTM 语言模型组成：前向和后向。使用这两种模型是为了使每个标记都可以具有两个上下文：左和右。 同样有趣的是作者如何获得初始单词表示（然后将其馈送到 LSTM）。让我们回想一下，在标准词嵌入层中，对于词汇表中的每个词，我们训练一个唯一的向量。在这种情况下， - 词嵌入不知道它们所包含的字符（例如，它们不知道单词represent, represents, represented, 和 representation在书面上是接近的） - OOV问题 为了解决这些问题，作者将单词表示为字符级网络的输出。正如我们从插图中看到的，这个 CNN 非常简单，由我们之前已经看到的组件组成：卷积、全局池化、highway connections和线性层。通过这种方式，单词表示通过构造知道它们的字符，我们甚至可以表示那些我们在训练中从未见过的单词。 ","date":"2021-12-12","objectID":"/elmo/:1:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"获取表示 训练模型后，我们可以使用它来获取单词表示。为此，对于每个单词，我们结合来自前向和后向 LSTM 的相应层的表示。通过连接这些前向和后向向量，我们构建了一个“知道”左右上下文的单词表示。 总体而言，ELMo 表示具有三层： 第 0 层（嵌入） - 字符级 CNN 的输出； 第 1 层- 来自前向和后向 LSTM 的第 1 层的连接表示； 第 2 层- 来自前向和后向 LSTM 的第 2 层的连接表示； 这些层中的每一层都对不同类型的信息进行编码：第 0 层 - 仅单词级别，第 1 层和第 2 层 - 上下文中的单词。比较第 1 层和第 2 层，第 2 层可能包含更多高级信息：这些表示来自相应 LM 的更高层。 由于不同的下游任务需要不同种类的信息，ELMo 使用特定于任务的权重来组合来自三层的表示。这些是为每个下游任务学习的标量。得到的向量，即所有层表示的加权和，用于表示一个单词。 ","date":"2021-12-12","objectID":"/elmo/:2:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"总结 CoVe和ELMo都用了上下文单词，解决了word2vec中多义词的问题。但他们主要是替换嵌入层，并保持特定于任务的模型架构几乎完好无损。这意味着例如，对于共指解决，必须使用为此任务设计的特定模型，用于词性标记 - 一些其他模型，用于问答 - 另一个非常特殊的模型等。对于这些任务中的每一个，专门研究它的研究人员不断改进特定于任务的模型架构。 与以前的模型相比，GPT/BERT 不是作为词嵌入的替代品，而是作为特定任务模型的替代品。在这个新设置中，首先使用大量未标记数据（纯文本）对模型进行预训练。然后，该模型在每个下游任务上进行微调。重要的是，现在在微调期间，您必须只使用任务感知输入转换（即以某种方式提供数据）， 而不是 修改模型架构。 ","date":"2021-12-12","objectID":"/elmo/:3:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["Machine Learning","集成学习"],"content":"Stacking ","date":"2021-11-25","objectID":"/stacking/:0:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","集成学习"],"content":"思想简介 简单得理解，就是对于多个学习器，分别对结果进行预测，然后将预测的结果作为特征，再对结果进行预测。 上一张经典的图： 以这个5折stacking为例： 首先将所有数据集生成测试集和训练集（假如训练集为10000,测试集为2500行），那么上层会进行5折交叉检验，使用训练集中的8000条作为训练集，剩余2000行作为验证集（橙色）。 每次验证相当于使用了蓝色的8000条数据训练出一个模型，使用模型对验证集进行验证得到2000条数据，并对测试集进行预测，得到2500条数据，这样经过5次交叉检验，可以得到中间的橙色的\\(5\\times 2000\\)条验证集的结果(相当于每条数据的预测结果)，\\(5\\times 2500\\)条测试集的预测结果。 接下来会将验证集的\\(5\\times 2000\\)条预测结果拼接成10000行长的矩阵，标记为A1，而对于\\(5\\times 2500\\)行的测试集的预测结果进行加权平均，得到一个2500一列的矩阵，标记为B1。 上面得到一个基模型在数据集上的预测结果A1、B1,这样当我们对3个基模型进行集成的话，相于得到了A1、A2、A3、B1、B2、B3六个矩阵。 之后我们会将A1、A2、A3并列在一起成10000行3列的矩阵作为training data,B1、B2、B3合并在一起成2500行3列的矩阵作为testing data，让下层学习器基于这样的数据进行再训练。 再训练是基于每个基础模型的预测结果作为特征（三个特征），次学习器会学习训练如果往这样的基学习的预测结果上赋予权重w，来使得最后的预测最为准确。 ","date":"2021-11-25","objectID":"/stacking/:1:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","集成学习"],"content":"伪代码 ","date":"2021-11-25","objectID":"/stacking/:2:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","回归算法"],"content":"参考：https://cuijiahua.com/blog/2017/11/ml_11_regression_1.html ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"什么是回归？ 回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。 HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio 这就是所谓的回归方程（regression equation），其中的0.0015和-0.99称为回归系数（regression weights），求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。 说到回归，一般都是指线性回归（linear regression），所以本文里的回归和线性回归代表同一个意思。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。 这里的线性可以是对变量的线性，也可以是对参数的线性。比如\\(y=x^2\\)可以认为是一个线性函数。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"正规方程推导 将向量表达形式转为矩阵表达形式，则有 \\(J(\\theta)=\\frac{1}{2}(X \\theta-y)^{2}\\) ，其中 \\(X\\) 为 \\(m\\) 行 \\(n\\) 列的矩阵（ \\(m\\) 为样本个数， \\(n\\) 为特 征个数)， \\(\\theta\\) 为 \\(n\\) 行1列的矩阵， \\(y\\) 为 \\(m\\) 行1列的矩阵，对 \\(J(\\theta)\\) 进行如下变换 \\[ \\begin{aligned} \u0026J(\\theta)=\\frac{1}{2}(X \\theta-y)^{T}(X \\theta-y) \\\\\\\\ \u0026=\\frac{1}{2}\\left(\\theta^{T} X^{T}-y^{T}\\right)(X \\theta-y) \\\\\\\\ \u0026=\\frac{1}{2}\\left(\\theta^{T} X^{T} X \\theta-\\theta^{T} X^{T} y-y^{T} X \\theta-y^{T} y\\right) \\end{aligned} \\] 接下来对 \\(J(\\theta)\\) 偏导，需要用到以下几个矩阵的求导法则: \\[ \\begin{aligned} \u0026\\frac{d A B}{d B}=A^{T} \\\\\\\\ \u0026\\frac{d X^{T} A X}{d X}=2 A X \\end{aligned} \\] 所以有: \\[ \\begin{aligned} \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta}=\\frac{1}{2}\\left(2 X^{T} X \\theta-X^{T} y-\\left(y^{T} X\\right)^{T}-0\\right) \\\\\\\\ \u0026=\\frac{1}{2}\\left(2 X^{T} X \\theta-X^{T} y-X^{T} y-0\\right) \\\\\\\\ \u0026=X^{T} X \\theta-X^{T} y \\\\\\\\ \u0026\\text { 令 } \\frac{\\partial J(\\theta)}{\\partial \\theta}=0 ｝ \\\\\\\\ {\\text { 则有 } \\theta=\\left(X^{T} X\\right)^{-1} X^{T} y} \\end{aligned} \\] 值得注意的是，上述公式中包含逆矩阵，也就是说，这个方程只在逆矩阵存在的时候使用，也即是这个矩阵是一个方阵，并且其行列式不为0。 除了这种方法，也可以使用最小二乘法来解决。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"局部加权线性回归 线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有小均方误差的无偏估 计。显而易见，如果模型欠拟合将不能取得好的预测效果。所以有些方法允许在估计中引入一 些偏差，从而降低预测的均方误差。 其中的一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。 普通线性回归: \\[ \\min J_R=\\sum_{i=1}^m\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 局部加权线性回归: \\[ \\min J_{L R}=\\sum_{i=1}^m \\theta_i\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 这里唯一的区别是加入了权重 \\(\\theta\\), 采用之前的最小二乘法求解权数 \\(\\mathbf{w}\\) : \\[ \\hat{\\mathbf{w}}^* =\\arg \\min_{\\hat{\\mathbf{w}}} \\theta(\\mathbf{y}-\\mathbf{X} \\hat{\\mathbf{w}})^T(\\mathbf{y}-\\mathbf{X} \\hat{\\mathbf{w}}) \\] 在该方法中，我们给待预测点附近的每个点赋予一定的权重。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解除回归系数W的形式如下： \\[ \\hat{w} = (X^T\\theta X)^{-1}X^T\\theta y \\] 其中W是一个矩阵，这个公式跟我们上面推导的公式的区别就在于W，它用来给每个店赋予权重。 LWLR使用”核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下： \\[ \\theta(i,i) = exp\\left ( \\frac{ (x^{(i)}-x)^2}{-2k^2}\\right) \\] 这样就构造了一个只含对角元素的权重矩阵 \\(w\\) ，并且点xi与 \\(x\\) 越接近， \\(\\theta(i, i)\\) 的值越大，当 \\(x i\\) 与 \\(x\\) 非常接近时， \\(\\theta(i, i)\\) 的值趋于 1 ，我们再回头看之前的优化式: \\[ \\min J_{L R}=\\sum_{i=1}^m \\theta_i\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 对于一个数据点，与其靠近的点，权重大，与其相距较远的点，权重小，从而优化问题会有所偏倚，靠近的点对该数据点的回归拟合起较大作用，而相距较远的 点由于权数很小，造成的影响基本可以忽略不计，这样就等同于每个点的回归都是基于与其相距较近的点为基础，而忽略了较远的点，这也就是同部加权线性回归局部的由来，因为它着重考虑目标点同部的数据点为回归基础. 可以看到，加权函数只有一个系数，那就是分母上的 \\(k\\) ，当K取很小时， exp得到的很多值均趋于 0 ，此时只有很少一部分样本用于训练，而当k取很大时， exp的值 不会很快趋于 0 ，从而会有一大部分点用于训练，我们可以通过调整k的值，决定这个‘局部’的大小究竟是多大 如果数据的特征比样本点还多应该怎么办？如果矩阵有多重共线性怎么办？很显然，此时我们不能再使用上文的方法进行计算了，因为矩阵X不是满秩矩阵，非满秩矩阵在求逆时会出现问题。 解决的方法就是正则化。在线性回归中，正则化主要有L1正则化与L2正则化，L1正则化对应LASSO回归，L2正则化对应岭回归。 #!/usr/bin/env python #-*- coding:utf-8 -*- from numpy import * import matplotlib.pyplot as plt \"\"\" 打开一个用tab键分隔的文本文件 parameters: fileName -文件名 return: dataMat -数据矩阵 labelMat -目标值向量 \"\"\" def loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t')) - 1 #得到列数，不包括最后一列，默认最后一列值为目标值 dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr =[] curLine = line.strip().split('\\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMat \"\"\" 计算最佳拟合直线 parameters: xArr -给定的输入值 yArr -给定的输出值 return: ws -回归系数 \"\"\" def standRegres(xArr,yArr): xMat = mat(xArr); yMat = mat(yArr).T #将数据保存到矩阵中 #计算x.T *x xTx = xMat.T @ xMat #使用linalg.det()方法来判断它的行列式是否为零，即是否可逆 if linalg.det(xTx) == 0.0: return #使用最小二乘法计算w值 ws = linalg.inv(xTx) @ xMat.T @ yMat return ws \"\"\" 计算局部加权线性回归系数 parameters: testPoint -待预测数据 xArr -给定输入值 yArr -给定输出值 k -高斯核的k值，决定对附近的点赋予多大的权重 return: testPoint * ws -回归预测的估计值 \"\"\" def lwlr(testPoint, xArr, yArr, k=1.0): xMat = mat(xArr); yMat = mat(yArr).T #读入数据到矩阵 m = shape(xMat)[0] #创建对角权重矩阵，该矩阵为方阵，矩阵维数为样本点个数 theta = eye(m, m) #遍历整个数据集 for i, x in enumerate(xMat): #计算待预测数据与每个样本点的差值 x_i = (testPoint - x) @ (testPoint - x).T #计算每个样本点对应的权重值，随着样本点与待预测点距离的递增，权重将以指数级衰减 theta[i][i] = exp(x_i / (-2 * k ** 2)) #计算x.T θ x xT_theta_x = xMat.T @ theta @ xMat #判断矩阵是否可逆 if linalg.det(xT_theta_x) == 0: return #使用最小二乘法计算w值 ws = linalg.inv(xT_theta_x) @ (xMat.T @ (theta @ yMat)) return testPoint*ws \"\"\" 测试函数 parameters: testArr -测试数据集 xArr -给定输入值 yArr -给定输出值 k -高斯核的k值 return: yHat -预测值 \"\"\" def lwlrTest(testArr, xArr, yArr,k=1.0): m = shape(xArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHat \"\"\" 计算预测误差的平方和 parameters: yArr -给定y值 yHatArr -预测y值 return: ((yArr-yHatArr)**2).sum() -误差矩阵 \"\"\" def rssError(yArr,yHatArr): return ((yArr-yHatArr)**2).sum() if __name__=='__main__': abX,abY = loadDataSet('linear_regression_abalone/abalone.txt') yHat01 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],0.1) yHat1 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],1) yHat10 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],10) print(\"使用局部加权线性回归预测误差：\") print(\"核为0.1时：\",rssError(abY[0:99],yHat01.T)) print(\"核为1时：\",rssError(abY[0:99],yHat1.T)) print(\"核为10时：\",rssError(abY[0:99],yHat10.T)) yHat01 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],0.1) yHat1 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],1) yHat10 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],10) print(\"使用局部加权线性回归预测误差在新数据上的表现：","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"岭回归 岭回归即我们所说的L2正则线性回归，在一般的线性回归最小化均方误差的基础上增加了一个参数w的L2范数的罚项，从而最小化罚项残差平方和： \\[ min\\mid \\mid Xw-y \\mid\\mid_2^2 + \\lambda \\mid\\mid w\\mid\\mid_2^2 \\] 简单说来，岭回归就是在普通线性回归的基础上引入单位矩阵。回归系数的计算公式变形如下： \\[ \\hat{w} = (X^TX+\\lambda I)^{-1}X^Ty \\] 式中，矩阵I是一个mxm的单位矩阵，加上一个λI从而使得矩阵非奇异，进而能对矩阵求逆。 当\\(\\lambda\\)过小，则相当于原来的正规方程，会造成过拟合，而\\(\\lambda\\)过大时，模型的方差会更小，但偏差会变大，所以岭回归的关键是找到一个合理的\\(\\lambda\\)值来平衡模型的方差和偏差。 岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入λ来限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也可以叫做缩减（shrinkage）。 缩减方法可以去掉不重要的参数，因此能更好地裂解数据。此外，与简单的线性回归相比，缩减法能够取得更好的预测效果。 图绘制了回归系数与log(λ)的关系。在最左边，即λ最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减成0；在中间部分的某个位置，将会得到最好的预测结果。想要得到最佳的λ参数，可以使用交叉验证的方式获得，文章的后面会继续讲解。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:4:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"Lasso Lasso对回归系数做了限定，对应的约束条件如下： \\[ \\sum_{k=1}^n|w_k| \\leq \\lambda \\] 而岭回归的约束条件如下： \\[ \\sum_{k=1}^nw_k^2 \\leq \\lambda \\] 唯一不同点在于，Lasso的约束条件用绝对值取代了平方和，虽然约束形式稍作变化，但结果大相径庭，细微的变化极大地增加了计算复杂度。所以用更简单的前向逐步回归来取代。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:5:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"前向逐步回归 前向逐步线性回归算法属于一种贪心算法，即每一步都尽可能减少误差。我们计算回归系数，不再是通过公式计算，而是通过每次微调各个回归系数，然后计算预测误差。那个使误差最小的一组回归系数，就是我们需要的最佳回归系数。 该算法的伪代码如下： 数据标准化，使其满足0均值和单位方差。 在每轮迭代中： 设置当前最小误差lowestError为正无穷 对每个特征 增大或缩小 改变一个系数得到新的W 计算新W下的误差 如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W 将W设置为新的Wbest ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:6:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"代码 使用的优化算法是梯度下降法，还有一个随机梯度下降 import numpy as np import matplotlib.pyplot as plt np.random.seed(42) w = np.array([2, 1, 4, 5, 3]) d = len(w) X = [] Y = [] for _ in range(1000000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X.append(x) Y.append(y) X = np.array(X) Y = np.array(Y) def mse(y_true, y_test): return ((y_true - y_test) ** 2) / len(y_true) def gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent(w, alpha, x, y): y_pred = x.dot(w) error = mse(y, y_pred).mean() grad = np.dot(x.T, gradient(y, y_pred)) w = w - alpha * grad return w, error def stochastic_gradient_descent(w, alpha, x, y, epoch): alpha_update = alpha for i in range(len(x)): y_pred = x[i].dot(w) grad = np.dot(x[i].T, (y_pred - y[i])) * 2 / len(x) w = w- alpha_update * grad alpha_update = alpha_update / (epoch+1) error = mse(y, x.dot(w)).mean() return w, error X_test = [] Y_test = [] for _ in range(10000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X_test.append(x) Y_test.append(y) X_test = np.array(X_test) Y_test = np.array(Y_test) def l2_mse(y_true, y_test, l, w): return ((y_true - y_test) ** 2) / len(y_true) + l * np.sum(w ** 2) def l2_gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent_with_l2(w, alpha, x, y, l): y_pred = x.dot(w) error = l2_mse(y, y_pred, l, w).mean() grad = np.dot(x.T, l2_gradient(y, y_pred)) w = w - alpha * grad - alpha * l * w *2 return w, error if __name__ == \"__main__\": train_loss = [] test_loss = [] print(\"Batch Gradient Descent\") for epoch in range(1000): w, error = batch_gradient_descent(w, 0.01, X, Y) # train y_pred = X_test.dot(w) error_test = mse(Y_test, y_pred).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-No-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-No-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-No-L2\") plt.plot(test_loss, label=\"Test-No-L2\") plt.legend() plt.show() # ============================================ train_loss = [] test_loss = [] print(\"Batch Gradient Descent with L2\") l = 0.0001 # lambda for epoch in range(1000): w, error = batch_gradient_descent_with_l2(w, 0.01, X, Y, l) # train y_pred = X_test.dot(w) error_test = l2_mse(Y_test, y_pred, l, w).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-L2\") plt.plot(test_loss, label=\"Test-L2\") plt.legend() plt.show() 其实很简单，就是数学公式的复现。 ## 总结 缩减方法（逐步线性回归或岭回归），就是将一些系数缩减成很小的值或者直接缩减为0。这样做，就增大了模型的偏差（减少了一些特征的权重），通过把一些特征的回归系数缩减到0，同时也就减少了模型的复杂度。消除了多余的特征之后，模型更容易理解，同时也降低了预测误差。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:7:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["算法题"],"content":"计数质数 https://leetcode-cn.com/problems/count-primes/ 一开始直接暴力，隐约感觉会超时，果然不出我所料 class Solution: def countPrimes(self, n: int) -\u003e int: counts = 0 for i in range(n): if self.isprime(i): counts += 1 return counts def isprime(self,n): from itertools import count if n \u003c=1: return False for i in count(2): if i* i \u003e n: return True if n % i == 0: return False 我还特意用了itertools库，没想到还是超时了 ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:0:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["算法题"],"content":"埃氏筛 class Solution: def countPrimes(self, n: int) -\u003e int: res = [1] * n count = 0 for i in range(2, n): if res[i]: count += 1 for j in range(i*i, n, i): res[j] = 0 return count 埃氏筛的原理：从 2 开始，将每个质数的倍数都标记为合数。同样的，标记到 根号n停止。 假设一个数 i 为质数时，那么此时大于 i 且是 i 的倍数的数一定不是质数，例如 2i，3i…。那么我们将这些不是质数的数进行标记。 这里需要注意，标记应该从 i * i 开始，而不是 2 * i 开始。因为对于每个数 i 来说，枚举是从小到大的，此时前面数字的倍数都已经进行了标记。对于 i 而言，2∗i 也肯定会被在枚举数字 2 时进行标记，[2, i) 区间的数同理。 ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:1:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["算法题"],"content":"欧拉筛（线性筛） 具体的证明不说了，背板子就行 class Solution: def countPrimes(self, n: int) -\u003e int: if n \u003c= 1: return 0 is_prime = [True] * n is_prime[0] = is_prime[1] = False res = [] for i in range(2, n): if is_prime[i]: res.append(i) j = 0 while j \u003c len(res) and (tmp:=i*res[j]) \u003c n: is_prime[tmp] = False if i % res[j] == 0: # 如果res[j]为当前数的约数，则i*res[j+1]等后面的数必然会在后面的计算得到。就结束循环。这减少了重复计算。 break j += 1 return len(res) ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:2:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["Machine Learning","分类算法"],"content":"感知机算法 感知机印象中没有系统学习过但是是一个很简单的算法，最近看了一下李航老师的统计学习方法，发现感知机的思想和svm十分类似，并且比svm简单的多，不需要间隔最大，只需要分开就可以。同时老师在课堂上面讲的版本也有点不一样，主要是计算上的不同，本质还是一样的。然后就打算整理一下这一块。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:0:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"感知机模型 假设输入空间（特征空间) 是 \\(\\mathcal{X} \\subseteq \\mathbf{R}^n\\), 输出空间是 \\(\\mathcal{Y}=\\\\{+1,-1\\\\}\\) 。输入 \\(x \\in \\mathcal{X}\\) 表示实例的特征向量, 对应于输入空间 (特征空间 ) 的点; 输出 \\(y \\in \\mathcal{Y}\\) 表示实例的类别。由输入空间到输出空间的如下函数 \\[ f(x)=\\operatorname{sign}(w \\cdot x+b) \\] 称为感知机。其中, \\(w\\) 和 \\(b\\) 为感知机模型参数, \\(w \\in \\mathbf{R}^n\\) 叫作权值 (weight) 或权值向 量 (weight vector), \\(b \\in \\mathbf{R}\\) 叫作偏置 (bias), \\(w \\cdot x\\) 表示 \\(w\\) 和 \\(x\\) 的内积。sign 是符号 函数, 即 \\[ \\operatorname{sign}(x)=\\left\\{\\begin{array}{cc} +1, \u0026 x \\geqslant 0 \\\\\\\\ -1, \u0026 x\u003c0 \\end{array}\\right. \\] ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:1:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"损失函数 假设训练数据集是线性可分的, 感知机学习的目标是求得一个能够将训练集正实 例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面, 即确定感知 机模型参数 \\(w, b\\), 需要确定一个学习策略, 即定义 (经验) 损失函数并将损失函数极 小化。 损失函数的一个自然选择是误分类点的总数。但是, 这样的损失函数不是参数 \\(w\\), \\(b\\) 的连续可导函数, 不易优化。损失函数的另一个选择是误分类点到超平面 \\(S\\) 的总距 离, 这是感知机所采用的。为此, 首先写出输入空间 \\(\\mathbf{R}^n\\) 中任一点 \\(x_0\\) 到超平面 \\(S\\) 的 距离: \\[ \\frac{1}{\\|w\\|}\\left|w \\cdot x_0+b\\right| \\] 这里, \\(\\|w\\|\\) 是 \\(w\\) 的 \\(L_2\\) 范数。 其次, 对于误分类的数据 \\(\\left(x_i, y_i\\right)\\) 来说, \\[ -y_i\\left(w \\cdot x_i+b\\right)\u003e0 \\] 成立。因为当 \\(w \\cdot x_i+b\u003e0\\) 时, \\(y_i=-1\\); 而当 \\(w \\cdot x_i+b\u003c0\\) 时, \\(y_i=+1\\) 。 因此, 误 分类点 \\(x_i\\) 到超平面 \\(S\\) 的距离是 \\[ -\\frac{1}{\\|w\\|} y_i\\left(w \\cdot x_i+b\\right) \\] 这样, 假设超平面 \\(S\\) 的误分类点集合为 \\(M\\), 那么所有误分类点到超平面 \\(S\\) 的总 距离为 \\[ -\\frac{1}{\\|w\\|} \\sum_{x_i \\in M} y_i\\left(w \\cdot x_i+b\\right) \\] 不考虑 \\(\\frac{1}{\\|w\\|}\\), 就得到感知机学习的损失函数。 即 \\[ L(w, b) = -\\sum_{x_i \\in M} y_i\\left(w \\cdot x_i+b\\right) \\] 使用梯度下降算法更新参数，对损失函数求导得到： \\[ \\begin{aligned} \u0026\\nabla_w L(w, b)=-\\sum_{x_i \\in M} y_i x_i \\\\\\\\ \u0026\\nabla_b L(w, b)=-\\sum_{x_i \\in M} y_i \\end{aligned} \\] 随机选取一个误分类点 \\(\\left(x_i, y_i\\right)\\), 对 \\(w, b\\) 进行更新: \\[ \\begin{gathered} w \\leftarrow w+\\eta y_i x_i \\\\\\\\ b \\leftarrow b+\\eta y_i \\end{gathered} \\] ## 算法流程 总结可得，感知机算法流程如下： 输入: 训练数据集 \\(T=\\left\\{\\left(x_1, y_1\\right),\\left(x_2, y_2\\right), \\cdots,\\left(x_N, y_N\\right)\\right\\\\\\}\\), 其中 \\(x_i \\in \\mathcal{X}=\\mathbf{R}^n, y_i \\in\\) \\(\\mathcal{Y}=\\\\{-1,+1\\\\}, i=1,2, \\cdots, N\\); 学习率 \\(\\eta(0\u003c\\eta \\leqslant 1)\\); 输出: \\(w, b\\); 感知机模型 \\(f(x)=\\operatorname{sign}(w \\cdot x+b)\\) 。 (1) 选取初值 \\(w_0, b_0\\); (2) 在训练集中选取数据 \\(\\left(x_i, y_i\\right)\\); (3) 如果 \\(y_i\\left(w \\cdot x_i+b\\right) \\leqslant 0\\), \\[ \\begin{aligned} \u0026w \\leftarrow w+\\eta y_i x_i \\\\\\\\ \u0026b \\leftarrow b+\\eta y_i \\end{aligned} \\] 转至 (2), 直至训练集中没有误分类点。 这很容易理解。就是求解最佳参数\\(w\\)和\\(b\\)，使用梯度下降算法，对于每个样本，如果其真实的标签与预测的结果符号不一致，也就是sign函数之前的结果不同号，则说明分类错误，则就需要更新参数，不断地继续更新直到所有的样本都分类正确。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:2:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"另一种表达方式 感知器: 用数据训练线性模型 \\(g({x})={w}^T {x}+w_0\\) 增广的样本向量: \\[ {y}=\\left(1 ; x_1 ; x_2 ; \\ldots ; x_d\\right) \\] 增广的权向量: \\[ {\\alpha}=\\left(w_0 ; w_1 ; \\ldots ; w_d\\right) \\] 线性判别函数: \\[ g({y})={\\alpha}^T {y} \\] 决策规则: 如果 \\(g({y})\u003e0\\), 则 \\(y \\in \\omega_0\\); 如果 \\(g({y})\u003c0\\), 则 \\(y \\in \\omega_1\\) 若定义新变量 \\(y^{\\prime}\\), 使 \\[ y_i^{\\prime}=\\left\\{\\begin{array}{lll} y_i, \\text { 若 } \u0026 {y}_i \\in \\omega_0 \\\\ -{y}_i, \\text { 若 } \u0026 {y}_i \\in \\omega_1 \\end{array} \\quad i=1,2, \\ldots, m\\right. \\] 样本可分性条件变为：存在 \\(\\alpha\\), 使 \\[ {\\alpha}^T {y}_i^{\\prime}\u003e0, i=1,2, \\ldots, m \\] \\(y^{\\prime}\\) 称作规范化增广样本向量, 仍记作 \\(y\\) 。 可以用这样的形式定义损失函数为： \\[ J(\\alpha) = \\sum_{\\alpha^Ty_k \\leq 0} (-\\alpha^Ty_k) \\] 其中w和b合并为了\\(\\alpha\\)。\\(y_k\\)为原来的x加上了1用于与偏置b对应。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"梯度下降迭代法求解 \\[ \\boldsymbol{\\alpha}(t+1)=\\boldsymbol{\\alpha}(t)-\\rho_t \\nabla J_P(\\boldsymbol{\\alpha}) \\] 下一时刻的权向量是把当前时刻的权向量向目标函数的负梯度方向调整一个修 正量, \\(\\rho_t\\) 为调整的步长 (“学习率”)。 \\[ \\nabla J_P(\\boldsymbol{\\alpha})=\\frac{\\partial J_P(\\boldsymbol{\\alpha})}{\\partial \\boldsymbol{\\alpha}}=\\sum_{\\alpha^T y_k \\leq 0}\\left(-y_k\\right) \\] 所以 \\[ \\alpha(t+1)=\\alpha(t)+\\rho_t \\sum_{\\alpha^T y_k \\leq 0} y_k \\] 即每次迭代时把错分的样本按照某个系数加到权向量上。 当没有错分样本时, 得到一个合适的解 $^* $ 。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:1","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"固定增量法 （1）任意选择初始权向量 \\(\\alpha(0)\\); (2) 对样本 \\(y_j\\), 若 \\(\\alpha(t)^T y_j \\leq 0\\), 则 \\(\\alpha(t+1)=\\alpha(t)+y_j\\) (假设 \\(\\left.\\rho_t=1\\right)\\), 否则继 续; (3) 对所有样本重复 (2), 直至对所有的样本都有 \\(\\alpha(t)^T y_j\u003e0\\), 即 \\(J_P(\\boldsymbol{\\alpha})=0\\) 与梯度下降法的区别就是每次只对一个样本更新，可以这样理解： 原始的数据，对于第二类则增广之后取负数就可以理解为前面第一种表达的\\(y_i\\left(w \\cdot x_i+b\\right)\\)， 大于0则说明分类正确，否则说明分类错误，就需要更新参数。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:2","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["NLP","库学习","transformers"],"content":"目的：手动将抽取出的样本堆叠起来，构造成batch Trainer函数有一个参数data_collator，其值也为一个函数，用于从一个list of elements来构造一个batch。 如下 trainer = CLTrainer( model=model, args=training_args, train_dataset=train_dataset if training_args.do_train else None, tokenizer=tokenizer, data_collator=data_collator, ) 其中data_collator为自定义的类，必须可调用，因此实现了__call__，要实现输入额外参数，就要用类的方式或者再嵌套匿名函数实现。 @dataclass class OurDataCollatorWithPadding: tokenizer: PreTrainedTokenizerBase padding: Union[bool, str, PaddingStrategy] = True max_length: Optional[int] = None pad_to_multiple_of: Optional[int] = None mlm: bool = True mlm_probability: float = data_args.mlm_probability def __call__(self, features: List[Dict[str, Union[List[int], List[List[int]], torch.Tensor]]]) -\u003e Dict[str, torch.Tensor]: special_keys = ['input_ids', 'attention_mask', 'token_type_ids', 'mlm_input_ids', 'mlm_labels'] # 必有的输入参数 bs = len(features) if bs \u003e 0: num_sent = len(features[0]['input_ids']) else: return flat_features = [] for feature in features: for i in range(num_sent): flat_features.append({k: feature[k][i] if k in special_keys else feature[k] for k in feature}) batch = self.tokenizer.pad( flat_features, padding=self.padding, max_length=self.max_length, pad_to_multiple_of=self.pad_to_multiple_of, return_tensors=\"pt\", ) if model_args.do_mlm: batch[\"mlm_input_ids\"], batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"]) batch = {k: batch[k].view(bs, num_sent, -1) if k in special_keys else batch[k].view(bs, num_sent, -1)[:, 0] for k in batch} if \"label\" in batch: batch[\"labels\"] = batch[\"label\"] del batch[\"label\"] if \"label_ids\" in batch: batch[\"labels\"] = batch[\"label_ids\"] del batch[\"label_ids\"] return batch 并且返回的必须是dict类型，必须要有特定的参数，即model.forward的必要参数。 而dataloader中的collate_fn比较自由，返回任意形式都可以。 collate_fn的用处: - 自定义数据堆叠过程 - 自定义batch数据的输出形式 ","date":"2021-11-12","objectID":"/data_collator/:0:0","tags":["NLP","transformers","库学习","data_collator"],"title":"情感分析","uri":"/data_collator/"},{"categories":["sklearn"],"content":"导入包 import numpy as np import pandas as pd from sklearn.model_selection import train_test_split import plotly.graph_objects as go ","date":"2021-11-09","objectID":"/knn/:1:0","tags":["sklearn","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["sklearn"],"content":"导入数据 data = pd.read_csv(\"./datasets/Social_Network_Ads.csv\") X = data.iloc[:,[2,3]].values Y = data.iloc[:,4].values # scatter = go.Scatter(x=X[:,0],y=X[:,1],mode='markers',marker={'color':Y}) # fig = go.Figure(scatter) # fig.show() X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0) ","date":"2021-11-09","objectID":"/knn/:2:0","tags":["sklearn","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["sklearn"],"content":"标准化 from sklearn.preprocessing import StandardScaler sca = StandardScaler() X_train = sca.fit_transform(X_train) X_test = sca.transform(X_test) ","date":"2021-11-09","objectID":"/knn/:3:0","tags":["sklearn","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["sklearn"],"content":"训练模型 from sklearn.neighbors import KNeighborsClassifier model = KNeighborsClassifier(n_neighbors=5,p=2) model.fit(X_train,Y_train) KNeighborsClassifier() ","date":"2021-11-09","objectID":"/knn/:4:0","tags":["sklearn","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["sklearn"],"content":"模型得分 model.score(X_test,Y_test) 0.93 ","date":"2021-11-09","objectID":"/knn/:5:0","tags":["sklearn","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["算法题"],"content":"最大数 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:0:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/largest-number/ ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:1:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 一开始直接暴力搜索，把所有的情况都列举然后比较，结果超时了，最后利用了自定义排序的方法 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:2:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def largestNumber(self, nums: List[int]) -\u003e str: class Comapre(str): def __lt__(self,other): return int(self+other) \u003e int(other+self) nums.sort(key=Comapre) return str(int(''.join(map(str,nums)))) 注意的是这里利用了自定义的比较类型，继承了str，也可以从functools里导入cmp_to_key方法来实现比较 python3之后不支持cmp，所用key函数并不直接比较任意两个原始元素，而是通过key函数把那些元素转换成一个个新的可比较对象，也就是元素的key，然后用元素的key代替元素去参与比较。如果原始元素本来就是可比较对象，比如数字、字符串，那么不考虑性能优化可以直接sort(key=lambda e: e)。不过这种基于key函数的设计倾向于每个元素的大小有个绝对标准，但有时却会出现单个元素并没有一个绝对的大小的情况，此时可以使用 functools.cmp_to_key构建基于多个元素的比较函数。 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:3:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["Deep Learning","循环神经网络系列"],"content":" image.png image.png ","date":"2021-11-02","objectID":"/lstm/:0:0","tags":["Deep Learning","循环神经网络系列","LSTM"],"title":"LSTM","uri":"/lstm/"},{"categories":["算法题"],"content":"检查平衡性 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:0:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/check-balance-lcci/ ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:1:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"思路： 算深度，然后作差是否大于1 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:2:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"代码： # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def isBalanced(self, root: TreeNode) -\u003e bool: if self.maxdepth(root) \u003c 1: return True if abs(self.maxdepth(root.left) - self.maxdepth(root.right)) \u003e 1: return False return self.isBalanced(root.right) and self.isBalanced(root.left) def maxdepth(self,root): if not root: return 0 return 1 + max(self.maxdepth(root.right),self.maxdepth(root.left)) ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:3:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["NLP"],"content":"语言模型 语言模型是一个很大的主题，很多nlp的任务都是基于语言模型进行的，因此理解语言模型是很重要的。 语言模型简单说就是 计算一个句子在语言中出现的概率。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"数学表示 一个语言模型通常构建字符串s的概率分布p(s)，这里p(s)反应字符串s作为一个句子出现的概率。 对于一个由m个基元（可以是字、词或者短语）构成的句子\\(s=w_1,w_2, \\dots w_m\\)，其概率计算公式可以表示为(乘法公式，详见bayes)： \\[ p(s) = p(w_1)p(w_2\\mid w_1)p(w_3\\mid w_1, w_2)\\cdots p(w_m\\mid w_1w_2\\dots w_{m-1})= \\prod_{i=1}^mp(w_i\\mid w_1\\cdots,w_{i-1}) \\] 但是很明显这个计算复杂度是极大的。 ## 评价指标 语言模型的常用评价指标是困惑度（perplexity）：在一个测试数据上的perplexity越低，说明建模的效果越好。perplexity计算公式如下： 简单来说，困惑度就是刻画一个语言模型预测一个语言样本的能力，其实际上就是计算每一个词得到的概率倒数的几何平均，即模型预测下一个词的平均可选择数量。 在实际应用中通常使用log的形式，即： ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"统计语言模型 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"n-gram 为了解决复杂度高的问题，因此引入马尔科夫假设，即当前词的预测概率只与之前n-1个词相关，基于此，语言模型可以修改如下： \\[ p(s) = p(w_1, w_2, \\dots, w_m) = \\prod_{i=1}^mp(w_i\\mid w_{i-n+1}, w_{i-1}) \\] 当n取1,2,3时，n-gram可以称为unigram、bigram、trigram。n越大复杂度越高。 n-gram model一般采用MLE进行参数估计： \\[ p(w_i\\mid w_{i-n+1}, \\cdots, w_{i-1}) = \\frac{C(w_{i-n+1}, \\cdots,w_{i-1}, w_i)}{C(w_{i-n+1}, \\cdots, w_{i-1})} \\] 即使训练语料再大，也存在参数为0的情况，这时候就需要引入数据平滑策略，其中最为常用的就是拉普拉斯平滑。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:1","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"拉普拉斯平滑 Add one 拉普拉斯平滑，即强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可。这个方法的弊端是，大部分n-gram都是没有出现过的，很容易为他们分配过多的概率空间。 #### Add-K 在Add-one的基础上做了一点小改动，原本是加一，现在加上一个小于1的常数K K。但是缺点是这个常数仍然需要人工确定，对于不同的语料库K可能不同。 k取1时与add one 相同 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:2","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"Kneser-Ney Smoothing 这是目前ngram模型效果最好的平滑方法。 ## 神经网络语言模型 具体可以看本博客有关神经网络语言模型的内容，NNLM是第一个出现的神经网络语言模型。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:3","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"隐马尔科夫模型 ","date":"2021-10-25","objectID":"/hmm/:0:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"介绍 HMM可以看做是处理序列模型的传统方法。 一般来说HMM解决三个问题： 评估观察序列概率。给定模型\\(\\lambda=(A,B,\\prod)\\)和观察序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，计算在模型\\(\\lambda\\)下观测序列O出现的概率\\(P(O\\lvert \\lambda)\\)，这个问题需要用到前向后向算法，属于三个问题中最简单的。 预测问题，也叫解码问题。即给定模型\\(\\lambda = (A,B,\\prod)\\)和观测序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，求在给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，这个问题属于三个问题中复杂度居中的算法。 模型参数学习问题。即给定观测序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，估计模型\\(\\lambda = (A,B,\\prod)\\)的参数，使得该模型下观测序列的条件概率\\(P(O\\lvert\\lambda)\\)最大，这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。属于三个问题中最复杂的。 ","date":"2021-10-25","objectID":"/hmm/:1:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"定义 设 \\(Q\\) 是所有可能的状态的集合, \\(V\\) 是所有可能的观测的集合: \\[ Q=\\left\\{q_1, q_2, \\cdots, q_N\\right\\}, \\quad V=\\left\\{v_1, v_2, \\cdots, v_M\\right\\} \\] 其中, \\(N\\) 是可能的状态数, \\(M\\) 是可能的观测数。 \\(I\\) 是长度为 \\(T\\) 的状态序列, \\(O\\) 是对应的观测序列: \\[ I=\\left(i_1, i_2, \\cdots, i_T\\right), \\quad O=\\left(o_1, o_2, \\cdots, o_T\\right) \\] \\(A\\) 是状态转移概率矩阵: \\[ A=\\left[a_{i j}\\right]_{N \\times N} \\] 其中, \\[ a_{i j}=P\\left(i_{t+1}=q_j \\mid i_t=q_i\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, N \\] 是在时刻 \\(t\\) 处于状态 \\(q_i\\) 的条件下在时刻 \\(t+1\\) 转移到状态 \\(q_j\\) 的概率。 \\(B\\) 是观测概率矩阵: \\[ B=\\left[b_j(k)\\right]_{N \\times M} \\] 其中, \\[ b_j(k)=P\\left(o_t=v_k \\mid i_t=q_j\\right), \\quad k=1,2, \\cdots, M ; \\quad j=1,2, \\cdots, N \\] 是在时刻 \\(t\\) 处于状态 \\(q_j\\) 的条件下生成观测 \\(v_k\\) 的概率。 \\(\\pi\\) 是初始状态概率向量: \\[ \\pi=\\left(\\pi_i\\right) \\] 其中, \\[ \\pi_i=P\\left(i_1=q_i\\right), \\quad i=1,2, \\cdots, N \\] 是时刻 \\(t=1\\) 处于状态 \\(q_i\\) 的概率。 隐马尔可夫模型由初始状态概率向量 \\(\\pi\\) 、状态转移概率矩阵 \\(A\\) 和观测概率矩阵 \\(B\\) 决定。 \\(\\pi\\) 和 \\(A\\) 决定状态序列, \\(B\\) 决定观测序列。因此, 隐马尔可夫模型 \\(\\lambda\\) 可以用三元 符号表示, 即 \\[ \\lambda=(A, B, \\pi) \\] \\(A, B, \\pi\\) 称为隐马尔可夫模型的三要素。 状态转移概率矩阵 \\(A\\) 与初始状态概率向量 \\(\\pi\\) 确定了隐藏的马尔可夫链, 生成不 可观测的状态序列。观测概率矩阵 \\(B\\) 确定了如何从状态生成观测, 与状态序列综合确 定了如何产生观测序列。 ","date":"2021-10-25","objectID":"/hmm/:2:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"两个基本假设 齐次马尔可夫性假设, 即假设隐藏的马尔可夫链在任意时刻 \\(t\\) 的状态只依赖 于其前一时刻的状态, 与其他时刻的状态及观测无关, 也与时刻 \\(t\\) 无关: \\[ P\\left(i_t \\mid i_{t-1}, o_{t-1}, \\cdots, i_1, o_1\\right)=P\\left(i_t \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T \\] 观测独立性假设, 即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状 态, 与其他观测及状态无关: \\[ P\\left(o_t \\mid i_T, o_T, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_t, i_{t-1}, o_{t-1}, \\cdots, i_1, o_1\\right)=P\\left(o_t \\mid i_t\\right) \\] ","date":"2021-10-25","objectID":"/hmm/:3:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"观测序列生成的过程 输入: 隐马尔可夫模型 \\(\\lambda=(A, B, \\pi)\\), 观测序列长度 \\(T\\); 输出: 观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 。 (1) 按照初始状态分布 \\(\\pi\\) 产生状态 \\(i_1\\); (2) 令 \\(t=1\\); (3) 按照状态 \\(i_t\\) 的观测概率分布 \\(b_{i_t}(k)\\) 生成 \\(o_t\\) : (4) 按照状态 \\(i_t\\) 的状态转移概率分布 \\(\\left\\{a_{i_t i_{t+1}}\\right\\}\\) 产生状态 \\(i_{t+1}, i_{t+1}=1,2, \\cdots, N\\); (5) 令 \\(t=t+1\\); 如果 \\(t\u003cT\\), 转步 (3); 否则, 终止。 ","date":"2021-10-25","objectID":"/hmm/:4:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"概率计算问题 ","date":"2021-10-25","objectID":"/hmm/:5:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"直接计算（复杂度太高） 给定模型 \\(\\lambda=(A, B, \\pi)\\) 和观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\), 计算观测序列 \\(O\\) 出现 的概率 \\(P(O \\mid \\lambda)\\) 。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为 \\(T\\) 的状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\), 求各个状态序列 \\(I\\) 与观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 的联合概率 \\(P(O, I \\mid \\lambda)\\), 然后对所有可能的状态序列求和, 得到 \\(P(O \\mid \\lambda)\\) 。 状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\) 的概率是: \\[ P(I \\mid \\lambda)=\\pi_{i_1} a_{i_1 i_2} a_{i_2 i_3} \\cdots a_{i_{T-1} i_T} \\] 对固定的状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\), 观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 的概率是: \\[ P(O \\mid I, \\lambda)=b_{i_1}\\left(o_1\\right) b_{i_2}\\left(o_2\\right) \\cdots b_{i_T}\\left(o_T\\right) \\] \\(O\\) 和 \\(I\\) 同时出现的联合概率为 \\[ \\begin{aligned} P(O, I \\mid \\lambda) \u0026=P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\\\\ \u0026=\\pi_{i_1} b_{i_1}\\left(o_1\\right) a_{i_1 i_2} b_{i_2}\\left(o_2\\right) \\cdots a_{i_{T-1} i_T} b_{i_T}\\left(o_T\\right) \\end{aligned} \\] 然后, 对所有可能的状态序列 \\(I\\) 求和, 得到观测序列 \\(O\\) 的概率 \\(P(O \\mid \\lambda)\\), 即 \\[ \\begin{aligned} P(O \\mid \\lambda) \u0026=\\sum_I P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\\\\ \u0026=\\sum_{i_1, i_2, \\cdots, i_T} \\pi_{i_1} b_{i_1}\\left(o_1\\right) a_{i_1 i_2} b_{i_2}\\left(o_2\\right) \\cdots a_{i_{T-1} i_T} b_{i_T}\\left(o_T\\right) \\end{aligned} \\] 这种算法复杂度太高，计算量太大，有效算法为前向算法和后向算法。 ","date":"2021-10-25","objectID":"/hmm/:5:1","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"前向算法 首先定义前向概率。 给定隐马尔可夫模型 \\(\\lambda\\), 定义到时刻 \\(t\\) 部分观测序列为 \\(o_1, o_2, \\cdots, o_t\\) 且状态为 \\(q_i\\) 的概率为前向概率, 记作 \\[ \\alpha_t(i)=P\\left(o_1, o_2, \\cdots, o_t, i_t=q_i \\mid \\lambda\\right) \\] 可以递推地求得前向概率 \\(\\alpha_t(i)\\) 及观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (观测序列概率的前向算法) 输入: 隐马尔可夫慔型 \\(\\lambda\\), 观测序列 \\(O\\); 输出: 观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (1) 初值 \\[ \\alpha_1(i)=\\pi_i b_i\\left(o_1\\right), \\quad i=1,2, \\cdots, N \\] （2）递推 对 \\(t=1,2, \\cdots, T-1\\), \\[ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^N \\alpha_t(j) a_{j i}\\right] b_i\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N \\] 终止 \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\alpha_T(i) \\] 前向算法, 步骤 (1) 初始化前向概率, 是初始时刻的状态 \\(i_1=q_i\\) 和观测 \\(o_1\\) 的 联合概率。步骤 (2) 是前向概率的递推公式, 计算到时刻 \\(t+1\\) 部分观测序列为 \\(o_1, o_2, \\cdots, o_t, o_{t+1}\\) 且在时刻 \\(t+1\\) 处于状态 \\(q_i\\) 的前向概率, 如图 \\(10.1\\) 所示。在式 (10.16) 的方括弧里, 既然 \\(\\alpha_t(j)\\) 是到时刻 \\(t\\) 观测到 \\(o_1, o_2, \\cdots, o_t\\) 并在时刻 \\(t\\) 处于状态 \\(q_j\\) 的前向概率, 那么乘积 \\(\\alpha_t(j) a_{j i}\\) 就是到时刻 \\(t\\) 观测到 \\(o_1, o_2, \\cdots, o_t\\) 并在时刻 \\(t\\) 处于 状态 \\(q_j\\) 而在时刻 \\(t+1\\) 到达状态 \\(q_i\\) 的联合概率。对这个乘积在时刻 \\(t\\) 的所有可能的 #### 统计学习方法中前向算法的例子 ### 后向算法 给定隐马尔可夫模型 \\(\\lambda\\), 定义在时刻 \\(t\\) 状态为 \\(q_i\\) 的条件下, 从 \\(t+1\\) 到 \\(T\\) 的部分观测序列为 \\(o_{t+1}, o_{t+2}, \\cdots, o_T\\) 的概率为后向概率, 记作 \\[ \\beta_t(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_T \\mid i_t=q_i, \\lambda\\right) \\] 可以用递推的方法求得后向概率 \\(\\beta_t(i)\\) 及观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (观测序列概率的后向算法) 输入: 隐马尔可夫模型 \\(\\lambda\\), 观测序列 \\(O\\); 输出: 观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (1) \\[ \\beta_T(i)=1, \\quad i=1,2, \\cdots, N \\] 对 \\(t=T-1, T-2, \\cdots, 1\\) \\[ \\beta_t(i)=\\sum_{j=1}^N a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N \\] 后向算法到这一步只用到了第二个观测值，还有第一个观测值没有用到。 因此最后要乘上。 (3) \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\pi_i b_i\\left(o_1\\right) \\beta_1(i) \\] ","date":"2021-10-25","objectID":"/hmm/:5:2","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"结合 利用前向概率和后向概率的定义可以将观测序列概率 \\(P(O \\mid \\lambda)\\) 统一写成 \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1 \\] 也可以写成： \\[ P(O\\mid \\lambda) = \\sum_{i=1}^N[\\alpha_t(i)\\beta_t(i)], \\quad t=1,2,\\cdots, T-1 \\] ","date":"2021-10-25","objectID":"/hmm/:5:3","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"一些概率和期望问题 利用前向概率和后向概率, 可以得到关于单个状态和两个状态概率的计算公式。 1. 给定模型 \\(\\lambda\\) 和观测 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 的概率。记 \\[ \\gamma_t(i)=P\\left(i_t=q_i \\mid O, \\lambda\\right) \\] 可以通过前向后向概率计算。事实上， \\[ \\gamma_t(i)=P\\left(i_t=q_i \\mid O, \\lambda\\right)=\\frac{P\\left(i_t=q_i, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)} \\] 由前向概率 \\(\\alpha_t(i)\\) 和后向概率 \\(\\beta_t(i)\\) 定义可知: \\[ \\alpha_t(i) \\beta_t(i)=P\\left(i_t=q_i, O \\mid \\lambda\\right) \\] 于是得到: \\[ \\gamma_t(i)=\\frac{\\alpha_t(i) \\beta_t(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\] 给定模型 \\(\\lambda\\) 和观测 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 且在时刻 \\(t+1\\) 处于状态 \\(q_j\\) 的概 率。记 \\[ \\xi_t(i, j)=P\\left(i_t=q_i, i_{t+1}=q_j \\mid O, \\lambda\\right) \\] 可以通过前向后向概率计算: \\[ \\xi_t(i, j)=\\frac{P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)}{\\sum_{i=1}^N \\sum_{j=1}^N P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)} \\] 而 \\[ P(i_t=q_i,i_{t+1}=q_j,O | \\lambda) = \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j) \\] 所以 \\[ \\xi_t(i, j)=\\frac{\\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j)} \\] 将 \\(\\gamma_t(i)\\) 和 \\(\\xi_t(i, j)\\) 对各个时刻 \\(t\\) 求和, 可以得到一些有用的期望值。 在观测 \\(O\\) 下状态 \\(i\\) 出现的期望值: \\[ \\sum_{t=1}^T \\gamma_t(i) \\] （2）在观测 \\(O\\) 下由状态 \\(i\\) 转移的期望值: \\[ \\sum_{t=1}^{T-1} \\gamma_t(i) \\] 在观测 \\(O\\) 下由状态 \\(i\\) 转移到状态 \\(j\\) 的期望值: \\[ \\sum_{t=1}^{T-1} \\xi_t(i, j) \\] ","date":"2021-10-25","objectID":"/hmm/:6:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"预测问题 ","date":"2021-10-25","objectID":"/hmm/:7:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"近似算法 近似算法的想法是, 在每个时刻 \\(t\\) 选择在该时刻最有可能出现的状态 $i_t^* $, 从而得 到一个状态序列 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\), 将它作为预测的结果。 给定隐马尔可夫模型 \\(\\lambda\\) 和观测序列 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 的概率 \\(\\gamma_t(i)\\) 是 \\[ \\gamma_t(i)=\\frac{\\alpha_t(i) \\beta_t(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\] 在每一时刻 \\(t\\) 最有可能的状态 $i_t^* $ 是 \\[ i_t^* =\\arg \\max_{1 \\leqslant i \\leqslant N}\\left[\\gamma_t(i)\\right], \\quad t=1,2, \\cdots, T \\] 从而得到状态序列 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 近似算法的优点是计算简单, 其缺点是不能保证预测的状态序列整体是最有可能 的状态序列, 因为预测的状态序列可能有实际不发生的部分。事实上, 上述方法得到 的状态序列中有可能存在转移概率为 0 的相邻状态, 即对某些 \\(i, j, a_{i j}=0\\) 时。尽管 如此, 近似算法仍然是有用的。 近似算法就是一种贪心的算法，每个时刻都取最有可能的状态，但整体序列并不一定是最优解。 ","date":"2021-10-25","objectID":"/hmm/:7:1","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"维特比算法 维特比算法实际上是用动态规划解隐马尔科夫模型预测问题，用动态规划求解概率最大路径，一条路径对应着一条状态序列。 首先导入两个变量 \\(\\delta\\) 和 \\(\\Psi\\) 。定义在时刻 \\(t\\) 状态为 \\(i\\) 的所有单个路径 \\(\\left(i_1, i_2, \\cdots, i_t\\right)\\) 中概率最大值为 \\[ \\delta_t(i)=\\max_{i_1, i_2, \\cdots, i_{t-1}} P\\left(i_t=i, i_{t-1}, \\cdots, i_1, o_t, \\cdots, o_1 \\mid \\lambda\\right), \\quad i=1,2, \\cdots, N \\] 后面的部分与前向算法的部分有点类似。 由定义可得变量 \\(\\delta\\) 的递推公式: \\[ \\begin{aligned} \\delta_{t+1}(i) \u0026=\\max_{i_1, i_2, \\cdots, i_t} P\\left(i_{t+1}=i, i_t, \\cdots, i_1, o_{t+1}, \\cdots, o_1 \\mid \\lambda\\right) \\\\\\\\ \u0026=\\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_t(j) a_{j i}\\right] b_i\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} \\] 定义在时刻 \\(t\\) 状态为 \\(i\\) 的所有单个路径 \\(\\left(i_1, i_2, \\cdots, i_{t-1}, i\\right)\\) 中概率最大的路径的 第 \\(t-1\\) 个结点为 \\[ \\Psi_t(i)=\\arg \\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\] 可以简单理解为找到使得从t-1的j到t的i式子\\(\\delta_{t-1}(j)a_{ji}\\)最大的j，也就是说\\(\\Psi_t(i)\\)代表t-1时刻的最佳状态值，如果t时刻的最佳状态值是i的话，那么t-1时刻的最佳状态值就是\\(\\Psi_t(i)\\)，后面回溯要用到 下面介绍维特比算法。 输入: 模型 \\(\\lambda=(A, B, \\pi)\\) 和观测 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\); 输出: 最优路径 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 （1）初始化 \\[ \\begin{gathered} \\delta_1(i)=\\pi_i b_i\\left(o_1\\right), \\quad i=1,2, \\cdots, N \\\\\\\\ \\Psi_1(i)=0, \\quad i=1,2, \\cdots, N \\end{gathered} \\] 前者和前向算法的初始化是一样的。 递推。对 \\(t=2,3, \\cdots, T\\) \\[ \\begin{array}{ll} \\delta_t(i)=\\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_i\\left(o_t\\right), \\quad i=1,2, \\cdots, N \\\\\\\\ \\Psi_t(i)=\\arg \\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} \\] 终止 \\[ \\begin{gathered} P^* =\\max_{1 \\leqslant i \\leqslant N} \\delta_T(i) \\\\\\\\ i_T^* =\\arg \\max_{1 \\leqslant i \\leqslant N}\\left[\\delta_T(i)\\right] \\end{gathered} \\] 这里得到的是最后一个时刻的最佳状态值，然后进行回溯。 最优路径回溯。对 \\(t=T-1, T-2, \\cdots, 1\\) \\[ i_t^* =\\Psi_{t+1}\\left(i_{t+1}^* \\right) \\] 求得最优路径 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 书上的例子 看一个例子就很容易理解了 #### 代码 def viterbi(obs, states, start_p, trans_p, emit_p): V = [{}] # 列表idx代表时间t，字典的键代表状态值，值代表概率 path = {} # 最佳路径 for y in states: V[0][y] = start_p[y] * emit_p[y].get(obs[0], 1e-5) path[y] = [0] # 都初始化为0 for t in range(1, len(obs)): V.append({}) for y in states: em_p = emit_p[y].get(obs[t], 1e-5) # 取出观测值对应的概率 (prob, state) = max([(V[t-1][y0]*trans_p[y0][y]*em_p, y0) for y0 in states]) V[t][y] = prob path[y] = path[y] + [state] # 记录路径，state是当前时间t状态为y时t-1的最佳状态，也就是从state转移到y的概率最大。如果最后时刻的最佳状态是y，则回溯从y开始，最后的状态也是y。 (prob, state) = max((V[len(obs)-1][y], y) for y in states) # 求最后时刻的最大概率和状态。 return path[state][1:] + [state] # 初始状态是0，所以去掉第一个0，再加上最后时刻的最大概率的状态，结果就是最佳路径。这里键对值的过程相当于回溯了。 A = { 0 : {0:0.5, 1:0.2, 2:0.3}, 1 : {0:0.3, 1:0.5, 2:0.2}, 2 : {0:0.2, 1:0.3, 2:0.5} } B = { 0: {'红': 0.5, '白': 0.5}, 1: {'红': 0.4, '白': 0.6}, 2: {'红': 0.7, '白': 0.3} } π = {0:0.2, 1:0.4, 2:0.4} viterbi(['红', '白', '红'], [0, 1, 2], π, A, B) 拿上面的例子进行实验，思路是完全按照李航老师书的思路来的。还是比较容易理解的，只是回溯的实现不太一样，这个严格来说不能叫做回溯。 ","date":"2021-10-25","objectID":"/hmm/:7:2","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"学习算法 ","date":"2021-10-25","objectID":"/hmm/:8:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"参考 《统计学习方法》李航 https://www.52nlp.cn/hmm-learn-best-practices-one-introduction https://www.cnblogs.com/pinard/p/6945257.html ","date":"2021-10-25","objectID":"/hmm/:9:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP"],"content":"What? TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。 TF-IDF有两层意思，一层是”词频”（Term Frequency，缩写为TF），另一层是”逆文档频率”（Inverse Document Frequency，缩写为IDF）。 假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。 但这样又会遇到了另一个问题，我们可能发现”量化”、“系统”、“架构”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。 当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。 ","date":"2021-10-24","objectID":"/tf-idf/:1:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"步骤 第一步，计算词频： \\[ 词频(TF) = \\frac{某个词在文章中出现次数}{文章的总词数} \\] 第二步，计算逆文档频率： \\[ 逆文档频率(IDF) = log(\\frac{语料库的文档总数}{包含该词的文档数+1}) \\] 1.为什么+1？是为了处理分母为0的情况。假如所有的文章都不包含这个词，分子就为0，所以+1是为了防止分母为0的情况。 2.为什么要用log函数？log函数是单调递增，求log是为了归一化，保证反文档频率不会过大。 3.会出现负数？肯定不会，分子肯定比分母大。 第三步，计算TF-IDF： \\[ TF-IDF = 词频(TF) \\times 逆文档频率(IDF) \\] ","date":"2021-10-24","objectID":"/tf-idf/:2:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"优缺点 TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。 ","date":"2021-10-24","objectID":"/tf-idf/:3:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"代码 corpus = ['this is the first document', 'this is the second second document', 'and the third one', 'is this the first document'] words_list = list() for i in range(len(corpus)): words_list.append(corpus[i].split(' ')) ","date":"2021-10-24","objectID":"/tf-idf/:4:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"手动实现 def manual(): from collections import Counter count_list = list() for i in range(len(words_list)): count = Counter(words_list[i]) count_list.append(count) import math def tf(word, count): return count[word] / sum(count.values()) def idf(word, count_list): n_contain = sum([1 for count in count_list if word in count]) return math.log(len(count_list) / (1 + n_contain)) def tf_idf(word, count, count_list): return tf(word, count) * idf(word, count_list) for i, count in enumerate(count_list): print(\"第 {} 个文档 TF-IDF 统计信息\".format(i + 1)) scores = {word : tf_idf(word, count, count_list) for word in count} sorted_word = sorted(scores.items(), key = lambda x : x[1], reverse=True) for word, score in sorted_word: print(\"\\tword: {}, TF-IDF: {}\".format(word, round(score, 5))) ","date":"2021-10-24","objectID":"/tf-idf/:4:1","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"gensim def gensim_work(): from gensim import corpora # 赋给语料库中每个词(不重复的词)一个整数id dic = corpora.Dictionary(words_list) # 创建词典 new_corpus = [dic.doc2bow(words) for words in words_list] # 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数 from gensim import models tfidf = models.TfidfModel(new_corpus) tfidf.save(\"tfidf.model\") # 载入模型 tfidf = models.TfidfModel.load(\"tfidf.model\") # 使用这个训练好的模型得到单词的tfidf值 res = [tfidf[temp] for temp in new_corpus] print(res) ","date":"2021-10-24","objectID":"/tf-idf/:4:2","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"sklearn def sklearn_work(): from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() tfidf = vectorizer.fit_transform(corpus) print(tfidf.toarray()) # 权重 print(vectorizer.get_feature_names()) # 单词 print(vectorizer.vocabulary_) # 词典 ","date":"2021-10-24","objectID":"/tf-idf/:4:3","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["pandas"],"content":"记录一下实训学到的内容 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:0:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"布尔索引 布尔索引不能使用and or not ，只能用\u0026 | ~ 因为只能用位操作符 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"花哨索引 arr = np.arange(32).reshape((8, 4)) arr array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) arr[[1, 5, 7, 2], [0, 3, 1, 2]] array([ 4, 23, 29, 10]) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:1","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"更常用的方式为 arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] # 行，列重置顺序 array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:2","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"pandas.cut import pandas as pd import numpy as np cars = pd.read_csv(\"second_cars_info.csv\",encoding=\"gbk\") final_ = [cars.Sec_price.min()] + list(np.linspace(10,100,10)) + [cars.Sec_price.max()] pd.cut(cars[\"Sec_price\"],bins=final_).value_counts().sort_index() # 对区间进行排序 # labels参数给每个区间贴上标签 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:2:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":".str的用法 可以对行列进行python字符串一样的操作 stud_alcoh[[\"Mjob\",'Fjob']].apply(lambda x:x.str.upper()) # 对Mjob Fjob两列变成大写 # 也可以用applymap stud_alcoh[[\"Mjob\",'Fjob']].applymap(lambda x:x.upper()) #区别就在于.str # 因为applymap只能对dataframe进行操作,apply可以对Dataframe和Series进行操作。 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:3:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"groupby与apply灵活运用 coffee = pd.read_excel(\"coffee.xlsx\") coffee.groupby(\"区域\").apply(lambda x:x[\"销售额\"].sum()) # 如果是DataFramegroupby对象调用apply，转换函数处理的就是每一个分组（dataframe） # 等价于 coffee.groupby(\"区域\")['销售额'].sum() 求各个区域的 销售总和、平均销售额、预计销售额与实际销售额的差异总和 def transfer(x_):# x_类型是dataframe res = pd.Series() res[\"销售总和\"] = x_[\"销售额\"].sum() res[\"平均销售额\"] = x_['销售额'].mean() res['差值'] = ( x_[\"销售额\"] - x_['预计销售额']).mean() return res # 返回的是一个series coffee.groupby(\"区域\").apply(transfer) 求出个区域中，高于该区域平均利润的记录 def transfer(x): # x(dataframe) mean_value = x[\"利润额\"].mean() condition = x[\"利润额\"] \u003e mean_value return x[condition] # dataframe c = coffee.groupby(\"区域\").apply(transfer) # 二维索引 # c.loc[(\"Central\",0)] ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:4:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"groupby也可以嵌套 def top_3_data(x):# x 表示DataFrame 每个区域的数据集 res = x.groupby(\"产品名称\").sum().sort_values(\"销售额\",ascending=False).iloc[:3] return res coffee.groupby(\"区域\").apply(top_3_data) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:4:1","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"nlargest nsmallest 求每一列最大的几个多最小的几个，一般配合groupby和apply使用 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:5:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"pd.Grouper grouper = pd.Grouper(key=\"订单日期\",freq=\"M\") 按月进行分组 。freq=\"Y\"就是按年分组 def top_sale_month(x): #x 表示DataFrame 每个产品的数据集 grouper = pd.Grouper(key=\"订单日期\",freq=\"M\") return x.groupby(grouper).sum().nlargest(1,\"销售额\") coffee.groupby(\"产品名称\").apply(top_sale_month).reset_index() reset_index()方法就是按照原来的index显示，不然就是按照分组的结果展示 ## plt.xticks plt.yticks 可以理解为自定义x轴的坐标 plt.figure(figsize=(12,8)) width = 0.2 plt.bar(np.arange(4)+0,np.random.randint(3,10,(4)),color='r',width=width) plt.bar(np.arange(4)+width*1,np.random.randint(3,10,(4)),color='g',width=width) plt.bar(np.arange(4)+width*2,np.random.randint(3,10,(4)),color='b',width=width) plt.xticks(np.arange(4)+width/2,list(\"abcd\")) # 将0 1 2 3 替换为 a b c d plt.show() ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:6:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"np.where np.where(condition, [x, y]),这里三个参数,其中必写参数是condition(判断条件),后边的x和y是可选参数.那么这三个参数都有怎样的要求呢? condition：array_like，bool,当为True时，产生x，否则产生y 情况1： np.where([[True, False], [True, True]], [[1, 2], [3, 4]], [[9, 8], [7, 6]]) 返回： array([[1, 8], [3, 4]]) 条件中第0个元素中的第0个元素是true,那么取x中的相对应元素1; 条件中第0个元素中的第1个元素是false,那么取y中的相对应元素8; 条件中第1个元素中的第0个元素是ture,那么取x中相对应的元素3; 条件中第1个元素中的第1个元素是ture,那么取x中相对应的元素4; 所以最后的结果中取出的元素是1,8,3,4. 情况2： x = np.arange(9.).reshape(3, 3) np.where(x\u003e5) # 返回的是索引 (array([2, 2, 2], dtype=int64), array([0, 1, 2], dtype=int64)) 第一个array是行坐标，第二个array为列坐标。 不想要索引想要具体的数值也很简单 x[np.where(x\u003e5)] array([6., 7., 8.]) np.where(x \u003c 5, x, -1) array([[ 0., 1., 2.], [ 3., 4., -1.], [-1., -1., -1.]]) 可见小于五的部分不变，大于5的则变成了-1 np.where常用于pandas的Series中。 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:7:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["Mathematical Modeling"],"content":"典型相关分析 参考：CCA 典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。 CCA使用的方法是将多维的X和Y都用线性变换为1维的X’和Y’，然后再使用相关系数来看X’和Y’的相关性。将数据从多维变到1位，也可以理解为CCA是在进行降维，将高维数据降到1维，然后再用相关系数进行相关性的分析。下面我们看看CCA的算法思想。 ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:0:0","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"算法思想 现在我们具体来讨论下CCA的算法思想。假设我们的数据集是X和Y，X为\\(n_1\\times m\\)的样本矩阵。Y为\\(n_2\\times m\\)的样本矩阵.其中m为样本个数，而\\(n_1,n_2\\)分别为X和Y的特征维度。 对于X矩阵，我们将其投影到1维，或者说进行线性表示，对应的投影向量或者说线性系数向量为a, 对于Y矩阵，我们将其投影到1维，或者说进行线性表示，对应的投影向量或者说线性系数向量为b, 这样X ,Y投影后得到的一维向量分别为X’,Y’。我们有 \\[ X'=a^TX,Y'=b^TY \\] 我们CCA的优化目标是最大化ρ(X′,Y′)得到对应的投影向量a,b，即 \\[ \\underbrace{argmax}_{a,b}\\frac{cov(X',Y')}{\\sqrt{D(X')D(Y')}} \\] 在投影前，我们一般会把原始数据进行标准化，得到均值为0而方差为1的数据X和Y。这样我们有： \\[ cov(X',Y')=cov(a^TX,b^TY)=a^Tcov(X,Y)b \\\\\\\\ D(X') = D(a^TX)= a^TD(X)a \\\\\\\\ D(Y') = D(b^TY)= b^TD(Y)b \\] 令\\(S_{XY} = cov(X,Y)\\)优化目标可以简化为 \\[ \\underbrace{argmax}_{a,b}\\frac{a^TS_{XY}b}{\\sqrt{a^TS_{XX}a}\\sqrt{b^TS_{YY}b}} \\] 由于分子分母增大相同的倍数，优化目标结果不变，我们可以采用和SVM类似的优化方法，固定分母，优化分子，具体的转化为： \\[ \\underbrace{argmax}_{a,b}\\quad a^TS_{XY}b \\\\\\\\ s.t.\\quad a^TS_{XX}a=1,b^TS_{YY}b=1 \\\\\\\\ \\text{因为已经标准化了，因此方差为0} \\] 也就是说，我们的CCA算法的目标最终转化为一个凸优化过程，只要我们求出了这个优化目标的最大值，就是我们前面提到的多维X和Y的相关性度量，而对应的a,ba,b则为降维时的投影向量，或者说线性系数。 这个函数优化一般有两种方法，第一种是奇异值分解SVD，第二种是特征分解，两者得到的结果一样。 ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:1:0","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"优化 ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:2:0","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"svd求解 不想敲公式了，直接截图吧 ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:2:1","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"特征分解 ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:2:2","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"CCA算法流程 这里我们对CCA的算法流程做一个总结，以SVD方法为准。 输入：各为m个的样本X和Y，X和Y的维度都大于1 输出：X,Y的相关系数\\(\\rho\\),X和Y的线性系数向量a和b 1.计算X的方差\\(S_{XX}\\)，Y的方差\\(S_{YY}\\)，X和Y的协方差\\(S_{XY}\\)，Y和X的协方差\\(S_{YX}=S_{XY}^T\\) 2.计算矩阵\\(M=S_{XX}^{-\\frac{1}{2}}S_{XY}S_{YY}^{-\\frac{1}{2}}\\) 3.对矩阵M进行奇异值分解，得到最大的奇异值\\(\\rho\\)，和最大的奇异值对应的左右奇异向量u,v 4.计算X和Y的线性向量a,b，\\(a=S_{XX}^{-\\frac{1}{2}}u, b=S_{YY}^{-\\frac{1}{2}}v\\) ","date":"2021-10-21","objectID":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/:3:0","tags":["Mathematical Modeling","典型相关分析"],"title":"典型相关分析","uri":"/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/"},{"categories":["算法题"],"content":"平衡括号字符串的最少插入次数 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:0:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-insertions-to-balance-a-parentheses-string/ ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:1:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 本题和前面的题属于同一系列的，都是平衡括号字符串，不过这个不是1:1 而是1:2 思路还是差不多，不过判断条件需要改变 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:2:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def minInsertions(self, s: str) -\u003e int: res,temp = 0,0 for i in s: if i == '(': temp += 2 if temp % 2 == 1: res += 1 temp -= 1 if i == ')': temp -= 1 if temp == -1: res += 1 temp = 1 return res + temp 开始还是初始化，temp代表需求的右括号的数量 如果有左括号的话，则让右括号的需求+2 因为一个左对应两个右 这里有个难点，如果需求的是奇数的话，则应添加一个右括号，然后让需求减1 如果是右括号，则需求 减1 如果需求的成了-1 的话 则在左边补上左括号 res++ 此时还需要一个右括号，则temp再初始化为1 最后还是输出 Q:为什么最后不是 temp == -2 res += 1 temp=0 呢？ 看看这个例子: \")))))))\" 这是7个右括号，最后减到最后的话，temp是个负数，影响了最后的结果。 所以还是要用原来的那样，-1的时候就进行判断，不用考虑奇偶的问题了 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:3:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["python"],"content":"可变与不可变 “-=”操作符会调用__isub__函数，而”-“操作符会调用__sub__函数，一般对于可变对象来说“-=”操作符会直接改变self自身。 import torch x1 = 1 x2 = 2 params = [x1, x2] for p in params: print(id(p), id(x1), id(x2)) p -= 4 print(id(p), id(x1), id(x2)) print(params) x1 = torch.Tensor([1]) x2 = torch.Tensor([2]) params = [x1, x2] for p in params: print(id(p), id(x1), id(x2)) p -= 4 print(id(p), id(x1), id(x2)) print(params) 9784896 9784896 9784928 9784768 9784896 9784928 9784928 9784896 9784928 9784800 9784896 9784928 [1, 2] 139752445458112 139752445458112 139752445458176 139752445458112 139752445458112 139752445458176 139752445458176 139752445458112 139752445458176 139752445458176 139752445458112 139752445458176 [tensor([-3.]), tensor([-2.])] 可以看到对于int类型，地址变换了，而torch类型，地址却没有变化。 p -= 4等价于p.sub_(4)。这个可变对象改变了自身。写成p = p - 4则会调用构造函数，并返回一个新的变量，也就不可能作用到原先的“可变对象”。 int类没有发生就地变化是因为它是一个不可变对象。 这是python “-” 与”-=“的一个坑 微妙的字符串 a = 'some_thing' b = 'some'+'_'+'thing' id(a),id(b) (1957716471920, 1957716471920) a = 'wtf' b = 'wtf' a is b True a = 'wtf!' b = 'wtf!' a is b False a,b = 'wtf!','wtf!' a is b True 'a'*20 is 'aaaaaaaaaaaaaaaaaaaa','a'*21 is 'aaaaaaaaaaaaaaaaaaaaa' (True, False) Cpython 在编译优化时, 某些情况下会尝试使用已经存在的不可变对象,成为字符串驻留 发生驻留之后, 许多变量可能指向内存中的相同字符串对象 所有长度为 0 和长度为 1 的字符串都被驻留. 字符串在编译时被实现 (‘wtf’ 将被驻留, 但是 ’‘.join([’w’, ’t’, ’f’] 将不会被驻留) 字符串中只包含字母，数字或下划线时将会驻留. 所以 ’wtf!’ 由于包含 ! 而未被驻留。 当在同一行将 a 和 b 的值设置为 “wtf!” 的时候, Python 解释器会创建一个新对象, 然后同时引用第二个变量. 常量折叠(constant folding) 是 Python 中的一种 窥孔优化(peephole optimization) 技术. 这意味着在编译时表达式 ‘a’*20 会被替换为 ‘aaaaaaaaaaaaaaaaaaaa’ 以减少运行时的时钟周期. 只有长度小于 20 的字符串才会发生常量折叠. a = 1 b = 1 a is b,id(a) == id(b) (True, True) is 是比较对象是否相同(is 表示对象标识符即 object identity)，即用 id() 函数查看的地址是否相同，如果相同则返回 True，如果不同则返回 False。is 不能被重载。 == 是比较两个对象的值是否相等，此操作符内部调用的是 _eq() 方法。所以 a==b 等效于a.___eq__(b)，所以 = 可以被重载 ","date":"2021-10-18","objectID":"/wtf/:1:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"是时候来点蛋糕了! some_dict = {} some_dict[5.5] = 'ruby' some_dict[5.0] = 'javascript' some_dict[5] = 'python' print(some_dict[5.0]) python 5 == 5.0,hash(5) == hash(5.0) (True, True) Python 字典通过检查键值是否相等和比较哈希值来确定两个键是否相同. 具有相同值的不可变对象在Python中始终具有相同的哈希值 ","date":"2021-10-18","objectID":"/wtf/:2:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"本质上,我们都一样 class WTF: pass print(WTF() == WTF(),WTF() is WTF()) print(hash(WTF()) == hash(WTF())) print(id(WTF()) == id(WTF())) False False True True 当调用 id 函数时, Python 创建了一个 WTF 类的对象并传给 id 函数. 然后 id 函数获取其id值 (也就是内存地址), 然后丢弃该对象. 该对象就被销毁了. 当我们连续两次进行这个操作时, Python会将相同的内存地址分配给第二个对象. 因为 (在CPython中) id 函数使用对象的内存地址作为对象的id值, 所以两个对象的id值是相同的. print(id(id(WTF())) == id(id(WTF()))) ##无论多少个ID都是True 原因就在上面 ##虽然id(id(WTF())) == id(id(WTF())) 但是id(WTF()) is id(WTF()) 返回True ##原因就是id这个函数调用的过程特殊性 print(id(WTF()) is id(WTF())) True False class WTF(object): def __init__(self): print(\"I\") def __del__(self): print(\"D\") WTF() is WTF() ##这时是两个对象一起创建，然后一起销毁，所以id不一样 I I D D False id(WTF()) == id(WTF()) ##这时候先创建一个销毁，然后再创建。对象销毁的顺序是造成所有不同之处的原因. I D I D True ","date":"2021-10-18","objectID":"/wtf/:3:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"为什么？ some_string = \"wtf\" some_dict = {} for i, some_dict[i] in enumerate(some_string): pass some_dict Python 语法 中对 for 的定义是: {0: 'w', 1: 't', 2: 'f'} for_stmt: 'for' exprlist 'in' testlist ':' suite ['else' ':' suite] 其中 exprlist 指分配目标. 这意味着对可迭代对象中的每一项都会执行类似 {exprlist} = {next_value} 的操作. for i in range(4): print(i) i = 10 0 1 2 3 ","date":"2021-10-18","objectID":"/wtf/:4:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"列表副本 list1 = [1,2,3,4,5] list2 = list1 list2[0] = 6 print(list1,list2) [6, 2, 3, 4, 5] [6, 2, 3, 4, 5] list1 = [1,2,3,4,5] list2 = list1[:] list2[0] = 6 print(list1,list2) [1, 2, 3, 4, 5] [6, 2, 3, 4, 5] ","date":"2021-10-18","objectID":"/wtf/:5:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"执行时机差异 array = [1, 8, 15] g = (x for x in array if array.count(x) \u003e 0) ##这时候x为[1,8,15]的解包 ##而后面的array变成了下面的 array = [2, 8, 22] print(list(g)) [8] 在生成器表达式中, in 子句在声明时执行, 而条件子句则是在运行时执行. 所以在运行前, array 已经被重新赋值为 [2, 8, 22], 因此对于之前的 1, 8 和 15, 只有 count(8) 的结果是大于 0 的, 所以生成器只会生成 8. array_1 = [1,2,3,4] g1 = (x for x in array_1) array_1 = [1,2,3,4,5] array_2 = [1,2,3,4] g2 = (x for x in array_2) array_2[:] = [1,2,3,4,5] print(list(g1)) print(list(g2)) [1, 2, 3, 4] [1, 2, 3, 4, 5] 第二部分中 g1 和 g2 的输出差异则是由于变量 array_1 和 array_2 被重新赋值的方式导致的. 在第一种情况下, array_1 被绑定到新对象 [1,2,3,4,5], 因为 in 子句是在声明时被执行的， 所以它仍然引用旧对象 [1,2,3,4]. 在第二种情况下, 对 array_2 的切片赋值将相同的旧对象 [1,2,3,4] 原地更新为 [1,2,3,4,5]. 因此 g2 和 array_2 仍然引用同一个对象(这个对象现在已经更新为 [1,2,3,4,5]). ","date":"2021-10-18","objectID":"/wtf/:6:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"出人意料的is a = 256 b = 256 a is b True a = 257 b = 257 ##256 是一个已经存在的对象, 而 257 不是 ##当你启动Python 的时候, -5 到 256 的数值就已经被分配好了. ##这些数字因为经常使用所以适合被提前准备好 a is b False a,b = 257,257 ##当 a 和 b 在同一行中使用相同的值初始化时，会指向同一个对象. print(a is b) print(id(a),id(b)) True 1957717387056 1957717387056 [] == [] True [] is [] ##两个空列表位于不同的内存地址 False ","date":"2021-10-18","objectID":"/wtf/:7:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"一蹴即至! row = [\"\"] * 3 board = [row] * 3 board [['', '', ''], ['', '', ''], ['', '', '']] board[0][0] = 'X' board ##这是因为之前对row做乘法导致的 [['X', '', ''], ['X', '', ''], ['X', '', '']] ##如何避免这种情况？ board = [['']*3 for _ in range(3)] board[0][0] = 'X' board [['X', '', ''], ['', '', ''], ['', '', '']] ","date":"2021-10-18","objectID":"/wtf/:8:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"麻烦的输出 funcs = [] res = [] for x in range(7): def func(): return x funcs.append(func) res.append(func()) func_res = [func() for func in funcs] print(func_res,res) [6, 6, 6, 6, 6, 6, 6] [0, 1, 2, 3, 4, 5, 6] power_x = [lambda x:x**i for i in range(11)] print([func(2) for func in power_x]) [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024] 在循环内部定义一个函数时, 如果该函数在其主体中使用了循环变量, 则闭包函数将与循环变量绑定, 而不是它的值. 因此, 所有的函数都是使用最后分配给变量的值来进行计算的. ","date":"2021-10-18","objectID":"/wtf/:9:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"连Python也知道爱是难言的 import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! love = this this is love True love is True False love is False False love is not True or False True love is not True or False;love is love True ","date":"2021-10-18","objectID":"/wtf/:10:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"三个引号 print('wtfpython''') wtfpython print(\"wtf\" \"python\") wtfpython ","date":"2021-10-18","objectID":"/wtf/:11:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"布尔你咋了? mixed_list = [False, 1.0, \"some_string\", 3, True, [], False] integers_found_so_far = 0 booleans_found_so_far = 0 for item in mixed_list: if isinstance(item, int): integers_found_so_far += 1 elif isinstance(item, bool): booleans_found_so_far += 1 integers_found_so_far 4 booleans_found_so_far 0 another_dict = {} another_dict[True] = \"JavaScript\" another_dict[1] = \"Ruby\" another_dict[1.0] = \"Python\" another_dict[True] 'Python' 布尔值是 int 的子类 some_iterable = ('a', 'b') def some_func(val): return \"something\" [x for x in some_iterable] ['a', 'b'] [(yield x) for x in some_iterable] \u003cgenerator object \u003clistcomp\u003e at 0x000001CC6FFC3888\u003e list([(yield x) for x in some_iterable]) ['a', 'b'] list(((yield x) for x in some_iterable)) ['a', None, 'b', None] list(some_func((yield x)) for x in some_iterable) ['a', 'something', 'b', 'something'] ","date":"2021-10-18","objectID":"/wtf/:12:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"消失的外部变量 e = 7 try: raise Exception() except Exception as e: pass print(e) ##error! ","date":"2021-10-18","objectID":"/wtf/:13:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"从有到无 some_list = [1, 2, 3] some_dict = { \"key_1\": 1, \"key_2\": 2, \"key_3\": 3 } some_list = some_list.append(4) some_dict = some_dict.update({\"key_4\": 4}) some_list some_dict 大多数修改序列/映射对象的方法, 比如 list.append, dict.update, list.sort 等等. 都是原地修改对象并返回 None. 这样做的理由是, 如果操作可以原地完成, 就可以避免创建对象的副本来提高性能. ","date":"2021-10-18","objectID":"/wtf/:14:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"迭代列表时删除元素 list_1 = [1, 2, 3, 4] list_2 = [1, 2, 3, 4] list_3 = [1, 2, 3, 4] list_4 = [1, 2, 3, 4] for idx, item in enumerate(list_1): del item for idx, item in enumerate(list_2): list_2.remove(item) for idx, item in enumerate(list_3[:]): list_3.remove(item) for idx, item in enumerate(list_4): list_4.pop(idx) list_1 ##没有修改list_1 [1, 2, 3, 4] list_2 ##每一次删除元素后 迭代的list_2也发生改变 比如第一次删除了1 list_2为[2,3,4]这时idx=1 所以下一个删除了3 [2, 4] list_3 ##迭代副本不会出现上述情况 [] list_4 [2, 4] ","date":"2021-10-18","objectID":"/wtf/:15:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"循环变量泄露 for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') 6 : for x inside loop 6 : x in global ## 这次我们先初始化x x = -1 for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') 6 : for x inside loop 6 : x in global x = 1 print([x for x in range(5)]) print(x, ': x in global') [0, 1, 2, 3, 4] 1 : x in global ","date":"2021-10-18","objectID":"/wtf/:16:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"当心默认的可变参数 def some_func(default_arg=[]): default_arg.append(\"some_string\") return default_arg some_func() ['some_string'] some_func() ['some_string', 'some_string'] some_func([]) ['some_string'] some_func() ['some_string', 'some_string', 'some_string'] Python中函数的默认可变参数并不是每次调用该函数时都会被初始化. 相反, 它们会使用最近分配的值作为默认值. 当我们明确的将 [] 作为参数传递给 some_func 的时候, 就不会使用 default_arg 的默认值, 所以函数会返回我们所期望的结果. some_func.__defaults__ (['some_string', 'some_string', 'some_string'],) 避免可变参数导致的错误的常见做法是将 None 指定为参数的默认值, 然后检查是否有值传给对应的参数. 例: def some_func(default_arg=None): if not default_arg: default_arg = [] default_arg.append(\"some_string\") return default_arg ","date":"2021-10-18","objectID":"/wtf/:17:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"同人不同命 a = [1, 2, 3, 4] b = a a = a + [5, 6, 7, 8] a [1, 2, 3, 4, 5, 6, 7, 8] b [1, 2, 3, 4] a = [1, 2, 3, 4] b = a a += [5, 6, 7, 8] a [1, 2, 3, 4, 5, 6, 7, 8] b [1, 2, 3, 4, 5, 6, 7, 8] a += b 并不总是与 a = a + b 表现相同. 类实现 op= 运算符的方式 也许 是不同的, 列表就是这样做的. 表达式 a = a + [5,6,7,8] 会生成一个新列表, 并让 a 引用这个新列表, 同时保持 b 不变. 表达式 a += [5,6,7,8] 实际上是使用的是 “extend” 函数, 所以 a 和 b 仍然指向已被修改的同一列表. a_var = 'global variable' def a_func(): print(a_var, '[ a_var inside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') global variable [ a_var inside a_func() ] global variable [ a_var outside a_func() ] a_var = 'global value' def a_func(): a_var = 'local value' print(a_var, '[ a_var inside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') local value [ a_var inside a_func() ] global value [ a_var outside a_func() ] a_var = 'global value' def a_func(): global a_var a_var = 'local value' print(a_var, '[ a_var inside a_func() ]') print(a_var, '[ a_var outside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') global value [ a_var outside a_func() ] local value [ a_var inside a_func() ] local value [ a_var outside a_func() ] a_var = 'global value' def outer(): a_var = 'enclosed value' def inner(): a_var = 'local value' print(a_var) inner() outer() local value a_var = 'global variable' def len(in_var): print('called my len() function') l = 0 for i in in_var: l += 1 return l def a_func(in_var): len_in_var = len(in_var) print('Input variable is of length', len_in_var) a_func('Hello, World!') called my len() function Input variable is of length 13 a = 'global' def outer(): def len(in_var): print('called my len() function: ', end=\"\") l = 0 for i in in_var: l += 1 return l a = 'local' def inner(): global len nonlocal a a += ' variable' inner() print('a is', a) print(len(a)) outer() print(len(a)) print('a is', a) a is local variable called my len() function: 14 called my len() function 6 a is global ","date":"2021-10-18","objectID":"/wtf/:18:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"大海捞针 x, y = (0, 1) if True else None, None x,y ((0, 1), None) ##正确做法 x,y = (0,1) if True else (None,None) x,y (0, 1) t = ('one', 'two') for i in t: print(i) t = ('one') for i in t: print(i) t = () print(t) one two o n e () ##明显上面的把t = ('one') t当成字符串了，正确做法如下 t = ('one',) ##注意逗号 for i in t: print(i) one ","date":"2021-10-18","objectID":"/wtf/:19:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["NLP"],"content":"共现矩阵 主要用于发现主题，解决词向量相近关系的表示 例如，语料库如下： - I like deep learning. - I like NLP. - I enjoy flying. 则共现矩阵如下(窗口大小为1) 例如：“I like”出现在第1，2句话中，一共出现2次，所以=2。 对称的窗口指的是，“like I”也是2次 将共现矩阵行(列)作为词向量表示后，可以知道like，enjoy都是在I附近且统计数目大约相等，他们意思相近。 ","date":"2021-10-15","objectID":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/:0:0","tags":["NLP","共现矩阵"],"title":"共现矩阵","uri":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/"},{"categories":["NLP"],"content":"代码 import numpy as np word2ind = {w:i for i,w in enumerate(words)} # word到key，words就是词汇表 M = np.zeros((num_words, num_words)) # num_words是词汇表的长度 for c in corpus: # 假设语料库是一个列表，元素为一段文本。遍历语料库 for idx, word in enumerate(c): # 遍历文本的每一个词，这里默认空格分词 for i in range(1, window_size+1): # 对窗口大 小进行遍历 left = idx - i # 自己与自己不算共现，所以这里要加减 right = idx + i if left \u003e= 0: # 左边元素 M[word2ind[word], word2ind[c[left]]] += 1 if right \u003c len(c): # 右边元素 M[word2ind[word], word2ind[c[right]]] += 1 ","date":"2021-10-15","objectID":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/:1:0","tags":["NLP","共现矩阵"],"title":"共现矩阵","uri":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/"},{"categories":["python"],"content":"两个数的交换 # a = 1 # b = 2 # temp = b # b = a # a = temp # print(a,b) a = 1 b = 2 a,b = b,a print(a,b) 2 1 ","date":"2021-10-10","objectID":"/skill/:1:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"格式化字符串 a = 17 name = \"wlb\" # print('%s is %d years old' % (name,a)) # print('{} is {} years old'.format(name,a)) print(f'{name} is {a} years old') #明显这个方法更简单 wlb is 17 years old ","date":"2021-10-10","objectID":"/skill/:2:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"yield与yield from def fib(n): a = 0 b = 1 for _ in range(n): yield a a,b = b,a+b for i in fib(10): print(i) #注释的内容与yield a效果相同，yield相当于使其成为一个迭代器 yield一个数后会立马传递出去，而return 要等列表都生成完毕后才会传出去 #他的优势在于一些耗时的操作 # 通过yield来进行dfs，由于没有实现__next__因此是个可迭代对象而不是一个迭代器 class Node: def __init__(self,value) -\u003e None: self._value = value self._node = [] def __repr__(self) -\u003e str: return f'Node({self._value})' def add_children(self,node:'Node') -\u003e 'Node': self._node.append(node) def __iter__(self): return iter(self._node) def dfs(self): yield self for i in self: yield from i.dfs() root = Node(0) children1 = Node(1) children2 = Node(2) root.add_children(children1) root.add_children(children2) children1.add_children(Node(3)) children1.add_children(Node(4)) children11 = Node(5) children2.add_children(children11) children11.add_children(Node(6)) for c in root.dfs(): print(c) from typing import Iterable def test_format(datas: Iterable[str], max_len: int): for data in datas: if len(data) \u003e max_len: yield data[:max_len] + '...' else: yield data print(list(test_format(['vllbc', 'test_for_this_function', 'good'],5))) # 把长度大于5的部分变成省略号 #子生成器 def average_gen(): total = 0 count = 0 average = 0 while True: new_num = yield average if new_num is None: break count += 1 total += new_num average = total/count return total,count,average # 委托生成器 def proxy_gen(): while True: total,count,average = yield from average_gen() # yield from后面是一个可迭代对象,此文后面的将多维数组转化为一维数组中flatten函数就用到了yield from，原理就是如果列表中一个元素是列表就yield from这个列表，否则就直接yield这个元素，也利用了递归的方法。如果子生成器退出while循环了，就执行return以获取返回值。 print(total,count,average) def main(): t = proxy_gen() next(t) print(t.send(10)) print(t.send(15)) print(t.send(20)) t.send(None) main() ","date":"2021-10-10","objectID":"/skill/:3:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"列表解析式 lists = [f\"http://www.baidu.com/page{n}\" for n in range(21)] lists#此方法在爬虫构造urls中非常常用 # lists = [f\"http://www.baidu.com/page{n}\" for n in range(21) if n%2==0] page偶数 # alp = \"abcdefghigklmnopqrstuvwxyz\" # ALP = [n.upper() for n in alp] 将小写转换为大写 ","date":"2021-10-10","objectID":"/skill/:4:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"enumerate lists = ['apple','banana','cat','dog'] for index,name in enumerate(lists): print(index,name) # 手动实现一下enumerate from typing import Iterable def enumerate_(Iterable:Iterable,start=0): yield from zip(range(start,start+len(Iterable)),Iterable) for i,item in enumerate_([1,2,3,4,5,6],9): print(i,item) ","date":"2021-10-10","objectID":"/skill/:5:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"字典的合并 dic1 = {'qq':1683070754, 'phone':123456789 } dic2 = { 'height':180, 'handsome':True } dic3 = {**dic1,**dic2} #合并两个字典 **叫做解包 #或者用dic1.update(dic2) 将dic2合并到dic1 相同键则dic2替代dic1 dic3 {'handsome': True, 'height': 180, 'phone': 123456789, 'qq': 1683070754} ","date":"2021-10-10","objectID":"/skill/:6:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"序列解包 name = \"wang lingbo\" xing,ming = name.split(\" \") #split返回一个序列，分别赋给xing 和ming print(xing,ming) #x,*y,z = [1,2,3,4,5] #x:1 z:5 y:[2,3,4] wang lingbo ","date":"2021-10-10","objectID":"/skill/:7:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"匿名函数lambda lists = [1,2,3,4,5,6] maps = map(lambda x:x*x,lists) print(maps) print(list(maps)) \u003cmap object at 0x000001911C8E03C8\u003e [1, 4, 9, 16, 25, 36] ","date":"2021-10-10","objectID":"/skill/:8:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"装饰器 def logging(level): def wapper(func): def inner_wapper(*args, **wbargs): print(f'{level} enter in {func.__name__}()') return func(*args, **wbargs) #不写return 也可以 return inner_wapper return wapper @logging('inner') def say(a): print('hello! {}'.format(a)) say('wlb') inner enter in say() hello! wlb import time def print_time(func): def wapper(*args,**wbargs): print(f'{func.__name__}()调用于{time.asctime(time.localtime(time.time()))}') return func(*args,**wbargs) #不写return 也可以 return wapper @print_time def my_name(name): print(f'look!{name}') my_name(\"wlb\") my_name()调用于Wed Dec 9 21:21:00 2020 look!wlb ","date":"2021-10-10","objectID":"/skill/:9:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"map、reduce、filter # map print(list(map(abs,[-1,-2,-3,-4,-5]))) #也可以自己定义函数或者用匿名函数 # reduce from functools import reduce #python3中需要从内置库导入 print(reduce(lambda x,y:x+y,list(map(int,str(131351412))))) # filter a = [1,2,3,4,5,6,7,8,9] new_a = filter(lambda x:x%2!=0,a) #filter就是筛选 list(new_a) # 这三个都是函数式编程中常用的函数 ","date":"2021-10-10","objectID":"/skill/:10:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"join() # lists = ['1','2','3','4','5'] # ''.join(lists) lists = [1,2,3,4,5] ''.join(list(map(str,lists))) #join只能是字符串列表，所以要map转换一下 '12345' ","date":"2021-10-10","objectID":"/skill/:11:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"将多维数组转换为一维 ab = [[1, 2, 3], [5, 8], [7, 8, 9]] print([i for item in ab for i in item]) #利用列表解析式 print(sum(ab, [])) # 利用sum函数 from functools import reduce print(reduce(lambda x,y:x+y,ab)) # 利用reduce from itertools import chain print(list(chain(*ab))) # 利用chain def flatten(items,ignore=(str,bytes)): for x in items: if isinstance(x,Iterable) and not isinstance(x,ignore): yield from flatten(x) else: yield x print(list(flatten(ab))) # 利用自己定义的函数 [1, 2, 3, 5, 8, 7, 8, 9] ","date":"2021-10-10","objectID":"/skill/:12:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"将一个列表倒序 lists = [2,4,3,2,5,4] lists[::-1] # list(reversed(lists)) ","date":"2021-10-10","objectID":"/skill/:13:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"随机生成密码 import random b = 8 t = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890' print(''.join(random.sample(t,b))) # 主要就是用sample这个方法来取多个随机值 0KmtEZSU ","date":"2021-10-10","objectID":"/skill/:14:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"断言 assert(True is True) #成功 print('yes') assert(True is False) #报错 print('no') yes ","date":"2021-10-10","objectID":"/skill/:15:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"合并列表 list1 = [1,2,31,13] list2 = [5,2,12,32] # list1.append(list2) # print(list1) #错误方法 list1.extend(list2) print(list1) #正确方法 [1, 2, 31, 13, 5, 2, 12, 32] a = [1,2,3,4,5] b = ['a','b','c','d','e'] fin = dict() for k,i in zip(a,b): fin[k] = i print(fin) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} # 或者 d = {} for i,d[i] in zip(a,b): pass print(d) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} 为什么？在WTFpython中有讲 # 或者 fin = dict(zip(a,b)) print(fin) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} ","date":"2021-10-10","objectID":"/skill/:16:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"对list进行解包 lists = ['dog','cat','you'] print(*lists) #想对一个列表进行zip操作时，可以这样 print(list(zip(*lists))) def test(*args): print(\"args:\",args) test(*lists) dog cat you [('d', 'c', 'y'), ('o', 'a', 'o'), ('g', 't', 'u')] args:('dog','cat','you') ","date":"2021-10-10","objectID":"/skill/:17:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"对类的一些操作 class Test: x = 1 y = 2 print(Test.x,Test.y) #==\u003eprint(Test().x,Test().y) class Test: def __init__(self,x,y): self.x = x self.y = y test = Test(1,2) print(test.x,test.y) 1 2 1 2 class Test: def __init__(self,maxlen): self.maxlen = maxlen self.lists = [] def put(self,*args): for i in args: if len(self.lists) \u003c= self.maxlen: self.lists.append(i) else: break def get(self): return self.lists.pop() def empty(self): if len(self.lists) != 0: return False else: return True def __len__(self): return len(self.lists) def __del__(self): print(\"del this class\") def printfs(self): return self.lists test = Test(10) test.put(1,2,3,4,5,6) print(test.empty()) print(len(test)) print(test.printfs()) test.__del__() #直接调用test还存在，__del__是析构函数，垃圾回收时就会调用a print(test) #del test #print(test) 这时候就会报错，因为del将test这个对象直接删除了 False 6 [1, 2, 3, 4, 5, 6] del this class \u003c__main__.Test object at 0x0000021B7DF33EB0\u003e del this class ","date":"2021-10-10","objectID":"/skill/:18:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"一些内置函数 all([True,True,False]) #False all([True,True,True]) #True any([True,True,False]) #True any([True,False,False])#True any([False,False]) #False import random for i in iter(lambda:random.randint(1,10),5): print(i) #相当于 while True: x = random.randint(1,10) print(x) if x == 5: break iter(object[, sentinel]) sentinel为可选参数，若不传入，则object必须为可迭代对象，传入则必须为可调用对象,当可调用对象的返回值为sentinel抛出异常，但for循环会处理这个异常，这常常用于IO操作 ​ #这是cookbook里面的一个例子 import sys f = open('xxx/xxx.txt') for chunk in iter(lambda:f.read(10),''): n = sys.stdout.write(chunk) #深入理解一下 import random class Test: def __init__(self): self.lists = [1,23,2,4,1,421,412] def __call__(self): return random.choice(self.lists) for i in iter(Test(),1): print(i) #这是可以正常输出的，因为实例化Test后是个可调用对象，返回列表的随机值，当返回1时则循环结束，如果把__call__魔法方法去了后，则会报错，如果想要不使用魔法方法的话可以用匿名函数 import random class Test: def __init__(self): self.lists = [1,23,2,4,1,421,412] # def __call__(self): # return random.choice(self.lists) for i in iter(lambda:random.choice(Test().lists),1): print(i) #总之，吹爆cookbook ","date":"2021-10-10","objectID":"/skill/:19:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"functools.partial #先看演示 from functools import partial def add(a,b): return a + b addOne = partial(add,1) addOne(2) #3 addOne(4) #5 #大概意思就是利用partial将函数的一个参数固定住了 def partial(func,*wargs): def wapper(*kargs): args = list(wargs) print(f\"args:{args}\") print(f\"kargs:{kargs}\") args.extend(kargs) print(f\"last:{args}\") return func(*args) return wapper def add(a,b,c): return a + b + c addone = partial(add,1,2) #此时addone相当于wapper print(addone(3)) #调用wrapper 3为传入的kargs #输出： args:[1, 2] kargs:(3,) last:[1, 2, 3] 6 #上面是partial函数的简化版本 #很明显的闭包操作，很容易就可以理解 #当然也可以转换为装饰器操作 from functools import wraps from functools import wraps,partial def out_wapper(*wargs): def partialout(func): return partial(func,*wargs) # 这是使用partial原理的 # @wraps(func) # def wrapper(*kargs): # args = list(wargs) # print(f\"args:{args}\") # print(f\"kargs:{kargs}\") # args.extend(kargs) # print(f\"last:{args}\") # return func(*args) # return wrapper return partialout @out_wapper(1,2) def add(a,b,c): return a + b + c print(add(3)) #6 #明显装饰器要麻烦一点实现，不过毕竟是封装好的函数，以后直接用就可以，不过了解这些有助于提高思维水平 ","date":"2021-10-10","objectID":"/skill/:20:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"@classmethod和@staticmethod class A(object): bar = 1 def func1(self): print ('foo') @classmethod def func2(cls): print ('func2') print (cls.bar) cls().func1() # 调用 foo 方法 A.func2() # 不需要实例化 func2 1 foo class A(object): # 属性默认为类属性（可以给直接被类本身调用） num = \"类属性\" # 实例化方法（必须实例化类之后才能被调用） def func1(self): # self : 表示实例化类后的地址id print(\"func1\") print(self) # 类方法（不需要实例化类就可以被类本身调用） @classmethod def func2(cls): # cls : 表示没用被实例化的类本身 print(\"func2\") print(cls) print(cls.num) cls().func1() # 不传递传递默认self参数的方法（该方法也是可以直接被类调用的，但是这样做不标准） def func3(): print(\"func3\") print(A.num) # 属性是可以直接用类本身调用的 # A.func1() 这样调用是会报错：因为func1()调用时需要默认传递实例化类后的地址id参数，如果不实例化类是无法调用的 A.func2() A.func3() class A(object): def foo(self, x): print(\"executing foo(%s,%s)\" % (self, x)) print('self:', self) @staticmethod def static_foo(x): print(\"executing static_foo(%s)\" % x) 问题：@staticmethod修饰的方法函数与普通的类外函数，为什么不直接使用普通函数？ @staticmethod是把函数嵌入到类中的一种方式，函数就属于类，同时表明函数不需要访问这个类。通过子类的继承覆盖，能更好的组织代码。 from pydantic import BaseModel from typing import Sequence class Test(BaseModel): text: Sequence[str] @classmethod def create(cls,text: Sequence[str]) -\u003e \"Test\": # classmethod常用构造函数 return cls(text=text) def to_tuple(self) -\u003e \"Test\": return Test(text=tuple(self.text)) @classmethod def join(cls, *Tests): return cls.create(sum([i.text for i in Tests],[])) test = Test.create(list(\"Hello world\")) t2 = Test.create(list(\"NIHAO\")) print(Test.join(test, t2)) # text=['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', 'N', 'I', 'H', 'A', 'O'] ","date":"2021-10-10","objectID":"/skill/:21:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"用类实现装饰器 #先看这样的代码 类实现装饰器要求类必须是可调用的 import time import functools class DelayFunc: def __init__(self, duration, func): self.duration = duration self.func = func def __call__(self, *args, **kwargs): print(f'Wait for {self.duration} seconds...') time.sleep(self.duration) return self.func(*args, **kwargs) def eager_call(self, *args, **kwargs): print('Call without delay') return self.func(*args, **kwargs) def delay(duration): \"\"\"装饰器：推迟某个函数的执行。同时提供 .eager_call 方法立即执行 \"\"\" # 此处为了避免定义额外函数，直接使用 functools.partial 帮助构造 # DelayFunc 实例 return functools.partial(DelayFunc, duration) @delay(2) def add(a,b): print(a,b) add(1,2) #延迟两秒输出3 相当于delay(2)(add)(1,2) add.eager_call(1,2) #不延迟输出3 相当于delay(2)(add).eager_call(1,2) #额，当然，想更深入理解的话，也可以这么写 def delay(duration): def partial(func): return DelayFunc(duration,func) return partial # 上面的就相当于 partial(DelayFunc,duration),缺的func参数就是要修饰的函数 @delay(2) def add(a,b): return a + b print(add(1,2)) 与纯函数相比，我觉得使用类实现的装饰器在特定场景下有几个优势： 实现有状态的装饰器时，操作类属性比操作闭包内变量更符合直觉、不易出错 实现为函数扩充接口的装饰器时，使用类包装函数，比直接为函数对象追加属性更易于维护 更容易实现一个同时兼容装饰器与上下文管理器协议的对象 ","date":"2021-10-10","objectID":"/skill/:22:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"BaseModel from pydantic import BaseModel,AnyUrl class Test(BaseModel): # 继承后可以用类属性创建实例 url: AnyUrl data: str def __str__(self): return self.url + self.data kwargs = { 'url': 'https://www.baidu.com', 'data': '/search' } print(Test(**kwargs)) ","date":"2021-10-10","objectID":"/skill/:23:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"python类型注释 from pydantic import BaseModel from typing import Any class cout(): def __init__(self, cls: \"Test\", text: str) -\u003e None: self.cls = cls self.text = text def __str__(self): return f\"{self.cls} {self.text}\" #程序到cout时 Test类并没有定义，但最后Test在变量空间中，所以加上引号 class Test(BaseModel): def __str__(self) -\u003e str: return \"I am Test Class\" print(cout(cls=Test(), text=\"hello world!\")) ","date":"2021-10-10","objectID":"/skill/:24:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"namedtuple from collections import namedtuple Test = namedtuple(\"Test\", ['name', 'age', 'sex']) def test_for_test(name: str, year: int, sex: str) -\u003e \"Test\": return Test( name=name.title(), age=2021 - year, sex=sex ) name,age,sex = test_for_test('wlb', 2002, 'male') print(name, age, sex) ","date":"2021-10-10","objectID":"/skill/:25:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"@property from pydantic import BaseModel class Test(): def __init__(self, cls, n): self.cls = cls self.n = n @property def to_string_cls(self): return self.cls @property def to_strings(self): return self.n class Test_For(BaseModel): num: int def __str__(self): return str(self.num) __repr__ = __str__ test = Test(Test_For, 22) print(test.to_string_cls(num=1)) # 1 print(test.to_strings) # 22 ","date":"2021-10-10","objectID":"/skill/:26:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"在边界处思考 from typing import Iterable from pydantic import BaseModel,conint,ValidationError class NumberInput(BaseModel): num: conint(ge=0, le=100) def input_a_number(): while True: n = input(\"输入一个数\") try: n = NumberInput(num=n) except ValidationError as e: print(e) continue n = n.num break return n print(input_a_number()) #要求输入一个0-100的数 这样是不是很优雅 ","date":"2021-10-10","objectID":"/skill/:27:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"super()进阶 今天学习cookbook8-8子类中扩展property 先贴一下代码 class Person: def __init__(self, name): self.name = name # 有意思的是 这里的self.name是@property修饰的 这行代码调用name.setter # Getter function @property def name(self): return self._name # Setter function @name.setter def name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._name = value # Deleter function @name.deleter def name(self): raise AttributeError(\"Can't delete attribute\") # 子类 class SubPerson(Person): @property def name(self): print('Getting name') return super().name @name.setter def name(self, value): print('Setting name to', value) super(SubPerson, SubPerson).name.__set__(self, value) @name.deleter def name(self): print('Deleting name') super(SubPerson, SubPerson).name.__delete__(self) 看到super(SubPerson, SubPerson)感到很疑惑，于是搜索资料大致搞明白了 通俗说默认的super(SubPerson,self) (直接写super()也可) 返回的是一个类的实例 \u003e 为了委托给之前定义的setter方法，需要将控制权传递给之前定义的name属性的 __set__() 方法。 不过，获取这个方法的唯一途径是使用类变量而不是实例变量来访问它。 这也是为什么我们要使用 super(SubPerson, SubPerson) 的原因。 从书中这句话可以看出 super(cls,cls)返回的是一个类 不是一个实例，super()的参数的作用就是用于定位位置 第一个cls必须是第二cls的父类或者二者相同，可以通过cls.__mro__查看继承顺序 比如在D里面super(A,D).__init__(self) 而__mro__ 为 (\u003cclass '__main__.D'\u003e, \u003cclass '__main__.A'\u003e, \u003cclass '__main__.B'\u003e, \u003cclass '__main__.C'\u003e, \u003cclass '__main__.Base'\u003e, \u003cclass 'object'\u003e) 那么就调用从A以后的类的__init__() 不过重点不在这里，重点是super(cls,cls)和super(cls,object)的区别 使用super(cls,cls)必须显示的传入self参数，即super(cls,cls).func(self,…)。总之其就是一个定位方法，别的作用我暂且不知。 ### 一个示例 class A: def say(self): print(\"I am A\") class B(A): def say(self): # super(B, B).say(self) super().say() class C(A): def say(self): print(\"I am C\") class D(B, C): def say(self): super().say() D().say() 上述的这段代码怎么修改D输出都是I am C，这是因为在B中super().say()相当于super(B,self).say()，而根据上述内容这是一个实例方法，其中self是D的实例，查看D的mro可以知道C在B的后面，所以根据super的作用，则会调用继承关系中B后面类的say方法，即C的say 方法，所以会得到匪夷所思的结果，将代码修改成注释的那样就可以解决这个问题，希望可以帮助理解。 ## dataclass from dataclasses import dataclass import random @dataclass(order=True) # 等于实现了各种比较方法例如=、\u003e、\u003c,排序函数都依赖比较两个对象 class A: n: int nums = [A(random.randint(1,10)) for _ in range(10)] nums = sorted(nums) print(nums, end='') x = '''hello''' print(x) dataclass可以自动添加__rapr__方法，不必自己实现 @dataclass(init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False) init：默认将生成 __init__ 方法。如果传入 False，那么该类将不会有 __init__ 方法。 repr：__repr__ 方法默认生成。如果传入 False，那么该类将不会有 __repr__ 方法。 eq：默认将生成 __eq__ 方法。如果传入 False，那么 __eq__ 方法将不会被 dataclass 添加，但默认为 object.__eq__。 order：默认将生成 __gt__、__ge__、__lt__、__le__ 方法。如果传入 False，则省略它们。 unsafe_hash：默认生成__hash__方法，用于构建可hashable的类 from dataclasses import dataclass @dataclass(unsafe_hash=True) class VisitRecordDC: first_name: str last_name: str phone_number: str # 跳过“访问时间”字段，不作为任何对比条件 date_visited: str = field(hash=False, compare=False) def find_potential_customers_v4(): return set(VisitRecordDC(**r) for r in users_visited_phuket) - \\ #求差集 set(VisitRecordDC(**r) for r in users_visited_nz) ","date":"2021-10-10","objectID":"/skill/:28:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"自定义format class Student: def __init__(self, name, age): self.name = name self.age = age def __format__(self, format_spec): if format_spec == 'long': return f'{self.name} is {self.age} years old.' elif format_spec == 'simple': return f'{self.name}({self.age})' raise ValueError('invalid format spec') vllbc = Student('vllbc', '18') print(f'{vllbc:simple}') print(f'{vllbc:long}') ","date":"2021-10-10","objectID":"/skill/:29:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"抽象类实践 import collections from abc import ABC, abstractmethod from typing import List customer = collections.namedtuple('customer', ['name', 'points']) class Goods(): def __init__(self, name: str, quantity: float, price: float) -\u003e None: self.name = name self.quantity = quantity self.price = price def total(self) -\u003e float: return self.quantity * self.price class Order(): def __init__(self, customer: customer, cart: List[Goods], prom=None) -\u003e None: self.customer = customer self.cart = cart self.prom = prom def total(self): if not hasattr(self, '__total'): self.__total = sum(i.total() for i in self.cart) return self.__total def due(self): if self.prom is None: discount = 0 else: discount = self.prom.discount(self) return self.total() - discount def __repr__(self) -\u003e str: return f'\u003cOrder total: {self.total():.2f} due: {self.due():.2f}\u003e' class Prom(ABC): # 抽象类 @abstractmethod def discount(self,order) -\u003e float: '''discount''' class discount1(Prom): def discount(self,order) -\u003e float: return order.total() * 0.05 if order.customer.points \u003e= 10000 else 0 john = customer(name='vllbc', points=100000) carts = [Goods(name='apple', quantity=5, price=10), Goods( name='banana', quantity=8, price=5), Goods(name='peach', quantity=4, price=8)] order = Order(customer=john, cart=carts,prom=discount1()) ","date":"2021-10-10","objectID":"/skill/:30:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"accumulate import itertools test_list = [i for i in range(1, 11)] for i in itertools.accumulate(test_list): print(i, end=\",\") # 1,3,6,10,15,21,28,36,45,55, print() for i in itertools.accumulate(test_list, lambda x, y: x * y): print(i, end=',') # 1,2,6,24,120,720,5040,40320,362880,3628800, ","date":"2021-10-10","objectID":"/skill/:31:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"异步装饰器 from functools import wraps import asyncio def decorator(func): @wraps(func) async def hello(*args, **kwargs): await asyncio.sleep(2) return await func(*args,**kwargs) return hello @decorator async def test(): print(\"hello\") asyncio.run(test()) ","date":"2021-10-10","objectID":"/skill/:32:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"bisect import bisect import time # BREAKPOINTS 必须是已经排好序的，不然无法进行二分查找 BREAKPOINTS = (1, 60, 3600, 3600 * 24) TMPLS = ( # unit, template (1, \"less than 1 second ago\"), (1, \"{units} seconds ago\"), (60, \"{units} minutes ago\"), (3600, \"{units} hours ago\"), (3600 * 24, \"{units} days ago\"), ) def from_now(ts): \"\"\"接收一个过去的时间戳，返回距离当前时间的相对时间文字描述 \"\"\" seconds_delta = int(time.time() - ts) unit, tmpl = TMPLS[bisect.bisect(BREAKPOINTS, seconds_delta)] # bisect类似于index方法，要是不存在会选择数值最接近的索引 return tmpl.format(units=seconds_delta // unit) ","date":"2021-10-10","objectID":"/skill/:33:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"contextlib from contextlib import contextmanager,ContextDecorator # contextmanager可以把一个函数变成一个上下文管理器，不需要自己去实现一个定义了__enter__和__exit__方法的类 @contextmanager def open_file(filename, methods=\"r\"): print(f\"打开了文件{filename}\") res_file = open(filename, mode=methods) # __enter__方法 这里也可以是自己定义的类 try: yield res_file # 相当于在__enter__方法里面返回self yield后面为空的话就不用as了 except Exception as e: print(\"有错误发生\", e) # __exit__方法里的错误处理 finally: res_file.close() # __exit__ with open_file(\"testvim.txt\") as fp: print(fp) ","date":"2021-10-10","objectID":"/skill/:34:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"读取大文件 from functools import partial def digits_(file,block_size=1024*8): # 分块读取 _read = partial(file.read, block_size) # 使用partial,也可以使用lambda:file.read(block_size) for line in iter(_read, \"\"): # 当读取完毕时退出 for s in line: if s.isdigit(): yield s # 使用yield def count_digits(fname): \"\"\"计算文件里包含多少个数字字符\"\"\" count = 0 with open(fname) as file: for _ in digits_(file=file): count+=1 return count ","date":"2021-10-10","objectID":"/skill/:35:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__exit__、__enter__ # def __enter__(self): # 该方法将在进入上下文时调用 # return self # def __exit__(self, exc_type, exc_val, exc_tb): # 该方法将在退出上下文时调用 # exc_type, exc_val, exc_tb 分别表示该上下文内抛出的异常类型、异常值、错误栈 # __enter__()：主要执行一些环境准备工作，同时返回一资源对象。如果上下文管理器open(\"test.txt\")的__enter__()函数返回一个文件对象。 # __exit__()：完整形式为__exit__(type, value, traceback),这三个参数和调用sys.exec_info()函数返回值是一样的，分别为异常类型、异常信息和堆栈。如果*执行体语句*没有引发异常，则这三个参数均被设为None。否则，它们将包含上下文的异常信息。__exit__()方法返回True或False,分别指示被引发的异常有没有被处理，如果返回False，引发的异常将会被传递出上下文。如果__exit__()函数内部引发了异常，则会覆盖掉执行体的中引发的异常。处理异常时，不需要重新抛出异常，只需要返回False，with语句会检测__exit__()返回False来处理异常。 ","date":"2021-10-10","objectID":"/skill/:36:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"enum from enum import IntEnum class Test(IntEnum): X = 2 Y = 1 print(2 == Test.X) ","date":"2021-10-10","objectID":"/skill/:37:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"pydantic数据验证 from pydantic import BaseModel, conint, ValidationError # pydantic主要功能就是作数据验证 from typing import ( List, Union, Optional, Dict ) class Test(BaseModel): name: Optional[str] sex: Union[str, List[str]] d: Dict[str, int] id: conint(ge=1,le=10) try: test = Test(name='wlb', sex='male', d={'dict':1}, id=1) print(test.dict(), test.__annotations__) # {'name': 'wlb', 'sex': 'male', 'd': {'dict': 1}, 'id': 1} {'name': typing.Union[str, NoneType], 'sex': typing.Union[str, typing.List[str]], 'd': typing.Dict[str, int], 'id': \u003cclass '__main__.ConstrainedIntValue'\u003e} except ValidationError: print(\"数据错误\") ","date":"2021-10-10","objectID":"/skill/:38:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"islice from itertools import islice def test(): t = 0 while True: yield t t += 1 for i in islice(test(), 10, 21, 2): print(i) ","date":"2021-10-10","objectID":"/skill/:39:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__iter__、__next__ class Range7: # 可迭代类型 只需要实现__iter__即可 def __init__(self,start,end) -\u003e None: self.start = start self.end = end def __iter__(self): return Range7iterator(self) class Range7iterator: #这是迭代器,一般的迭代器只能调用一次 def __init__(self,rangeobj) -\u003e None: self.rangeobj = rangeobj self.cur = rangeobj.start def __iter__(self): return self def __next__(self): while True: if self.cur \u003e self.rangeobj.end: raise StopIteration if self.is_7(self.cur): res = self.cur self.cur += 1 return res self.cur += 1 def is_7(self,num): if num == 0: return False return num%7==0 or \"7\" in str(num) for i in Range7(1,100): print(i,end=\" \") #可迭代对象不一定是迭代器，但迭代器一定是可迭代对象 # 对可迭代对象使用 iter() 会返回迭代器，迭代器则会返回它自身 # 每个迭代器的被迭代过程是一次性的，可迭代对象则不一定 # 可迭代对象只需要实现 __iter__ 方法，而迭代器要额外实现 __next__ 方法 ","date":"2021-10-10","objectID":"/skill/:40:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"求字典的最大值 prices = { 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75 } print(max(zip(prices.values(),prices.keys()))) print(max(prices.items(),key=lambda x:x[1])) print(max(prices,key=lambda k:prices[k])) ","date":"2021-10-10","objectID":"/skill/:41:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"注意循环变量 funcs = [] res = [] for x in range(7): def func(x=x): # 去掉x=x则出现[6,6,6,6,6,6] 在循环内部定义一个函数时, 如果该函数在其主体中使用了循环变量, 则闭包函数将与循环变量绑定, 而不是它的值. 因此, 所有的函数都是使用最后分配给变量的值来进行计算的. return x funcs.append(func) res.append(func()) func_res = [f() for f in funcs] print(func_res) def create_mult(): res = [] for i in range(5): def func(x, i=i): # 去掉i=i则全输出8，原因和上面一样 return x * i res.append(func) return res for cr in create_mult(): print(cr(2)) ","date":"2021-10-10","objectID":"/skill/:42:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"空对象模式 修改前 import decimal class CreateAccountError(Exception): \"\"\"Unable to create a account error\"\"\" class Account: \"\"\"一个虚拟的银行账号\"\"\" def __init__(self, username, balance): self.username = username self.balance = balance @classmethod def from_string(cls, s): \"\"\"从字符串初始化一个账号\"\"\" try: username, balance = s.split() balance = decimal.Decimal(float(balance)) except ValueError: raise CreateAccountError('input must follow pattern \"{ACCOUNT_NAME} {BALANCE}\"') if balance \u003c 0: raise CreateAccountError('balance can not be negative') return cls(username=username, balance=balance) def caculate_total_balance(accounts_data): \"\"\"计算所有账号的总余额 \"\"\" result = 0 for account_string in accounts_data: try: user = Account.from_string(account_string) except CreateAccountError: pass else: result += user.balance return result accounts_data = [ 'piglei 96.5', 'cotton 21', 'invalid_data', 'roland $invalid_balance', 'alfred -3', ] print(caculate_total_balance(accounts_data)) ","date":"2021-10-10","objectID":"/skill/:43:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"空对象模式简介 额外定义一个对象来表示None ","date":"2021-10-10","objectID":"/skill/:43:1","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"好处 它可以加强系统的稳固性，能有有效地防止空指针报错对整个系统的影响，使系统更加稳定。 它能够实现对空对象情况的定制化的控制，能够掌握处理空对象的主动权。 它并不依靠Client来保证整个系统的稳定运行。 它通过isNone对==None的替换，显得更加优雅，更加易懂。 import decimal class Account: \"\"\"一个虚拟的银行账号\"\"\" def __init__(self, username, balance): self.username = username self.balance = balance @classmethod def from_string(cls, s): \"\"\"从字符串初始化一个账号\"\"\" try: username, balance = s.split() balance = decimal.Decimal(float(balance)) except ValueError: # raise CreateAccountError('input must follow pattern \"{ACCOUNT_NAME} {BALANCE}\"') return NullAccount() if balance \u003c 0: return NullAccount() return cls(username=username, balance=balance) def caculate_total_balance(accounts_data): \"\"\"计算所有账号的总余额 \"\"\" return sum(Account.from_string(s).balance for s in accounts_data) class NullAccount: # 要返回的空对象 username = \"\" # 当发生错误时username的值 balance = 0 # 当发生错误时balance的值 def re_Null(): return NotImplementedError accounts_data = [ 'piglei 96.5', 'cotton 21', 'invalid_data', 'roland $invalid_balance', 'alfred -3', ] print(caculate_total_balance(accounts_data)) ","date":"2021-10-10","objectID":"/skill/:43:2","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"pathlib from pathlib import Path # 把txt文件重命名为csv文件 def unify_ext_with_pathlib(path): for fpath in Path(path).glob(\"*.txt\"): fpath.rename(fpath.with_suffix(\".csv\")) print(Path(\".\") / \"test_pathlib.py\") # Path类型可以使用/运算符 print(Path(\"testvim.txt\").read_text()) # 直接读取文件内容 # .resolve() 取绝对路径 # with_name() 修改文件名 with_suffix()修改后缀名 # 把当前目录下的文件批量重命名 # import os # from pathlib import Path # p = Path(\".\") # for filepath in p.glob(\"test_*.py\"): # name = filepath.with_name(str(filepath).replace(\"test_\",\"\")) # filepath.rename(name) ","date":"2021-10-10","objectID":"/skill/:44:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"单元测试 def say_hello(name=None): if name: return f\"hello {name}\" return \"hello world\" import unittest from typing import List class sayhellotest(unittest.TestCase): def setUp(self,nums:List[int] = 0): return super().setUp() def tearDown(self): return super().tearDown() def test_sayhello(self): rv = say_hello() self.assertEqual(rv,\"hello world\") def test_to_name(self): rv = say_hello(\"wlb\") self.assertEqual(rv,\"hello wlb\") if __name__ == '__main__': unittest.main() ","date":"2021-10-10","objectID":"/skill/:45:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"takewhile和dropwhile from itertools import dropwhile,takewhile # 你想遍历一个可迭代对象，但是它开始的某些元素你并不感兴趣，想跳过它们，用dropwhile with open('testvim.txt','r') as fp: for i in dropwhile(lambda i:i.startswith(\"#\"),fp): # 跳过前面#号开头的 print(i) with open(\"testvim.txt\",\"r\") as fp: for i in takewhile(lambda i:i.startswith(\"#\"),fp): # 遍历带#号开头的，遇到不是#号开头的就退出循环，可以当做break使用 # 相当于 if not i.startwith(\"#\"): break print(i) ","date":"2021-10-10","objectID":"/skill/:46:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"装饰器可以装饰方法 import random import wrapt # 为第三方库 def provide_number(min_num, max_num): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): # 参数含义： # # - wrapped：被装饰的函数或类方法 # - instance： # - 如果被装饰者为普通类方法，该值为类实例 # - 如果被装饰者为 classmethod 类方法，该值为类 # - 如果被装饰者为类/函数/静态方法，该值为 None # # - args：调用时的位置参数（注意没有 * 符号） # - kwargs：调用时的关键字参数（注意没有 ** 符号） # num = random.randint(min_num, max_num) # 无需关注 wrapped 是类方法或普通函数，直接在头部追加参数 args = (num,) + args return wrapped(*args, **kwargs) return wrapper @provide_number(1, 100) def print_random_number(num): print(num) class Foo: @provide_number(1, 100) def print_random_number(self, num): print(num) Foo().print_random_number() print_random_number() # 使用 wrapt 模块编写的装饰器，相比原来拥有下面这些优势： # 嵌套层级少：使用 @wrapt.decorator 可以将两层嵌套减少为一层 # 更简单：处理位置与关键字参数时，可以忽略类实例等特殊情况 # 更灵活：针对 instance 值进行条件判断后，更容易让装饰器变得通用 ","date":"2021-10-10","objectID":"/skill/:47:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"___getattribute__ ____getattribute__仅在新式类中可用，重载__getattrbute__方法对类实例的每个属性访问都有效。 class ClassA: x = 'a' def __init__(self): self.y = 'b' def __getattribute__(self, item): return '__getattribute__' if __name__ == '__main__': a = ClassA() # 使用实例直接访问存在的类属性时,会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.x) # 使用实例直接访问实例存在的实例属性时,会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.y) # 使用实例直接访问实例不存在的实例属性时,也会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.z) 由于__getattr__只针对未定义属性的调用，所以它可以在自己的代码中自由地获取其他属性，而__getattribute__针对所有的属性运行，因此要十分注意避免在访问其他属性时，再次调用自身的递归循环。 当在__getattribute__代码块中，再次执行属性的获取操作时，会再次触发__getattribute__方法的调用，代码将会陷入无限递归，直到Python递归深度限制（重载__setter__方法也会有这个问题）。 示例代码（无限递归）： class ClassA: x = 'a' def __getattribute__(self, item): print('__getattribute__') return self.item if __name__ == '__main__': a = ClassA() a.x 运行结果引发异常。 同时，也没办法通过从__dict__取值的方式来避免无限递归 class ClassA: x = 'a' def __getattribute__(self, name): return self.__dict__[name] if __name__ == '__main__': a = ClassA() # 无限递归 a.x 为了避免无限递归，应该把获取属性的方法指向一个更高的超类，例如object（因为__getattribute__只在新式类中可用，而新式类所有的类都显式或隐式地继承自object，所以对于新式类来说，object是所有新式类的超类）。 修改代码（避免无限递归循环）： class ClassA: x = 'a' def __getattribute__(self, item): print('__getattribute__') return super().__getattribute__(self, item) if __name__ == '__main__': a = ClassA() print(a.x) 结果： __getattribute__ a ","date":"2021-10-10","objectID":"/skill/:48:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"___getattr__、__setattr__ 区别 getattribute 和 getattr，前者是任何通过 x.y 访问实例的属性时都会调用的特殊方法，而后者则是在正常访问形式下无法找到的情况下才会被调用。 class Chain(object): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain('%s/%s' % (self._path, path)) def __str__(self): return self._path def users(self,name): return Chain(f\"{self._path}/users/{name}\") __repr__ = __str__ chain = Chain(\"vllbc\") print(chain.x.x.x.x.x) # out: vllbc/x/x/x/x/x 另外，当同时定义__getattribute__和__getattr__时，__getattr__方法不会再被调用，除非显示调用__getattr__方法或引发AttributeError异常。可以在__getattribute__中抛出异常来调用getattr。 ","date":"2021-10-10","objectID":"/skill/:49:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__getitem__ ","date":"2021-10-10","objectID":"/skill/:50:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"元类 ''' 元类就是控制类的创建的类 ''' class ModelMetaclass(type): def __new__(cls, name, bases, attrs): if name == 'Model': return type.__new__(cls, name, bases, attrs) print(f\"found model {name}\") maps = dict() for k, v in attrs.items(): if isinstance(v, Field): print(f\"Found mapping {k} ==\u003e {v}\") maps[k] = v for k, v in maps.items(): attrs.pop(k) attrs['__mappings__'] = maps attrs['__table__'] = name return type.__new__(cls, name, bases, attrs) class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return '\u003c%s:%s\u003e' % (self.__class__.__name__, self.name) class StringField(Field): def __init__(self, name, column_type='TXT'): super().__init__(name, column_type) class IntegerField(Field): def __init__(self, name, column_type='INT'): super().__init__(name, column_type) class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Model' object has no attribute '%s'\" % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.items(): fields.append(k) params.append('?') args.append(getattr(self, k, None)) sql = 'insert into %s (%s) values (%s)' % ( self.__table__, ','.join(fields), ','.join(params)) print('SQL: %s' % sql) print('ARGS: %s' % str(args)) class User(Model): # 定义类的属性到列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password') # 创建一个实例： u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd') # 保存到数据库： u.save() ","date":"2021-10-10","objectID":"/skill/:51:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"关于hash # set中的元素要求必须是可哈希的，但set本身是不可哈希的 a = set([1,2,3,4]) a.add([1,2,3]) # 会报错，因为列表是不可哈希的 hash(a) # 会报错，因为set本身是不可哈希的 b = tuple([1,2,3,4]) # 元组本身是可哈希的 c = tuple([1,2,3,[4,5]]) # 如果元组里面有不可哈希的元素 那么整个元组也是不可哈希的了 # 在类中定义__hash__方法就可以变成可哈希的类，注意避免返回可能重复的hash值 class My_Hash: def __hash__(self) -\u003e int: return 111 # 还有简便的方法就是使用dataclass类，可以省时省力，本博客也有介绍。 ","date":"2021-10-10","objectID":"/skill/:52:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"处理列表越界 假如你请求的不是某一个元素，而是一段范围的切片。那么无论你指定的范围是否有效，程序都只会返回一个空列表 []，而不会抛出任何错误。 了解了这点后，你会发现像下面这种边界处理代码根本没有必要： def sum_list(l, limit): \"\"\"对列表的前 limit 个元素求和 \"\"\" # 如果 limit 过大，设置为数组长度避免越界 if limit \u003e len(l): limit = len(l) return sum(l[:limit]) 因为做切片不会抛出任何错误，所以不需要判断 limit 是否超出范围，直接做 sum 操作即可： def sum_list(l, limit): return sum(l[:limit]) ","date":"2021-10-10","objectID":"/skill/:53:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"or操作符 在很多场景下，我们可以利用 or 的特点来简化一些边界处理逻辑。看看下面这个例子： context = {} # 仅当 extra_context 不为 None 时，将其追加进 context 中 if extra_context: context.update(extra_context) # 等同于 context.update(extra_context or {}) 因为 a or b or c or … 这样的表达式，会返回这些变量里第一个布尔值为真的值，直到最后一个为止。 含义为当extra_context为None时，会返回{} ## 字典的键 在python里面，Python 字典通过检查键值是否相等和比较哈希值来确定两个键是否相同.具有相同值的不可变对象(可哈希)在Python中始终具有相同的哈希值。 注意: 具有不同值的对象也可能具有相同的哈希值（哈希冲突）。 下面这个例子 some_dict = {} some_dict[5.5] = \"Ruby\" some_dict[5.0] = \"JavaScript\" some_dict[5] = \"Python\" \u003e\u003e\u003e some_dict[5.5] \"Ruby\" \u003e\u003e\u003e some_dict[5.0] \"Python\" \u003e\u003e\u003e some_dict[5] \"Python\" 因为Python将 5 和 5.0 识别为 some_dict 的同一个键, 所以已有值 “JavaScript” 就被 “Python” 覆盖了. ","date":"2021-10-10","objectID":"/skill/:54:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"类__init__参数快速赋给self def __init__(self, x=1) -\u003e None: super().__init__() self.__dict__.update(locals()) pass 即可，此时不需要self.x=x也可以直接使用self.x ","date":"2021-10-10","objectID":"/skill/:55:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["算法题"],"content":"长度最小的子数组 ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:0:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-size-subarray-sum/ ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:1:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"思路： ​ 一开始想的是直接排序，然后从后面开始遍历，因为要求最小的 然后出错了，，，，， class Solution: def minSubArrayLen(self, s: int, nums: List[int]) -\u003e int: nums.sort() end = len(nums) - 1 while end \u003e 0: for i in range(end,-1,-1): if sum(nums[i:end+1]) \u003e= s: return len(nums[i:end+1]) end -= 1 return 0 在 213 [12,28,83,4,25,26,25,2,25,25,25,12] 出了错，结果试了一下排序后的列表 [2, 4, 12, 12, 25, 25, 25, 25, 25, 26, 28, 83] 结果居然是对的，说明是我的代码的问题，不应该排序 那么该怎么办呢 class Solution: def minSubArrayLen(self, s: int, nums: List[int]) -\u003e int: if not nums: return 0 left = 0 right = 0 res = float('inf') while right \u003c len(nums): while sum(nums[left:right+1]) \u003e= s: res = min(res, right-left +1) left += 1 else: right += 1 if res == float('inf'): return 0 return res 思路也差不多，也是用到了双指针。不过必须注意要判断最小的这个条件啊。 ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:2:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["Machine Learning","分类算法"],"content":"SVM ","date":"2021-09-20","objectID":"/svm/:0:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"kernel ","date":"2021-09-20","objectID":"/svm/:1:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"介绍 其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。 一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超平面。 围绕这个，那么我们所做的就是要在Feature Space套用原本在线性可分情况下的Input Space中使用过的优化方法，来找到那个Maximaizing Margin的超平面。原理机制一模一样，是二次规划，唯一不同是代入数据的不同，将原来的\\(x_i\\)替换成了高维空间中的\\(\\phi(x_i)\\)，这就是映射函数，映射到高维空间。 具体的技巧(trick)，就是简化计算二次规划中间的一步内积计算。也即中间步骤有一步必须求得\\(\\phi(x_i) \\phi(x_j)\\)，我们可以定义核函数\\(K(x_i,x_j) = \\phi(x_i)\\phi(x_j)\\)，这样我们不需要显式计算每一个\\(\\phi(x_i)\\)，甚至不需要知道它的形式，就可以直接计算结果出来。 也就是说，核函数、内积、相似度这三个词是等价的。因为inner product其实就是一种similarity的度量。核函数和映射是无关的。 ","date":"2021-09-20","objectID":"/svm/:1:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"例子 举一个例子： 考虑一个带有特征映射的二维输入空间 \\(\\chi \\subseteq \\mathbb{R}^{2}\\) : 特征映射二维到三维: \\(\\quad \\Phi: x=\\left(x_{1}, x_{2}\\right) \\rightarrow \\Phi(x)=\\left(x_{1}^{2}, x_{2}^{2}, \\sqrt{2} x_{1} x_{2}\\right) \\in F=\\mathbb{R}^{3}\\) 特征空间中的内积： \\[ \\begin{aligned} \\langle\\Phi(x), \\Phi(z)\\rangle \u0026=\\left\\langle\\left(x_{1}^{2}, x_{2}^{2}, \\sqrt{2} x_{1} x_{2}\\right),\\left(z_{1}^{2}, z_{2}^{2}, \\sqrt{2} z_{1} z_{2}\\right)\\right\\rangle \\\\\\\\ \u0026=x_{1}^{2} z_{1}^{2}+x_{2}^{2} z_{2}^{2}+2 x_{1} x_{2} z_{1} z_{2} \\\\\\\\ \u0026=\\left\\langle x_{1} z_{1}+x_{2} z_{2}\\right\\rangle^{2} \\\\\\\\ \u0026=\\langle x, z\\rangle^{2} \\end{aligned} \\] 根据上面可得，核函数\\(k(x,z) = \\langle x,z \\rangle^2=\\phi(x)^T \\phi(z)\\) 而这里为什么映射函数是这样的形式呢，其实可以是反推出来的，我也不知道，反正凑巧通过这种映射函数可以得到这个核函数。 ","date":"2021-09-20","objectID":"/svm/:1:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"常用核函数理解 以高斯核函数为例， \\[ \\kappa\\left(x_{1}, x_{2}\\right)=\\exp \\left(-\\frac{\\left|x_{1}-x_{2}\\right|^{2}}{2 \\sigma^{2}}\\right) \\] 我们假设 \\(\\sigma=1\\) ，则 \\[ \\begin{aligned} \\kappa\\left(x_{1}, x_{2}\\right) \u0026=\\exp \\left(-\\frac{\\left|x_{1}-x_{2}\\right|^{2}}{2 \\sigma^{2}}\\right) \\\\\\\\ \u0026=\\exp \\left(-\\left(x_{1}-x_{2}\\right)^{2}\\right) \\\\\\\\ \u0026=\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right) \\exp \\left(2 x_{1} x_{2}\\right) \\\\\\\\ \u0026 \\text { Taylor } \\\\\\\\ \u0026=\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right)\\left(\\sum_{i=0}^{\\infty} \\frac{\\left(2 x_{1} x_{2}\\right)^{i}}{i !}\\right) \\\\\\\\ \u0026=\\sum_{i=0}^{\\infty}\\left(\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right) \\sqrt{\\left.\\frac{2^{i}}{i !} \\sqrt{\\frac{2^{i}}{i !}} x_{1}^{i} x_{2}^{i}\\right)}\\right.\\\\\\\\ \u0026=\\sum_{i=0}^{\\infty}\\left(\\left[\\exp \\left(-x_{1}^{2}\\right) \\sqrt{\\frac{2^{i}}{i !}} x_{1}^{i}\\right]\\left[\\exp \\left(-x_{2}^{2}\\right) \\sqrt{\\frac{2^{i}}{i !}} x_{2}^{i}\\right]\\right) \\\\\\\\ \u0026=\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) \\end{aligned} \\] w这不，已经有了定义的那种形式，对于 \\(\\phi(x)\\) ，由于 \\[ \\phi(x)=\\exp \\left(-x^{2}\\right) \\cdot\\left(1, \\sqrt{\\frac{2^{1}}{1 !}} x, \\sqrt{\\frac{2^{2}}{2 !}} x^{2}, \\cdots\\right) \\] 所以，可以映射到任何一个维度上。 ","date":"2021-09-20","objectID":"/svm/:1:3","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"核函数类别 其实常用的就那几个，高斯核函数最为常用。 ","date":"2021-09-20","objectID":"/svm/:1:4","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"参考 https://www.cnblogs.com/damin1909/p/12955240.html https://blog.csdn.net/mengjizhiyou/article/details/103437423 ","date":"2021-09-20","objectID":"/svm/:1:5","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性可分支持向量机 ","date":"2021-09-20","objectID":"/svm/:2:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性可分 在二维空间上，两类点被一条直线完全分开叫做线性可分。 ","date":"2021-09-20","objectID":"/svm/:2:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"最大间隔超平面 以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。 ","date":"2021-09-20","objectID":"/svm/:2:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"支持向量 样本中距离超平面最近的一些点，这些点叫做支持向量。 ","date":"2021-09-20","objectID":"/svm/:2:3","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"最优化问题 SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。任意超平面可以用下面这个线性方程来描述： \\[ w^Tx+b=0 \\] 二维空间点(x,y)到直线\\(Ax+By+C=0\\)的距离公式为： \\[ \\frac{|Ax+By+C|}{\\sqrt{A^2+B^2}} \\] 扩展到n维空间中，\\(x=(x_1, x_2,\\dots, x_n)\\)到直线\\(w^Tx+b=0\\)的距离为： \\[ \\frac{|w^Tx+b|}{||w||} \\] 如图所示，根据支持向量的定义我们知道，支持向量到超平面的距离为 d，其他点到超平面的距离大于 d。 于是我们有这样的一个公式： 之后得到: 分母都是正数，因此可以令它为1。 合并得： 至此我们就可以得到最大间隔超平面的上下两个超平面： 每个支持向量到超平面的距离可以写为： 所以我们得到： 最大化这个距离： 这里乘上 2 倍也是为了后面推导，对目标函数没有影响。刚刚我们得到支持向量\\(y(w^Tx+b) = 1\\)，所以我们得到： \\[ \\max\\frac{2}{||w||} \\] 对目标进行转换： \\[ \\min\\frac{1}{2}||w||^2 \\] 所以得到的最优化问题是： ","date":"2021-09-20","objectID":"/svm/:2:4","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"对偶问题 拉格朗日乘数法、拉格朗日对偶和KKT条件 参考：https://zhuanlan.zhihu.com/p/38163970 给定约束优化问题： \\[ \\begin{aligned} \u0026\\min f(x) \\\\\\\\ \u0026 s.t. g(x) = 0 \\end{aligned} \\] 为方便分析，假设 f 与 g 是连续可导函数。Lagrange乘数法是等式约束优化问题的典型解法。定义Lagrangian函数 \\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] 其中 λ 称为Lagrange乘数。Lagrange乘数法将原本的约束优化问题转换成等价的无约束优化问题 计算 L 对 x 与 λ 的偏导数并设为零，可得最优解的必要条件： 接下来是不等式约束： \\[ \\begin{aligned} \u0026 \\min f(x) \\\\ \u0026 s.t. g(x) \\leq 0 \\end{aligned} \\] 据此我们定义可行域(feasible region)\\(K=x\\in R^n | g(x)\\leq 0\\)。设$x^* $为满足条件的最佳解，分情况讨论： \\(g(x^* ) \u003c 0\\)，最佳解位于K的内部，为内部解，这时的约束是无效的。 \\(g(x^* ) = 0\\)，最佳解落在K的边界，称为边界解，此时的约束是有效的。 这两种情况的最佳解具有不同的必要条件。 具有不同的必要条件： 内部解：在约束条件无效的情况下，\\(g(x)\\)不起作用，约束优化问题退化为无约束优化问题，因此$x^* \\(满足\\)= 0$ 边界解：在约束有效的情况下，约束不等式变为等式\\(g(x)=0\\)。此时拉格朗日函数在$x^* \\(的梯度为0，即\\)f = -g\\(，\\)f(x)\\(的极小值在边界取到，那么可行域内部的\\)f(x)\\(应该都是大于这个极小值，则\\)f(x)\\(的方向是可行域内部。而\\)g\\(的方向为可行域外部，因为约束条件是\\)g(x) \\(，也就是可行域外部都是\\)g(x) \u003e 0\\(，所以梯度方向就是指向函数增加的方向。说明两个函数的梯度方向相反，要想上面的等式成立，必须有\\)\\(，这就是对偶可行性。 因此，不论是内部解或边界解， \\)g(x)=0$ 恒成立 整合上述两种情况，最佳解的必要条件包括Lagrangian函数的定常方程式、原始可行性、对偶可行性，以及互补松弛性： 这就是KKT条件。 上面结果可推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题(或称非线性规划)： 定义Lagrangian 函数 则KKT条件为 应用 已知svm优化的主要问题： 那么求解线性可分的 SVM 的步骤为： 步骤1： 构造拉格朗日函数： 步骤2： 利用强对偶性转化： 现对参数 w 和 b 求偏导数： 具体步骤： 在前面的步骤中即为： 我们将这个结果带回到函数中可得： 也就是说： 步骤3： 由上述过程需要满足KKT条件（\\(\\alpha\\)就是本文中的\\(\\lambda\\)）： 易得，当\\(\\lambda_i\\)大于0，则必有\\(y_if(x_i)=1\\),所对应的样本点是一个支持向量，即位于最大间隔边界上。 我们可以看出来这是一个二次规划问题，问题规模正比于训练样本数，我们常用 SMO(Sequential Minimal Optimization) 算法求解。 SMO(Sequential Minimal Optimization)，序列最小优化算法，其核心思想非常简单：每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的应用。 我们刚说了 SMO 算法每次只优化一个参数，但我们的优化目标有约束条件，没法一次只变动一个参数。所以我们选择了一次选择两个参数。具体步骤为： 选择两个需要更新的参数\\(\\lambda _i\\)和\\(\\lambda_j\\)，固定其他参数。于是我们有以下约束： 其中\\(c = -\\sum_{k\\neq i, j}\\lambda_ky_k\\)， 因此可以得出\\(\\lambda_j = \\frac{c-\\lambda_iy_i}{y_j}\\)，这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是\\(\\lambda_i\u003e0\\) 对于仅有一个约束条件的最优化问题，我们完全可以在\\(\\lambda_i\\)上对优化目标求偏导，令导数为零，从而求出变量值\\(\\lambda_{inew}\\)，从而求出\\(\\lambda_{jnew}\\) 多次迭代直至收敛。 通过 SMO 求得最优解$^* $ 步骤4： 我们求偏导数时得到： 由上式可求得 w。 由于所有\\(\\lambda_i\u003e0\\)的点都是支持向量，可以随便找一个支持向量代入\\(y_s(w^Tx_s+b)=1\\)，求出b即可。 两边同时乘以\\(y_s\\)，最后得\\(b = y_s-wx_s\\) 为了更具鲁棒性，我们可以求得支持向量的均值： 步骤5： w 和 b 都求出来了，我们就能构造出最大分割超平面：\\(w^Tx+b=0\\) 分类决策函数：\\(f(x) = sign(w^Tx+b)\\) 将新样本点导入到决策函数中既可得到样本的分类。 ","date":"2021-09-20","objectID":"/svm/:2:5","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性支持向量机与软间隔 ","date":"2021-09-20","objectID":"/svm/:3:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"软间隔 在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们应该怎么办？比如下面这个： 于是我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面，即允许出现分类错误的样本： 我们允许部分样本点不满足约束条件： \\[ y_i(w^Tx_i+b) \\geq 1 \\] 则优化目标变成了 \\[ \\min_{w, b} \\frac{1}{2}\\|{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left({w}^{\\mathrm{T}} {x}_{i}+b\\right)-1\\right), \\] 其中 \\(C\u003e0\\) 是一个常数, \\(\\ell_{0 / 1}\\) 是 “ \\(0 / 1\\) 损失函数” \\[ \\ell_{0 / 1}(z)= \\begin{cases}1, \u0026 \\text { if } z\u003c0 \\\\\\\\ 0, \u0026 \\text { otherwise. }\\end{cases} \\] 显然, 当 \\(C\\) 为无穷大时, \\(\\xi_i\\)必然无穷小，如此一来线性svm就又变成了线性可分svm，当\\(C\\)为有限值时，才会允许部分样本不遵循约束条件 然而, \\(\\ell_{0 / 1}\\) 非凸、非连续, 数学性质不太好, 使得不易直接求解. 于 是, 人们通常用其他一些函数来代替 \\(\\ell_{0 / 1}\\), 称为 “替代损失” (surrogate loss). 替代损失函数一般具有较好的数学性质, 如它们通常是凸的连续函数且是 \\(\\ell_{0 / 1}\\) 的上界. 给出了三种常用的替代损失函数: hinge 损失: \\(\\ell_{\\text {hinge }}(z)=\\max(0,1-z)\\); 指数损失(exponential loss): \\(\\ell_{\\exp }(z)=\\exp (-z)\\); 对率损失(logistic loss): \\(\\ell_{\\log }(z)=\\log (1+\\exp (-z))\\). 若采用 hinge 损失, 则变成 \\[ \\min_{w, b} \\frac{1}{2}\\|{w}\\|^{2}+C \\sum_{i=1}^{m} \\max\\left(0,1-y_{i}\\left({w}^{\\mathrm{T}} {x}_{i}+b\\right)\\right) \\] 为了度量这个间隔软到何种程度，我们为每个样本引入一个松弛变量\\(\\xi_i\\)，令\\(\\xi_i \\geq 0\\)，且\\(1-y_i(w^Tx_i+b)-\\xi_i\\leq 0\\)，如下图： ","date":"2021-09-20","objectID":"/svm/:3:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"优化目标与求解 优化目标： 步骤1： 构造拉格朗日函数： 步骤2： 分别求导，得出以下关系： 将这些关系带入拉格朗日函数中，得到： 则： 我们可以看到这个和硬间隔的一样，只是多了个约束条件。 然后使用SMO算法求$^* $ 软间隔KKT条件 其中\\(\\alpha\\)对应本文的\\(\\lambda\\)，\\(\\mu\\)对应本文的\\(\\mu\\) 因此由第三个式子得必有\\(\\lambda_i =0\\)或者\\(y_if(x_i) - 1+\\xi_i \\geq 0\\) \\(\\lambda_i=0\\)，则该样本对其没有任何影响。 \\(\\lambda_i \u003e 0\\)，则样本为支持向量。 若\\(\\lambda_i \u003cC\\),则\\(\\mu_i \u003e 0\\)，进而有\\(\\xi_i=0\\)，则样本恰在最大间隔边界上。也是支持向量。 若\\(\\lambda_i=C\\),则有\\(\\mu_i=0\\)，此时若\\(\\xi_i\\leq 1\\)，则样本落在最大间隔内部。若\\(\\xi_i\u003e1\\)则样本被错误分类。 再看一下下面这图就理解了。 步骤3： 然后我们通过上面两个式子求出 w 和 b，最终求得超平面 这边要注意一个问题，在间隔内的那部分样本点是不是支持向量？ 我们可以由求参数 w 的那个式子可看出，只要 \\(\\lambda_i \u003e 0\\)的点都能影响我们的超平面，因此都是支持向量。 ","date":"2021-09-20","objectID":"/svm/:3:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"非线性支持向量机 我们刚刚讨论的硬间隔和软间隔都是在说样本的完全线性可分或者大部分样本点的线性可分。 但我们可能会碰到的一种情况是样本点不是线性可分的，比如： 这种情况的解决方法就是：将二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分，比如： 对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。 我们用 x 表示原来的样本点，用\\(\\phi(x)\\)表示 x 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为: \\(f(x) = w\\phi(x)+b\\) 对于非线性 SVM 的对偶问题就变成了： 区别就在于优化目标中的内积。 ","date":"2021-09-20","objectID":"/svm/:4:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"核函数 我们不禁有个疑问：只是做个内积运算，为什么要有核函数的呢？ 这是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘全部计算好，这样的计算量太大了。 但如果我们有这样的一核函数\\(k(x,y) = (\\phi(x), \\phi(y))\\)，x与y在特征空间中的内积，就等于它们在原始空间中通过函数\\(k(x,y)\\)计算的结果，我们就不需要知道映射函数和计算高维空间中的内积了。 有关内容看本文一开始对kernel的介绍。 ","date":"2021-09-20","objectID":"/svm/:4:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"总结 SVM是深度学习流行之前的首选分类方法，在许多任务上都有很好的效果，稍微修改后可以用于回归任务中。总结一下svm算法的优缺点。 ","date":"2021-09-20","objectID":"/svm/:5:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"优点 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题； 能找出对任务至关重要的关键样本（即：支持向量）； 采用核技巧之后，可以处理非线性分类/回归任务； 最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。 ","date":"2021-09-20","objectID":"/svm/:5:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"缺点 训练时间长。当采用 SMO 算法时，每次都需要挑选一对参数 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为\\(O(N^2)\\) 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。 ## 参考 https://zhuanlan.zhihu.com/p/77750026 ","date":"2021-09-20","objectID":"/svm/:5:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["算法题"],"content":"字符串的排列 ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:0:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/permutation-in-string/ ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:1:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"思路： 滑动窗口加字典 ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:2:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution(object): def checkInclusion(self, s1, s2): counter1 = collections.Counter(s1) N = len(s2) left = 0 right = len(s1) - 1 counter2 = collections.Counter(s2[0:right]) while right \u003c N: counter2[s2[right]] += 1 if counter1 == counter2: return True counter2[s2[left]] -= 1 if counter2[s2[left]] == 0: del counter2[s2[left]] left += 1 right += 1 return False ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:3:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["pandas","api"],"content":"类似于pandas的apply，就是在某一维上进行定义的函数操作 apply_along_axis(func1d, axis, arr, *args, **kwargs) 官网的例子 def my_func(a): return (a[0] + a[-1]) * 0.5 b = np.array([[1,2,3], [4,5,6], [7,8,9]]) np.apply_along_axis(my_func, 0, b) # 结果 array([ 4., 5., 6.]) # 结果 array([ 2., 5., 8.]) ","date":"2021-09-03","objectID":"/apply_along_axis/:0:0","tags":["pandas","api","apply_along_axis"],"title":"apply_along_axis","uri":"/apply_along_axis/"},{"categories":["算法题"],"content":"采购方案 ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:0:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/4xy4Wx/ ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:1:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"思路： 题目很简单，思想就是双指针，感觉是个双指针的典型例子就写了下来 先对数组进行从小到大排序，然后双指针从两边移动，如果一直大于target就一直左移right 然后right - left就是所有成立的数目，再移动left 进行筛选 ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:2:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"代码： class Solution: def purchasePlans(self, nums: List[int], target: int) -\u003e int: nums.sort() left = 0 right = len(nums) - 1 res = 0 while left \u003c right and left \u003c len(nums): while left \u003c right and nums[right] + nums[left] \u003e target: right -= 1 res += right - left left += 1 return res % (1000000007) ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:3:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"位运算的一个应用 翻了翻以前用python刷leetcode的记录，最后刷的一道题是这样的 https://leetcode-cn.com/problems/single-number/ 叫只出现一次的数字，当时看题感觉非常简单啊！直接搞就行了 当时一开始我的做法是这样的 class Solution: def singleNumber(self, nums): for i in set(nums): if nums.count(i) ==1: return i 信心满满的提交，结果发现TLE了。。 然后看超时案例的输入，没有一个数字重复，也就是说我的set跟没有一样，所以说肯定不能这么做。 然后又想到了哈希表 class Solution: def singleNumber(self, nums): dic={} for num in nums: if num in dic.keys(): dic[num]+=1 else: dic[num]=1 for i in dic.keys(): if dic[i] ==1: return i 这样也算是AC了。本以为这个题就这么结束了，结果无意中看到了别的题解震惊了 代码数量比我短的多得多。 然后就认识到了位运算的魔力。。 先上代码： class Solution: def singleNumber(self, nums): a = 0 for i in nums: a^=i return a 简单的一个异或运算就达到了目的 真是太神奇了！ 找到相关资料 交换律：a ^ b ^ c \u003c=\u003e a ^ c ^ b 任何数于0异或为任何数 0 ^ n =\u003e n 相同的数异或为0: n ^ n =\u003e 0 也就是说相同的数就异或为0了，达到了去重的目的。 ","date":"2021-08-23","objectID":"/%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/:0:0","tags":["算法题","只出现一次的数字"],"title":"只出现一次的数字","uri":"/%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/"},{"categories":["sklearn"],"content":"​ import numpy as np from sklearn.model_selection import train_test_split,cross_val_score from sklearn import datasets from sklearn.neighbors import KNeighborsClassifier data = datasets.load_iris() X = data.data Y = data.target k_scores = [] for k in range(1,31): model = KNeighborsClassifier(n_neighbors=k) #scores = cross_val_score(model,X,Y,cv=10,scoring=\"accuracy\") # for classification loss = -cross_val_score(model,X,Y,cv=10,scoring=\"neg_mean_squared_error\") # for regression k_scores.append(loss.mean()) plt.plot(range(1,31),k_scores) plt.show() ","date":"2021-08-15","objectID":"/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/:0:0","tags":["sklearn","交叉验证"],"title":"交叉验证","uri":"/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"},{"categories":["NLP","概率图模型"],"content":"概率图模型概述 ","date":"2021-08-13","objectID":"/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/:0:0","tags":["NLP","概率图模型","概率图模型概述"],"title":"概率图模型概述","uri":"/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"},{"categories":["面经"],"content":"B树 B树就是B-树，以前还以为这是两种树，现在才知道这俩就是一个东西。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:0:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"基本概念 所有的叶子结点都出现在同一层上，并且不带信息(可以看做是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空)。 每个结点包含的关键字个数有上界和下界。用一个被称为 B-树的 最小度数 的固定整数 t≥2 来表示这些界 ，其中 t 取决于磁盘块的大小： a.除根结点以外的每个结点必须至少有 t−1 个关键字。因此，除了根结点以外的每个内部结点有 t 个孩子。如果树非空，根结点至少有一个关键字。 每个结点至多包含 2t−1 个关键字。 一个包含x个关键字的结点有x+1个孩子。 一个结点中所有的关键字升序排列，两个关键字\\(k_1\\)和\\(k_2\\)之间的孩子结点的所有关键字key在\\((k_1, k_2)\\)的范围内。 其中最小度数和B树的阶不一样： 度：一个结点含有的子结点的个数称为该结点的度 阶：一棵树的最大孩子数 最小度minimum degree（t）：用来衡量结点的关键字数量范围 阶 order（m）：衡量B树中的结点的最大孩子数 关系如下： \\[ t = ceil(\\frac{m}{2}) \\quad m = 2t \\] 可以简单理解为最小度是孩子数的最小值，阶是孩子数的最大值。最小度-1是节点关键字数的最小值，阶-1是节点关键字数的最大值。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:1:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"查找 查找很简单，对每个键中的索引进行比较然后查找就可以。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:2:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"插入 伪代码： 初始化 x 作为根结点 当 x 不是叶子结点，执行如下操作： 找到 x 的下一个要被访问的孩子结点 y 如果 y 没有满，则将结点 y 作为新的 x 如果 y 已经满了，拆分 y ，结点 x 的指针指向结点 y 的两部分。 如果 k 比 y 中间的关键字小， 则将 y 的第一部分作为新的 x ，否则将 y 的第二部分作为新的 x ，当将 y 拆分后，将 y 中的一个关键字移动到它的父结点 x 当中。 当 x 是叶子结点时，第二步结束； 由于我们已经提前查分了所有结点，x 必定至少有一个额外的关键字空间，进行简单的插入即可。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:3:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"删除 待删除的关键字 k 在结点 x 中，且 x 是叶子结点，删除关键字k 待删除的关键字 k 在结点 x 中，且 x 是内部结点，分一下三种情况 1). 如果位于结点 x 中的关键字 k 之前的第一个孩子结点 y 至少有 t 个关键字，则在孩子结点 y 中找到 k 的前驱结点 k0 ，递归地删除关键字 k0 ，并将结点 x 中的关键字 k 替换为 k0 直接前驱：当前关键字左侧指针 所指子树中“最右下”的元素 删除 B-树中的关键字 G ，G 的前一个孩子结点 y 为 [D、E、F] ，包含 3个关键字，满足情况一，关键字 G 的直接前驱为关键 F ，删除 F ，然后将 G 替换为 F . 2).y 所包含的关键字少于 t 个关键字，则检查结点 x 中关键字 k 的后一个孩子结点 z 包含的关键字的个数，如果 z 包含的关键字的个数至少为 t 个，则在 z 中找到关键字 k 的直接后继 K0 ,然后删除 K0 ，并将关键 k 替换为 K0 . 直接后继：当前关键字右侧指针 所指子树中“最左下”的元素 删除 B-树中的关键字 C , y 中包含的关键字的个数为 2 个，小于 t = 3 ,结点 [C、G、L] 中的 关键字 C 的后一个孩子 z 为 [D、E、F] 包含 3 个关键字，关键字 C 的直接后继为 D ，删除 D ，然后将 C 替换为 D . 3). 如果 y 和 z 都只包含 t -1 个关键字，合并关键字 k 和所有 z 中的关键字到 结点 y 中，结点 x 将失去关键字 k 和孩子结点 z，y 此时包含 2t -1 个关键字，释放结点 z 的空间并递归地从结点 y 中删除关键字 k. 删除关键字 C , 结点 y 包含 2 个关键字 ，结点 z 包含 2 个关键字，均等于 t - 1 = 2 个， 合并关键字 C 和结点 z 中的所有关键字到结点 y 当中： 之后直接删除C即可。 如果关键字 k 不在当前在内部结点 x 中，则确定必包含 k 的子树的根结点 x.c(i) （如果 k 确实在 B-树中）。如果 x.c(i) 只有 t - 1 个关键字，必须执行下面两种情况进行处理： 首先我们得确认什么是当前内部结点 x ，什么是 x.c(i) ,如下图所示， P 现在不是根结点，而是完整 B-树的一个子树的根结点： 1). x.c(i) 仅包含 t - 1 个关键字且 x.c(i) 的一个兄弟结点包含至少 t 个关键字，则将 x 的某一个关键字下移到 x.c(i) 中，将 x.c(i) 的相邻的左兄弟或右兄弟结点中的一个关键字上移到 x 当中，将该兄弟结点中相应的孩子指针移到 x.c(i) 中，使得 x.c(i) 增加一个额外的关键字。 我们以删除结点 [A、B] 中的结点 B 为例，上图中 x.c(i) 包含 2 个关键字，即 t - 1 个关键字， x.c(i) 的一个兄弟结点 [H、J、K] 包含 3 个关键字（满足至少 t 个关键字的要求），则将兄弟结点 [H、J、K] 中的关键字 H 向上移动到 x 中， 将 x 中的关键字 C 下移到 x.c(i) 中；删除关键字 B . 2).如果 x.c(i) 及 x.c(i) 的所有相邻兄弟都只包含 t - 1 个关键字，则将 x.c(i) 与 一个兄弟合并，即将 x 的一个关键字移动至新合并的结点，使之成为该结点的中间关键字，将合并后的结点作为新的 x 结点 . 以此图为例： 上面的图标明了相应的 x 及 x.c(i) ，我们以删除关键字 D 为例，此时当前内部结点 x 不包含关键字 D , 确定是第三种情况，我们可以确认关键 D 一定在结点 x 的第一个孩子结点所在的子树中，结点 x 的第一个孩子结点所在子树的跟结点为 x.c(i) 即 [C、L] . 其中 结点 [C、L] 及其相邻的兄弟结点 [T、W] 都只包含 2 个结点（即 t - 1) ，则将 [C、L] 与 [T、W] 合并，并将结点 x 当中仅有的关键字 P 合并到新结点中；然后将合并后的结点作为新的 x 结点，递归删除关键字 D ，发现D 此时在叶子结点 y 中，直接删除，就是 1. 的情况。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:4:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["算法题"],"content":"和为s的连续正数序列 ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:0:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"题目: https://leetcode-cn.com/problems/he-wei-sde-lian-xu-zheng-shu-xu-lie-lcof/ ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:1:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"思路： 滑动窗口即可，滑动窗口就是选取数组的一部分来进行操作，left 和 right只能向右移动 ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:2:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def findContinuousSequence(self, target: int) -\u003e List[List[int]]: res = [] left,right = 1,2 while left \u003c= target // 2: # 优化 减少时间复杂度 if sum(range(left,right+1)) \u003c target: # 小于target 右指针移动 right += 1 elif sum(range(left,right+1)) \u003e target: # 大于target 左指针移动 left += 1 else: res.append(list(range(left,right+1))) # 相等的话 两个指针都移动 right += 1 left += 1 return res ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:3:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"合并区间 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:0:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/merge-intervals/ ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:1:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"思路： ​ 一开始思路想的是，根据每一个区间的left排序后，然后比较每一个数，再向前更新，然后写了半天，一直WA，感觉这个思路不太行了 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:2:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"代码： ​ 先贴上错误的代码： class Solution: def merge(self, res: List[List[int]]) -\u003e List[List[int]]: if not res: return [] res.sort(key=lambda i:i[0]) n = len(res) - 1 for i in range(0,n): if res[i][1] \u003c res[i+1][0]: continue else: res[i+1] = [res[i][0],max(res[i][1],res[i+1][1])] res[i] = res[i-1] ress = [] for i in res: if i not in ress: ress.append(i) return ress 在[[1,4],[0,2],[3,5]] ​ 出错了 输出： [[3,5],[0,5]] 预期结果： [[0,5]] 应该是思路的错误 后来觉得不应该在原数组上操作 又改了如下，终于过了 class Solution: def merge(self, intervals: List[List[int]]) -\u003e List[List[int]]: if not intervals: return [] intervals.sort(key=lambda i:i[0]) res = [] for i in intervals: if len(res) == 0 or res[-1][1] \u003c i[0]: res.append(i) else: res[-1][1] = max(res[-1][1],i[1]) return res 这个思路就是先创造一个空数组res 然后如果数组为空或者题设的条件不成立的时候，把原数组的值加进去，要是条件成立的话，则将目前区间的right改为目前区间的right和原数组的right之间的最大值，预防[[1,4],[2,3]]这种情况。注意这个也是按left排序的 我上面代码的思路和这个是一样的，看来类似的题目尽量不要在原数组上面操作，除非题目要求 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:3:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["sklearn","feature_extraction"],"content":"TfidfTransformer() 输入：词频TF 输出：词频逆反文档频率TF-IDF（即词频TF与逆反文档频率IDF的乘积，IDF的标准计算公式为 ：idf=log[n/(1+df)]，其中n为文档总数，df为含有所计算单词的文档数量，df越小，idf值越大，也就是说出现频率越小的单词意义越大） 1.计算词频TF，通过函数CountVectorizer()来完成，以该文档为输入，并得到词频 tf 输出； 2.计算词频逆反文档频率TF-IDF，通过函数TfidfTransformer()来实现，以第一步的词频 tf 输出为输入，并得到 tf-idf 格式的输出。 cv = CountVectorizer() tfidf = TfidfTransformer().fit_transform(cv.fit_transform(corpus)) 将两步简化为一步就是函数 TfidfVectorizer() 所实现的功能了 TfidfVectorizer() 输入：文档 输出：该文档的词频逆反文档频率TF-IDF TfidfVectorizer().fit_transform(corpus) = TfidfTransformer().fit_transform(CountVectorizer().fit_transform(corpus)) ","date":"2021-07-25","objectID":"/tfidftransformer/:0:0","tags":["sklearn","feature_extraction","TfidfTransformer"],"title":"TfidfTransformer","uri":"/tfidftransformer/"},{"categories":["Machine Learning","降维算法"],"content":"SVD奇异值分解 参考：https://www.cnblogs.com/pinard/p/6251584.html ","date":"2021-07-16","objectID":"/svd/:0:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["Machine Learning","降维算法"],"content":"特征值与特征向量 首先回顾特征值与特征向量\\(Ax=\\lambda x\\) \\(\\lambda\\) 是矩阵A的一个特征值，x是矩阵A的特征值\\(\\lambda\\)对应的特征向量。 求出特征值与特征向量可以将矩阵A进行特征分解。如果求出了A的n个特征值，以及这n个特征值所对应的特征向量\\({w_1,w_2,\\dots,w_n}\\)，如果这n个特征向量线性无关，则矩阵A就可以用下式进行表示： \\[ A = W\\sum W^{-1} \\] 其中W为这n个特征向量所张成的\\(n\\times n\\)维矩阵，\\(\\sum\\)为这n个特征值为主对角线的矩阵。 我们一般会把n个特征向量标准化，即\\(w_i^Tw_i=1\\)，此时W的n个特征向量为标准正交基，满足\\(W^TW=I\\)，即\\(W^T=W^{-1}\\)，也就是说W为酉矩阵。 这样特征分解表达式可以写为\\(A=W\\sum W^T\\) 特征分解要求A必须为方阵，如果行列不相同则使用SVD进行分解。 ","date":"2021-07-16","objectID":"/svd/:1:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["Machine Learning","降维算法"],"content":"SVD 假设A为一个\\(m\\times n\\)的矩阵，那么定义A的SVD为： \\[ A = U\\sum V^T \\] 其中U是一个\\(m\\times n\\)的矩阵，\\(\\sum\\)是一个\\(m\\times n\\)的矩阵，除了主对角线上的元素全为0，主对角线上的每个元素成为奇异值，V是一个\\(n\\times n\\)的矩阵，U和V都是酉矩阵。 先得到\\(m\\times m\\)的方阵\\(AA^T\\),然后进行特征值分解，\\((AA^T)u_i=\\lambda_iu_i\\) 将\\(AA^T\\)的所有特征向量张成一个\\(m\\times m\\)的矩阵U，就是SVD公式里面的U矩阵了。 后得到\\(n\\times n\\)的方阵\\(A^TA\\),然后进行特征值分解，\\((A^TA)v_i=\\lambda_iv_i\\) 将\\(A^TA\\)的所有特征向量张成一个\\(n\\times n\\)的矩阵V，就是SVD公式里面的V矩阵了。 特征值和奇异值满足以下关系\\(\\sigma_i = \\sqrt{\\lambda_i}\\)，意思就是我们可以通过求\\(A^TA\\)的特征值取平方根来求奇异值。 ","date":"2021-07-16","objectID":"/svd/:2:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["NLP"],"content":"词嵌入 ","date":"2021-07-16","objectID":"/word-embedding/:0:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"介绍 词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。 词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。 在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中语法分析器和文本情感分析等的效果。 以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word representation)，可以粗略得把它分为三个阶段： 一、特征工程阶段，以词袋模型为典型代表。 二、浅层表证阶段，以word2vec为典型代表。 三、深层表征阶段，以基于transformer的Bert为典型代表。 本文介绍了一、二两部分内容 ","date":"2021-07-16","objectID":"/word-embedding/:1:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"语言模型 一句话，语言模型是这样一个模型：对于任意的词序列，它能够计算出这个序列是一句话的概率。 具体的详见本博客有关语言模型的文章。 为什么要介绍语言模型，因为NLP的做预训练一般选择用语言模型任务来做的。 语言模型主要有： N-gram LM、FeedForward Neural Network LM、RNN LM和GPT系列 本文会涉及到Neural Network LM，其本质是一个语言模型，词向量只是其在训练过程中的一个副产物。 ","date":"2021-07-16","objectID":"/word-embedding/:2:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"特征工程阶段(基于计数的方法) ","date":"2021-07-16","objectID":"/word-embedding/:3:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"One-Hot 最简单的方法是将单词表示为 one-hot 向量：对于词汇表中的第 i 个单词，向量在第 i 个维度上具有 1，在其余维度上具有 0。在机器学习中，这是表示分类特征的最简单方法。 One-Hot的缺点很明显，它对自己代表的词一无所知，没有捕捉到词的意义。 ","date":"2021-07-16","objectID":"/word-embedding/:3:1","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"词袋模型 即无视词语的顺序，只关心出现的次数，以下都属于词袋模型。在文本匹配领域也有类似的字面意义的匹配。本博客有相关内容。 ### TFIDF 权重也可以作为词向量表示。也可以计算文本相似度等，本博客有相关内容。 ","date":"2021-07-16","objectID":"/word-embedding/:3:2","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"共现矩阵 详见本博客共现矩阵 ### PPMI 详见本博客互信息相关博文。 中文名字为正点互信息，公式如下 \\[ PPMI(w, c) = max(0, PMI(w, c)) \\\\\\\\ PMI(w, c) = log\\frac{P(w,c)}{P(w)P(c)} = log \\frac{N(w,c)|(w,c)|}{N(w)N(c)} \\] 事实证明，word2vec被证明可以隐式逼近PMI矩阵的因式分解。 『Neural Word Embedding as Implicit Matrix Factorization』这篇论文就详细讨论了这个问题。 ","date":"2021-07-16","objectID":"/word-embedding/:3:3","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"主题模型 这个更是重量级，见本博客。 ","date":"2021-07-16","objectID":"/word-embedding/:3:4","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"浅层表征阶段(基于推理的方法) ","date":"2021-07-16","objectID":"/word-embedding/:4:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"NNLM 介绍 先来张经典的图片 这就是大名鼎鼎的神经网络语言模型（其它的语言模型详见本博客语言模型部分） 学习任务是输入某个句中单词\\(W_t=i\\)前面句子的t-1个单词，要求网络正确预测单词\\(W_t=i\\)，即最大化： \\[ P(W_t=i | W_1, W_2, \\dots W_{(t-1)}; \\theta) \\] 前面任意单词 \\(W_i\\)用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 \\(C(W_i)\\)，每个单词的 \\(C(W_i)\\) 拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。 这个\\(C(W_i)\\)是什么？ 这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。 所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。 参数解释 训练样本(Context(w), w),w是语料C的每一个词，Context(w)为取其前n-1个词 投影层向量\\(X_w\\): 将该训练样本(Context(w), w)的前n-1个词的词向量首尾拼接在一起，这里的词向量可以用独热编码表示。\\(X_w\\)的形状为 \\((n-1)\\times m\\)，这里的m为词汇表的所有词的个数。 隐藏层向量\\(Z_w\\): \\[ Z_w = tanh(WX_w+p) \\] 输出层向量\\(y_w\\): 维度为N=|D|,即词典D中词的个数。 \\[ y_w = Uz_w + q \\] 在对\\(y_w\\)做softmax后，\\(y_w\\)的分量就表示当前词是w的概率 \\[ p(w|Context(w)) = \\frac{e^{y_w,iw}}{\\sum_{i-1}^N e^{y^{w, i}} } \\] 优点与缺点 NNLM相对于N-grams语言模型，有以下优点： 词语与词语间的相似度可以通过词向量来体现 基于词向量的模型自带『平滑化』功能，无需额外处理。 当然也有缺点，缺点就是计算量太大。 下面就重点介绍一下word2vec，相比于NNLM来说这是专门训练词向量的一种工具，而词向量对于NNLM来说只是一个副产物，其本质还是一个语言模型。 代码 代码来自https://github.com/graykode/nlp-tutorial import torch import torch.nn as nn import torch.optim as optim def make_batch(): input_batch = [] target_batch = [] for sen in sentences: word = sen.split() # space tokenizer input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input target = word_dict[word[-1]] # create (n) as target, We usually call this 'casual language model' input_batch.append(input) target_batch.append(target) return input_batch, target_batch # Model class NNLM(nn.Module): def __init__(self): super(NNLM, self).__init__() self.C = nn.Embedding(n_class, m) self.H = nn.Linear(n_step * m, n_hidden, bias=False) self.d = nn.Parameter(torch.ones(n_hidden)) self.U = nn.Linear(n_hidden, n_class, bias=False) self.W = nn.Linear(n_step * m, n_class, bias=False) self.b = nn.Parameter(torch.ones(n_class)) def forward(self, X): X = self.C(X) # X : [batch_size, n_step, m] X = X.view(-1, n_step * m) # [batch_size, n_step * m] tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden] output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class] return output if __name__ == '__main__': n_step = 2 # number of steps, n-1 in paper n_hidden = 2 # number of hidden size, h in paper m = 2 # embedding size, m in paper sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"] word_list = \" \".join(sentences).split() # ['i', 'like', 'dog', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk'] word_list = list(set(word_list)) # ['i', 'like', 'dog', 'love', 'coffee', 'hate', 'milk'] word_dict = {w: i for i, w in enumerate(word_list)} # {'i':0, 'like':1, 'dog':2, 'love':3, 'coffee':4, 'hate':5, 'milk':6} number_dict = {i: w for i, w in enumerate(word_list)} # {0:'i', 1:'like', 2:'dog', 3:'love', 4:'coffee', 5:'hate', 6:'milk'} n_class = len(word_dict) # number of Vocabulary, just like |V|, in this task n_class=7 model = NNLM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) input_batch, target_batch = make_batch() input_batch = torch.LongTensor(input_batch) target_batch = torch.LongTensor(target_batch) # Training for epoch in range(5000): optimizer.zero_grad() output = model(input_batch) # output : [batch_size, n_class], target_batch : [batch_size] loss = criterion(output, target_batch) if (epoch + 1) % 1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Predict predict = model(input_batch).data.argmax(1, keepdim=True) # Test print([sen.split()[:2] for sen in sentences], '-\u003e', [number_dict[n.item()] for n in predict.squeeze()]) ","date":"2021-07-16","objectID":"/word-embedding/:4:1","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"word2vec Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。 Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。 (skip-gram与CBOW只是word2vec的变体。) 训练思路 Word2Vec 是一个模型，其参数是词向量。这些参数针对某个目标进行迭代优化。目标迫使词向量“知道”一个词可能出现的上下文：训练向量以预测相应词的可能上下文。正如您从分布假设中所记得的那样，如果向量“知道”上下文，它们就“知道”单词的含义。 获取一个巨大的文本语料库； 使用滑动窗口浏览文本，一次移动一个单词。在每一步，都有一个中心词和上下文词（此窗口中的其他词）； 对于中心词，计算上下文词的概率； 调整向量以增加这些概率。 推导 以Skip-grams模型为例，首先清楚几个概念，中心词、背景词（上下文词）、负采样词。中心词就是我们的输入，因为skip-grams相当于在一句话中扣去一个词，然后用这个词预测这句话的其余词。形式上给人的感觉就是一对多，这里的一句话其实不是一句话，是我们设定的窗口大小，比如一句话”I miss you very much”， 设置中心词为you，窗口大小为1，那么背景词就是”miss”和”very”。那么对于我们的模型来说，miss和very就是正例，就是我们的预测值(sigmoid后)的值接近于1的，而其余的词就是负例，就是使其值接近于0的。所以负采样就是从这些负例中随机抽取一些负例，不然每次都要计算单词表中所有单词的sigmoid值，这个计算量很大，而使用负采样就大大缩小了计算量。 目标函数：负似然对数 对于每个位置\\(t=1,\\dots, T\\)，在文本语料库中，Word2Vec在给定中心词的m大小窗口内预测上下文词\\(w_t\\): \\[ Likelihood=L(\\theta)=\\prod_{t=1}^T\\prod_{-m\\leq j\\leq m, j\\neq0} P(w_{t+j} | w_t, \\theta) \\] 目标函数\\(J(\\theta)\\)为平均负对数似然 \\[ Loss = J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta) \\] 对于每个单词w我们都有两个向量： \\(v_w\\)当w是中心词 \\(u_w\\)当w时背景词 因为word2vec是一个专门训练词向量的神经网络，输入为onehot向量，经过第一层W的计算得到隐藏层，就相当于查表，数值为中心词的词向量，然后再与第二层的W计算得到输出，输出的维度和输入的维度相同，因此输出层的结果在计算上就是\\(u_c^Tv_w\\)。即输入层对应的中心词的词向量*输出层对应背景词的词向量，再经过softmax就是对应背景词的概率。 训练完毕后，我们只使用\\(v_w\\)，即只使用从输入层到隐藏层的权重。 对于中心词w，上下文词c的概率为： \\[ P(c|w) = \\frac{exp(u_c^Tv_w)}{\\sum_{o\\in V}exp(u_o^Tv_w)} \\] 这就是softmax函数。 训练 \\(\\theta^{new} = \\theta^{old} - \\alpha \\nabla_\\theta J(\\theta)\\) 一次进行一次更新，每次更新都是针对一对中心词和其中一个背景词。损失函数： \\[ Loss = J(\\theta) = -\\frac{1}{T}logL(\\theta)= -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta)\\\\\\\\=\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0}J_{t,j}(\\theta). \\] 其中\\(J_{t,j}(\\theta) = -logP(w_{t+j}|w_t, \\theta)\\) 以”I miss you very much”这句话为例子，中心词为”you”，其中一个背景词为miss，则损失项为 \\[ J_{t,j}(\\theta) = -logP(miss|you) = -log\\frac{exp(u_{miss}^Tv_{you})}{\\sum_{o\\in V}exp(u_o^Tv_{you})} = \\\\\\\\-u_{miss}^Tv_{you}+log\\sum_{o\\in V} exp(u_o^Tv_{you}) \\] 这是中心词对应其中一个背景词的损失函数，如果要求总的损失，则将所有背景词的损失相加，然后将所有样本的损失求平均，就是上面的Loss。其实这就是交叉熵损失函数，是个多分类问题，预测的target相当于miss对应的数字，也就是序列值，具体的pytorch代码为 loss = (-output_layer[:, Y] + torch.log(torch.sum(torch.exp(output_layer), dim=1))).mean() 这里的output_layer就是神经网络的输出层，注意这里的都是对batch操作，这里求出的loss是这个batch上最终的loss，而不是一对中心词与背景词的loss。理解了这个简单的word2vec模型就算理解了。 下面求一下梯度： 注意这里的c为center word, o为上下文词 \\[ \\frac{\\partial logP(w_o|w_c)}{\\partial v_c} = \\frac{\\partial}{\\partial v_c}log\\frac{exp(u_0^Tv_c)}{\\sum_{i=1}^{|V|}exp(u_i^Tv_c)}\\\\\\\\= \\frac{\\partial}{\\partial v_c}log\\, exp(u_o^Tv_c) - \\frac{\\partial}{\\partial v_c}log\\sum_{i=1}^{|V|}exp(u_i^Tv_c) \\] 左边的： \\[ \\frac{\\partial}{\\partial v_c}log\\, exp(u_o^Tv_c)=\\frac{\\partial}{\\partial v_c}u_o^Tv_c=u_o \\] 第二部分推导 \\[ \\begin{aligned} \\frac{\\partial}{\\partial {v}_{c}} \\log \\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right) \u0026=\\frac{1}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} \\cdot \\frac{\\partial}{\\partial {v}_{c}} \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\\\\\\\ \u0026=\\frac{1}{A} \\cdot \\sum_{x=1}^{|V|} \\frac{\\partial}{\\partial {v}_{c}} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\\\\\\\ \u0026=\\frac{1}{A} \\cdot \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\frac{\\partial}{\\partial {v}_{c}} {u}_{x}^{T} {v}_{c} \\\\\\\\ \u0026=\\frac{1}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) {u}_{x} \\\\\\\\ \u0026=\\sum_{x=1}^{|V|} \\frac{\\exp \\left({u}_{x}^{T} {v}_{c}\\right)}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} {u}_{x} \\\\\\\\ \u0026=\\sum_{x=1}^{|V|} P\\left(w_{x} \\mid w_{c}\\right) {u}_{x} \\end{aligned} \\] 综上所述 \\[ \\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial {v}_{c}}={u}_{o}-\\sum_{j \\in V} P\\left(w_{j} \\mid w_{c}\\right) {u}_{j} \\] 通过上面计算得到梯度后，我们可以使用随机梯度下降来不断迭代模型参数\\(v_c\\)","date":"2021-07-16","objectID":"/word-embedding/:4:2","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"GloVe GloVe 模型是基于计数的方法和预测方法（例如 Word2Vec）的组合。模型名称 GloVe 代表“Global Vectors”，体现了它的思想：该方法利用 语料库中的全局信息来学习向量。 正如我们之前看到的，最简单的基于计数的方法使用共现计数来衡量单词 w 和上下文c之间的关联：\\(N(w,c)\\)。 GloVe 也使用这些计数来构建损失函数： \\[ J(\\theta) = \\sum_{w,c\\in V}f(N(w,c)) \\cdot(u_c^Tv_w+b_c+\\bar{b_w}-logN(w,c)^2) \\] 其中： \\[ f(x) = \\begin{cases} (\\frac{x}{x_{max}})^{0.75}, \\quad if \\;x \u003c x_{max} \\\\\\\\ 1, \\quad if \\; x\\geq x_{max} \\end{cases} \\] 具体的推导可以看这个博客：https://www.cnblogs.com/Lee-yl/p/11172255.html 可以看到GloVe并没有使用神经网络的方法。 与 Word2Vec 类似，我们也有不同的 中心词和上下文词向量——这些是我们的参数。此外，该方法对每个词向量都有一个标量偏置项。 特别有趣的是 GloVe 控制稀有词和频繁词影响的方式：每对 ( w , c ) 的损失以如下方式加权 罕见事件受到惩罚， 非常频繁的事件不会被过度加权。 ","date":"2021-07-16","objectID":"/word-embedding/:4:3","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"关于word embedding word embedding只能通过语言模型的训练获取吗？ 不是的；事实上任何NLP任务都可以在训练过程中获取word embedding，甚至特定任务下的word embedding在特定任务中的使用效果还会更好（如基于fasttext的文本分类任务）。之所以我们平时提及的word embedding均是在语言模型任务中产生，私以为主要有这么几个原因：a).语言模型是无监督任务，存在海量训练语料，无需标注成本；b).语言模型任务本身要求较高，训练过程中可以学习大量的语义知识，进而生成高质的word representation。 如何评估word embedding好坏？ 有两种方式：第一种，把word embedding融入现有系统中，看其对系统性能的提升；第二，从语言学的角度对word embedding进行分析，如相似度、语义偏移等。更细节的可以参考这里。 ## 参考 参考： https://blog.csdn.net/malefactor/article/details/83961886 https://www.zybuluo.com/Dounm/note/591752 https://lena-voita.github.io/nlp_course/word_embeddings.html https://wmathor.com/index.php/archives/1430/ https://zhuanlan.zhihu.com/p/27234078 Rong X . word2vec Parameter Learning Explained[J]. Computer Science, 2014. ","date":"2021-07-16","objectID":"/word-embedding/:5:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["算法题"],"content":"三数之和 ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:0:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/3sum/solution/ ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:1:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"思路： ​ 第一眼看就想到了用双指针，注意重复数值的处理问题，算是一个滑动窗口问题 ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:2:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"代码： class Solution: def threeSum(self, nums: List[int]) -\u003e List[List[int]]: res = [] if len(nums) \u003c 3: return [] nums.sort() for i, num in enumerate(nums): if num \u003e 0: return res if i \u003e 0 and nums[i] == nums[i-1]: continue left, right = i+1, len(nums) - 1 while left \u003c right: temp = nums[i] + nums[left] + nums[right] if temp == 0: res.append([nums[i], nums[left], nums[right]]) while left \u003c right and nums[right-1] == nums[right]: right -= 1 while left \u003c right and nums[left+1] == nums[left]: left += 1 left += 1 right -= 1 if temp \u003e 0: right -=1 if temp \u003c 0: left += 1 return res ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:3:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["Machine Learning","性能指标"],"content":"# ROC曲线 了解什么是ROC曲线和AUC之前，要先了解什么是混淆矩阵。 混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下： 称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。 预测正确的为True（真），预测错误的为False（伪）。 因此有了True Postive Rate、False Postive Rate两个概念 \\(TPRate = \\frac{TP}{TP+FN}\\) \\(FPRate = \\frac{FP}{FP+TN}\\) TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。 FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。 按照定义，AUC即ROC曲线下的面积，而ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图: 这样的分类器和瞎猜没啥区别，我们可以看成AUC的最小值为0.5。 而我们希望分类器达到的效果是：对于真实类别为1的样本，分类器预测为1的概率（即TPRate），要大于真实类别为0而预测类别为1的概率（即FPRate），即y＞x。这是理所当然的，分类器肯定要分对的嘛。 我们知道，在二分类（0，1）的模型中，一般我们最后的输出是一个概率值，表示结果是1的概率。那么我们最后怎么决定输入的x是属于0或1呢？我们需要一个 阈值，超过这个阈值则归类为1，低于这个阈值就归类为0。 所以，不同的阈值会导致分类的结果不同，也就是混淆矩阵不一样了，FPR和TPR也就不一样了。所以当阈值从0开始慢慢移动到1的过程，就会形成很多对(FPR, TPR)的值，将它们画在坐标系上，就是所谓的ROC曲线了。 看这张图： 当阈值选取为0.5时，阈值右边的视为 预测为正例 ，阈值左边的视为 预测为负例 。由于准确度为\\(\\frac{TP+TN}{all}\\)，因此可得此时的准确度为90%。 看这张图： 阈值设定为0.6，即右边视为 预测为正例 ，红色的为实际为正例，蓝色的为实际为负例，因此很容易得到FP=0，因为阈值右边没有蓝色区域，可以这么理解。 当蓝色区域与红色区域基本重叠时，ROC曲线就和接近y=x这条线了。 其实，AUC表示的是正例排在负例前面的概率 我们知道阈值可以取不同，也就是说，分类的结果会受到阈值的影响。如果使用AUC的话，因为阈值变动考虑到了，所以评估的效果更好。 最后说说AUC的优势，AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。 ","date":"2021-06-26","objectID":"/roc%E6%9B%B2%E7%BA%BF/:1:0","tags":["Machine Learning","性能指标","ROC曲线"],"title":"ROC曲线","uri":"/roc%E6%9B%B2%E7%BA%BF/"},{"categories":["Machine Learning","性能指标"],"content":"AUC计算 ","date":"2021-06-26","objectID":"/roc%E6%9B%B2%E7%BA%BF/:2:0","tags":["Machine Learning","性能指标","ROC曲线"],"title":"ROC曲线","uri":"/roc%E6%9B%B2%E7%BA%BF/"},{"categories":["算法题"],"content":"删除排序数组中的重复项 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:0:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"删除排序数组中的重复项1 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/ ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:1","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"思路： ​ 双指针，定义 nums[0...i] 为为非重复数列，遍历整个数列不断的维护这个定义 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:2","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"代码： ​ class Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: start = 0 for i in range(len(nums)): if nums[i] != nums[start]: start += 1 nums[i],nums[start] = nums[start],nums[i] return start + 1 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:3","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"删除排序数组中的重复项2 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii/ ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:1","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"思路： ​ 也是利用双指针，一个指针用于遍历数组元素，一个指针指向要拷贝赋值的索引位置 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:2","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"代码： ​ class Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: if len(nums) \u003c= 2: #长度小于等于2时 return len(nums) count = 1 #用于重复的计数 j = 1 #指向多余重复的元素 for i in range(1,len(nums)): if nums[i] == nums[i-1]: count += 1 #重复了就加一 if count \u003e 2: #如果重复两次以上就pass掉，等着被替换 pass else: nums[j] = nums[i] j += 1 else: nums[j] = nums[i] #如果不相等了 把多余重复的那个替换掉了 count = 1 #重置计数 j += 1 return j 这是一种思路比较清晰的写法 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:3","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["python"],"content":"import threading import time ","date":"2021-06-13","objectID":"/asyncio/:0:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"多线程例子 def spider(): #..... time.sleep(0.02) def main1(): for i in range(100): spider() def main2(): thread_list = [] for i in range(100): thread = threading.Thread(target = spider) thread.start() thread_list.append(thread) for t in thread_list: t.join() if __name__ == \"__main__\": start = time.time() main1() end = time.time() print(\"time1 :{:.4f}\".format(end-start)) start = time.time() main2() end = time.time() print(\"time2 :{:4f}\".format(end-start)) time1 :2.0523 time2 :0.037929 ","date":"2021-06-13","objectID":"/asyncio/:1:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"yield def fib(n): a,b = 0,1 while b\u003cn: a,b = b,a+b yield a print(fib(100)) for i in fib(100): print(i) \u003cgenerator object fib at 0x000002B1A7AA1E60\u003e 1 1 2 3 5 8 13 21 34 55 89 ","date":"2021-06-13","objectID":"/asyncio/:2:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"协程 GEN_CREATED 创建完成，等待执行 GEN_RUNNING 解释器正在执行 GEN_SUSPENDED 在 yield 表达式处暂停 GEN_CLOSE 执行结束，生成器停止 import inspect def generator(): i = \"激活生成器\" while True: try: value = yield i except ValueError: print(\"OVER\") i = value g = generator() print(inspect.getgeneratorstate(g)) #查看状态 next(g) # next(g)相当于g.send(None) 可以用后面的语句来预缴携程 GEN_CREATED '激活生成器' inspect.getgeneratorstate(g) #查看生成器状态 'GEN_SUSPENDED' g.send(\"hello world\") 'hello world' 暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值，也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i g.throw(ValueError) #抛出异常 结束 OVER 'hello world' g.close() inspect.getgeneratorstate(g) #关闭了 'GEN_CLOSED' ","date":"2021-06-13","objectID":"/asyncio/:3:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"预激协程 from functools import wraps def corcut(func): @wraps(func) def wrapper(*args,**kw): g = func(*args,**kw) next(g) return g return wrapper @corcut #装饰器 def generator(): i = \"激活生成器\" while True: try: value = yield i except ValueError: print(\"OVER\") i = value g = generator() print(inspect.getgeneratorstate(g)) #此时已经用装饰器将生成器激活了 GEN_SUSPENDED @corcut def generator(): l = [] while True: value = yield if value == \"CLOSE\": break l.append(value) return l g = generator() for i in ['a','b','CLOSE']: try: g.send(i) except StopIteration as e: value = e.value value ['a', 'b'] ","date":"2021-06-13","objectID":"/asyncio/:4:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"yield from用法 from itertools import chain c = chain({'one','two','three'},list(\"abc\")) for i in c: print(i) three two one a b c def chains1(*args): for i in args: for n in i: yield n def chains2(*args): for i in args: yield from i #i为可迭代对象，避免嵌套循环 c1 = chains1({\"one\",\"two\",\"three\"},list(\"abc\")) for i in c1: print(i) print(\"\\n\") c2 = chains2({\"one\",\"two\",\"three\"},list(\"abc\")) for i in c2: print(i) three two one a b c three two one a b c ","date":"2021-06-13","objectID":"/asyncio/:5:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"转移控制权 from functools import wraps from faker import Faker import time def corout(func): @wraps(func) def wapper(*args,**kw): g = func(*args,**kw) next(g) return g return wapper # 子生成器 def generator(): l = [] while True: i = yield if i == \"CLOSE\": break l.append(i) return sorted(l) # 委托生成器 @corout def generator2(): while True: l = yield from generator() print(\"排序后的列表\",l) print(\"-----------------\") # 客户端 if __name__ == \"__main__\": fake = Faker().country_code nest_country = [[fake() for i in range(3)] for j in range(3)] for country in nest_country: print('国家代号列表：', country) c = generator2() for i in country: c.send(i) c.send(\"CLOSE\") 国家代号列表： ['AM', 'ZA', 'BG'] 排序后的列表 ['AM', 'BG', 'ZA'] ----------------- 国家代号列表： ['UG', 'BE', 'SI'] 排序后的列表 ['BE', 'SI', 'UG'] ----------------- 国家代号列表： ['SC', 'KI', 'KI'] 排序后的列表 ['KI', 'KI', 'SC'] ----------------- yield显然不只是用来减小循环次数的，引用一下《流畅的python》中关于yield from 的意义： - 子生成器产出的值都直接传给委派生成器的调用方（即客户端代码）。 - 使用 send() 方法发给委派生成器的值都直接传给子生成器。如果发送的值是 None，那么会调用子生成器的 next() 方法。如果发送的值不是 None，那么会调用子生成器的 send() 方法。如果调用的方法抛出 StopIteration 异常，那么委派生成器恢复运行。任何其他异常都会向上冒泡，传给委派生成器。 - 生成器退出时，生成器（或子生成器）中的 return expr 表达式会触发 StopIteration(expr) 异常抛出。 - yield from 表达式的值是子生成器终止时传给 StopIteration异常的第一个参数。 为什么yield可以转移控制权，可以看一下这一段伪代码： 注意这里的6是委托生成器向子生成器发送_s，而_s是调用方向委托生成器发送的，发送后得到结果_y，并在下一个循环yield即抛出给调用方。 ## asyncio模块 import time import asyncio def one(): start = time.time() @asyncio.coroutine #1 def do_something(): #2 print(\"start ------\") time.sleep(0.1) #3 print(\"doing something\") loop = asyncio.get_event_loop() #4 coroutine = do_something() #5 loop.run_until_complete(coroutine) #6 end = time.time() print(\"消耗时间:{:.4f}\".format(end-start))#7 one() start ------ doing something 消耗时间:0.1012 代码说明： 1、使用协程装饰器创建协程函数 2、协程函数 3、模拟 IO 操作 4、创建事件循环。每个线程中只能有一个事件循环，get_event_loop 方法会获取当前已经存在的事件循环，如果当前线程中没有，新建一个 5、调用协程函数获取协程对象 6、将协程对象注入到事件循环，协程的运行由事件循环控制。事件循环的 run_until_complete 方法会阻塞运行，直到任务全部完成。协程对象作为 run_until_complete 方法的参数，loop 会自动将协程对象包装成任务来运行。后面我们会讲到多个任务注入事件循环的情况 7、打印程序运行耗时 import time import asyncio def two(): start = time.time() @asyncio.coroutine def do_something(): print(\"start ------\") time.sleep(0.1) print(\"doing something\") loop = asyncio.get_event_loop() coroutine = do_something() task = loop.create_task(coroutine) #1 print(\"task是不是Task的示例？\",isinstance(task,asyncio.Task)) #2 print(\"task state\",task._state) #3 loop.run_until_complete(task) #4 print(\"take state\",task._state) end = time.time() print(\"消耗时间:{:.4f}\".format(end-start)) two() task是不是Task的示例？ True task state PENDING start ------ doing something take state FINISHED 消耗时间:0.1013 1、事件循环的 create_task 方法可以创建任务，另外 asyncio.ensure_future 方法也可以创建任务，参数须为协程对象 2、task 是 asyncio.Task 类的实例，为什么要使用协程对象创建任务？因为在这个过程中 asyncio.Task 做了一些工作，包括预激协程、协程运行中遇到某些异常时的处理 **3、task 对象的 _state 属性保存当前任务的运行状态，任务的运行状态有 PENDING 和 FINISHED 两种** 4、将任务注入事件循环，阻塞运行 ","date":"2021-06-13","objectID":"/asyncio/:6:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"async / await import functools def three(): start = time.time() #@asyncio.coroutine async def do_something(): #1 print(\"start doing\") time.sleep(0.1) print(\"done\") def callback(name,task): #2 print(\"call back:{}\".format(name)) print(\"call back:{}\".format(task._state)) loop = asyncio.get_event_loop() coroutine = do_something() task = loop.create_task(coroutine) task.add_done_callback(functools.partial(callback, 'vllbc')) #3 loop.run_until_complete(task) end = time.time() print(\"total time {:.4f}\".format(end-start)) three() start doing done call back:vllbc call back:FINISHED total time 0.1013 代码说明： 1、使用 async 关键字替代 asyncio.coroutine 装饰器创建协程函数 2、回调函数，协程终止后需要顺便运行的代码写入这里，回调函数的参数有要求，最后一个位置参数须为 task 对象 3、task 对象的 add_done_callback 方法可以添加回调函数，注意参数必须是回调函数，这个方法不能传入回调函数的参数，这一点需要通过 functools 模块的 partial 方法解决，将回调函数和其参数 name 作为 partial 方法的参数，此方法的返回值就是偏函数，偏函数可作为 task.add_done_callback 方法的参数 def four(): start = time.time() async def do_something(name,t): print(\"start !\u003e\u003e\",name) await asyncio.sleep(t) #1 print('Stop coroutine', name) return 'Coroutine {} OK'.format(name) #2 loop = asyncio.get_event_loop() coroutine1 = do_something('wlb',3) #3 coroutine2 = do_something('yyh',1) task1 = loop.create_task(coroutine1) #4 task2 = loop.create_task(coroutine2) gather = asyncio.gather(task1,task2) #5 loop.run_until_complete(gather) print(\"task1\",task1.result()) print(\"task2\",task2.result()) #result = loop.run_until_complete(gather) #这里result就是两个返回值组成的列表 即['task1 Coroutine wlb OK','task2 Coroutine yyh OK'] end = time.time() print(\"total time:{:.4f}\".format(end-start)) four() start !\u003e\u003e wlb start !\u003e\u003e yyh Stop coroutine yyh Stop coroutine wlb task1 Coroutine wlb OK task2 Coroutine yyh OK total time:3.0022 代码说明： 1、await 关键字等同于 Python 3.4 中的 yield from 语句，后面接协程对象。asyncio.sleep 方法的返回值为协程对象，这一步为阻塞运行。asyncio.sleep 与 time.sleep 是不同的，前者阻塞当前协程，即 corowork 函数的运行，而 time.sleep 会阻塞整个线程，所以这里必须用前者，阻塞当前协程，CPU 可以在线程内的其它协程中执行 2、协程函数的 return 值可以在协程运行结束后保存到对应的 task 对象的 result 方法中 3、创建两个协程对象，在协程内部分别阻塞 3 秒和 1 秒 4、创建两个任务对象 5、将任务对象作为参数，asyncio.gather 方法创建任务收集器。注意，asyncio.gather 方法中参数的顺序决定了协程的启动顺序 6、将任务收集器作为参数传入事件循环的 run_until_complete 方法，阻塞运行，直到全部任务完成 7、任务结束后，事件循环停止，打印任务的 result 方法返回值，即协程函数的 return 值 到这一步，大家应该可以看得出，上面的代码已经是异步编程的结构了，在事件循环内部，两个协程是交替运行完成的。简单叙述一下程序协程部分的运行过程： -\u003e 首先运行 task1 -\u003e 打印 [corowork] Start coroutine ONE -\u003e 遇到 asyncio.sleep 阻塞 -\u003e 释放 CPU 转到 task2 中执行 -\u003e 打印 [corowork] Start coroutine TWO -\u003e 再次遇到 asyncio.sleep 阻塞 -\u003e 这次没有其它协程可以运行了，只能等阻塞结束 -\u003e task2 的阻塞时间较短，阻塞 1 秒后先结束，打印 [corowork] Stop coroutine TWO -\u003e 又过了 2 秒，阻塞 3 秒的 task1 也结束了阻塞，打印 [corowork] Stop coroutine ONE -\u003e 至此两个任务全部完成，事件循环停止 -\u003e 打印两个任务的 result -\u003e 打印程序运行时间 -\u003e 程序全部结束 ","date":"2021-06-13","objectID":"/asyncio/:7:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"await的理解 从上文中也可以看到await其实是从yield from中转变过来的，当在代码中看到await时，可以知道当前协程要去运行await后面的任务，此时控制权回到了event loop手中，去执行其它的任务，当前面的任务完成了以后，则转去执行前面await后面的代码。注意当await直接跟一个coroutline时，此时相当于去yield from，会卡在那里，并不会实现真正的异步，所以要先将coroutline变为task或者future就可以直接await。 ## 异步编程 ","date":"2021-06-13","objectID":"/asyncio/:7:1","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"一个买土豆的例子 import asyncio import random # potato类 class Potato: # 生成土豆 @classmethod def make(cls, num, *args, **kws): potatos = [] for i in range(num): potatos.append(cls.__new__(cls, *args, **kws)) return potatos all_potatos = Potato.make(5) ## 这是一个异步生成器，可以用async for迭代，nums为想买的数量。 async def take_photos(nums): count = 0 while True: # 如果没有土豆了，挂起当前任务请求生成土豆任务。 if len(all_potatos) == 0: await askfor_photos() else: photo = all_potatos.pop() # 如果有土豆将土豆抛出去 yield photo count += 1 if count == nums : break async def askfor_photos(): await asyncio.sleep(2) all_potatos.append(Potato.make(5)) async def buy_photos(): bucket = [] async for p in take_photos(50): bucket.append(p) print(f\"Go photo {id(p)}\") loop = asyncio.get_event_loop() loop.run_until_complete(buy_photos()) ","date":"2021-06-13","objectID":"/asyncio/:7:2","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"requests例子 import asyncio import requests import time # 相当于委托生成器 async def result(url): res = await request_url(url) print(url, res) # 相当于子生成器 async def request_url(url): res = requests.get(url) print(url) await asyncio.sleep(2) print(\"execute_time:\", time.time() - start) return res url_list = [\"https://www.csdn.net/\", \"https://vllbc.top/\", \"https://www.baidu.com/\", ] # 以下相当于调用方 start = time.time() print(f\"start_time:{start}\\n\") task = [result(url) for url in url_list] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(task)) endtime = time.time() - start print(\"\\nendtime:\", time.time()) print(\"all_execute_time:\", endtime) ","date":"2021-06-13","objectID":"/asyncio/:7:3","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["算法题"],"content":"移除元素 还是以前刷过的题 https://leetcode-cn.com/problems/remove-element/ 以前的思路早忘了 然后我重新做了一下，一开始就一行代码 class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: return len(list(filter(lambda x:x!=val,nums))) ​ 然后发现输出和正确输出不一样。于是看了了下面的提示，然后改了改 class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: for i in range(nums.count(val)): nums.remove(val) return len(nums) 但这算不上叫算法，利用双指针做法如下： class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: left = 0 for i in range(len(nums)): if nums[i] != val: nums[left], nums[i] = nums[i], nums[left] left += 1 return left ","date":"2021-06-12","objectID":"/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/:0:0","tags":["算法题","移除元素"],"title":"移除元素","uri":"/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/"},{"categories":["sklearn"],"content":"import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset = pd.read_csv('./datasets/Social_Network_Ads.csv') X = dataset.iloc[:, [2, 3]].values y = dataset.iloc[:, 4].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) from sklearn.tree import DecisionTreeClassifier classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) classifier.fit(X_train, y_train) DecisionTreeClassifier(criterion='entropy', random_state=0) classifier.score(X_test,y_test) 0.91 ","date":"2021-05-25","objectID":"/decision-tree/:0:0","tags":["sklearn","Decision Tree"],"title":"Decision Tree","uri":"/decision-tree/"},{"categories":["NLP"],"content":"参考：https://blog.csdn.net/asialee_bird/article/details/96894533 TextRank算法是一种基于图的用于关键词抽取和文档摘要的排序算法，由谷歌的网页重要性排序算法PageRank算法改进而来，它利用一篇文档内部的词语间的共现信息(语义)便可以抽取关键词，它能够从一个给定的文本中抽取出该文本的关键词、关键词组，并使用抽取式的自动文摘方法抽取出该文本的关键句。 ","date":"2021-05-20","objectID":"/textrank/:0:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"PageRank算法 PageRank算法通过计算网页链接的数量和质量来粗略估计网页的重要性，算法创立之初即应用在谷歌的搜索引擎中，对网页进行排名。 PageRank算法的核心思想如下： （1）链接数量：如果一个网页被越多的其他网页链接，说明这个网页越重要，即该网页的PR值（PageRank值）会相对较高； （2）链接质量：如果一个网页被一个越高权值的网页链接，也能表明这个网页越重要，即一个PR值很高的网页链接到一个其他网页，那么被链接到的网页的PR值会相应地因此而提高。 ","date":"2021-05-20","objectID":"/textrank/:1:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"TextRank算法 TextRank算法的基本思想是将文档看作一个词的网络，该网络中的链接表示词与词之间的语义关系。 TextRank算法主要包括：关键词抽取、关键短语抽取、关键句抽取。 ","date":"2021-05-20","objectID":"/textrank/:2:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（1）关键词抽取（keyword extraction） 关键词抽取是指从文本中确定一些能够描述文档含义的术语的过程。对关键词抽取而言，用于构建顶点集的文本单元可以是句子中的一个或多个字；根据这些字之间的关系（比如：在一个框中同时出现）构建边。根据任务的需要，可以使用语法过滤器（syntactic filters）对顶点集进行优化。语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集。 ","date":"2021-05-20","objectID":"/textrank/:2:1","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（2）关键短语抽取（keyphrase extration） 关键词抽取结束后，我们可以得到的N个关键词，在原始文本中相邻的关键词构成关键短语。因此，从get_keyphrases函数的源码中我们可以看到，它先调用get_keywords抽取关键词，然后分析关键词是否存在相邻的情况，最后确定哪些是关键短语。 ","date":"2021-05-20","objectID":"/textrank/:2:2","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（3）关键句抽取（sentence extraction） 句子抽取任务主要针对的是自动摘要这个场景，将每一个sentence作为一个顶点，根据两个句子之间的内容重复程度来计算他们之间的“相似度”，以这个相似度作为联系，由于不同句子之间相似度大小不一致，在这个场景下构建的是以相似度大小作为edge权重的有权图。 ","date":"2021-05-20","objectID":"/textrank/:2:3","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["python"],"content":"matplotlib.pyplot学习 ","date":"2021-05-02","objectID":"/pyplot/:0:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"绘图标记 import matplotlib.pyplot as plt import numpy as np ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4]) plt.plot(ypoints, marker = 'o') # \"o\"代表实心圆 plt.show() maker可用的符号如下： ","date":"2021-05-02","objectID":"/pyplot/:1:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"fmt参数 fmt = '[marker][line][color]' 例如 o:r，o 表示实心圆标记，: 表示虚线，r 表示颜色为红色。 ","date":"2021-05-02","objectID":"/pyplot/:2:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线类型 线的类型可以使用 linestyle 参数来定义，简写为 ls。 ","date":"2021-05-02","objectID":"/pyplot/:3:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线的宽度 线的宽度可以使用 linewidth 参数来定义，简写为 lw，值可以是浮点数，如：1、2.0、5.67 等。 ","date":"2021-05-02","objectID":"/pyplot/:3:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"颜色类型 ","date":"2021-05-02","objectID":"/pyplot/:4:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线的颜色 线的颜色可以使用 color 参数来定义，简写为 c。 ","date":"2021-05-02","objectID":"/pyplot/:4:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"标记大小与颜色 我们可以自定义标记的大小与颜色，使用的参数分别是： markersize，简写为 ms：定义标记的大小。 markerfacecolor，简写为 mfc：定义标记内部的颜色。 markeredgecolor，简写为 mec：定义标记边框的颜色。 import matplotlib.pyplot as plt import numpy as np ypoints = np.array([6, 2, 13, 10]) plt.plot(ypoints, marker = 'o', ms = 20) plt.show() ","date":"2021-05-02","objectID":"/pyplot/:4:2","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 轴标签和标题 我们可以使用 xlabel() 和 ylabel() 方法来设置 x 轴和 y 轴的标签。 import numpy as np import matplotlib.pyplot as plt x = np.array([1, 2, 3, 4]) y = np.array([1, 4, 9, 16]) plt.plot(x, y) plt.xlabel(\"x - label\") plt.ylabel(\"y - label\") plt.show() ","date":"2021-05-02","objectID":"/pyplot/:5:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"标题 我们可以使用 title() 方法来设置标题 也可以自定义字体样式，通过传入fontdict参数 import matplotlib.pyplot as plt import numpy as np font = {\"color\":\"blue\",\"size\":20} ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4]) plt.plot(ypoints,marker=\"o\",color=\"r\",linestyle=\"-.\") plt.title(\"test\",fontdict=font) plt.xlabel(\"x\",fontdict=font) plt.ylabel(\"y\",fontdict=font) plt.show() ","date":"2021-05-02","objectID":"/pyplot/:5:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 网格线 我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。 grid() 方法语法格式如下： matplotlib.pyplot.grid(b=None, which='major', axis='both', ) 参数说明： b：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。 which：可选，可选值有 ‘major’、‘minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。 axis：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），‘x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。 **kwargs：可选，设置网格样式，可以是 color=‘r’, linestyle=‘-’ 和 linewidth=2，分别表示网格线的颜色，样式和宽度。 以下实例添加一个简单的网格线，并设置网格线的样式，格式如下： grid(color = 'color', linestyle = 'linestyle', linewidth = number) 参数说明： color：’b’ 蓝色，‘m’ 洋红色，‘g’ 绿色，‘y’ 黄色，‘r’ 红色，‘k’ 黑色，‘w’ 白色，‘c’ 青绿色，‘#008000’ RGB 颜色符串。 linestyle：’‐’ 实线，‘‐‐’ 破折线，‘‐.’ 点划线，‘:’ 虚线。 linewidth：设置线的宽度，可以设置一个数字。 ","date":"2021-05-02","objectID":"/pyplot/:6:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 绘制多图 我们可以使用 pyplot 中的 subplot() 和 subplots() 方法来绘制多个子图。 subpot() 方法在绘图时需要指定位置，subplots() 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。 ","date":"2021-05-02","objectID":"/pyplot/:7:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"subplot subplot(nrows, ncols, index, **kwargs) subplot(pos, **kwargs) subplot(**kwargs) subplot(ax) 以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 1…N ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 index 来设置。 设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为： (1, 1), (1, 2) 设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为： (1, 1), (1, 2) (2, 1), (2, 2) ","date":"2021-05-02","objectID":"/pyplot/:7:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"subplots() subplots() 方法语法格式如下： matplotlib.pyplot.subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) 参数说明： nrows：默认为 1，设置图表的行数。 ncols：默认为 1，设置图表的列数。 sharex、sharey：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、‘all’、‘row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，‘row’ 设置每个子图行共享一个 x 轴或 y 轴，‘col’：设置每个子图列共享一个 x 轴或 y 轴。 squeeze：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N1 或 1N 个子图，返回一个 1 维数组，对于 N*M，N\u003e1 和 M\u003e1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。 subplot_kw：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。 gridspec_kw：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。 **fig_kw：把详细的关键字参数传给 figure() 函数。 常用技巧 x = np.linspace(0, 2*np.pi, 400) y = np.sin(x**2) fig, axs = plt.subplots(2, 2) for i in axs.flatten(): # axs.flatten()将二维数组变为一维，方便循环。 i.plot(x, y) ","date":"2021-05-02","objectID":"/pyplot/:7:2","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 散点图 我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。 scatter() 方法语法格式如下： matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs) 参数说明： x，y：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。 s：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。 c：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。 marker：点的样式，默认小圆圈 ‘o’。 cmap：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。 norm：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。 vmin，vmax：：亮度设置，在 norm 参数存在时会忽略。 alpha：：透明度设置，0-1 之间，默认 None，即不透明。 linewidths：：标记点的长度。 edgecolors：：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。 plotnonfinite：：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。 **kwargs：：其他参数。 设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组。 如果要显示颜色条，需要使用 plt.colorbar() 方法： ","date":"2021-05-02","objectID":"/pyplot/:8:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 柱形图 我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。 bar() 方法语法格式如下： matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs) 参数说明： x：浮点型数组，柱形图的 x 轴数据。 height：浮点型数组，柱形图的高度。 width：浮点型数组，柱形图的宽度。 bottom：浮点型数组，底座的 y 坐标，默认 0。 align：柱形图与 x 坐标的对齐方式，‘center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align=‘edge’。 **kwargs：：其他参数。 垂直方向的柱形图可以使用 barh() 方法来设置： ","date":"2021-05-02","objectID":"/pyplot/:9:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 饼图 我们可以使用 pyplot 中的 pie() 方法来绘制饼图。 pie() 方法语法格式如下： matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0, frame=False, rotatelabels=False, *, normalize=None, data=None)[source] 参数说明： x：浮点型数组，表示每个扇形的面积。 explode：数组，表示各个扇形之间的间隔，默认值为0。 labels：列表，各个扇形的标签，默认值为 None。 colors：数组，表示各个扇形的颜色，默认值为 None。 autopct：设置饼图内各个扇形百分比显示格式，%d%% 整数百分比，%0.1f 一位小数， %0.1f%% 一位小数百分比， %0.2f%% 两位小数百分比。 labeldistance：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 \u003c1则绘制在饼图内侧。 pctdistance：：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。 shadow：：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。 radius：：设置饼图的半径，默认为 1。 startangle：：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 =90 则从 y 轴正方向画起。 counterclock：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。 wedgeprops ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops={‘linewidth’:5} 设置 wedge 线宽为5。 textprops ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。 center ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。 frame ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。 rotatelabels ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。 ","date":"2021-05-02","objectID":"/pyplot/:10:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["Machine Learning"],"content":"范数的定义 \\[ {\\lVert x \\rVert}_p := \\left(\\sum_{i=1}^n{\\lvert x_i\\rvert}^p\\right)^{\\frac{1}{p}} \\] ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:1:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"标准化与归一化 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"定义 归一化和标准化都是对数据做变换的方式，将原始的一列数据转换到某个范围，或者某种形态。 \u003e归一化：将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]； 标准化：将数据变换为均值为0，标准差为1的分布。切记，并非一定是正态的； 中心化：还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:1","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"差异 归一化会严格限定变换后的数据的范围，标准化没有严格的区间，变换后的数据没有范围，只是其均值为0，标注差为1. 归一化对数据的缩放比例只与极值有关，而标准化缩放比例与所有的数据都有关。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:2","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"标准化、归一化的好处 统计建模中，如回归模型，自变量\\(X\\)的量纲不一致导致了回归系数无法直接解读或者错误解读；需要将\\(X\\)都处理到统一量纲下，这样才可比； 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果； 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:3","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"决策边界 所谓决策边界就是能够把样本正确分类的一条边界，主要有线性决策边界(linear decision boundaries)和非线性决策边界(non-linear decision boundaries)。注意：决策边界是假设函数的属性，由参数决定，而不是由数据集的特征决定。下面主要举一些例子，形象化的来说明线性决策边界和非线性决策边界。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:3:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"方差与偏差 当我们的模型表现不佳时，通常是出现两种问题，一种是 高偏差 问题，另一种是 高方差 问题。识别它们有助于选择正确的优化方式，所以我们先来看下 偏差 与 方差 的意义。 偏差: 描述模型输出结果的期望与样本真实结果的差距。 方差: 描述模型对于给定值的输出稳定性。 就像打靶一样，偏差描述了我们的射击总体是否偏离了我们的目标，而方差描述了射击准不准。接下来让我们通过各种情况下 训练集 和 交叉验证集 的 误差 曲线来直观地理解 高偏差 与 高方差 的意义。 对于 多项式回归，当次数选取较低时，我们的 训练集误差 和 交叉验证集误差 都会很大；当次数选择刚好时，训练集误差 和 交叉验证集误差 都很小；当次数过大时会产生过拟合，虽然 训练集误差 很小，但 交叉验证集误差 会很大（ 关系图如下 ） 对于 正则化 参数，使用同样的分析方法，当参数比较小时容易产生过拟合现象，也就是高方差问题。而参数比较大时容易产生欠拟合现象，也就是高偏差问题。 偏差和方差与数据噪声之和就是模型的泛化能力 模型的期望预测（这里x指所有的样本，期望预测为该模型的所有预测结果的期望。也可以表示有多个模型同时对x一个样本进行预测，期望预测为所有模型预测的期望）： 样本数相同的不同训练集产生的方差（可以理解为测试集预测结果与训练集输出期望之间的方差，也可以直接理解为一个模型中所有的预测与预测期望之间的平方差）： 噪声（这里的噪声为人工标注的错误。）： 期望输出与真实标记的差别称为偏差(也有两种理解，一种是多模型的预测期望与真实值之间的偏差，还有一种就直接是单模型的预测输出（因为单模型的预测期望就是它的输出了）与真实值之间的平方差就可以记为偏差的平方，其实这里应理解为多模型的情况 泛化误差也就是期望风险。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"参考 参考1 参考2 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:1","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"改进策略 [高方差] 采集更多的样本数据，(参考正规方程) [高方差] 减少特征数量，去除非主要的特征 [高偏差] 引入更多的相关特征 [高偏差] 采用多项式特征 [高偏差] 减小正则化参数 λ [高方差] 增加正则化参数 λ ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:2","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"学习曲线 无论你是要检查你的学习算法是否正常工作或是要改进算法的表现，学习曲线 都是一个十分直观有效的工具。学习曲线 的横轴是样本数，纵轴为 训练集 和 交叉验证集 的 误差 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:5:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Deep Learning","损失函数"],"content":"Softmax理解 主要记录了在使用softmax这个函数中遇到的一些问题，比较基础，但确实困扰了一段时间。 在学习word2vec中, 使用的一般都是如下的损失函数： \\[ \\begin{aligned} Loss = J(\\theta) = -\\frac{1}{T}logL(\\theta)= -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta) \\\\\\\\ =\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0}J_{t,j}(\\theta). \\end{aligned} \\] \\[ \\begin{aligned} J_{t,j}(\\theta) = -logP(miss|xwh) = -log\\frac{exp(u_{miss}^Tv_{xwh})}{\\sum_{o\\in V}exp(u_o^Tv_{xwh})} = \\\\\\\\ -u_{miss}^Tv_{xwh}+log\\sum_{o\\in V} exp(u_o^Tv_{xwh}) \\end{aligned} \\] 但是说起交叉熵往往是下面的式子： \\[ L = -\\sum_{c=1}^Cy_clog(p_c) \\] 在学习的时候就疑惑，这两种形式有什么区别与联系呢，最近看到一篇文章正好解答了这个疑惑。 下面给出结论： 第一种形式是只针对正确类别的对应点输出，将这个位置的softmax即概率最大化，而第二种形式是直接衡量真实分布和实际输出之间的距离，因为交叉熵就是由KL散度变形得来的。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:0:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"交叉熵 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"信息量 一条信息的信息量大小和它的不确定性有很大的关系。一句话如果需要很多外部信息才能确定，我们就称这句话的信息量比较大。 将信息的定义为下： \\(I(x_0) = -log(p(x_0))\\) ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:1","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"熵 信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。公式如下： \\[ H(X) = -\\sum_{i=1}^np(x_i)log(p(x_i)) \\] ### 相对熵 相对熵也称之为KL散度，用于衡量对于同意随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)通常描述样本的真实分布，例如[1, 0, 0, 0]表示样本属于第一类，而q(x)常常用于表示预测的分布，例如[0.7, 0.1, 0.1, 0.1] KL散度的定义公式如下： \\[ D_{KL}(p||q) = \\sum_{i=1}^np(x_i)log(\\frac{p(x_i)}{q(x_i)}) \\] KL越小则说明二者越接近 ### 交叉熵 将KL散度变形 \\[ D_{KL}(p||q) = \\sum_{i=1}^np(x_i)log(p(x_i)) -\\sum_{i=1}^np(x_i)log(q(x_i)) = -H(p(x)) - \\sum_{i=1}^np(x_i)log(q(x_i)) \\] 后半部分就是我们的交叉熵，常常用于评估predict和label之间的差别。 ## 理解 以一个三分类的问题来说，假设真实分布为[0, 1, 0]，则对于第二个式子来说 \\[ L = -\\sum_{c=1}^Cy_clog(p_c) = -0 \\times log(p_0) - 1\\times log(p_1) - 0\\times log(p_3) = -log(p_1) \\] 对于第一个式子就是 \\[ loss_1 = -log(\\frac{e^{z_1}}{\\sum_{c=1}^C e^{z_c}}) = -log(p_1) = -z_1+log\\sum_{c=1}^Ce^{z_c} \\] 所以说实际上这俩是一样的，只是出发点不一样。在skip-gram中，使用的就是第一种式子，对它来说，正确的类别就是背景词，就直接将背景词的概率最大，即损失函数最小。 loss = (-output_layer[:, Y] + torch.log(torch.sum(torch.exp(output_layer), dim=1))).mean() 这行代码就是对第一个式子的实现，需要注意的是所有的操作都是基于batch的。负采样的形式有一些不同，这里不再讨论，到这里我才终于稍稍理解了softmax这个函数。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:2","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"与二分类的联系 具体的说二分类就是两个类别，可以使用上述的方法定义，对于正类和负类都用不同的概率表示，也可以只计算正类，负类就等于1-正类，上述出现的y都是向量的形式，对于二分类，向量的长度就是2，就可以直接展开，最后的结果就是熟知的损失函数的形式。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:2:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"参考 https://zhuanlan.zhihu.com/p/105722023 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:3:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["算法题"],"content":"验证二叉搜索树 https://leetcode-cn.com/problems/validate-binary-search-tree/ # Definition for a binary tree node. # class TreeNode: # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution: def isValidBST(self, root: TreeNode) -\u003e bool: return self.search(root,-(232),232) def search(self,root,mins,maxs): if root == None: return True if root.val \u003e mins and root.val \u003c maxs: pass else: return False return all([self.search(root.left,mins,root.val),self.search(root.right,root.val,maxs)]) 最后用了个all 也是简洁了代码 ","date":"2021-04-22","objectID":"/%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:0:0","tags":["算法题","验证二叉搜索树"],"title":"验证二叉搜索树","uri":"/%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["Deep Learning","循环神经网络系列"],"content":"回看博客，发现深度学习的笔记空荡荡，才发觉一直没有详细得进行笔记，但也感觉确实没有什么可以记录的东西，都是一些网络和模型，具体的trick倒是记录了一些，不过为了博客的美观，还是写一些东西吧，最主要的还是一些小知识。 (MLP就不记录了) RNN其实很类似于一个自回归模型，用历史观测预测下一个观测，我们可以假设不适用所有的历史预测下一个，而是部分的历史，也就是说序列满足马尔科夫条件，而RNN，即循环神经网络，就符合一阶马尔科夫模型，即当前观测仅仅与前一个的观测有关，模型图如下： ","date":"2021-04-21","objectID":"/rnn/:0:0","tags":["Deep Learning","循环神经网络系列","RNN"],"title":"RNN","uri":"/rnn/"},{"categories":["others"],"content":"范围+文本对象 ","date":"2021-04-20","objectID":"/vim/:0:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"范围 内部：i ，意指 inner 外部：a ，英文单词 a，一个的意思 ","date":"2021-04-20","objectID":"/vim/:1:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"文本对象 ( 或 ) ：一对 () b ：一对 () { 或 } ：一对 {} B ：一对 {} [ 或 ] ：一对 [] \u003c 或 \u003e ：一对 \u003c\u003e t ：tag （HTML 或 XML）标签 ' 或 ' ：一对 '' \" 或 \" ：一对 \"\" ` 或 ` ：一对 `` w ：一个单词 s ：一个句子；以 . ! ? 结尾即为一个句子 p ：一个段落；以一个换行符间隔即为一个段落(一般用于对函数体操作) 比如i'代表'内部，iw代表光标所在单词，注意到，加上了范围i或者a，则原本的w含义也发生了变化，注意这是两个东西，原本的w为一个移动，而这里的w代表文本对象即单词。 move ","date":"2021-04-20","objectID":"/vim/:2:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"some easy hjkl，左下上右 wbeWBE，单词级别 0和$，行首和行尾 ^和g _，行首和行尾，但不算空白字符 H和L，页面相关，基本就是映射到^和g _ %括号匹配 ","date":"2021-04-20","objectID":"/vim/:3:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"search 光标移到要搜索的单词，*向下查找，#向上查找 /向下模糊搜索，?向上模糊搜索，配合n和N f + 字符：自左往右移动光标到下一个匹配的字符中 F + 字符：自右往左移动光标到下一个匹配的字符中 t + 字符：自左往右移动光标到下一个匹配的字符的前一个字符中 T + 字符：自右往左移动光标到下一个匹配的字符的后一个字符中 ;：重复执行上一个搜索命令 ,： 与上一个命令方向相反地执行上一个搜索命令 动词 ","date":"2021-04-20","objectID":"/vim/:4:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"some easy d: 删除 c: 删除并进入insert模式 y: 复制 ys, ds, cs，vim-surround插件，对应添加括号，删除括号，修改括号 .重复上一个命令 u撤销上条命令 p粘贴剪切板内容 r替换 ","date":"2021-04-20","objectID":"/vim/:5:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"g+？整理 g : 定位到上次编辑位置 g c 注释 g h 弹出变量或者函数的详细信息 gu和gU 小写或大写，g u u和g U U整行小写大写 g b 多选相同的word gg文件首，G文件尾 3G或3gg或:3，跳到第3行 数量词 1234567...表示命令执行的次数。 结合起来吧 ","date":"2021-04-20","objectID":"/vim/:6:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"动词 + move d h，删除前一个字符 d w，删除字符到下一个单词开头 d tV，删除字符到下一个v字符前面 d 2fb，删除字符到第二个b，且包括这个b，等同于2 dfb ","date":"2021-04-20","objectID":"/vim/:7:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["others"],"content":"动词 + （范围） +文本对象 ys w'，给当前到下一个单词前添加'号 ds '，删除包裹的符号 cs [(，将[]修改为() yssb，整行添加括号，重复两次代表对行操作，b在前面介绍说了是文本对象，代表括号。 ysiw'，出现了两个文本对象，第一个与前面的i结合为iw代表单词，第二个文本对象代表即位要添加的引号，注意和第一条的区别，w所代表的不同。 gUw，大写直到下一个单词前，是动词+move，gUiw，大写光标所在单词，是动词+范围+文本对象。 ","date":"2021-04-20","objectID":"/vim/:8:0","tags":["others","vim"],"title":"vim","uri":"/vim/"},{"categories":["Deep Learning","损失函数"],"content":"L1 Loss 也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。 优点： 对离群点（Outliers）或者异常值更具有鲁棒性。 缺点： 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:1:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["Deep Learning","损失函数"],"content":"L2 Loss 也称为Mean Squred Error，即均方差（MSE），它衡量的是预测值与真实1值之间距离的平方和，作用范围同为0到正无穷。 优点： 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。 缺点： 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。 提一嘴，真的不理解鲁棒性这个翻译的含义，后来才知道是音译，真的是，毒害了多少人。。理解成稳定性就行。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:2:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["Deep Learning","损失函数"],"content":"Smooth L1 Loss 以上两种方法都有各自的优缺点，大部分的情况下其实二者都不使用，因此需要新的损失函数。即平滑的L1损失（SLL)。 SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|\u003e1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。 通过对这三个损失函数进行求导可以发现，L1损失的导数为常数，如果不及时调整学习率，那么当值过小时，会导致模型很难收敛到一个较高的精度，而是趋向于一个固定值附近波动。反过来，对于L2损失来说，由于在训练初期值较大时，其导数值也会相应较大，导致训练不稳定。最后，可以发现Smooth L1在训练初期输入数值较大时能够较为稳定在某一个数值，而在后期趋向于收敛时也能够加速梯度的回传，很好的解决了前面两者所存在的问题。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:3:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["Mathematical Modeling","插值算法"],"content":"插值算法 ","date":"2021-04-17","objectID":"/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC/:0:0","tags":["Mathematical Modeling","插值算法","拉格朗日插值"],"title":"拉格朗日插值","uri":"/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC/"},{"categories":["Mathematical Modeling","插值算法"],"content":"拉格朗日插值法 对于这几个点，想找到一根穿过他们的曲线。 我们可以合理地假设，这根曲线是一个二次多项式 \\[ y = a_0 + a_1x + a_2x^2 \\] 可以通过下面的方程组来解这个二次多项式： \\[ \\begin{cases} y_1 = a_0+a_1x_1+a_2x_1^2 \\\\\\\\ y_2 = a_0+a_1x_2+a_2x_2^2 \\\\\\\\ y_3 = a_0+a_1x_3+a_2x_3^2 \\end{cases} \\] 下面开始阐述拉格朗日的思考 第一根曲线 \\(f_1(x)\\) ，在\\(x_1\\) 点处，取值为1，其余两点取值为0 第二根曲线 \\(f_2(x)\\) ，在 \\(x_2\\) 点处，取值为1，其余两点取值为0 第三根曲线 \\(f_3(x)\\) ，在 \\(x_3\\) 点处，取值为1，其余两点取值为0 那么 \\[ f(x) = y_1f_1(x)+y_2f_2(x)+y_3f_3(x) \\] 可以一一穿过这三个点 ### 推导 用符号表示\\(f_i(x_j), i=1,2,3,j=1,2,3\\) 需要满足的条件为 \\[ f_i(x_j) = \\begin{cases} 1 \u0026 i=j \\\\\\\\ 0 \u0026 i\\neq j \\end{cases} \\] 则有 \\[ f_i(x) = \\prod_{j\\neq i}^{1\\leq j\\leq 3}\\frac{x-x_j}{x_i-x_j} \\] 最终得到 \\[ f(x) = \\sum_{i=1}{3}y_if_i(x) \\] 这就是拉格朗日插值法，推广到更多点的插值也很容易 ","date":"2021-04-17","objectID":"/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC/:1:0","tags":["Mathematical Modeling","插值算法","拉格朗日插值"],"title":"拉格朗日插值","uri":"/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC/"},{"categories":["算法题"],"content":"无重叠区间 https://leetcode-cn.com/problems/non-overlapping-intervals/ 利用了贪心 移除的数目就是总数目减去条件成立的数目 class Solution: def eraseOverlapIntervals(self, intervals: List[List[int]]) -\u003e int: if len(intervals) == 0: return 0 res = 0 mins = -float(\"inf\") for i in sorted(intervals,key=lambda i:i[1]): if i[0] \u003e= mins: res += 1 mins = i[1] return len(intervals) - res 注意是根据end进行排序的，引用别人的解释@HONGYANG 比如你一天要参加几个活动，这个活动开始的多早其实不重要，重要的是你结束的多早，早晨7点就开始了然后一搞搞一天，那你今天也就只能参加这一个活动；但如果这个活动开始的不早，比如9点才开始，但是随便搞搞10点就结束了，那你接下来就还有大半天的时间可以参加其他活动。 这就是为啥要着眼于end，而不是start。 贪心就是考虑当前最优解 ","date":"2021-04-16","objectID":"/%E6%97%A0%E9%87%8D%E5%8F%A0%E5%8C%BA%E9%97%B4/:0:0","tags":["算法题","无重叠区间"],"title":"无重叠区间","uri":"/%E6%97%A0%E9%87%8D%E5%8F%A0%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"对角线遍历 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:0:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/diagonal-traverse/ ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:1:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"思路： ​ 每个对角线的两索引之和是一样的 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:2:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"代码： class Solution: def findDiagonalOrder(self, matrix: List[List[int]]) -\u003e List[int]: if not matrix: return [] hashs = collections.defaultdict(list) row, col = len(matrix), len(matrix[0]) for i in range(row): for j in range(col): hashs[j + i].append(matrix[i][j]) res = [] flag = True for k, v in sorted(hashs.items()): if flag: res.extend(v[::-1]) else: res.extend(v) flag = not flag return res 注意flag的作用 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:3:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["Machine Learning","降维算法"],"content":"主成分分析(PCA) 主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。注意的是PCA属于无监督学习。 PCA降维的原则是投影方差最大。 使用PCA时如果有不同种类的数据，PCA会把这些数据混合在一起降维。 PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据(x(1),x(2),…,x(m))。我们希望将这m个数据的维度从n维降到n’维，希望这m个n’维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n’维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这n’维的数据尽可能表示原来的数据呢？ ","date":"2021-04-12","objectID":"/pca/:0:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"协方差矩阵 在统计学中，方差是用来度量单个随机变量的离散程度，而协方差则一般用来刻画两个随机变量的相似程度，其中，方差的计算公式为: \\[ \\sigma_x^2 = \\frac{1}{n}\\sum_{i=1}^n(x-\\bar{x})^2 \\] 协方差的公式为: \\[ \\sigma(x,y) = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) \\] 根据方差的定义，给定d个随机变量\\(x_k,k=1,2,\\dots,d\\)，这些随机变量的方差为 \\[ \\sigma(x_k,x_k) = \\frac{1}{n-1}\\sum_{i=1}^n(x_{ki}-\\bar{x_k})^2,k=1,2,\\dots,d \\] 因此可以求出两两之间的协方差 \\[ \\sigma(x_m,x_k) = \\frac{1}{n-1}\\sum_{i=1}^n(x_{mi}-\\bar{x_m})(x_{ki}-\\bar{x_k}) \\] 因此，协方差矩阵为 \\[ \\sum = \\begin{bmatrix} \\sigma(x_1,x_1) \u0026 \\cdots \u0026 \\sigma(x_1,x_d) \\\\\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\\\\ \\sigma(x_d,x_1) \u0026 \\cdots \u0026 \\sigma(x_d,x_d) \\end{bmatrix} \\] ","date":"2021-04-12","objectID":"/pca/:1:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"拉格朗日乘数法优化 设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系： \\[ D = \\frac{1}{m}YY^T \\\\\\\\ =\\frac{1}{m}(PX)(PX)^T \\\\\\\\ =\\frac{1}{m}PXX^TP^T \\\\\\\\ =P(\\frac{1}{m}XX^T)P^T \\\\\\\\ =PCP^T \\] 我们令P=\\(w^T\\),令原本的协方差为A，于是我们有优化目标如下： \\[ \\begin{cases} \\max{w^TAw} \\\\\\\\ s.t. w^Tw = 1 \\end{cases} \\] 然后构造拉格朗日函数： \\[ L(w) = w^TAw + \\lambda(1-w^Tw) \\] 对w求导： \\[ Aw = \\lambda w \\] 则方差\\(D(x) = w^TAw = \\lambda w^Tw = \\lambda\\) 于是我们发现，x 投影后的方差就是协方差矩阵的特征值。我们要找到最大方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量，次佳就是第二大特征值对应的特征向量，以此类推。 ","date":"2021-04-12","objectID":"/pca/:2:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"对角矩阵 由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质： 实对称矩阵不同特征值对应的特征向量必然正交。 设特征向量\\(\\lambda\\) 重数为 r，则必然存在 r 个线性无关的特征向量对应于 \\(\\lambda\\) ，因此可以将这 r 个特征向量单位正交化。 实对称矩阵一定可以对角化 由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 \\(e_1,e_2,\\cdots,e_n\\)，我们将其按列组成矩阵： \\(E=(e_1,e_2,\\cdots,e_n)\\)。 对于协方差矩阵C有以下结论: \\[ E^TCE = \\begin{bmatrix} \\lambda_1 \\\\\\\\ \u0026\\lambda_2 \\\\\\\\ \u0026\u0026 \\ddots \\\\\\\\ \u0026\u0026\u0026 \\lambda_n \\end{bmatrix} \\] 注：因为E为正交矩阵，则\\(E^{-1}\\) = \\(E^T\\) ,这个过程成为相似对角化，\\(\\lambda_n\\)为C的特征值，对应的特征向量为\\(e_n\\) 这都是线代的基础知识。 ","date":"2021-04-12","objectID":"/pca/:3:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"SVD 复制一下将协方差矩阵写成中心化的形式： \\[ \\begin{align}S\u0026=\\frac{1}{N}\\sum\\limits_{i=1}^N(x_i-\\overline{x})(x_i-\\overline{x})^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}(x_1-\\overline{x},x_2-\\overline{x},\\cdots,x_N-\\overline{x})(x_1-\\overline{x},x_2-\\overline{x},\\cdots,x_N-\\overline{x})^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}(X^T-\\frac{1}{N}X^T\\mathbb{I}_{N1}\\mathbb{I}_{N1}^T)(X^T-\\frac{1}{N}X^T\\mathbb{I}_{N1}\\mathbb{I}_{N1}^T)^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^T(E_N-\\frac{1}{N}\\mathbb{I}_{N1}\\mathbb{I}_{1N})(E_N-\\frac{1}{N}\\mathbb{I}_{N1}\\mathbb{I}_{1N})^TX\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^TH_NH_N^TX\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^TH_NH_NX=\\frac{1}{N}X^THX \\end{align} \\] 对中心化后的数据集进行奇异值分解： \\[ HX=U\\Sigma V^T,U^TU=E_N,V^TV=E_p,\\Sigma:N\\times p \\] 于是： \\[ S=\\frac{1}{N}X^THX=\\frac{1}{N}X^TH^THX=\\frac{1}{N}V\\Sigma^T\\Sigma V^T \\] 因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值\\(\\Sigma^2\\)和特征向量 \\(V\\)，在新坐标系中的坐标就是： \\[ HX\\cdot V \\] ## 步骤 总结一下 PCA 的算法步骤： 设有 m 条 n 维数据。 对所有的样本进行中心化： \\(x^{(i)} = x^{(i)}-\\frac{1}{m}\\sum_{j=1}^mx^{(j)}\\) 计算样本的协方差矩阵 \\(XX^T\\) 对矩阵\\(XX^T\\)进行特征值分解 4）取出最大的n’个特征值对应的特征向量\\((w_1,w_2,...,w_{n′})\\), 将所有的特征向量标准化后，组成特征向量矩阵W。 5）对样本集中的每一个样本\\(x^{(i)}\\),转化为新的样本\\(z^{(i)}=W^Tx^{(i)}\\) 6) 得到输出样本集\\(D^′=(z^{(1)},z^{(2)},...,z^{(m)})\\) ","date":"2021-04-12","objectID":"/pca/:4:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"实例 假设我们的数据集有10个二维数据(2.5,2.4), (0.5,0.7), (2.2,2.9), (1.9,2.2), (3.1,3.0), (2.3, 2.7), (2, 1.6), (1, 1.1), (1.5, 1.6), (1.1, 0.9)，需要用PCA降到1维特征。 首先我们对样本中心化，这里样本的均值为(1.81, 1.91),所有的样本减去这个均值向量后，即中心化后的数据集为(0.69, 0.49), (-1.31, -1.21), (0.39, 0.99), (0.09, 0.29), (1.29, 1.09), (0.49, 0.79), (0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)。 现在我们开始求样本的协方差矩阵。 然后求出特征值\\((0.0490833989,1.28402771)\\)，对应的特征向量为 \\((0.735178656,0.677873399)^T\\),\\((−0.677873399,−0.735178656)^T\\)，,由于最大的k=1个特征值为1.28402771,对于的k=1个特征向量为\\((−0.677873399,−0.735178656)^T\\). 则我们的W=\\((−0.677873399,−0.735178656)^T\\)。 们对所有的数据集进行投影\\(z^{(i)}=W^Tx^{(i)}\\)，得到PCA降维后的10个一维数据集为：(-0.827970186， 1.77758033， -0.992197494， -0.274210416， -1.67580142， -0.912949103， 0.0991094375， 1.14457216, 0.438046137， 1.22382056) ","date":"2021-04-12","objectID":"/pca/:5:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"性质 缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段； 降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果； 过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合； 特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立； ","date":"2021-04-12","objectID":"/pca/:6:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"代码 # 手动实现PCA算法 import numpy as np def pca(data): \"\"\" 主成分分析 :param data: 数据集 :return: \"\"\" # 数据集的行数 num_data, num_feat = data.shape # 对每一列的数据进行平均值的计算 mean_vec = np.mean(data, axis=0) # 对数据集中每一行的数据进行平均值的计算 data_mean_centered = data - mean_vec # 计算协方差矩阵 sigma = np.dot(data_mean_centered.T, data_mean_centered) / num_data # 计算特征值和特征向量 eig_val, eig_vec = np.linalg.eig(sigma) # 对特征值进行排序 eig_pairs = [(np.abs(eig_val[i]), eig_vec[:, i]) for i in range(len(eig_val))] eig_pairs.sort(key=lambda x: x[0], reverse=True) # 要降维的维数，这里以2为例。将特征向量以列向量形式拼合。 matrix_w = np.hstack([eig_pairs[i][1].reshape(-1, 1) for i in range(2)]) # 将原始数据进行投影。 res = data.dot(matrix_w) print(res) pca(np.array([[1, 2, 5], [3, 4, 6], [5, 6, 9], [3, 2 ,5]])) [[ 4.78637704 -1.46926342] [ 7.62278435 -0.82094287] [11.68194836 -0.79767573] [ 5.77746434 0.2656909 ]] 参考文章 https://zhuanlan.zhihu.com/p/77151308 https://www.cnblogs.com/pinard/p/6239403.html ","date":"2021-04-12","objectID":"/pca/:7:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["NLP"],"content":"Bert BERT 的模型架构非常简单，你已经知道它是如何工作的：它只是 Transformer 的编码器。新的是训练目标和 BERT 用于下游任务的方式。 我们如何使用纯文本训练（双向）编码器？我们只知道从左到右的语言建模目标，但它仅适用于每个标记只能使用以前的标记（并且看不到未来）的解码器。BERT 的作者提出了其他未标记数据的训练目标。在讨论它们之前，让我们先看看 BERT 作为 Transformer 编码器的输入。 训练输入：带有特殊标记的句子对 在训练中，BERT 看到用特殊的标记分隔符 [SEP] 分隔的句子对。为了让模型轻松区分这些句子，除了标记和位置嵌入之外，它还使用了段嵌入。 另一个特殊标记是 [CLS] 。顾名思义，它就是表示整个句子的类别的token。在训练中，它用于我们接下来会看到的 NSP 目标。一旦模型被训练，它就会被用于下游任务。 ","date":"2021-04-08","objectID":"/bert/:0:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"预训练目标：NSP Next Sentence Prediction (NSP) 目标是一个二元分类任务。根据特殊标记[CLS] 的最后一层表示 ，该模型预测这两个句子是否是某些文本中的连续句子。 输入： [CLS] 这个人去了 [MASK] 商店 [SEP] 他买了一加仑 [MASK] 牛奶 [SEP] 标签： isNext 输入： [CLS] 男子去了 [MASK] 商店 [SEP] 企鹅 [MASK] 正在飞行##less 鸟 [SEP] 标签： notNext 该任务教模型理解句子之间的关系。正如我们稍后将看到的，这将使 BERT 能够用于需要某种推理的复杂任务。 ","date":"2021-04-08","objectID":"/bert/:1:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"预训练目标：MLM（掩蔽语言模型） BERT 有两个训练目标，其中最重要的是 Masked Language Modeling (MLM) 目标。对于 MLM 目标，在步骤中会发生以下情况： 选择一些标记 （每个标记以 15% 的概率被选中） 替换这些选定的标记 （使用特殊标记 [MASK] (p=80%)，随机标记 (p=10%)，原始标记（保持不变）(p=10%)） 预测原始标记（计算损失） 其思想来自于完形填空，也借鉴了CBOW的思想。 MLM 仍然是语言建模：目标是根据文本的某些部分预测句子/文本中的一些标记。为了更清楚，让我们将 MLM 与标准的从左到右的语言建模目标进行比较 在每一步，标准的从左到右的 LMs 根据之前的标记预测下一个标记。这意味着最终表示，即来自最终层的用于预测的表示，仅编码先前的上下文，即它们 看不到未来。 不同的是，MLM可以一次看到整个文本，但有些标记被破坏了：这就是 BERT 是双向的原因。请注意，为了让 ELMo 知道左右上下文，作者必须训练两个不同的单向 LM(即双向LSTM)，然后将它们的表示连接起来。在 BERT 中，我们不需要这样做：一个模型就足够了。 注意一些细节，在代码实现的时候，注意特殊的标记如[SEP][CLS] 等不要替换， 还有[PAD] ","date":"2021-04-08","objectID":"/bert/:2:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"数据集构建代码 class BERTDataset(Dataset): def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True): self.vocab = vocab self.seq_len = seq_len self.on_memory = on_memory self.corpus_lines = corpus_lines self.corpus_path = corpus_path self.encoding = encoding with open(corpus_path, \"r\", encoding=encoding) as f: if self.corpus_lines is None and not on_memory: for _ in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines): self.corpus_lines += 1 if on_memory: self.lines = [line[:-1].split(\"\\t\") for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)] # 一行有两个句子，分隔符是\\t self.corpus_lines = len(self.lines) if not on_memory: self.file = open(corpus_path, \"r\", encoding=encoding) self.random_file = open(corpus_path, \"r\", encoding=encoding) for _ in range(random.randint(self.corpus_lines if self.corpus_lines \u003c 1000 else 1000)): self.random_file.__next__() def __len__(self): return self.corpus_lines def __getitem__(self, item): t1, t2, is_next_label = self.random_sent(item) # is_next_label: 1 or 0，1代表t2是相邻句子，0代表不是相邻句子 t1_random, t1_label = self.random_word(t1) # mlm任务 t2_random, t2_label = self.random_word(t2) # [CLS] tag = SOS tag, [SEP] tag = EOS tag t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index] t2 = t2_random + [self.vocab.eos_index] t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index] t2_label = t2_label + [self.vocab.pad_index] segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len] bert_input = (t1 + t2)[:self.seq_len] # 截断 bert_label = (t1_label + t2_label)[:self.seq_len] padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))] #pad bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding) output = {\"bert_input\": bert_input, \"bert_label\": bert_label, \"segment_label\": segment_label, \"is_next\": is_next_label} return {key: torch.tensor(value) for key, value in output.items()} def random_word(self, sentence): # 对sent token进行mask并返回mask后的label tokens = sentence.split() output_label = [] for i, token in enumerate(tokens): prob = random.random() if prob \u003c 0.15: prob /= 0.15 # 80% randomly change token to mask token if prob \u003c 0.8: tokens[i] = self.vocab.mask_index # 10% randomly change token to random token elif prob \u003c 0.9: tokens[i] = random.randrange(len(self.vocab)) # 10% randomly change token to current token else: tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index) output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index)) # 被mask掉的token作为label else: tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index) output_label.append(0) # label为0，这样计算loss时不用考虑，因为可以设置nn.NLLLoss(ignore_index=0) return tokens, output_label def random_sent(self, index): # 根据idx选择某对句子并随机返回相邻或不相邻的句子。 t1, t2 = self.get_corpus_line(index) # output_text, label(isNotNext:0, isNext:1) if random.random() \u003e 0.5: return t1, t2, 1 else: return t1, self.get_random_line(), 0 def get_corpus_line(self, item): # 通过item idx选择某对句子 if self.on_memory: return self.lines[item][0], self.lines[item][1] else: line = self.file.__next__() if line is None: self.file.close() self.file = open(self.corpus_path, \"r\", encoding=self.encoding) line = self.file.__next__() t1, t2 = line[:-1].split(\"\\t\") return t1, t2 def get_random_line(self): # 随机选一行 if self.on_memory: return self.lines[random.randrange(len(self.lines))][1] line = self.file.__next__() if line is None: self.file.close() self.file = open(self.corpus_path, \"r\", encoding=self.encoding) for _ in range(random.randint(self.corpus_lines if self.corpus_lines \u003c 1000 else 1000)): self.random_file.__next__() line = self.random_file.__next__() return line[:-1].split(\"\\t\")[1] ","date":"2021-04-08","objectID":"/bert/:3:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"微调 ","date":"2021-04-08","objectID":"/bert/:4:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"分类 对于分类任务直接取第一个[CLS] token的final hidden state，然后加一层权重后softmax输出。 \\[ P = softmax(CW^T) \\] ","date":"2021-04-08","objectID":"/bert/:4:1","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"其它任务 其它任务需要一些调整 ","date":"2021-04-08","objectID":"/bert/:4:2","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"适配器(Adapter) 到目前为止，我们只考虑了将知识从预训练模型（例如 BERT）转移到下游任务的标准方法：微调。“微调”意味着您采用预训练模型并以相当小的学习率训练您感兴趣的任务（例如，情感分类）。这意味着首先，您更新整个（大型）模型，其次，对于每个任务，您需要微调预训练模型的单独副本。最后，对于几个下游任务，您最终会得到很多大型模型 - 这是非常低效的！ Apdater-Bert的想法是将task-specific layer放在预训练模型中间，也就是加入Adapter结构，然后冻结住预训练模型参数，最后我们fientuning的时候，只更新Apdater、layerNorm以及与具体任务相关的layer的参数。具体结构图如下： 左图是Adapter-BERT中的transformer layer，我们可以看到每一个transformer layer增加了两个Adapter layer，分别加在LayerNorm之前，当然了，在进行LayerNorm之前，我们需要进行讲Apdater layer的输出进行残差连接。 右图是Adapter layer的具体结构示意 \u003e这里为什么要用残差连接？主要是因为当初始化的时候，权重都很小，残差连接可以保证模型输出与预训练模型相同。 ","date":"2021-04-08","objectID":"/bert/:5:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"总结 总之BERT就只有这么多新的特性，或者说创新，但是它一经问世就成为了新的霸主，可见效果之好，BERT还有很多细节上的问题，后面看到或者学习到的时候会继续记录下来。 ## 一些问题 ","date":"2021-04-08","objectID":"/bert/:6:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"为什么 Bert 的三个 Embedding 可以进行相加？ 因为三个 embedding 相加等价于三个原始 one-hot 的拼接再经过一个全连接网络。和拼接相比，相加可以节约模型参数。 引用苏建林老师的话： \u003e Embedding的数学本质，就是以one hot为输入的单层全连接。 也就是说，世界上本没什么Embedding，有的只是one hot。 假设 token Embedding 矩阵维度是 [4,768]；position Embedding 矩阵维度是 [3,768]；segment Embedding 矩阵维度是 [2,768]。 对于一个字，假设它的 token one-hot 是[1,0,0,0]；它的 position one-hot 是[1,0,0]；它的 segment one-hot 是[1,0]。 那这个字最后的 word Embedding，就是上面三种 Embedding 的加和。 如此得到的 word Embedding，和concat后的特征：[1,0,0,0,1,0,0,1,0]，再过维度为 [4+3+2,768] = [9, 768] 的全连接层，得到的向量其实就是一样的。 ","date":"2021-04-08","objectID":"/bert/:6:1","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"transformers中bert模型的输出 ","date":"2021-04-08","objectID":"/bert/:6:2","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["others"],"content":"常用命令 ","date":"2021-04-04","objectID":"/git/:1:0","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"diff image.png git diff：当工作区有改动，临时区为空，diff的对比是“工作区”与最后一次commit提交的仓库的共同文件”；当工作区有改动，临时区不为空，diff对比的是“工作区”与“暂存区”的共同文件”。 git diff --cached：显示暂存区与最后一次commit的改动。 git diff \u003c分支1\u003e \u003c分支2\u003e 显示两个分支最后一次commit的改动。 ","date":"2021-04-04","objectID":"/git/:1:1","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"init git init初始化一个仓库 ","date":"2021-04-04","objectID":"/git/:1:2","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"add git add .将除.gitignore中声明的所有文件加入暂存区 也可以git add 特定文件，将文件加入暂存区。 ","date":"2021-04-04","objectID":"/git/:1:3","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"commit image.png git commit -m \"提交说明\" 提交到工作区 git commit --amend 修改上次的提交记录 ","date":"2021-04-04","objectID":"/git/:1:4","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"push git push 提交到远程库。可以指定分支提交 比如：git push origin main 指定为main分支 ","date":"2021-04-04","objectID":"/git/:1:5","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"pull git pull 将改动拉倒本地库，并合并，默认为Merge合并 如果加上rebase参数则合并方式变为rebase合并。 ","date":"2021-04-04","objectID":"/git/:1:6","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"log git log 查看提交历史 git log --oneline简洁输出 ","date":"2021-04-04","objectID":"/git/:1:7","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"branch git branch xxx 建立分支xxx 然后用git checkout xxx 切换到分支xxx 或者用git checkout -b xxx 完成同样的操作 ","date":"2021-04-04","objectID":"/git/:1:8","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"checkout image.png git checkout 回退版本即head分离 但master不变 checkout可以切换分支，也可以head分离，但是分支的位置不变，但后面的reset和revert都会改变分支 ","date":"2021-04-04","objectID":"/git/:1:9","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"reset git reset --hard HEAD^回退一个版本（master也改变） git reset --hard HEAD~n 回退n个版本 git reset --hard xxxxxx xxxxxx为版本号 即为git log显示的每一个版本号 一般为前六位 reset的参数有三种，其作用如下： 最危险但最常用的就是hard。soft也常用来修改某个提交，只修改提交，之后再进行提交即可达到修改的目的。 ","date":"2021-04-04","objectID":"/git/:1:10","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"revert git revert命令用于撤消对仓库提交历史的更改。其他“撤消”命令，例如 git checkout 和 git reset，将HEAD和分支引用指针移动到指定的提交。git revert也需要一个指定的提交，但是，它并不会将 ref 指针移动到这个提交。revert 操作将采用反转指定的提交的更改，并创建一个新的“还原提交”。然后更新 ref 指针以指向新的还原提交，使其成为分支的HEAD。(创建一个新的commit结点) ","date":"2021-04-04","objectID":"/git/:1:11","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"merge git merge 比如在master分支里 执行git merge xxx 将xxx分支合并到master中，一般项目开发，一人一个分支，最后提交的时候合并再提交。不过更推荐用git rebase方法，这样合并后的分支更加直观。一般是进行三方合并。 ","date":"2021-04-04","objectID":"/git/:1:12","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"branch 最常用的就是创建和删除分支 git branch -f master~3将分支强制回退三个版本，但head不动 ","date":"2021-04-04","objectID":"/git/:1:13","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"cherry-pick 当需要另一个分支的所有改动时，用git merge，但当需要部分改动时候，要用git cherry-pick xxx xxx为哈希值或者分支名，指定为分支名时候，将分支的最新改动合并过来 ","date":"2021-04-04","objectID":"/git/:1:14","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"rebase 当不知道提交的哈希值时，可以用git rebase -i HEAD~x 来可视化管理，可以调整提交的顺序，可以删除不想要的提交，或合并提交 这是线性化的自动的 cherry-pick git rebase xx1 xx2将xx2分支上的提交记录放到xx1后面 此外活用git rebase -i ,进行可视化的操作。 ### fetch git fetch获取远程仓库的数据，不会改变你本地仓库的状态，不会更新你的master,也没有更改你的本地磁盘文件，可以理解为单纯的下载操作。而git pull相当于git fetch + git merge即抓取后合并 ","date":"2021-04-04","objectID":"/git/:1:15","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"reflog git reflog 查看操作记录，这个操作可以撤销不小心用git reset回退版本的操作 ","date":"2021-04-04","objectID":"/git/:1:16","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"底层内容 这几天系统学习了一下git的底层内容，通透了很多，记录一些。贴一下原博客：https://www.lzane.com/tech/git-internal/ 首先看一个图 首先要明确有四种object，第一种是记录文件内容，第二种是记录目录结构，第三种是记录提交信息，第四种是记录tag信息，第四种无关紧要。 从下面开始看，最下面记录的文件内容，注意只记录文件内容，不包括文件名等其它内容。是一个blob类型的节点，将文件的内容信息经过SHA1哈希算法得到对应的哈希值作为这个object在Git仓库中的唯一身份证。 然后再往上的三角形记录的是仓库的目录结构，它将当前的目录结构打了一个快照。从它储存的内容来看可以发现它储存了一个目录结构（类似于文件夹），以及每一个文件（或者子文件夹）的权限、类型、对应的身份证（SHA1值）、以及文件名。 再往上就是记录的提交的信息，它储存的是一个提交的信息，包括对应目录结构的快照tree的哈希值，上一个提交的哈希值，提交的作者以及提交的具体时间，最后是该提交的信息。 还有分支的信息和Head。HEAD、分支和普通的Tag可以理解为一个指针，指向对应commit的sha1值。 仓库有三个分区：工作目录、index索引区域、Git仓库。 当文件被修改后，只是工作目录发生了改变，其余两个是没有任何变化的。当运行git add xxx命令后，即将xxx文件加入了索引区域，此时新建了一个blob object，并且将原来指向xxx指向了新建的blob Object，记住索引索引的是add的所有文件，这时运行git commit，会生成一个tree object，然后创建commit object，将分支等信息指向新的commit。注意每次commit都是储存的全新的文件快照而不是变更部分。 ","date":"2021-04-04","objectID":"/git/:2:0","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["others"],"content":"参考 图解Git (marklodato.github.io) 这才是真正的Git——Git内部原理 - LZANE | 李泽帆（靓仔） ","date":"2021-04-04","objectID":"/git/:3:0","tags":["others","git"],"title":"git","uri":"/git/"},{"categories":["算法题"],"content":"编辑距离 ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:0:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"定义 编辑距离(Edit Distance)是针对两个字符串S1和S2的差异程度进行量化，计算方式是看至少需要多少次的处理才能将S1变成S2（和S2变成S1是等价的），用 EditDis(S1,S2)表示。 其中处理的方式有三种： 1.插入一个字符 2.删除一个字符 3.替换一个字符 这是严格意义上的距离，满足”距离三公理”： 1.对称性，EditDis(S1,S2) = EditDis(S2,S1) 2.非负性，EditDis(S1,S2) \u003e= 0, 当且仅当S1=S2时，等号成立 3.三角不等式，EditDis(S1,S2) + EditDis(S1,S3) \u003e= EditDis(S2,S3) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:1:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"动态规划求解 令 S1.substr(i)表示 S1前i个字符构成的子串，S2.substr(j)表示S2前j个字符构成的子串 \\(dp[i][j]\\)表示S1.substr(i)和S2.substr(j)的编辑距离。 注意，这句话的另一层含义是：当我们计算出了EditDis(S1,S2)则默认了S1.substr(i)已经通过三种处理方式变成了S2.substr(j)。 所以，当我们计算\\(dp[i+1][j+1]\\)时，我们可以利用\\(dp[i][j]\\)，\\(dp[i+1][j]\\)，\\(dp[i][j+1]\\)的信息。 ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:2:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"过程 1.确定最后一步： 令，S1的长度为len1，S2的长度为len2 \\(dp[len1][len2]\\)有三种方式可以实现，一种是S1.substr(len1-1)插入一个字符使之等于S2（\\(dp[len1][len2-1] + 1\\)），一种是S2.substr(len2)删除一个字符使之等于S1（\\(dp[len1-1][len2] + 1\\)），另一种是替换最后一个字符使S1和S2相等（\\(dp[len1-1][len2-1] + 1\\)） 确定转移方程: 3.确定边界和初始状态 我们设定Dp二维数组大小是（Len+1） * （Len2+1），第0行代表 S1为空串，第0列代表S2为空串。 显然，S1变成为空串需要的每次操作是\\(dp[i][0]=i\\) S2变成为空串需要的每次操作是\\(dp[0][j] = j\\) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:3:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"代码 class Solution: def minDistance(self, word1: str, word2: str) -\u003e int: import numpy as np len1, len2 = len(word1), len(word2) dp = [[0]*(len2+1) for _ in range(len1+1)] # 定义 for i in range(1,len1+1): dp[i][0] = i # 边界 for j in range(1,len2+1): dp[0][j] = j # 边界 for i in range(1,len1+1): for j in range(1,len2+1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+1) return int(dp[len1][len2]) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:4:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["Mathematical Modeling"],"content":"因子分析 可以看成主成分分析（PCA）的发展和拓展 如果数据之间有较强的相关性，我们就可以把它们打包到一起作为一个值。这就是所谓的数据降维。 有较强的相关性，是我们可以做因子分析的前提条件。 林登(Linden)根据他收集的来自139名运动员的比赛数据，对第二次世界大战以来奥林匹克十项全能比赛的得分作了因子分析研究。这十个全能项目为：100米跑(x1)，跳远(x2)，铅球(x3)，跳高(x4)，400米跑(x5)，11米跨栏(x6)，铁饼(x7)，撑杆跳高(x8)，标枪(x9)，1500米跑(x10)。经标准化后所作的因子分析表明，十项得分基本上可归结于他们的 短跑速度 、 爆发性臂力 、 爆发性腿力 和 耐力 这四个方面，每一方面都称为一个因子。 因子分析的一般模型为： \\[ \\begin{cases} x_1 = u_1 + a_{11}f_1 + a_{12}f_2 + \\dots + a_{1m}f_m + \\epsilon_1 \\\\\\\\ x_2 = u_2 + a_{21}f_1 + a_{22}f_2 + \\dots + a_{2m}f_m + \\epsilon_2 \\\\\\\\ \\vdots \\\\\\\\ x_p = u_p + a_{p1}f_1 + a_{p2}f_2 + \\dots + a_{pm}f_m + \\epsilon_p \\end{cases} \\] 其中f被称为公共因子，\\(\\epsilon\\)被称为特殊因子，他们都是无法观测的随机变量。 可以用矩阵的形式记为\\(x=u+Af+\\epsilon\\) 其中\\(f=(f_1,f_2,\\dots,f_m)^T\\)被称为公因子向量，\\(\\epsilon = (\\epsilon_1,\\epsilon_2,\\dots,\\epsilon_m)^T\\)被称为特殊因子向量，\\(A_{p*m}=(a_{ij})\\) 成为因子载荷矩阵。 主要说一下应用，原理不再赘述。 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:0:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"参数估计 为了建立因子模型，我们需要估计出因子载荷矩阵\\(A_{p*m}=(a_{ij})\\)，以及个性方差矩阵\\(D = diag(\\sigma_1^2,\\sigma_2^2,\\dots,\\sigma_p^2)\\) SPSS中提供的方法有主成分法、未加权的最小平方法、综合最小平方法、最 大似然法、主轴因子法、Alpha因式分解法和映像因子法。 常用为主成分法。这一步可以得出来原始的因子载荷矩阵 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:1:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"因子旋转 得到因子模型后，其中的公共因子不一定能反映问题的实质特征，为了能更好地解释每一个公共因子的实际意义，且减少解释的主观性，可以通过因子旋转达到目的。因子旋转分为正交旋转与斜交旋转，经过正交旋转而得到的新的公共因子仍然保持彼此独立的性质，而斜交旋转得到的公共因子是相关的(违背了最初的假定，因此可以看作传统因子分析的拓展)，其实际意义更容易解释。但不论是正交旋转还是斜交旋转，都应当使新公共因子的载荷系数的绝对值尽可能接近0或1（这里默认了我们从相关系数矩阵进行计算）。 SPSS中也有一些因子旋转的方法，比如最大方差法，直接Oblimin，最大四次方值，最大平衡值或最优斜交。 常用的是最大方差法。 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:2:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"因子得分 因子分析是将变量表示为公共因子和特殊因子的线性组合；此外，我们可以反过来将公共因子表示为原变量的线性组合，即可得到因子得分。 \\[ \\begin{cases} f_1 = b_{11}x_1 + b_{12}x_2 + \\dots + b_{1p}x_p \\\\\\\\ f_2 = b_{21}x_1 + b_{22}x_2 + \\dots + b_{2p}x_p \\\\\\\\ \\vdots f_m = b_{m1}x_1 + b_{m2}x_2 + \\dots + b_{mp}x_p \\end{cases} \\] 计算因子得分有三种方法，分别为回归、Bartlett、和Anderson-Rubin，常用的为第三种方法。 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:3:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"KMO检验与巴特利特球形检验 KMO检验是Kaiser, Meyer和Olkin提出的，该检验是对原始变量之间的简单相关系数和偏相关系数的相对大小进行检验，主要应用于多元统计的因子分析。 KMO统计量是取值在0和1之间，当所有变量间的简单相关系数平方和远远大于偏相关系数平方和 时，KMO值越接近于1，意味着变量间的相关性越强，原有变量越适合作因子分析；当所有变量 间的简单相关系数平方和接近0时，KMO值越接近于0,意味着变量间的相关性越弱，原有变量越 不适合作因子分析。 其中，Kaiser给出一个KMO检验标准：KMO\u003e0.9,非常适合；0.8\u003cKMO\u003c0.9,适合； 0.7\u003cKMO\u003c0.8, 一般；0.6\u003cKMO\u003c0.7,不太适合；KMO\u003c0.5,不适合。 巴特利特球形检验是一种检验各个变量之间相关性程度的检验方法。一般在做因子分析之前都要进行巴特利特球形检验，用于判断变量是否适合用于做因子分析。巴特利特球形检验是以变量的相关系数矩阵为出发点的。它的原假设是相关系数矩阵是一个单位阵（不适合做因子分析，指标之间的相关性太差，不适合降维），即相关系数矩阵对角线上的所有元素都是1，所有非对角线上的元素都为0。巴特利特球形检验的统计量是根据相关系数矩阵的行列式得到的。如果该值较大，且其对应的p值小于用户心中的显著性水平（一般为0.05），那么应该拒绝原假设，认为相关系数不可能是单位阵，即原始变量之间存在相关性，适合于作因子分析。相反不适合作因子分析。 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:4:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Mathematical Modeling"],"content":"实际操作 在实际操作中，最少要对数据进行两次分析。 第一次的因子分析用于参考，来确定公共因子的数量。通过碎石检验来确定。 碎石检验（scree test）是根据碎石图来决定因素数的方法。Kaiser提出，可通过直接观察特征值的变化来决定因素数。当某个特征值较前一特征值的值出现较大的下降，而这个特征值较小，其后面的特征值变化不大，说明添加相应于该特征值的因素只能增加很少的信息，所以前几个特征值就是应抽取的公共因子数。 得到公共因子数目后再对数据进行重新分析。 然后根据旋转后的因子模型进行解释，可以理解为给因子起名。 下面是一些实际操作spss生成的表格与图片 KMO 和巴特利特检验 KMO 取样适切性量数。 .909 巴特利特球形度检验 近似卡方 719.113 自由度 28 显著性 .000 可以看出可以进行因子分析。 公因子方差 初始 100米(s) 1.000 200米(s) 1.000 400米(s) 1.000 800米(min) 1.000 1500米(min) 1.000 5000米(min) 1.000 10000米(min) 1.000 马拉松(min) 1.000 提取方法：主成分分析法。 可以看到提取后的方差很接近初始的公因子方差，则说明我们选择的2个因子是最主要的因子。 通过碎石图可以看出来前两个因子是最主要的，这里的组件就是因子，翻译的问题。 100米(s) | .817 | .531 | | — | 200米(s) | .867 | .432 | 400米(s) | .915 | .233 | 800米(min) | .949 | .012 | 1500米(min) | .959 | -.131 | 5000米(min) | .938 | -.292 | 10000米(min) | .944 | -.287 | 马拉松(min) | .880 | -.411 | 这是原始的成分矩阵，可见两个因子对不同变量的可解释性比较差。 100米(s) | .274 | .935 | | – | 200米(s) | .376 | .893 | 400米(s) | .543 | .773 | 800米(min) | .712 | .627 | 1500米(min) | .813 | .525 | 5000米(min) | .902 | .389 | 10000米(min) | .903 | .397 | 马拉松(min) | .936 | .261 | 这是旋转后的成分矩阵，这样就可以很好的解释我们的变量。 本例中的第1个公共因子更能代表后面五个变量，我们可以称为长跑因子（或耐力因子）； 第2个公共因子更能代表前三个变量，我们可称为短跑因子（爆发力因子）。 最后因子得分 100米(s) | -.300 | .540 | | —- | 200米(s) | -.222 | .459 | 400米(s) | -.068 | .291 | 800米(min) | .100 | .103 | 1500米(min) | .207 | -.019 | 5000米(min) | .324 | -.161 | 10000米(min) | .321 | -.156 | 马拉松(min) | .406 | -.269 | 这是成分得分系数矩阵。通过代入可以得到f1和f2 和主成分分析一样，我们可以用因子得分f1和f2作为两个新的变量，来进行后续的建模（例如聚类、回归等） 注意：因子分析模型不能用于综合评价，尽管有很多论文是这样写的，但这是 存在很大的问题的。例如变量的类型、选择因子的方法、旋转对最终的影响都 是很难说清的。 ","date":"2021-04-03","objectID":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/:5:0","tags":["Mathematical Modeling","因子分析"],"title":"因子分析","uri":"/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"},{"categories":["Deep Learning","项目练习"],"content":"# 练习1-具有神经网络思维的Logistic回归 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:0","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"1 数据预处理 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:0","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"1.1 数据加载和查看 在开始之前，我们有需要引入的库： numpy ：是用Python进行科学计算的基本软件包。 h5py：是与H5文件中存储的数据集进行交互的常用软件包。 matplotlib：是一个著名的库，用于在Python中绘制图表。 import numpy as np import matplotlib.pyplot as plt import h5py /opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 接下来，需要运行load_dataset函数来读取数据文件中所存储的数据，并返回： train_set_x_orig ：保存的是训练集里面的图像数据(209张64x64的图像)； train_set_y_orig ：保存的是训练集的图像对应的分类值，其中0表示不是猫，1表示是猫。 test_set_x_orig ：保存的是测试集里面的图像数据(50张64x64的图像)； test_set_y_orig ： 保存的是测试集的图像对应的分类值。 classes ： 保存的是以字节类型保存的两个字符串数据，数据为：[b’non-cat’ b’cat’]。 现在我们就要把这些数据加载到主程序里面： def load_dataset(): train_dataset = h5py.File('train_catvnoncat.h5', \"r\") train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels test_dataset = h5py.File('test_catvnoncat.h5', \"r\") test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0])) return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = load_dataset() 我们可以看一下加载的文件里面的图片都是些什么样子的以及存储的分类值。可以看到，当我们改变index值时，会出现不同索引值的图片，且train_set_y的值代表了该图像是否为一只猫。 index = 56 plt.imshow(train_set_x_orig[index]) print(\"train_set_y=\" + str(train_set_y[0][index])) print(\"y=\" + str(train_set_y[:,index]) + \", it's a \" + classes[np.squeeze(train_set_y[:,index])].decode(\"utf-8\") + \"' picture\") train_set_y=1 y=[1], it's a cat' picture index = 1 plt.imshow(train_set_x_orig[index]) print(\"train_set_y=\" + str(train_set_y[0][index])) print(\"y=\" + str(train_set_y[:,index]) + \", it's a \" + classes[np.squeeze(train_set_y[:,index])].decode(\"utf-8\") + \"' picture\") train_set_y=0 y=[0], it's a non-cat' picture ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:1","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"1.2 数据变换 接下来看一看我们加载的数据的具体信息，首先介绍三个变量的含义： - m_train ：训练集里图片的数量。 - m_test ：测试集里图片的数量。 - num_px ： 训练、测试集里面的图片的宽度和高度（均为64x64）。 train_set_x_orig 是一个维度为(m_​​train，num_px，num_px，3）的数组。 m_train = train_set_y.shape[1] #训练集里图片的数量。 m_test = test_set_y.shape[1] #测试集里图片的数量。 num_px = train_set_x_orig.shape[1] #训练、测试集里面的图片的宽度和高度（均为64x64）。 #现在看一看我们加载的东西的具体情况 print (\"训练集的数量: m_train = \" + str(m_train)) print (\"测试集的数量 : m_test = \" + str(m_test)) print (\"每张图片的宽/高 : num_px = \" + str(num_px)) print (\"每张图片的大小 : (\" + str(num_px) + \", \" + str(num_px) + \", 3)\") print (\"训练集_图片的维数 : \" + str(train_set_x_orig.shape)) print (\"训练集_标签的维数 : \" + str(train_set_y.shape)) print (\"测试集_图片的维数: \" + str(test_set_x_orig.shape)) print (\"测试集_标签的维数: \" + str(test_set_y.shape)) 训练集的数量: m_train = 209 测试集的数量 : m_test = 50 每张图片的宽/高 : num_px = 64 每张图片的大小 : (64, 64, 3) 训练集_图片的维数 : (209, 64, 64, 3) 训练集_标签的维数 : (1, 209) 测试集_图片的维数: (50, 64, 64, 3) 测试集_标签的维数: (1, 50) 为了方便，我们要把维度为(64,64,3)的numpy数组重新构造为(64*64*3,1)的数组，要乘以3的原因是每张图片是由64x64像素构成的，而每个像素点由（R，G，B）三原色构成的，所以要乘以3。 在此之后，我们的训练和测试数据集是一个numpy数组，每列代表一个平坦的图像，应该有m_train和m_test列。 当你想将形状（a，b，c，d）的矩阵X平铺成形状（b * c * d，a）的矩阵X_flatten时，可以使用以下代码： #X_flatten = X.reshape(X.shape [0]，-1).T ＃X.T是X的转置 #将训练集的维度降低并转置。 train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T #将测试集的维度降低并转置。 test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T 这一段意思是指把数组变为209行的矩阵（因为训练集里有209张图片）我们可以用-1使得程序算出来是12288列，我再最后用一个T表示转置，这就变成了12288行，209列。 测试集的操作也一样。 然后我们看看降维之后的情况是怎么样的： print (\"训练集降维最后的维度： \" + str(train_set_x_flatten.shape)) print (\"训练集_标签的维数 : \" + str(train_set_y.shape)) print (\"测试集降维之后的维度: \" + str(test_set_x_flatten.shape)) print (\"测试集_标签的维数 : \" + str(test_set_y.shape)) 训练集降维最后的维度： (12288, 209) 训练集_标签的维数 : (1, 209) 测试集降维之后的维度: (12288, 50) 测试集_标签的维数 : (1, 50) 为了表示彩色图像，必须为每个像素指定红色，绿色和蓝色通道（RGB），因此像素值实际上是从0到255范围内的三个数字的向量。 机器学习中一个常见的预处理步骤是对数据集进行居中和标准化，这意味着可以减去每个示例中整个numpy数组的平均值，然后将每个示例除以整个numpy数组的标准偏差。 但对于图片数据集，它更简单，更方便，几乎可以将数据集的每一行除以255（像素通道的最大值），因为在RGB中不存在比255大的数据，所以我们可以放心的除以255，让标准化的数据位于[0,1]之间。 train_set_x = train_set_x_flatten / 255 test_set_x = test_set_x_flatten / 255 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:2","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2 神经网络的搭建 数据处理完毕后，我们需要开始搭建神经网络。 以下是数学表达式： 对于 \\(x(i)\\): \\[ {z}^{(i)}={w}^{T}{x}^{(i)}+b \\] \\[ \\hat{y}^{(i)}=a(i)=sigmoid(z(i)) \\] \\[ L\\left( \\hat{y},y \\right)=-y\\log(\\hat{y})-(1-y)\\log (1-\\hat{y}) \\] 然后通过对所有训练样例求和来计算成本: \\[ J\\left( w,b \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}L\\left( \\hat{y}^{(i)},y^{(i)} \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left( -y^{(i)}\\log \\hat{y}^{(i)}-(1-y^{(i)})\\log (1-\\hat{y}^{(i)}) \\right) \\] 建立神经网络的主要步骤是： 1. 定义模型结构（例如输入特征的数量） 2. 初始化模型的参数 3. 循环： 1. 计算当前损失（正向传播） 2. 计算当前梯度（反向传播） 3. 更新参数（梯度下降） ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:0","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.1 Sigmoid函数 现在构建sigmoid()，需要使用 \\(sigmoid(w ^ T x + b)\\) 计算来做出预测。 其中，sigmoid代表一个常用的逻辑函数为S形函数（Sigmoid function），公式为： \\[ g\\left( z \\right)=\\frac{1}{1+e^{-z}} \\] 接下来，你需要编写代码实现Sigmoid函数，编写后试着测试一些值，如果x的正值较大，则函数值应接近1；如果x的负值较大，则函数值应接近0。而对于x等于0时，则函数值为0.5。 ###在这里填入代码### def sigmoid(z): \"\"\" 参数： z - 任何大小的标量或numpy数组。 返回： s - sigmoid（z） \"\"\" s = 1/(1+np.exp(-z)) return s #测试sigmoid() print(\"====================测试sigmoid====================\") print (\"sigmoid(0) = \" + str(sigmoid(0))) print (\"sigmoid(9.2) = \" + str(sigmoid(9.2))) ====================测试sigmoid==================== sigmoid(0) = 0.5 sigmoid(9.2) = 0.9998989708060922 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:1","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.2 初始化参数 接下来，在搭建神经网络之前，我们需要初始化参数w和b，将w初始化为指定维度的零向量，b初始化为0。 def initialize_with_zeros(dim): \"\"\" 此函数为w创建一个维度为（dim，1）的0向量，并将b初始化为0。 参数： dim - 我们想要的w矢量的大小（或者这种情况下的参数数量） 返回： w - 维度为（dim，1）的初始化向量。 b - 初始化的标量（对应于偏差） \"\"\" w = np.zeros((dim,1)) b = 0 #使用断言来确保我要的数据是正确的 assert(w.shape == (dim, 1)) #w的维度是(dim,1) assert(isinstance(b, float) or isinstance(b, int)) #b的类型是float或者是int return (w , b) ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:2","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.3 成本函数和梯度 对参数进行初始化后，可以接着实现前向和后向传播的成本函数及其梯度，用于后续的参数学习，最小化成本。 接下来，你需要实现propagate函数，该函数用于实现前向传播的成本计算和后向传播的梯度计算。 参数\\(w\\)和\\(b\\)梯度的求解： \\[ \\frac{\\partial J}{\\partial w}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{\\partial J}{\\partial a^{(i)}}\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial w} \\] \\[ \\frac{\\partial J}{\\partial a^{(i)}}= -\\frac{y}{a^{(i)}}+\\frac{1-y}{1-a^{(i)}} \\] \\[ \\frac{\\partial g(z)}{\\partial z}=-\\frac{1}{(1+e^{-z})^2}(-e^{-z})=\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}\\times(1-\\frac{1}{1+e^{-z}})=g(z)(1-g(z)) \\] 所以 \\[ \\frac{\\partial a^{(i)}}{\\partial z^{(i)}}=a^{(i)}(1-a^{(i)}) \\] \\[ \\frac{\\partial z^{(i)}}{\\partial w}=x^{(i)} \\] 可得， \\[ \\frac{\\partial J}{\\partial w}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(a^{(i)}-y)x^{(i)} \\] 求和可以使用numpy的dot函数通过内积计算来实现。 同样地，推导可得， \\[ \\frac{\\partial J}{\\partial b}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(a^{(i)}-y) \\] 要点： 参数列表和返回值需要与函数说明中相同，其中返回值dw,db需要以字典的形式进行返回； 在函数中需要实现正向传播计算成本和反向传播计算梯度。 ###在这里填入代码### def propagate(w, b, X, Y): \"\"\" 实现前向和后向传播的成本函数及其梯度。 参数： w - 权重，大小不等的数组（num_px * num_px * 3，1） b - 偏差，一个标量 X - 矩阵类型为（num_px * num_px * 3，训练数量） Y - 真正的“标签”矢量（如果非猫则为0，如果是猫则为1），矩阵维度为(1,训练数据数量) 返回： cost- 逻辑回归的负对数似然成本 dw - 相对于w的损失梯度，因此与w相同的形状 db - 相对于b的损失梯度，因此与b的形状相同 \"\"\" m = X.shape[1] #正向传播，计算激活值。 A = sigmoid(np.dot(w.T,X) + b) #计算成本 cost = -(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) #反向传播 dw = (1/m)*np.dot(X,(A-Y).T) db = (1/m)*np.sum(A-Y) #使用断言确保我的数据是正确的 assert(dw.shape == w.shape) assert(db.dtype == float) cost = np.squeeze(cost) assert(cost.shape == ()) #创建一个字典，把dw和db保存起来。 grads ={\"dw\":dw,\"db\":db} return (grads , cost) #测试一下propagate print(\"====================测试propagate====================\") #初始化一些参数 w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]]) grads, cost = propagate(w, b, X, Y) print (\"dw = \" + str(grads[\"dw\"])) print (\"db = \" + str(grads[\"db\"])) print (\"cost = \" + str(cost)) ====================测试propagate==================== 6.000064773192205 dw = [[0.99993216] [1.99980262]] db = 0.49993523062470574 cost = 6.000064773192205 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:3","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.4 优化函数 接下来，你需要定义optimize函数通过使用propagate函数计算成本和梯度来最小化成本，并学习最优参数w和b。对于参数 \\(\\theta\\) ，更新规则是 $ = - d$（梯度下降法），其中 \\(\\alpha\\) 是学习率。 要点： - 参数列表和返回列表如函数说明中所示，注意返回值的数据类型； - 我们需要写下两个步骤并遍历它们： 1. 计算当前参数的成本和梯度，使用propagate（）。 2. 使用w和b的梯度下降法则更新参数。 ###在这里填入代码### def optimize(w , b , X , Y , num_iterations , learning_rate , print_cost = False): \"\"\" 此函数通过运行梯度下降算法来优化w和b 参数： w - 权重，大小不等的数组（num_px * num_px * 3，1） b - 偏差，一个标量 X - 维度为（num_px * num_px * 3，训练数据的数量）的数组。 Y - 真正的“标签”矢量（如果非猫则为0，如果是猫则为1），矩阵维度为(1,训练数据的数量) num_iterations - 优化循环的迭代次数 learning_rate - 梯度下降更新规则的学习率 print_cost - 每100步打印一次损失值 返回： params - 包含权重w和偏差b的`字典` grads - 包含权重和偏差相对于成本函数的梯度的`字典` costs - 优化期间计算的所有成本`列表`，将用于绘制学习曲线。 \"\"\" costs = [] for i in range(num_iterations): grads, cost = propagate(w,b,X,Y) dw = grads[\"dw\"] db = grads[\"db\"] w = w - learning_rate*dw #更新参数w b = b - learning_rate*db #更新参数b #记录成本 if i % 100 == 0: costs.append(cost) #打印成本数据 if (print_cost) and (i % 100 == 0): print(\"迭代的次数: %i ， 误差值： %f\" % (i,cost)) params = { \"w\" : w, \"b\" : b } grads = { \"dw\": dw, \"db\": db } return (params , grads , costs) #测试optimize print(\"====================测试optimize====================\") w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]]) params , grads , costs = optimize(w , b , X , Y , num_iterations=100 , learning_rate = 0.009 , print_cost = False) print (\"w = \" + str(params[\"w\"])) print (\"b = \" + str(params[\"b\"])) print (\"dw = \" + str(grads[\"dw\"])) print (\"db = \" + str(grads[\"db\"])) ====================测试optimize==================== w = [[0.1124579 ] [0.23106775]] b = 1.5593049248448891 dw = [[0.90158428] [1.76250842]] db = 0.4304620716786828 ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:4","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.5 实现预测函数 通过优化函数可以输出已学习的w和b的值，我们可以使用w和b来预测数据集X的标签。 接下来，你需要实现预测函数predict（）。 计算预测有两个步骤： 1. 计算\\(\\bar{Y}=A=\\sigma(w^TX+b)\\) 2. 将a的值变为0（如果激活值\u003c= 0.5）或者为1（如果激活值\u003e 0.5） 然后将预测值存储在向量Y_prediction中。 ###在这里填入代码### def predict(w , b , X ): \"\"\" 使用学习逻辑回归参数logistic （w，b）预测标签是0还是1， 参数： w - 权重，大小不等的数组（num_px * num_px * 3，1） b - 偏差，一个标量 X - 维度为（num_px * num_px * 3，训练数据的数量）的数据 返回： Y_prediction - 包含X中所有图片的所有预测【0 | 1】的一个numpy数组（向量） \"\"\" #计算预测猫在图片中出现的概率 m = X.shape[1] #图片的数量 Y_prediction = np.zeros((1,m)) #计预测猫在图片中出现的概率 A = sigmoid(np.dot(w.T,X)+b) for i in range(A.shape[1]): #将概率a [0，i]转换为实际预测p [0，i] Y_prediction[0,i] = \"1\" if A[0,i] \u003e 0.5 else \"0\" #使用断言 assert(Y_prediction.shape == (1,m)) return Y_prediction #测试predict print(\"====================测试predict====================\") w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]]) print(\"predictions = \" + str(predict(w, b, X))) ====================测试predict==================== predictions = [[1. 1.]] ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:5","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"2.6 构建神经网络模型 回顾搭建神经网络模型的步骤： 1. 定义模型结构（例如输入特征的数量） 2. 初始化模型的参数 3. 循环： 1. 计算当前损失（正向传播） 2. 计算当前梯度（反向传播） 3. 更新参数（梯度下降） 我们目前已经实现了参数的初始化、成本和梯度的计算、参数更新函数以及预测函数。接下来，你需要搭建完整的神经网络模型，定义model()函数。 要点： - 参数列表和返回列表如函数说明所示； - 需要分别计算训练集和测试集预测的准确率并输出。 def model(X_train , Y_train , X_test , Y_test , num_iterations = 2000 , learning_rate = 0.5 , print_cost = False): \"\"\" 通过调用之前实现的函数来构建逻辑回归模型 参数： X_train - numpy的数组,维度为（num_px * num_px * 3，m_train）的训练集 Y_train - numpy的数组,维度为（1，m_train）（矢量）的训练标签集 X_test - numpy的数组,维度为（num_px * num_px * 3，m_test）的测试集 Y_test - numpy的数组,维度为（1，m_test）的（向量）的测试标签集 num_iterations - 表示用于优化参数的迭代次数的超参数 learning_rate - 表示optimize（）更新规则中使用的学习速率的超参数 print_cost - 设置为true以每100次迭代打印成本 返回： d - 包含有关模型信息的字典。 \"\"\" w , b = initialize_with_zeros(X_train.shape[0]) parameters , grads , costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost) #从字典“参数”中检索参数w和b w , b = parameters[\"w\"],parameters[\"b\"] #预测测试/训练集的例子 Y_prediction_test = predict(w,b,X_test) Y_prediction_train = predict(w,b,X_train) #打印训练后的准确性 print(\"训练集准确性：\" , format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100) ,\"%\") print(\"测试集准确性：\" , format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100) ,\"%\") d = { \"costs\" : costs, \"Y_prediction_test\" : Y_prediction_test, \"Y_prediciton_train\" : Y_prediction_train, \"w\" : w, \"b\" : b, \"learning_rate\" : learning_rate, \"num_iterations\" : num_iterations } return d print(\"====================测试model====================\") #这里加载的是真实的数据，请参见上面的代码部分。 d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) ====================测试model==================== 迭代的次数: 0 ， 误差值： 0.693147 迭代的次数: 100 ， 误差值： 0.584508 迭代的次数: 200 ， 误差值： 0.466949 迭代的次数: 300 ， 误差值： 0.376007 迭代的次数: 400 ， 误差值： 0.331463 迭代的次数: 500 ， 误差值： 0.303273 迭代的次数: 600 ， 误差值： 0.279880 迭代的次数: 700 ， 误差值： 0.260042 迭代的次数: 800 ， 误差值： 0.242941 迭代的次数: 900 ， 误差值： 0.228004 迭代的次数: 1000 ， 误差值： 0.214820 迭代的次数: 1100 ， 误差值： 0.203078 迭代的次数: 1200 ， 误差值： 0.192544 迭代的次数: 1300 ， 误差值： 0.183033 迭代的次数: 1400 ， 误差值： 0.174399 迭代的次数: 1500 ， 误差值： 0.166521 迭代的次数: 1600 ， 误差值： 0.159305 迭代的次数: 1700 ， 误差值： 0.152667 迭代的次数: 1800 ， 误差值： 0.146542 迭代的次数: 1900 ， 误差值： 0.140872 训练集准确性： 99.04306220095694 % 测试集准确性： 70.0 % ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:6","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning","项目练习"],"content":"3 模型结果分析 在上述的模型结果中，我们可以看到模型在训练集和测试集上的不同表现。当我们修改学习率和迭代次数时，准确率会有些许的变化。现在我们将模型训练过程中的成本优化过程可视化，直观的观察模型训练过程。 #绘制图 costs = np.squeeze(d['costs']) plt.plot(costs) plt.ylabel('cost') plt.xlabel('iterations (per hundreds)') plt.title(\"Learning rate =\" + str(d[\"learning_rate\"])) plt.show() 可以看到在每次的迭代过程中，成本值都在降低，说明模型参数正在被学习。 ### 3.1 学习率的选择 在模型参数中有一个“学习率”的概念，学习率决定了模型更新参数的速度，如果学习率设置的过高，模型可能会“超过”最小值，反之，则会造成过慢的收敛速度。 接下来，可以比较一下在你的模型上选用不同学习率时模型的变化。 learning_rates = [0.01, 0.001, 0.0001] models = {} for i in learning_rates: print (\"learning rate is: \" + str(i)) models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False) print ('\\n' + \"-\" + '\\n') for i in learning_rates: plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"])) plt.ylabel('cost') plt.xlabel('iterations') legend = plt.legend(loc='upper center', shadow=True) frame = legend.get_frame() frame.set_facecolor('0.90') plt.show() learning rate is: 0.01 训练集准确性： 99.52153110047847 % 测试集准确性： 68.0 % - learning rate is: 0.001 训练集准确性： 88.99521531100478 % 测试集准确性： 64.0 % - learning rate is: 0.0001 训练集准确性： 68.42105263157895 % 测试集准确性： 36.0 % - ","date":"2021-04-01","objectID":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:0","tags":["Deep Learning","项目练习","具有逻辑回归思维的神经网络"],"title":"具有逻辑回归思维的神经网络","uri":"/%E5%85%B7%E6%9C%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%9D%E7%BB%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Machine Learning","聚类算法"],"content":"通过sklearn模块实现 import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.datasets import load_iris %matplotlib inline X,y = make_blobs(n_samples=100,n_features=2,centers=[[-1,-1],[0,0],[1,1],[2,2]],cluster_std=[0.4,0.2,0.2,0.2])#使用make_blobs生成训练数据,生成100个样本,每个样本2个特征,共4个聚类,聚类中心分别为[-1,-1],[0,0],[1,1],[2,2],聚类方差分别为0.4,0.2,0.2,0.2 plt.scatter(X[:,0],X[:,1])#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征 plt.show() kmeans = KMeans(n_clusters=3)#生成kmeans分类器,聚类数量为3,其余参数使用默认值。 y_pred = kmeans.fit_predict(X)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(X[:,0],X[:,1],c=y_pred)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征,将各聚类结果显示为不同的颜色 plt.show() kmeans = KMeans(n_clusters=4)#生成kmeans分类器,聚类数量为4,其余参数使用默认值。 y_pred = kmeans.fit_predict(X)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(X[:,0],X[:,1],c=y_pred)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征,将各聚类结果显示为不同的颜色 plt.show() iris = load_iris() #导入iris数据集,iris数据集包含了150个样本,分别属于3类,每个样本包含4个特征 data_train=iris.data #iris样本集的样本特征 label_train=iris.target #iris样本集的样本标签 kmeans = KMeans(n_clusters=3)#生成kmeans分类器,聚类数量为3,其余参数使用默认值。 y_predict = kmeans.fit_predict(data_train)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(data_train[:,0],data_train[:,2],c=y_predict)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第三维特征,将各聚类结果显示为不同的颜色 plt.show() ","date":"2021-03-28","objectID":"/kmeans/:1:0","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["Machine Learning","聚类算法"],"content":"手动实现 import numpy as np import matplotlib.pyplot as plt center = np.array([[0,0],[0,1]]) cls_num = 2 X = np.array([[0,0],[0,1],[2,1],[2,3],[3,4],[1,0]]) max_iter = 10 cls = np.zeros(X.shape[0]) run = True while run and max_iter \u003e 0: for i,x in enumerate(X): tmp = np.argmin(np.sum(np.power(x - center,2),axis=1)) cls[i] = tmp run = False # 重新计算聚类中心 for i in range(cls_num): data = X[cls==i] # 取相同类别的样本 new_center = np.mean(data,axis=0) # 对相同类别的x和y取平均值 if np.sum(np.abs(center[i]-new_center),axis=0) \u003e 1e-4: center[i] = new_center # 更新中心 run = True max_iter -= 1 plt.scatter(X[:,0],X[:,1],c=cls) plt.show() ## kernel kmeans 已知kmeans的核心公式为下： 这里可以看到,实际上就是计算每个样本点簇中心的距离,然后判断出到哪个簇中心距离最短,然后分给那个簇。然后下次迭代时,簇中心就按照新分配的点重新进行计算了,然后所有的点再同样计算样本点到簇中心的距离,重新分配到不同的簇中。所以这样不断迭代下去,就能够收敛了,得到最后的聚类效果。 核k-means,概括地来说,就是将数据点都投影到了一个高维的特征空间中（为啥要这么做呢,主要是突显出不同样本中的差异）,然后再在这个高维的特征空间中,进行传统的k-means聚类。主要的思想就是这么简单,比起传统普通的k-means就多了一个步核函数的操作。所以它的公式也与传统k-means很相近： 但实际中很难计算,因此需要改造, 改造成为 ","date":"2021-03-28","objectID":"/kmeans/:2:0","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["Machine Learning","聚类算法"],"content":"代码 class KernelKmeans: def __init__(self, n_clusters, max_iter, kernel): self.n_clusters = n_clusters self.max_iter = max_iter self.kernel = kernel def fit(self, X): self.centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)] for i in range(self.max_iter): distances = np.array([self.kernel(X, centroid) for centroid in self.centroids]) self.labels = np.argmin(distances, axis=0) self.centroids = np.array([X[self.labels == j].mean(axis=0) for j in range(self.n_clusters)], dtype=np.float32) def predict(self, X): distances = np.array([self.kernel(X, centroid) for centroid in self.centroids]) return np.argmin(distances, axis=0) def gaussian_kernel(X, y): return np.exp(-np.linalg.norm(X - y, axis=1)**2 / 2) ","date":"2021-03-28","objectID":"/kmeans/:2:1","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["算法题"],"content":"翻转二叉树 开始学习二叉树了 先来个简单题 https://leetcode-cn.com/problems/invert-binary-tree/ 很简单 # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def invertTree(self, root: TreeNode) -\u003e TreeNode: if root == None: return None temp = root.left root.left = root.right root.right = temp self.invertTree(root.left) self.invertTree(root.right) return root ","date":"2021-03-24","objectID":"/%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91/:0:0","tags":["算法题","翻转二叉树"],"title":"翻转二叉树","uri":"/%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"对称二叉树 ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/symmetric-tree/ ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:1:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"思路： 利用双向队列，每次把对称的两个对应的节点放入队列中，然后取出来比较，如果值不相等则返回false,如果一边为空 一边不为空也返回false 符合条件的话就继续搜索 ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:2:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"代码： # Definition for a binary tree node. # class TreeNode: # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution: def isSymmetric(self, root: TreeNode) -\u003e bool: from collections import deque d = deque() d.append((root,root)) while d: left,right = d.popleft() if not left and not right: continue elif not left or not right: return False elif left.val != right.val: return False else: d.append((left.left,right.right)) d.append((left.right,right.left)) return True ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:3:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"合并两个有序列表 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:0:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/merge-two-sorted-lists/ ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:1:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"思路： 利用递归的思想，比较两个当前值，因为是有序链表 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:2:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"代码： # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -\u003e ListNode: if l1 == None: return l2 if l2 == None: return l1 if l1.val \u003c= l2.val: l1.next = self.mergeTwoLists(l1.next,l2) return l1 else: l2.next = self.mergeTwoLists(l1,l2.next) return l2 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:3:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["面经"],"content":"关于损失函数的问题，之前也有很多疑惑。看了网上的很多博客，有从很多角度出发来讲解的，看的也是云里雾里。现在大致做一下整理。 对于最小二乘，为什么损失函数是那种形式呢，这里可以假设误差符合正态分布，则y也符合正态分布，则从概率的角度来看，减小预测误差也就是最大化\\(P(Y|X, w)\\)。可以看一下白板推导中的推导。 可以看到最终的结果就是我们熟悉的MSE损失函数，也就是说mse是正态分布的极大似然估计。 而对于LR来说(这里以多分类为例，对于二分类问题，使用的是logit loss)，交叉熵是假设模型分布为多项式分布，即为多项式分布的极大似然估计。 下面说明为啥分类问题中使用交叉熵会更好： MSE无差别关注全部类别上预测概率和真实概率的差。 交叉熵关注的是正确类别的预测概率。 分类问题中,模型的输出空间是概率分布,但目标输出空间是样例的类别,也就是说我们最终目标是获得正确的类别. 因此使用交叉熵的效果会更好。 ","date":"2021-03-09","objectID":"/lr%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5/:0:0","tags":["面经","LR为什么用交叉熵"],"title":"LR为什么用交叉熵","uri":"/lr%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5/"},{"categories":["信息检索"],"content":"协同过滤是推荐系统里面一个常用的算法，因为信息检索领域和推荐系统领域其实是有点相似的（对我目前的认知来说，对两个领域的了解都不深），所以就把协同过滤的笔记整理到了信息检索的目录下。现在主要的都是深度学习的方法了吧，双塔模型什么的。还不是太了解，继续学习。 协同过滤的内核就是一个有关物品和用户的共现矩阵，一般来说，矩阵上的元素是用户对于商品的评价或者说喜爱度。 一般来说，协同过滤分为三种类型，第一种是基于用户的协同过滤，第二种是基于项目的协同过滤，第三种是基于模型的协同过滤。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:0:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"基于用户的协同过滤 基于用户的协同过滤主要考虑的是用户与用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。 缺点： 当用户数大于物品数时，用户相似度矩阵的开销特别大。 当用户行为过于稀疏，找到相似用户的准确性很低 解释性不强。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:1:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"基于项目的协同过滤 基于项目的协同过滤与基于用户的协同过滤类似，不过这时需要转向找到物品与物品之间的相似度，只有找到了目标用户对某些物品的评分，我们就可以对相似度高的类比物品进行预测，将评分最高的若干个相似物品推荐给用户。 也有一些缺点： 当物品数远大于用户数时，物品相似度矩阵的开销会特别大 当有新物品时，没有办法在不离线更新物品相似度表的情况下将新物品推荐给客户。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:2:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"相似度计算 因为得到共现矩阵后，计算相似度就是向量之间计算相似度，有以下几种方法： Jaccard系数 Cosine相似度，用夹角大小表示相似程度。 Pearson相关性系数，概率论讲过。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:3:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"基于模型的协同过滤 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:4:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"矩阵分解 因为原始的矩阵非常稀疏，所以一个想法就是生成一个隐变量，将用户和物品映射到这个隐变量表示空间上。 矩阵分解的主要过程，就是先 分解 协同过滤生成的共现矩阵，生成用户和物品的隐向量，再通过用户和物品隐向量间的相似性进行推荐。 其实和主题模型中LSA很像，学过LSA就很容易理解。 矩阵分解主要有两种做法 SVD 梯度下降 SVD不多说，但它的主要问题很明显，SVD要求原始矩阵是稠密的，但很显然共现矩阵并不稠密，而且无法进行正则化，容易过拟合，因此可以用梯度下降来计算分解矩阵。 梯度下降的最基本的模型是： 矩阵分解进一步加强了协同过滤的泛化能力，它把协同过滤中的共现矩阵分解成了用户矩阵和物品矩阵，从用户 矩阵中提取出用户隐向量，从物品矩阵中提取出物品隐向量，再利用它们之间的内积相似性进行推荐排序。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:4:1","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"基于深度学习的协同过滤 参考文本匹配概述 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:5:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"基于表示学习的模型 结构如下: 用户和物品分别通过神经网络生成各自的Embedding向量，即表示向量。将其作为中间产物，然后再通过内积等交互函数得到匹配分数，进行排序推荐。 DSSM就是一个基于表示学习的模型。 ### 基于匹配方法学习的模型 基于匹配方法学习的深度推荐模型结构如下： 也就是文本匹配中的基于交互型的。 是一个端到端的模型。 NCF就是一个端对端的模型。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:5:1","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"总结 协同过滤在深度学习出现之前在推荐领域非常常用，但其缺点也很明显因为它使用非常稀疏的矩阵进行预测，所以泛化能力很弱，虽然使用了矩阵分解等技术，但是使用内积方式来处理用户和物品的交叉，因此拟合能力非常弱，所以出现了神经网络的方法，比如NeuralCF等。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:6:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["信息检索"],"content":"参考 https://zhuanlan.zhihu.com/p/69888124有关计算可以看这个。 ","date":"2021-03-06","objectID":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/:7:0","tags":["信息检索","协同过滤"],"title":"协同过滤","uri":"/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"},{"categories":["算法题"],"content":"可获得的最大点数 ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:0:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/maximum-points-you-can-obtain-from-cards/ ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:1:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 滑动窗口题目，限定窗口大小然后滑动即可 ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:2:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def maxScore(self, cardPoints: List[int], k: int) -\u003e int: n = len(cardPoints) # 滑动窗口大小为 n-k windowSize = n - k # 选前 n-k 个作为初始值 s = sum(cardPoints[:windowSize]) minSum = s for i in range(windowSize, n): # 滑动窗口每向右移动一格，增加从右侧进入窗口的元素值，并减少从左侧离开窗口的元素值 s += cardPoints[i] - cardPoints[i - windowSize] minSum = min(minSum, s) return sum(cardPoints) - minSum ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:3:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"滑动窗口中位数 ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:0:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/sliding-window-median/ ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:1:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 很明显的滑动窗口，首先定义一个求中位数的匿名函数，然后一点一点求出来 ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:2:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def medianSlidingWindow(self, nums: List[int], k: int) -\u003e List[float]: median = lambda a: (a[(len(a)-1)//2] + a[len(a)//2]) / 2 res = [] for i in range(len(nums)-k+1): res.append(median(sorted(nums[i:i+k]))) return res ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:3:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["NLP"],"content":"Seq2Seq中的Attention ","date":"2021-02-23","objectID":"/attention/:0:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"缺陷 在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。 不仅编码器很难将所有信息放入一个向量中——这对解码器来说也很困难。解码器只看到源的一种表示。但是，在每个生成步骤中，源的不同部分可能比其他部分更有用。但在目前的情况下，解码器必须从相同的固定表示中提取相关信息——这不是一件容易的事。 这个时候就需要引入注意力机制了，注意这里的注意力机制和transformer中的self-attention是不一样的。下面详细介绍一下。注意几个名词：注意力得分、注意力权重。其中注意力得分即score的计算有多种方法，权重就是对得分进行softmax归一化。 ","date":"2021-02-23","objectID":"/attention/:1:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"attention 注意机制是神经网络的一部分。在每个解码器步骤中，它决定哪些源部分更重要。在此设置中，编码器不必将整个源压缩为单个向量 - 它为所有源标记提供表示（例如，所有 RNN 状态而不是最后一个）。 步骤： - 接受注意输入：解码器状态\\(h_t\\)以及所有编码器状态\\(s_1,s_2,\\dots,s_m\\) - 计算每个编码器状态的注意力分数\\(s_k\\)，注意力分数表示它对解码器状态\\(h_t\\)的相关性，使用注意力函数，接收一个解码器状态和一个编码器状态并返回一个标量分数，即图中的\\(score(h_t,s_k)\\) - 计算注意力权重：即概率分布- 使用Softmax函数 - 计算注意力输出：具有注意力机制的编码器状态的加权和 即为如图所示内容为如何计算注意力。 注意我们提到的注意力函数，这里的注意力分数的计算有很多种方法，下面介绍几种比较常见的办法： - 点积： 最简单的办法。 - 双线性函数 - 多层感知机 注意后两者都有要优化的参数的，第一个点积是直接运算，因此很简单。 在应用时可以直接将注意力的结果传输到最后的softmax，也可以将原始的\\(h_t\\)合并，下面介绍几种变体。 ","date":"2021-02-23","objectID":"/attention/:2:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"Bahdanau Model - 编码器使用双向的RNN - 利用上一时刻的隐层状态计算注意力输出c，然后和隐层状态一起作为当前时刻的输入，再得到结果\\(\\hat{y}\\)。这里再说一下训练的过程中当前步的输入使用的是真实的\\(y\\)，测试的时候才会使用上一步的输出作为输入。可以将上下文向量c（也就是注意力输出）与\\(x\\)拼接后作为输入。 - 注意力得分使用的是感知机。 这里引用一下李沐大佬的代码： class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention = d2l.AdditiveAttention( num_hiddens, num_hiddens, num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU( embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): # outputs的形状为(batch_size，num_steps，num_hiddens). # hidden_state的形状为(num_layers，batch_size，num_hiddens) outputs, hidden_state = enc_outputs return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs的形状为(batch_size,num_steps,num_hiddens). # hidden_state的形状为(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # 输出X的形状为(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) # 转换是为了方便后面循环计算。 outputs, self._attention_weights = [], [] for x in X: # query的形状为(batch_size,1,num_hiddens) query = torch.unsqueeze(hidden_state[-1], dim=1) # -1是指在最后一层最后时刻的隐藏状态，作为query # context的形状为(batch_size,1,num_hiddens) context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # 在特征维度上连结 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # 将x变形为(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # 全连接层变换后，outputs的形状为 # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights ","date":"2021-02-23","objectID":"/attention/:3:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"Luong Model 这个模型的编码器比较常规，使用当前状态计算注意力输出，然后解码器中将隐层状态与注意力输出做一步结合，这样得到了新的隐层状态，然后再传递得到预测结果。 ","date":"2021-02-23","objectID":"/attention/:4:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"注意力对齐 可以看到解码器关注的源token。 到此seq2seq中的attention就介绍完毕了，其实还有很多细节，以后遇到了会持续补充。 Self-attention 移步transformer。Transformer KV cache KV cache # MQA 标准的mha中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 MQA (Multi Queries Attention): MQA比较极端，只保留一个KV Head，多个Query Heads共享相同的KV Head。这相当于不同Head的Attention差异，全部都放在了Query上，需要模型仅从不同的Query Heads上就能够关注到输入hidden states不同方面的信息。这样做的好处是，极大地降低了KV Cache的需求，但是会导致模型效果有所下降。（层内共享） # GQA 如上图所示，GQA就是在MHA和MQA之间做了一个平衡。对query heads进行分组，分成几组就对应多少个kv heads，然后每一组内的query Heads共享相同的KV head。 GQA可以在减少计算量和KV Cache同时确保模型效果不受到大的影响。 # online attention ### 3-pass \\(\\mathsf{NO}\\)TATIONS \\(\\{m_i\\}{:}\\max_{j=1}^i\\left\\{x_j\\right\\}\\), with initial value \\(m_0=-\\infty.\\) \\(\\{d_i\\}{:}\\sum_{j=1}^ie^{x_j-m_N}\\), with initial value \\(d_0=0,d_N\\) is the denominator of safe softmax. \\(\\{a_i\\}{:\\text{ the final softmax value}}.\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[m_i\\leftarrow\\max\\left(m_{i-1},x_i\\right)\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[d_i\\leftarrow d_{i-1}+e^{x_i-m_N}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[a_i\\leftarrow\\frac{e^{x_i-m_N}}{d_N}\\] \\(\\mathbf{end}\\) 这是3step计算attention的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于sram中没有足够的存储空间，因此需要多次访存。 ### online attention \\[\\begin{aligned} d_i^{\\prime}\u0026 =\\sum_{j=1}^ie^{x_j-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_i}\\right)+e^{x_i-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_{i-1}}\\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\\\ \u0026= d_{i-1}' e^{m_{i-1}-m_i}+e^{x_i-m_i} \\end{aligned}\\] 找到迭代式之后就可以从3step降到2step \\[\\begin{aligned}\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026m_i\u0026\u0026\\leftarrow\u0026\\max\\left(m_{i-1},x_i\\right)\\\\\u0026\u0026\u0026d_i^{\\prime}\u0026\u0026\\leftarrow\u0026d_{i-1}^{\\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\\\\u0026\\mathbf{end}\\\\\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026a_i\\leftarrow\u0026\u0026\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026\\mathbf{end}\\end{aligned}\\] 好像FLOPs计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的scale x值，也就是pre-softmax logits，由于需要O(N^2)的显存无法放在SRAM中。因此： 1. 要么提前计算好x，保存在全局显存中，需要O(N^2)的显存，容易爆显存。 2. 要么在算法中online计算，每次循环中去load一部分Q，K到片上内存，计算得到x。 Attention优化的目标就是避开第一种情况，尽可能节省显存，否则，LLM根本无法处理类似100K以上这种long context的情况。而对于第二种情况，我们不需要保存中间矩阵x，节省了显存，但是计算没有节省，并且增加了HBM IO Accesses（需要不断地load Q, K）。此时，2-pass算法相对于3-pass算法，可以减少一次整体的load Q, K以及减少一次对 xi 的online recompute，因为在2-pass的第一个pass中， xi 是被两次计算共享的。类似online-softmax这种算法，对应到Attention中的应用，就是Memory Efficient Attention（注意不是FlashAttention）。 # flash attention safe softmax并没有1-pass算法，那么Attention会不会有呢？有！这就是FlashAttention！ 在使用online attention的情况下，从头开始计算attention score的过程如下： \\(\\operatorname{NOTATIONS}\\) \\(Q[k,:]:\\)the \\(k\\)-th row vector of \\(Q\\) matrix. \\(\\begin{aligned}O[k,:]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\\\\\mathbf{V}[i,i]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\end{aligned}\\) \\(V[i,:]{:\\text{ the }i\\text{-th row of }V\\text{ matrix}}.\\) \\(\\{\\boldsymbol{o}_i\\}{:}\\sum_{j=1}^ia_jV[j,:]\\), a row vector storing partial aggregation result \\(A[k,:i]\\times V[:i,:]\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}x_i\u0026\\leftarrow\\quad Q[k,:]\\:K^T[:,i]\\\\m_i\u0026\\leftarrow\\quad\\max\\left(m_{i-1},x_i\\right)\\\\d_i'\u0026\\leftarrow\\quad d_{i-1}'e^{m_{i-1}-m_i}+e^{x_i-m_i}\\end{aligned}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}\u0026a_i\\:\\leftarrow\\:\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026o_i\\:\\leftarrow\\:o_{i-1}+a_i\\:V[i,:\\:]\\end{aligned}\\] \\(\\mathbf{end}\\) \\[O[k,:]\\leftarrow\\boldsymbol{o}_N\\] 优化思路和online attention一样，将\\(o_{i}\\)的计算简化以便于可以写成迭代式。 原来的\\(o_{i}\\)使用以下方式计算，依赖于全局的\\(m_{N}\\)和\\(d_{N}\\)。 \\[\\boldsymbol{o}_i:=\\sum_{j=1}^i\\left(\\frac{e^{x_j-m_N}}{d_N^{\\prime}}V[j,:]\\right)\\] 将其改写成如下形式： \\[\\boldsymbol{o}_i^{\\prime}:=\\left(\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:]\\right)\\] 这样按照上面的方式拓展下去，可以找到一个循环迭代式。 \\[\\begin{aligned} \\mathbf{o}_i^{\\prime}\u0026 =\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d'}V[j,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:] \\right)+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}","date":"2021-02-23","objectID":"/attention/:5:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"PMI 点互信息 对于两个单词之间的PMI来说，可以这样计算： \\[ PMI(w,c) = \\log \\frac{p(w,c)}{p(w)p(c)} = \\log \\frac{N(w,c) |w,c|}{N(w)N(c)} \\] ## MI 在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息(Mutual Information)是度量两个事件集合之间的相关性(mutual dependence)。互信息最常用的单位是bit。 ","date":"2021-02-22","objectID":"/ppmi/:1:0","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["NLP"],"content":"定义 正式地，两个离散随机变量 X 和 Y 的互信息可以定义为： 其中 p(x,y) 是 X 和 Y 的联合概率分布函数，而p(x)和p(y)分别是 X 和 Y 的边缘概率分布函数。 在结果上互信息与信息增益是一样的，下面是详细的推导。 应用到文本特征选择： U、C都是二值随机变量，当文档包含词项t时，U的取值为1，否则0；当文档属于类别c时，C的取值1，否则0。简单的理解就是对于文本来说，每一个token就是它的特征，取值只有有或者没有，也就是0或者1，互信息常用于文本特征的选择，也就 是选择有价值的token。在贝叶斯文本分类中用到了，特此记录。 \\[ I_k = \\sum_{\\tilde{y}=0}^1 \\bigg(p(X = k | Y = \\tilde{y})p(Y = \\tilde{y}) \\log\\frac{p(X = k | Y = \\tilde{y})}{p(X=k)} + (1-p(X = k | Y = \\tilde{y}))p(Y = \\tilde{y}) \\log\\frac{1 - p(X = k | Y = \\tilde{y})}{1 - p(X = k)}\\bigg), \\] 公式如上。\\(I_k\\)意味着单词k与Y之间的互信息。 示例代码如下： def calculateMI(dtm_ham_train, dtm_spam_train): ham_sums = np.sum(dtm_ham_train, axis=0) ham_probs = ham_sums / np.sum(ham_sums) spam_sums = np.sum(dtm_spam_train, axis=0) spam_probs = spam_sums / np.sum(spam_sums) all_sums = ham_sums + spam_sums all_probs = all_sums / sum(all_sums) mi = [] for i in range(len(all_probs)): if all_probs[i] == 0 or np.isnan(all_probs[i]): mi.append(0) else: mi.append(.5 * ham_probs[i] * np.log(ham_probs[i] / all_probs[i]) + .5 * (1 - ham_probs[i]) * np.log((1 - ham_probs[i])/(1 - all_probs[i])) + .5 * spam_probs[i] * np.log(spam_probs[i] / all_probs[i]) + .5 * (1 - spam_probs[i]) * np.log((1 - spam_probs[i])/(1 - all_probs[i]))) mi = np.array(mi) mi = np.where(np.isnan(mi), 0, mi) return mi ","date":"2021-02-22","objectID":"/ppmi/:1:1","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["NLP"],"content":"PPMI ","date":"2021-02-22","objectID":"/ppmi/:2:0","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["算法题"],"content":"至少有k个重复字符的最长字串 ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:0:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/longest-substring-with-at-least-k-repeating-characters/ ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:1:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： 利用递归，如果s中字符c的数目小于k,则以c作分割，分成的字串再次调用函数形成递归，然后从众多结果中找寻最大长度的。 ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:2:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"代码： class Solution(object): def longestSubstring(self, s, k): if len(s) \u003c k: return 0 for c in set(s): if s.count(c) \u003c k: return max(self.longestSubstring(t, k) for t in s.split(c)) return len(s) ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:3:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"最长递增子序列 ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:0:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/longest-increasing-subsequence/ ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:1:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"思路： 动态规划 定义dp[i]为到nums[i]的最长递增子序列的长度，全部都初始化为1,因为本身就是长度为1的递增子序列 ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:2:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def lengthOfLIS(self, nums: List[int]) -\u003e int: dp = [1 for _ in range(len(nums))] for i in range(1,len(nums)): for j in range(i): if nums[j] \u003c nums[i]: dp[i] = max(dp[i],dp[j]+1) return max(dp) ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:3:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"外观数列 https://leetcode-cn.com/problems/count-and-say/ 这题有意思 可以打表，不过打表的过程也相当于做出来了 class Solution: def countAndSay(self,n: int) -\u003e str: if n == 1: return '1' s = self.countAndSay(n - 1) n,res = 0,'' for ii,ss in enumerate(s): if ss != s[n]: res += str(ii-n) + s[n] n = ii res += str(len(s) - n) + s[-1] return res print(Solution().countAndSay(3)) 思路： ​ 递归，将上一层计算出来的东西作为迭代对象。 ","date":"2021-02-16","objectID":"/%E5%A4%96%E8%A7%82%E6%95%B0%E5%88%97/:0:0","tags":["算法题","外观数列"],"title":"外观数列","uri":"/%E5%A4%96%E8%A7%82%E6%95%B0%E5%88%97/"},{"categories":["pandas","api"],"content":"调用为np.lib.stride_tricks.as_strided() 可以分割一个数组为不同的shape块，有个问题就是什么是strides呢？可以看个例子： a = np.arange(9, dtype=np.int32).reshape(3,3) print(a) ''' [[0 1 2] [3 4 5] [6 7 8]] ''' print(a.strides) ''' (12, 4) ''' 这里（12， 4）中的12表示在内存中a[n, 0]到a[n+1, 0]跨过多少byte，4表示在内存中a[n, 0]到a[n, 1]跨过多少byte。 32int需要4byte是众所周知。 看一下函数的参数： numpy.lib.stride_tricks.as_strided(x, shape=None, strides=None, subok=False, writeable=True) x就是我们要分割的矩阵，可以当做是一个蓝图，shape，strides都是新矩阵的属性，也就是说这个函数按照给定的shape和strides来划分x，返回一个新的矩阵。 对于X： 如果卷积核的大小是2x2，stride为1，那么就需要把矩阵X转换为包含如下4个小矩阵的新矩阵A： 很明显A的维度为(2,2,2,2)。 所以shape可以确定，但strides还不确定： A = as_strided(X, shape=(2,2,2,2), strides) 下面确定strides，从图中可以确定最低维的为4，因为所有数据都在X上，所以A的各个维度的跨度都要根据X来确定，而不是A中，以1和4为例子，在X中的距离为12字节，所以现在可以确定后两维：（?,?,12,4）。 再看更高维度: 从X中可以看到，第二维的距离为4。 第一维也不多说，是12。 最后可以strides =（12，4，12，4）。 这就是整个分析的过程，可以方便卷积操作，不是嘛。 再看一个例子就结束了，估计以后会忘，记录下来。 意思就是将一个向量拓展成这样的形式，用循环的方法很容易实现： def sliding_stack_py(v, k): \"Stack sliding windows of v of length k.\" rows = [] for i in range(len(v) - k + 1): rows.append(v[i : (i + k)]) return np.array(rows) 但如果不能使用循环呢，就可以用刚说的这个函数了: def sliding_stack_np(v, k): return np.lib.stride_tricks.as_strided(v, shape=(len(v) - k + 1, k), strides=(v.strides[0], v.strides[0])) 因为原向量是1维的，所以转换后的strides为[4,4]。希望可以帮助理解。 ","date":"2021-02-14","objectID":"/as_strided/:0:0","tags":["pandas","api","as_strided"],"title":"as_strided","uri":"/as_strided/"},{"categories":["NLP"],"content":"NER(命名实体识别) 参考：https://www.jianshu.com/p/16e1f6a7aaef 命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。 举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息 人名：小明，时间：早上8点，地点：学校。 ","date":"2021-02-06","objectID":"/ner/:0:0","tags":["NLP","NER"],"title":"NER","uri":"/ner/"},{"categories":["算法题"],"content":"阶乘函数后K个零(首个困难题) ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:0:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/preimage-size-of-factorial-zeroes-function/ ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:1:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"思路： 首先先写个判断阶乘后有多少个零的函数，思路就是找所有相乘的数中因数有5的个数。 然后再用二分查找，找到有K个0的左界和右界，然后相减即可，就是要找的数目 class Solution: def preimageSizeFZF(self, K: int) -\u003e int: return self.findright(K) - self.findleft(K) def whatzero(self,n): dis = 5 res = 0 while dis \u003c= n: res += n // dis dis *= 5 return res def findleft(self,K): mins,maxs = 0,sys.maxsize while (mins \u003c maxs): mid = mins + (maxs-mins) // 2 if self.whatzero(mid) \u003c K: mins = mid + 1 elif self.whatzero(mid) \u003e K: maxs = mid else: maxs = mid return mins def findright(self,K): mins,maxs = 0,sys.maxsize while (mins \u003c maxs): mid = mins + (maxs-mins) // 2 if self.whatzero(mid) \u003c K: mins = mid + 1 elif self.whatzero(mid) \u003e K: maxs = mid else: mins = mid + 1 return maxs 注意这里的最大值要初始化为sys库里的maxsize 用float(“inf”)会返回nan值 ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:2:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"打家劫舍 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:0:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"打家劫舍I ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/house-robber/ ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:1","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"思路: 一个简单题，不过踩了特例的坑。。可以暴力解决 也可以动态规划 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:2","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"代码: 暴力解决 class Solution: def rob(nums): if nums == []: return 0 if len(nums) == 1: return nums[0] if len(nums) == 2: return max(nums[0],nums[1]) maxs = [] #max[i]代表到i+1家的最大价钱 maxs.append(nums[0]) maxs.append(nums[1]) for i in range(2,len(nums)): maxs.append(max(maxs[:i-1])+nums[i]) #从头到这家前面的第二家最大的价钱加上这一家的价钱 return max(maxs) 动态规划 class Solution: def rob(self, nums: List[int]) -\u003e int: if len(nums) \u003c= 2: return max(nums) dp = [0] * len(nums) dp[0], dp[1] = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): dp[i] = max(dp[i-1], dp[i-2] + nums[i]) return max(dp) ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:3","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"打家劫舍II ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/house-robber-ii/ ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:1","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"思路： 跟上面的题目非常类似，只是加了一个限制条件，就是第一家和最后一家不能同时打劫。 这里先写一个函数，表示从start 到end 范围里面的最大值，然后在主函数里面进行选择 如果打劫第一家，就不能打劫最后一家以及不打劫第一家去打劫最后一家，这两者之间的最大值 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:2","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"代码： class Solution: def rob(self, nums: List[int]) -\u003e int: if len(nums) == 1: return nums[0] return max(self.dp(0,len(nums)-2,nums),self.dp(1,len(nums)-1,nums)) def dp(self,start,end,nums): dp = [0 for _ in range(len(nums)+2)] for i in range(end,start-1,-1): dp[i] = max(dp[i+1],dp[i+2]+nums[i]) return dp[start] ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:3","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["python"],"content":"import threading import time 简单的创建 def run(n): print(\"task\", n) time.sleep(1) print('2s') time.sleep(1) print('1s') time.sleep(1) print('0s') time.sleep(1) if __name__ == '__main__': t1 = threading.Thread(target=run, args=(\"t1\",)) t2 = threading.Thread(target=run, args=(\"t2\",)) t1.start() t2.start() 通过类创建 class MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"task\", self.n) time.sleep(1) print('2s') time.sleep(1) print('1s') time.sleep(1) print('0s') time.sleep(1) if __name__ == \"__main__\": t1 = MyThread(\"t1\") t2 = MyThread(\"t2\") t1.start() t2.start() 对比没有join()和join()的区别 def run(n): print(\"task\", n) time.sleep(1) #此时子线程停1s print('3') time.sleep(1) print('2') time.sleep(1) print('1') if __name__ == '__main__': t = threading.Thread(target=run, args=(\"t1\",)) t.setDaemon(True) #把子进程设置为守护线程，必须在start()之前设置 t.start() print(\"end\") def run(n): print(\"task\", n) time.sleep(1) #此时子线程停1s print('3') time.sleep(1) print('2') time.sleep(1) print('1') if __name__ == '__main__': t = threading.Thread(target=run, args=(\"t1\",)) t.setDaemon(True) #把子进程设置为守护线程，必须在start()之前设置 t.start() t.join() # 设置主线程等待子线程结束 print(\"end\") 锁的应用 def run(n, semaphore): semaphore.acquire() #加锁 time.sleep(1) print(\"run the thread:%s\\n\" % n) semaphore.release() #释放 if __name__ == '__main__': num = 0 semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行 for i in range(22): t = threading.Thread(target=run, args=(\"t-%s\" % i, semaphore)) t.start() while threading.active_count() != 1: pass # print threading.active_count() else: print('--') 事件类 event = threading.Event() def lighter(): count = 0 event.set() #初始值为绿灯 while True: if 5 \u003c count \u003c=10 : event.clear() # 红灯，清除标志位 print(\"\\33[41;1mred light is on...\\033[0m\") elif count \u003e 10: event.set() # 绿灯，设置标志位 count = 0 else: print(\"\\33[42;1mgreen light is on...\\033[0m\") time.sleep(1) count += 1 def car(name): while True: if event.is_set(): #判断是否设置了标志位（绿灯） print(\"[%s] running...\"%name) time.sleep(1) else: print(\"[%s] sees red light,waiting...\"%name) event.wait()#如果变为绿灯 print(\"[%s] green light is on,start going...\"%name) light = threading.Thread(target=lighter,) light.start() car = threading.Thread(target=car,args=(\"MINI\",)) car.start() queue队列 import threading import queue,time q=queue.Queue(maxsize=10) def Producer(name): count=1 while True: q.put(\"骨头 %s\"%count) print(\"{}生产了骨头\".format(name),count) count+=1 time.sleep(1) def Consumer(name): while True: print(\"[%s] 取到 [%s] 并且吃了它。。。\"%(name,q.get())) time.sleep(1) p=threading.Thread(target=Producer,args=('wlb',)) c=threading.Thread(target=Consumer,args=(\"dog\",)) c1=threading.Thread(target=Consumer,args=(\"cat\",)) p.start() c.start() c1.start() 互斥锁 由于线程之间是进行随机调度，并且每个线程可能只执行n条执行之后，当多个线程同时修改同一条数据时可能会出现脏数据，所以，出现了线程锁，即同一时刻允许一个线程执行操作。线程锁用于锁定资源，你可以定义多个锁, 像下面的代码, 当你需要独占某一资源时，任何一个锁都可以锁这个资源，就好比你用不同的锁都可以把相同的一个门锁住是一个道理。 由于线程之间是进行随机调度，如果有多个线程同时操作一个对象，如果没有很好地保护该对象，会造成程序结果的不可预期，我们也称此为“线程不安全”。 为了方式上面情况的发生，就出现了互斥锁(Lock) from threading import Thread,Lock import os,time def work(): global n lock.acquire() temp=n time.sleep(0.1) n=temp-1 lock.release() if __name__ == '__main__': lock=Lock() n=100 l=[] for i in range(100): p=Thread(target=work) l.append(p) p.start() for p in l: p.join() 信号量 互斥锁同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去。 import threading import time def run(n, semaphore): semaphore.acquire() #加锁 time.sleep(1) print(\"run the thread:%s\\n\" % n) semaphore.release() #释放 if __name__ == '__main__': num = 0 semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行 for i in range(22): t = threading.Thread(target=run, args=(\"t-%s\" % i, semaphore)) t.start() while threading.active_count() != 1: pass # print threading.active_count() else: print('--') GIL（Global Interpreter Lock）全局解释器锁 在非python环境中，单核情况下，同时只能有一个任务执行。多核时可以支持多个线程同时执行。但是在python中，无论有多少核，同时只能执行一个线程。究其原因，这就是由于GIL的存在导致的。 GIL的全称是Global Interpreter Lock(全局解释器锁)，来源是python设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到GIL，我们可以把GIL看作是“通行证”，并且在","date":"2021-01-21","objectID":"/thread/:0:0","tags":["python","thread"],"title":"thread","uri":"/thread/"},{"categories":["算法题"],"content":"丑数系列 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:0:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"1.丑数 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ugly-number/ ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:1","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"思路： 就是让这个数字不断地除以2.3.5 如果最后变成了1 就说明是个丑数 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:2","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def isUgly(self, num: int) -\u003e bool: if num\u003c=-231 or num\u003e=231-1: return False while num \u003e1: if num %2 == 0: num=int(num/2) elif num %3 ==0: num =int(num/3) elif num %5 ==0: num=int(num/5) else: break if num == 1: return True else: return False ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:3","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"丑数II ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ugly-number-ii/ ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:1","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"思路： 利用三指针，维护i2 i3 i5三个指针分别指向2 3 5 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:2","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def nthUglyNumber(self, n: int) -\u003e int: res = [1] # 先初始化为1 i2 = i3 = i5 = 0 # 初始化为0 for i in range(1,n): mins = min(res[i2]*2,res[i3]*3,res[i5]*5) # 从小到大找 res.append(mins) if res[i] == res[i2]*2: i2 += 1 if res[i] == res[i3]*3: i3 += 1 if res[i] == res[i5]*5: i5 += 1 return res[n-1] ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:3","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"梯度提升决策树(GBDT) GBDT(Gradient Boosting Decision Tree)是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案。 ","date":"2021-01-06","objectID":"/gbdt/:0:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"回归树 选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小 值时的(j,s)对。其中Rm是被划分的输入空间， \\(\\mathrm{cm}\\) 是空间Rm对应的固定输出值。 \\[ \\min_{j, s}\\left[\\min_{c_{1}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min_{c_{2}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}\\right] \\] 用选定的(j,s)对，划分区域并决定相应的输出值 \\[ \\begin{gathered} R_{1}(j, s)=\\{x \\mid x^{(j)} \\leq s\\}, R_{2}(j, s)=\\{x \\mid x^{(j)}\u003es\\} \\\\\\\\ \\hat{c}_{m}=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}(j, s)} y_{i} \\\\\\\\ x \\in R_{m}, m=1,2 \\end{gathered} \\] 继续对两个子区域调用上述步骤，将输入空间划分为 \\(M\\) 个区域R1,R2,..,Rm，生成决策树。 \\[ f(x)=\\sum_{m=1}^{M} \\hat{c}_{m} I\\left(x \\epsilon R_{m}\\right) \\] 当输入空间划分确定时，可以用平方误差来表示回归树对于训练数据的预测方法，用平方误差最小 的准则求解每个单元上的最优输出值。 ","date":"2021-01-06","objectID":"/gbdt/:1:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"提升树 梯度提升树是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。 提升树算法: (1) 初始化 \\(f_{0}(x)=0\\) (2) 对 \\(m=1,2, \\ldots, M\\) (a) 计算残差 \\[ r_{m i}=y_{i}-f_{m-1}(x), i=1,2, \\ldots, N \\] 为什么残差是这种形式？ 当采用平方误差损失函数时, \\[ L(y, f(x))=(y-f(x))^2 \\] 其损失变为 \\[ \\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_m\\right)\\right) \u0026=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_m\\right)\\right]^2 \\\\\\\\ \u0026=\\left[r-T\\left(x ; \\Theta_m\\right)\\right]^2 \\end{aligned} \\] 这里, \\[ r=y-f_{m-1}(x) \\] 也可以用上一步的残差（定义的上一步的标签）减去拟合上一步残差的回归树。即： \\[ r_{mi} = r_{(m-1)i} - h_{m-1}(x) \\] 易证明这两种形式等价。 (b) 拟合残差 \\(r_{m i}\\) 学习一个回归树，得到 \\(h_{m}(x)\\) (c) 更新 \\(f_{m}(x)=f_{m-1}+h_{m}(x)\\) (3) 得到回归问题提升树 \\[ f_{M}(x)=\\sum_{m=1}^{M} h_{m}(x) \\] ","date":"2021-01-06","objectID":"/gbdt/:2:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"GBDT GBDT与提升树不同的是GBDT使用负梯度来近似残差。 GBDT算法: (1) 初始化弱学习器 \\[ f_{0}(x)=\\arg \\min_{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) \\] 对 \\(m=1,2, \\ldots, M\\) 有: 对每个样本 \\(i=1,2, \\ldots, N\\) ，计算负梯度，即残差 \\[ r_{i m}=-\\left[\\frac{\\left.\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} \\] 一般回归的损失函数就是均方误差，但缺点是对于outlier比较敏感。 因此可以选择使用MAE或者Huber loss。所以说负梯度并不等于残差，损失函数选择MSE的时候才可以划等号。 将上步得到的残差作为样本新的真实值，并将数据 \\(\\left(x_{i}, r_{i m}\\right), i=1,2, . . N\\) 作为下棵树的训练数据，得到一颗新的回归树 \\(f_{m}(x)\\) 其对应的叶子节点区域为 \\(R_{j m}, j=1,2, \\ldots, J\\) 。其中 \\(J\\) 为回归树的叶子节点的个数。 对叶子区域 \\(j=1,2, . . J\\) 计算最佳拟合值 \\[ \\Upsilon_{j m}=\\underbrace{\\arg \\min}_{\\Upsilon} \\sum_{x_{i} \\in R_{j m}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\Upsilon\\right) \\] 也可以理解为： \\[ \\Upsilon_{jm} = \\underbrace{\\arg \\min}_{\\Upsilon} \\sum_{x_i \\in R_{jm}} L(r_{im}, \\Upsilon) \\] 损失函数L为MSE时，就与回归树的构建类似，最佳拟合值就是划分的叶子节点的均值。可以简单理解为提升树和GBDT的区别就是计算残差的方式不同。 更新强学习器 \\[ f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} \\Upsilon_{j m} I\\left(x \\in R_{j m}\\right) \\] 得到最终学习器 \\[ f(x)=f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M} \\sum_{j=1}^{J} \\Upsilon_{j m} I\\left(x \\in R_{j m}\\right) \\] 实例可以看参考里面。 ","date":"2021-01-06","objectID":"/gbdt/:3:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"与梯度下降算法的关系 ","date":"2021-01-06","objectID":"/gbdt/:4:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"代码 import numpy as np class RegressionTree: def __init__(self, max_depth=2, min_samples_split=2): self.max_depth = max_depth self.min_samples_split = min_samples_split self.tree = {} def fit(self, X, y): self.X = X self.y = y self.n_features = X.shape[1] self.n_samples = X.shape[0] self.tree = self._build_tree(X, y) def predict(self, X): return np.array([self._predict(inputs) for inputs in X]) def _build_tree(self, X, y, depth=0): m = X.shape[0] n = X.shape[1] # 1. 终止条件 if m \u003c= self.min_samples_split or depth \u003e= self.max_depth: return self._leaf(y) # 2. 找到最优分裂特征和特征值 feature, value = self._best_split(X, y) # 3. 构建子树 left_idx, right_idx = self._split(X, feature, value) left = self._build_tree(X[left_idx, :], y[left_idx], depth + 1) right = self._build_tree(X[right_idx, :], y[right_idx], depth + 1) return {\"feature\": feature, \"value\": value, \"left\": left, \"right\": right} def _leaf(self, y): return np.mean(y) def _best_split(self, X, y): m = X.shape[0] n = X.shape[1] min_mse = np.inf best_feature = None best_value = None for feature in range(n): values = np.unique(X[:, feature]) for value in values: y1 = y[X[:, feature] \u003c value] y2 = y[X[:, feature] \u003e= value] mse = np.mean(y1) - np.mean(y2) if mse \u003c min_mse: min_mse = mse best_feature = feature best_value = value return best_feature, best_value def _split(self, X, feature, value): left_idx = np.argwhere(X[:, feature] \u003c value).flatten() right_idx = np.argwhere(X[:, feature] \u003e= value).flatten() return left_idx, right_idx class GBDT: def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3): self.n_estimators = n_estimators self.learning_rate = learning_rate self.max_depth = max_depth self.trees = [] def fit(self, X, y): y_pred = np.zeros_like(y, dtype=np.float) for i in range(self.n_estimators): tree = RegressionTree(self.max_depth) tree.fit(X, -self.gradient(y, y_pred)) y_pred += self.learning_rate * tree.predict(X) self.trees.append(tree) def predict(self, X): y_pred = np.zeros((X.shape[0], ), dtype=np.float) for tree in self.trees: y_pred += self.learning_rate * tree.predict(X) return y_pred def gradient(self, y_true, y_pred): return y_true - y_pred 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？ Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。 对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近。所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。 对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。 ## 参考 https://blog.csdn.net/zpalyq110/article/details/79527653 https://zhuanlan.zhihu.com/p/280222403 ","date":"2021-01-06","objectID":"/gbdt/:5:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["算法题"],"content":"两两交换链表中的节点 ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:0:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/swap-nodes-in-pairs/ ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:1:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"思路: 先把第二位储存起来，然后将后面的递归操作后，再把第二位指向第一位，完成换位 ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:2:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"代码： # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: #假设为[1,2,3,4] def swapPairs(self, head: ListNode) -\u003e ListNode: if not head or not head.next: #递归出口 return head newnode = head.next #储存第二位2 head.next = self.swapPairs(head.next.next) #此时为[1,4,3] newnode.next = head #[2,1,4,3] return newnode ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:3:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"种花问题（新年快乐!2021第一题） 新年快乐！2021年第一题，每日一题！希望2021年LC和github可以全绿！加油！ https://leetcode-cn.com/problems/can-place-flowers/ 代码如下： class Solution: def canPlaceFlowers(self, flowerbed: List[int], n: int) -\u003e bool: flowerbed = [0] + flowerbed + [0] for i in range(1,len(flowerbed)-1): if flowerbed[i-1] == 0 and flowerbed[i] == 0 and flowerbed[i+1] == 0: n -= 1 flowerbed[i] = 1 return n \u003c= 0 思路很暴力，就是三个0在一起就可以插进去。。 主要是边界问题，这里构造了两个边界 新的一年开开心心，完成自己的目标，让自己更优秀！ ","date":"2021-01-01","objectID":"/%E7%A7%8D%E8%8A%B1%E9%97%AE%E9%A2%98%E6%96%B0%E5%B9%B4%E5%BF%AB%E4%B9%902021%E7%AC%AC%E4%B8%80%E9%A2%98/:0:0","tags":["算法题","种花问题（新年快乐!2021第一题）"],"title":"种花问题（新年快乐!2021第一题）","uri":"/%E7%A7%8D%E8%8A%B1%E9%97%AE%E9%A2%98%E6%96%B0%E5%B9%B4%E5%BF%AB%E4%B9%902021%E7%AC%AC%E4%B8%80%E9%A2%98/"},{"categories":["算法题"],"content":"零钱兑换 https://leetcode-cn.com/problems/coin-change/ 以我目前的水平做出来有点吃力，看了思路才做出来 class Solution: def coinChange(self, coins: List[int], amount: int) -\u003e int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(amount+1): for coin in coins: # if i \u003e= coin: dp[i] = min(dp[i],dp[i-coin]+1) return -1 if (dp[-1] == float(\"inf\")) else dp[-1] 伪代码如下 # 伪码框架 def coinChange(coins: List[int], amount: int): # 定义：要凑出金额 n，至少要 dp(n) 个硬币 def dp(n): # 做选择，选择需要硬币最少的那个结果 for coin in coins: res = min(res, 1 + dp(n - coin)) return res # 题目要求的最终结果是 dp(amount) return dp(amount) ","date":"2020-10-10","objectID":"/%E9%9B%B6%E9%92%B1%E5%85%91%E6%8D%A2/:0:0","tags":["算法题","零钱兑换"],"title":"零钱兑换","uri":"/%E9%9B%B6%E9%92%B1%E5%85%91%E6%8D%A2/"},{"categories":["算法题"],"content":"去除重复字母 一开始看到题目感觉挺简单的，没想到对现在的我挺有难度。。 https://leetcode-cn.com/problems/remove-duplicate-letters/ #1 class Solution: def removeDuplicateLetters(s: str): res = \"\" while s: #用递归也可以 loc = min(map(s.rindex,s)) #s.rindex是返回列表各值最后出现的索引 求这个最小的索引 a = min(s[:loc+1]) #求字典序最小的 res += a s = s[s.index(a):].replace(a,\"\") #把已经加入的和与其重复的都去掉了 return res #2 #遍历字符串，压入栈，如果遇到比栈顶小的元素且当前字符后面还有与栈顶相同的元素时，移除栈顶元素 class Solution: def removeDuplicateLetters(s: str) -\u003e str: stack = [] for i, t in enumerate(s): if t in stack: continue while stack !=[] and t \u003c stack[-1] and s[i:].find(stack[-1]) != -1: stack.pop() stack.append(t) return \"\".join(stack) 两个方法，第二个方法更好想点。第一个方法是copy的 ","date":"2020-09-02","objectID":"/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%AD%97%E6%AF%8D/:0:0","tags":["算法题","去除重复字母"],"title":"去除重复字母","uri":"/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%AD%97%E6%AF%8D/"},{"categories":null,"content":" 好久没折腾博客了，总觉得mkdocs的可定义功能太少了，而且不太美观，于是将博客从mkdocs迁移到Hugo了，原本是想用hexo的，但是图片的引用改过来的话太麻烦了，所以选择了hugo。花费了一点时间迁移，主要是工程量有点大，之前写的博文内容不符合hugo的规范，所以写了一些脚本用于迁移，总体还是比较顺利，本来是想主要是想创造一个舒服的写作环境。目前是obsidian+hugo，然后部署在github pages上面。 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"关于本站 从大一开始就折腾博客了，从一开始只会python语言，到现在会了一点点机器学习，把博客当做自己的成长过程吧，其实大部分都是直接照搬网上的东西，不过自己复现了一些代码，我认为理论与实践要统一，不能只有理论知识，但没有理论去实践，就算照着写出来也一头雾水，做调参侠，但要知道每个参数的意义。 不知道写啥了，先空着。 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"}]