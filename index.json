[{"categories":["工具"],"content":"贴一下可以玩的shortcode。 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:0:0","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"音乐播放 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:0","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"播放列表 夏日口袋专辑： ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:1","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"播放单曲 最爱的一首（我是紬厨）： ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:1:2","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"视频播放 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:2:0","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"bilibili 有多P可以选择集数 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:2:1","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"admonition 类型有：note、abstract、info、tip、success、question、warning、failure、danger、bug、example、quote。 技巧 一个 技巧 横幅 ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:3:0","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["工具"],"content":"mapbox ","date":"2023-03-07","objectID":"/shortcode%E7%BD%AE%E9%A1%B6/:4:0","tags":["工具"],"title":"shortcode(置顶)","uri":"/shortcode%E7%BD%AE%E9%A1%B6/"},{"categories":["Benchmark","reading"],"content":"这篇论文发表于2024年6月，来自亚利桑那大学、微软研究院、艾伦人工智能研究所等顶尖机构，是一项关于大型语言模型（LLM）能力边界探索的严谨、扎实的量化研究。它并没有提出一个全新的、性能超群的模型，而是像一位严谨的实验物理学家，设计了一套精巧的实验装置，来精确测量并回答一个基础且重要的问题：当前最先进的语言模型，在多大程度上可以取代传统的手工编码，直接作为一个动态世界的“模拟器”？ 论文的核心贡献在于，它首次为这个问题提供了定量的答案，而不仅仅是定性的描述或个例的展示。为此，作者们构建了一个全新的基准测试集 BYTESIZED32-State-Prediction (BYTESIZED32-SP)，其中包含了超过76,000个从文本游戏中提取的“状态转换”样本。这就像是为语言模型建立了一个“物理实验室”，每一个样本都是一次“实验”，模型需要预测在一个给定的世界状态（State）下，当一个动作（Action）发生后，世界会变成什么新的状态。 为了更精细地剖析模型的能能力，作者们设计了一个巧妙的评估框架。他们将复杂的“世界演化”过程 F 分解为三个关键部分： 1. Fact (Action-driven transition)：由智能体的动作直接引起的状态变化。例如，玩家执行“拿起杯子”的动作，那么杯子就从桌子上转移到了玩家手中。这是对模型理解直接因果关系能力的考验。 2. Fenv (Environment-driven transition)：由环境内在规律驱动的状态变化。例如，玩家打开了水龙头，即使玩家不再做任何动作，水会因为物理规律自动注满水槽里的杯子。这考验的是模型对世界背景知识和物理常识的理解。 3. FR (Game progress)：对游戏进程（如得分、游戏是否结束）的预测。 通过分别测试模型在这三项任务上的表现，论文得以深入洞察模型能力的强项与短板。实验以强大的 GPT-4 模型为主要研究对象，并系统地评估了不同条件下的性能，例如提供由人类专家编写的规则、由LLM自己生成的规则，或者不提供任何规则。 实验结果揭示了深刻的洞见。正如论文在摘要中所言： \u003e We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. \u003e (我们在这个数据集上测试了GPT-4，发现尽管其性能令人印象深刻，但若无进一步创新，它仍是一个不可靠的世界模拟器。) dynamic是指状态发生变化的情况，static是状态没有发生变化的情况。左侧LLM、Human代表制定的不同规则来约束行为。Full是指预测完整的状态转换，而Diff是指预测两个状态的差异。 具体来说，关键数据（如论文表2所示）表明，GPT-4在模拟由动作直接驱动的转换（Fact）时表现尚可，在有明确规则指导下，对动态变化的预测准确率最高能达到77.1%。然而，一旦涉及到需要理解环境内在规律的转换（Fenv），其准确率便骤降至49.7%。这意味着，模型能够较好地理解“你做了A，就导致B”这种直接指令，但对于“因为A发生了，所以环境中的C会随之发生D”这种间接、隐含的动态，模型的把握能力就差很多。 更有说服力的是，论文进行了细致的错误分析（如图2所示）。分析发现，模型的错误主要集中在那些需要算术（arithmetic）、常识（common-sense）或科学知识（scientific knowledge）的属性上。例如，对于简单的布尔值属性（如“是否开启”），模型预测得很好；但对于需要计算的“温度”变化、需要常识判断的相机“光圈”设置，模型的表现就差强人意。 这引出了论文最核心的结论：单步预测的微小误差会在多步模拟中累积放大，导致模拟结果迅速偏离真实情况。 论文用一个生动的例子阐述了这一点：即使模型在动态变化上的单步最佳准确率为59.9%，在连续模拟10步之后，整体的准确率将下降到 \\(0.599^{10}\\)，即不到1%。这清晰地表明，目前的LLM在作为可靠的世界模拟器方面，还有很长的路要走。 总而言之，这篇论文通过构建新基准、设计精巧的评估框架和进行全面的量化实验，为“LLM作为世界模拟器”这一前沿课题提供了首个系统性的、数据驱动的深刻洞见。它不仅揭示了当前SOTA模型的能力边界，也为未来的研究指明了具体的挑战和方向。 接下来，我将按照您提出的六个问题，逐一进行详细解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是定量地评估当前的大型语言模型（LLM）在没有任何专门训练的情况下，作为基于文本的虚拟世界模拟器的能力和局限性。它试图回答：LLM能否准确预测在一个给定世界状态下，执行一个动作后，世界将如何演变？ 解决的实际问题：该研究旨在解决构建虚拟环境中的一个核心痛点——开发成本高昂且耗时。在AI研究，尤其是强化学习、规划和机器人学领域，研究人员需要大量的、高质量的虚拟环境来训练和测试智能体。传统上，这些环境需要由人类专家花费数周甚至数月的时间手动编码，定义每一个对象、每一个动作及其背后的复杂逻辑。如果LLM能够胜任这项工作，将极大地降低开发门槛，实现“用自然语言描述来即时生成一个可交互的虚拟世界”。 对行业发展的重要意义： 加速AI研究：若LLM能成为可靠的模拟器，AI研究者可以快速创建和迭代复杂的测试环境，从而加速在规划、推理和决策等领域的研究进程。 变革游戏开发：对于游戏产业，这意味着一种全新的内容生成范式。游戏设计师可以用自然语言来描述游戏世界的规则和动态，快速生成游戏原型，甚至创造出能够动态演化、真正“活”起来的游戏世界，极大地丰富玩家的体验。 推动通用人工智能（AGI）：构建和理解世界模型（World Models）被认为是通往AGI的关键一步。一个能够准确模拟世界的系统，意味着它在某种程度上理解了世界的运行规律。因此，这项研究也是对LLM作为世界模型潜力的一次基础性探底，其结果对评估我们距离AGI还有多远具有参考价值。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。 这篇论文的创新不在于提出一个新模型，而在于提出了一套新的评估思路和研究框架。 新的思路：直接模拟 (Direct Simulation) 的量化评估 在本文之前，利用LLM进行世界建模主要有两种思路。一种是神经符号（neurosymbolic）方法，即让LLM生成符号化的代码（如Python程序），然后由确定性的代码执行器来模拟世界。而本文聚焦于第二种思路，也是研究较少的直接模拟，即LLM直接生成下一个世界状态的文本描述（本文中为JSON格式）。 特点与优势：这种思路的优势在于其灵活性和通用性，理论上可以模拟任何能用语言描述的动态。而本文的贡献在于，它没有停留在概念层面，而是首次为“直接模拟”这条技术路线建立了一套严谨的量化评估体系，让人们可以清晰地看到它的成效和瓶颈。 新的方法与框架： LLM-Sim 任务的分解框架：如前文所述，将模拟任务 F 分解为 Fact (动作驱动), Fenv (环境驱动), 和 FR (游戏进程) 是本文方法论上的核心创新。 \u003e To better understand LLM’s ability to model each of these transitions, we further decompose the simulator function F into three steps… \u003e (为了更好地理解LLM建模每种转换的能力，我们将模拟器函数F进一步分解为三个步骤…) 优势：这种分解使得分析变得极为精细。如果模型整体表现不佳，我们可以定位到具体是哪个环节出了问题。实验结果也证明了这种分解的价值：LLM在Fact上表现尚可，但在Fenv上表现糟糕，这清晰地指出了模型能力的短板在于对环境内在规律的理解不足。 BYTESIZED32-SP 基准数据集：这是一个专门为状态预测任务构建的大规模、高质量数据集。它源自于BYTESIZED32语料库，包含了31个不同的文本游戏，覆盖了各种常识和初级科学推理概念。 优势：相比于之前依赖于特定游戏或小规模例子的研究，这个大规模数据集保证了评估结果的通用性和鲁棒性，使得出的结论更具说服力。 双重预测范式：全状态预测 (Full State) vs. 状态差异预测 (State Difference)：论文还测试了两种不同的输出模式。前者要求模型生成完整的下一个状态，后者只要求模型生成发生变化的部分。 优势：这可以用来评估模型输出的简洁性和对“变化”的捕捉能力。有趣的是，实验发现两种范式各有优劣，这为未来如何设计更高效的提示（Prompt）提供了参考。 3. 论文通过什么实验来验证所提出方法的有效性？实验是如何设计的？实验数据和结果如何？请引用关键数据加以说明。 论文的实验设计非常系统和严谨，通过控制变量法，全面地测试了GPT-4在LLM-Sim任务上的表现。 实验设计： 核心任务：在BYTESIZED32-SP数据集上，给定当前状态st、动作at和上下文c，预测下一个状态st+1。 主要变量： 模拟类型：F (完整模拟), Fact (仅动作驱动), Fenv (仅环境驱动)。 规则上下文：提供“人类专家编写的规则”、“LLM生成的规则”或“无规则”。 状态变化类型：动态 ","date":"2025-07-28","objectID":"/can-language-models-serve-as-text-based-world-simulators/:0:0","tags":["Reading","Benchmark"],"title":"Can Language Models Serve as Text-Based World Simulators?","uri":"/can-language-models-serve-as-text-based-world-simulators/"},{"categories":["LLM-RL","reading"],"content":"这篇论文的核心贡献是提出了一种名为 组序列策略优化 (Group Sequence Policy Optimization, GSPO) 的新型强化学习（RL）算法，旨在解决在训练大型语言模型（特别是混合专家模型, Mixture-of-Experts, MoE）时普遍存在的训练不稳定甚至模型崩溃的痛点。这不仅仅是一次微小的算法改进，而是一次对现有主流RL优化范式的根本性反思与重构，其核心思想是 “将优化的基本单元与奖励的基本单元对齐”。 在深入理解GSPO之前，我们需要了解它试图取代的先前方法，尤其是组相对策略优化 (Group Relative Policy Optimization, GRPO)。传统的RL算法，如经典的近端策略优化 (PPO)，在应用于大模型时，通常需要一个独立的、与模型本身差不多大的“价值模型”（Value Model）来评估每个决策（即每个token）的好坏，这带来了巨大的计算和内存开销。为了解决这个问题，GRPO被提出，它巧妙地通过在同一查询（query）生成的多个回答（response）之间进行“相对比较”，从而绕开了价值模型。具体来说，它会比较一个回答的奖励（reward）与同组其他回答的平均奖励，从而计算出优势（advantage），并用这个优势来指导模型更新。 然而，Qwen团队发现，GRPO虽然思路巧妙，却存在一个致命的“设计缺陷”。正如论文在第3节“动机”中所述，GRPO的根基——重要性采样（importance sampling）被“误用”了。重要性采样是一种统计学技巧，目的是用一个分布（行为分布）的采样来估计另一个分布（目标分布）下的期望值。它的有效性依赖于对大量样本的平均。但GRPO却在词元级别 (token-level) 上应用了它。 In contrast, GRPO applies the importance weight … at each token position t. Since this weight is based on a single sample yi,t from each next-token distribution … it fails to perform the intended distribution-correction role. Instead, it introduces high-variance noise into the training gradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. 换句话说，对于一个长序列中的每一个词元，GRPO都计算一个“重要性权重”，试图校正新旧策略的差异。但因为每个词元的生成只是“一次采样”，这种校正不仅无效，反而引入了巨大的、不稳定的噪声。当模型变得庞大、回答序列变长时，这种噪声会累积并被PPO家族算法中的“裁剪机制”（clipping mechanism）放大，最终导致训练过程“灾难性且不可逆转地崩溃”。 GSPO正是为了解决这个根本问题而设计的。它的核心创新点，如论文第4.1节所述，是将重要性采样的应用从“词元级别”提升到了“序列级别” (sequence-level)。既然奖励是针对整个回答序列给出的，那么策略更新的校正也应该在整个序列的层面上进行。GSPO定义了一个基于整个序列似然度（sequence likelihood）的重要性比率 s_i(θ)（公式8），这个比率衡量的是新策略生成整个回答序列的概率与旧策略生成该序列概率的比值。 \\[ s_i(\\theta) = \\left( \\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\theta_{\\text{old}}}(y_i|x)} \\right) ^ {\\frac{1}{\\mid y_{i}\\mid}} \\] 基于这个序列级别的重要性比率，GSPO对整个序列的优势函数 Â_i 进行裁剪（clipping）和优化（公式6）。这样做的好处是显而易见的：它从根本上避免了GRPO在词元级别引入的噪声累积问题。如论文的梯度分析部分（公式11 vs 公式13）所揭示的，GRPO对序列中每个词元的梯度赋予了不同的权重，而GSPO则对同一序列中的所有词元赋予了相同的权重，这个权重由整个序列的表现决定。这极大地增强了训练的稳定性。 论文通过一系列详实的实验证明了GSPO的优越性。最引人注目的莫过于图1展示的在AIME’24（数学）、LiveCodeBench（编程）等高难度任务上的训练曲线。GSPO不仅稳定地提升性能，而且在相同的计算资源下，其训练效率和最终性能都显著优于精心调优后的GRPO。 一个非常“反直觉”但又深刻的发现体现在图2中。GSPO在训练中裁剪掉的词元比例（Clipping Fraction）高达15%，而GRPO仅为0.13%，两者相差超过100倍。传统的观念认为，裁剪掉过多的样本意味着浪费数据，会降低训练效率。但GSPO的实验结果恰恰相反：“大规模的裁剪反而带来了更高的训练效率”。 This counter-intuitive finding — that clipping a much larger fraction of tokens leads to superior training efficiency — further indicates that GRPO’s token-level gradient estimates are inherently noisy and inefficient for sample exploitation. In contrast, GSPO’s sequence-level approach provides a more reliable and effective learning signal. 这雄辩地证明了GRPO的词元级梯度估计充满了噪声，而GSPO的序列级信号则更加可靠和有效。此外，论文还强调了GSPO在训练MoE模型时的巨大优势（第5.3节）。MoE模型在训练中存在“专家激活不一致”的问题，即梯度更新后，模型对同一个输入激活的专家组合会发生变化，这对于GRPO的词元级重要性权重是灾难性的。而GSPO只关心整个序列的似然度，对底层专家激活的变化不敏感，从而天然地解决了MoE模型的训练稳定性问题，甚至不再需要像“路由回放”（Routing Replay）这样复杂的额外技巧。 总而言之，GSPO通过一个看似简单却直击问题本质的改变——将优化单元从词元级提升至序列级——成功解决了大规模RL训练中的核心稳定性难题，为更大规模、更强能力的LLM（尤其是MoE架构）的持续发展铺平了道路，是LLM训练算法领域一个坚实而优雅的进步。 接下来，我将按照您提出的六个问题，逐一进行更详细的解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：设计一种新的、更稳定、更高效的强化学习（RL）算法，用于大型语言模型的训练。 要解决的实际问题： 训练不稳定性：现有SOTA（State-of-the-Art）的RL算法，如GRPO，在训练超大规模语言模型（如拥有数百亿甚至更多参数的MoE模型）时，会遭遇严重的稳定性问题，频繁导致训练过程突然恶化，即“模型崩溃”。 效率瓶颈：GRPO算法因其内在的噪声问题，导致样本利用效率不高，需要更多的计算资源才能达到理想的性能。 MoE模型训练难题：MoE模型在RL训练中存在独特的“专家激活波动性”问题，GRPO等传统方法难以应对，需要复杂的辅助策略（如Routing Replay）才能勉强收敛。 对行业发展的重要意义： 解锁更大规模的RL训练：稳定是规模化的前提。如果投入数千张GPU进行数周的RL训练，结果却因为算法不稳定而崩溃，这将是巨大的资源浪费。GSPO这样的稳定算法是推动LLM能力边界（例如在数学、编程等复杂推理任务上）的基石，使得更大规模的RL投资成为可能。 简化RL训练设施与流程：GSPO因为其内在的稳定性，特别是在MoE训练上，不再需要“路由回放”等复杂的“补丁”，这可以显著简化RL的训练代码和系统架构（RL Infrastructure），降低维护成本，提高研发效率。 推动MoE架构的普及：MoE被认为是未来扩展LLM能力的关键路径之一。GSPO解决了其在RL阶段的一个核心训练难题，将极大地促进MoE模型在更多场景下的应用和发展。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 核心新思路：“奖励与优化单元对齐” (Aligning the unit of optimization with the unit of reward)。这是整篇论文的哲学基石。既然奖励（如一个数学题是否做对）是基于整个生成序列来评估的，那么用于策略更新的重要性采样和裁剪等核心机制也应该在序列级别上进行，而不是在更细的词元级别上。 具体新方法：GSPO (Group Sequence Policy Optimization) 算法。 与GRPO的对比与优势： | 特性 | GRPO (之前的方法) | GSPO (新方法) | 优势 | | :— | :— | :— | :— | | 重要性比率 (Importance Ratio) | 词元级别：为每个token计算一个w_i,t(θ) | 序列级别：为整个sequence计算一个s_i(θ) |","date":"2025-07-28","objectID":"/group-sequence-policy-optimization/:0:0","tags":["Reading","LLM-RL"],"title":"Group Sequence Policy Optimization","uri":"/group-sequence-policy-optimization/"},{"categories":["RLHF","LLM"],"content":"这篇由字节跳动Seed团队在2025年4月发表的论文，直面了当前大模型领域中一个核心且棘手的难题：如何通过强化学习（Reinforcement Learning, RL）高效、稳定地提升模型在复杂推理任务上的能力。当前，以链式思考（Chain-of-Thought, CoT）为代表的推理技术是实现通用人工智能（AGI）的关键路径，而如何让模型学会更长、更可靠的推理链，是业界公认的瓶颈。 传统的强化学习方法在应用于大型语言模型（LLM）时，通常分为两大流派：无价值模型（value-model-free）和基于价值模型（value-model-based）的方法。近年来，如DAPO等无价值模型方法因其简单稳定而成为主流。它们绕过了训练一个价值模型（Value Model）——即一个用于预测在某个状态下未来总回报期望值的网络——的复杂过程，直接将整个推理序列的最终奖励（比如答案正确与否）分配给序列中的每一步。这种方法虽然规避了难题，但也触及了性能的天花板。原因在于，它无法实现精确的信用分配（credit assignment），即无法分辨出一个推理步骤中的具体哪个词或哪个想法是导致最终成败的关键。 这篇论文的核心论点，正是对当前主流趋势的一次“拨乱反正”。作者们认为，基于价值模型的方法拥有更高的性能上限，因为它能为每一步决策提供更精细、更低方差的指导信号。其面临的挑战并非无解，而是需要被系统性地识别和攻克。论文精准地指出了阻碍价值模型训练的三大核心挑战： 长序列上的价值模型偏差（Value Model Bias over Long Sequences）：价值模型通常由奖励模型（Reward Model）初始化，但二者目标不一致。奖励模型旨在为完整的句子打分，而价值模型需要评估不完整句子的未来潜力，这导致了严重的初始化偏差。 训练中异构序列长度（Heterogeneous Sequence Lengths during Training）：推理任务中，模型生成的文本长度差异巨大。传统的广义优势估计（Generalized Advantage Estimation, GAE）方法使用固定的λ参数，无法同时适应长短不一的序列，导致对短序列估计方差过高，对长序列估计偏差过大。 验证器任务中奖励信号的稀疏性（Sparsity of Reward Signal）：在数学、代码等推理任务中，奖励通常是二元的（0或1），并且只在推理链的末尾给出。模型需要进行大量探索才能偶尔获得一个正向反馈，学习效率极低。 为了系统性地解决上述问题，论文提出了VAPO框架。它并非单一的算法革新，而是一套集大成的“组合拳”，包含了七项关键技术优化，共同将基于价值模型的强化学习推向了新的高度。这七项技术如同一组精密配合的齿轮，协同作用，最终实现了1+1\u003e2的效果。 VAPO框架的卓越性能在AIME 2024（一个高难度的数学推理竞赛数据集）上得到了充分验证。如下图所示，基于同样的Qwen2.5-32B模型，VAPO的性能显著超越了之前的SOTA方法DAPO。 In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. VAPO不仅在最终得分（60.4分 vs DAPO的约50分）上取得了巨大优势，更重要的是，它在训练效率和稳定性上表现优异，仅用约5000步训练就达到了SOTA水平，且多次实验结果一致，没有出现训练崩溃。这充分证明，只要方法得当，基于价值模型的强化学习是一条更具潜力的技术路线。论文通过详尽的消融实验（Ablation Study），逐一验证了其提出的七个模块各自不可或缺的贡献，为后续研究者提供了清晰、可复现的技术蓝图。 Figure 1 from paper 图1：VAPO在AIME 2024上的得分与DAPO对比，展示了其在性能和训练效率上的显著优势。 接下来，我将按照您提出的六个维度，对这篇论文进行更深入的拆解和分析。 一、论文的研究目标是什么？ 核心研究目标：论文旨在创建一个高效且可靠的、基于价值模型的强化学习框架（即VAPO），专门用于提升大语言模型在高级推理任务（特别是长链式思考任务）上的性能。 想要解决的实际问题：当前业界在利用强化学习提升大模型推理能力时，普遍遇到了瓶颈。 性能瓶颈：以DAPO为代表的无价值模型方法，虽然稳定，但由于无法进行精确的信用分配，其性能提升已接近极限。模型很难学会进行超过一定长度和复杂度的推理。 训练瓶颈：直接应用基于价值模型的传统强化学习算法（如PPO），在长链式思考任务上极易失败，表现为训练崩溃（training collapse），即模型性能急剧下降，甚至只会生成无意义的短回答。 对行业发展的重要意义： 指明技术方向：这篇论文为整个行业探索大模型推理能力的提升，指出了一个更具潜力的方向。它证明了克服价值模型训练的困难是完全可行的，其回报是远超无价值模型方法所能达到的性能高度。 推动复杂任务自动化：强大的推理能力是实现科学发现自动化、编写复杂软件、进行法律分析等高级认知任务的前提。VAPO这类框架的出现，将极大加速AI在这些专业领域的应用落地。 降低SOTA模型训练门槛：通过提升训练效率和稳定性，VAPO实际上降低了训练出顶尖推理模型的计算成本和时间成本，使得更多机构有能力参与到前沿模型的研发中。 二、论文提出了哪些新的思路、方法或模型？ VAPO的核心思想是“直面问题，系统解决”。它没有回避价值模型训练的困难，而是将其拆解为三个子问题，并针对性地提出了七项技术改进。这些技术共同构成了VAPO框架，其特点和优势在于其系统性和协同效应。 We pinpoint three key challenges that plague value-model-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges… 以下是VAPO框架中七个关键组件的详细解析： 价值预训练 (Value-Pretraining) 思路：为了解决价值模型的初始化偏差问题，在正式的PPO训练开始前，增加一个预训练阶段。此阶段固定策略模型，生成样本，然后用蒙特卡洛回报（Monte-Carlo return）——即从当前步到结束的实际累积奖励——来监督价值模型的训练。 优势：这相当于先让价值模型“见过”真实的回报长什么样，对其进行有效“热身”，极大地缓解了从奖励模型继承来的偏差，是防止训练初期崩溃的关键第一步。 解耦的GAE (Decoupled-GAE) 思路：在计算优势函数时，为价值网络和策略网络的更新使用不同的λ参数。具体地，价值网络更新使用 λ_critic = 1.0，策略网络更新使用一个较小的值如 λ_policy = 0.95。 优势：λ=1.0 意味着价值函数的更新目标是无偏的蒙特卡洛回报，这能确保价值模型学到更准确的长期价值。而较小的 λ_policy 能引入一定的偏差以降低方差，从而加速策略的收敛。这种“解耦”设计兼顾了价值学习的准确性和策略学习的稳定性。 长度自适应GAE (Length-Adaptive GAE) 思路：这是对Decoupled-GAE的进一步创新。作者发现固定的 λ_policy 无法适应长短不一的序列。因此，他们设计了一个动态的λ，使其随序列长度l自适应调整。公式为： \\[ \\lambda_{\\text{policy}} = 1 - \\frac{1}{\\alpha l} \\] 优势：对于短序列，λ值较低，估计偏向于低偏差、高方差的蒙特卡洛方法；对于长序列，λ值接近1，估计偏向于高偏差、低方差的引导（bootstrapping）方法。这种自适应机制完美地平衡了不同长度序列下的偏差-方差权衡，是VAPO的一大亮点。 词元级策略梯度损失 (Token-Level Policy Gradient Loss) 思路：传统的PPO损失函数先在序列内部对所有词元的损失进行平均，再对一个批次内的所有序列求平均。这导致长序列中的单个词元对总损失的贡献过小。VAPO修改了损失函数，直接在整个批次的所有词元上进行平均。 优势：确保了每个词元（token）在梯度更新中拥有平等的“话语权”。这使得模型能够更有效地关注并修正长序列中的错误，防止因权重被稀释而导致的问题被忽视。 更高的裁剪上界 (Clip-Higher) 思路：在PPO的裁剪目标函数中，将裁剪范围的上下界ε解耦为 ε_low 和 ε_high，并设置一个更大的上界（如ε_high = 0.28, ε_low = 0.2）。 优势：更大的ε_high为提升","date":"2025-07-27","objectID":"/vapo/:0:0","tags":["LLM","RLHF"],"title":"vapo","uri":"/vapo/"},{"categories":["verl","coding"],"content":"详细讲解一下verl中的RLHFDataset，它继承自torch的Dataset，需要实现__getitem__来返回数据。 ","date":"2025-07-26","objectID":"/dataset/:0:0","tags":["coding","verl"],"title":"Dataset","uri":"/dataset/"},{"categories":["verl","coding"],"content":"初始化 class RLHFDataset(Dataset): \"\"\" Load and preprocess RLHF data from Parquet files. - Caches files locally. - Reads into a HuggingFace Dataset and tokenizes prompts. - Optionally handles images/videos via a ProcessorMixin. - Filters prompts over a max length. - Supports resuming from checkpoints. Args: data_files (str or list): Path(s) to Parquet file(s). tokenizer (PreTrainedTokenizer): For the tokenization of text to token IDs. config (DictConfig): Options like cache_dir, prompt_key, max_prompt_length, truncation, etc. processor (ProcessorMixin, optional): Multimodal preprocessor for images/videos. \"\"\" def __init__( self, data_files: str | list[str], tokenizer: PreTrainedTokenizer, config: DictConfig, processor: Optional[ProcessorMixin] = None, ): if not isinstance(data_files, list | ListConfig): data_files = [data_files] self.data_files = copy.deepcopy(data_files) self.original_data_files = copy.deepcopy(data_files) # use for resume self.tokenizer = tokenizer self.processor = processor self.config = config self.cache_dir = os.path.expanduser(config.get(\"cache_dir\", \"~/.cache/verl/rlhf\")) self.prompt_key = config.get(\"prompt_key\", \"prompt\") self.image_key = config.get(\"image_key\", \"images\") self.video_key = config.get(\"video_key\", \"videos\") self.max_prompt_length = config.get(\"max_prompt_length\", 1024) self.return_raw_chat = config.get(\"return_raw_chat\", False) self.return_full_prompt = config.get(\"return_full_prompt\", False) self.truncation = config.get(\"truncation\", \"error\") self.filter_overlong_prompts = config.get(\"filter_overlong_prompts\", True) self.num_workers = config.get(\"filter_overlong_prompts_workers\", max(1, os.cpu_count() // 4)) self.num_workers = min(self.num_workers, os.cpu_count()) self.use_shm = config.get(\"use_shm\", False) self.chat_template_func = config.get(\"chat_template_func\", None) self.need_tools_kwargs = config.get(\"need_tools_kwargs\", False) self.filter_prompts = config.get(\"filter_prompts\", True) self.serialize_dataset = False self.return_multi_modal_inputs = config.get(\"return_multi_modal_inputs\", True) self._download() self._read_files_and_tokenize() （来自deepwiki） download方法就是把hdfs文件或者本地文件缓存到缓存路径下。 接下来重点看一下read_files_and_tokenize方法： def _read_files_and_tokenize(self): dataframes = [] for parquet_file in self.data_files: # read parquet files and cache dataframe = datasets.load_dataset(\"parquet\", data_files=parquet_file)[\"train\"] dataframes.append(dataframe) self.dataframe: datasets.Dataset = datasets.concatenate_datasets(dataframes) print(f\"dataset len: {len(self.dataframe)}\") self.dataframe = self.maybe_filter_out_long_prompts(self.dataframe) def maybe_filter_out_long_prompts(self, dataframe: datasets.Dataset = None): # filter out too long prompts if self.filter_overlong_prompts: tokenizer = self.tokenizer processor = self.processor prompt_key = self.prompt_key image_key = self.image_key video_key = self.video_key if processor is not None: from verl.utils.dataset.vision_utils import process_image, process_video def doc2len(doc) -\u003e int: messages = self._build_messages(doc) raw_prompt = self.processor.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) images = [process_image(image) for image in doc[image_key]] if image_key in doc else None videos = [process_video(video) for video in doc[video_key]] if video_key in doc else None return len(processor(text=[raw_prompt], images=images, videos=videos)[\"input_ids\"][0]) else: def doc2len(doc) -\u003e int: return len(tokenizer.apply_chat_template(doc[prompt_key], add_generation_prompt=True)) dataframe = dataframe.filter( lambda doc: doc2len(doc) \u003c= self.max_prompt_length, num_proc=self.num_workers, desc=f\"Filtering prompts longer than {self.max_prompt_length} tokens\", ) print(f\"filter dataset len: {len(dataframe)}\") return dataframe 实现了读取parquet文件，然后再根据传入的prompt_length筛选掉prompt长度超过length的样本。返回的是dataframe。 然后就到了最重要的__getitem__方法，来构造我们需要的数据： def __getitem__(self, item): \"\"\" Note that we also return the raw_input_ids so that it can be co","date":"2025-07-26","objectID":"/dataset/:1:0","tags":["coding","verl"],"title":"Dataset","uri":"/dataset/"},{"categories":["大模型分布式","LLM"],"content":"verl中的device_mesh verl中有3个device_mesh，分别是： - 训练用的FSDP mesh（通常是一维） - 推理用的rollout mesh（包含tp维度） - Ulysses序列并行的mesh（dp×sp） ","date":"2025-07-26","objectID":"/device_mesh/:1:0","tags":["LLM","大模型分布式"],"title":"device_mesh","uri":"/device_mesh/"},{"categories":["大模型分布式","LLM"],"content":"fsdp mesh ","date":"2025-07-26","objectID":"/device_mesh/:2:0","tags":["LLM","大模型分布式"],"title":"device_mesh","uri":"/device_mesh/"},{"categories":["大模型分布式","LLM"],"content":"rollout mesh ","date":"2025-07-26","objectID":"/device_mesh/:3:0","tags":["LLM","大模型分布式"],"title":"device_mesh","uri":"/device_mesh/"},{"categories":["大模型分布式","LLM"],"content":"ulysses mesh ","date":"2025-07-26","objectID":"/device_mesh/:4:0","tags":["LLM","大模型分布式"],"title":"device_mesh","uri":"/device_mesh/"},{"categories":["Reasoning","reading"],"content":"好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇名为《Towards Effective Code-Integrated Reasoning》的论文 (arXiv:2505.24480v1)。这篇论文系统性地探讨了一个在当前大模型研究中至关重要的前沿方向：如何让模型更稳定、更有效地利用外部工具（特别是代码解释器）来完成复杂的推理任务。 首先，让我们对这篇论文进行一个全面的、深入的解读。 这篇论文的核心是关于代码集成推理（Code-Integrated Reasoning, CIR），即大语言模型在解决问题时，能够根据需要自主生成代码，并通过调用一个代码解释器（Code Interpreter）来执行代码、获取结果，再将这个结果整合回自身的推理链条中，以辅助后续的思考和判断。这个范式并非全新概念，但之前的研究往往停留在“让模型学会使用工具”的层面，而忽略了这一过程的内在挑战。这篇论文最大的贡献，就是系统性地识别并解决了在训练这类模型时遇到的不稳定性（instability）问题。 作者指出，使用工具增强的强化学习（Tool-augmented Reinforcement Learning）是教会模型使用工具的主流方法。简单来说，就是让模型不断尝试解决问题，如果它使用了工具并最终答对了，就给予奖励；答错了就给予惩罚。通过这种方式，模型会逐渐“领悟”到何时、以及如何使用工具。然而，这个过程远比想象的要复杂。论文深刻地揭示了三大核心挑战： 1. 交互边界的扰动（Interaction boundary disruptions）：模型如何精确地知道一段代码从哪里开始、到哪里结束？如果边界判断失误，送去执行的代码可能不完整或包含无关内容，导致执行失败或返回错误结果，从而干扰模型的学习。 2. 分布偏移（Distributional shifts）：模型自己生成的推理文本和工具返回的执行结果在文风、格式和内容上存在巨大差异（比如，模型思考的是自然语言，而工具返回的是一个冷冰冰的数字或者报错信息）。这种“画风突变”会打断模型连贯的思考流程，导致学习过程不稳。 3. 响应同质化（Response homogenization）：在固定的交互次数限制下，模型可能会“偷懒”，只学会一种或几种能稳定拿到奖励的“套路”，而不再探索更多样、更优的解题策略，这也就是强化学习中常说的“模式崩溃（mode collapse）”。 为了解决这些问题，论文提出了一套双管齐下的增强训练策略，其精髓在于在“探索”与“稳定”之间取得精妙的平衡。 在维持稳定性（Stability maintenance）方面，他们提出了三项关键技术： * 精确匹配交互边界：放弃了以往简单的“停止符”方案（如遇到output就执行），而是采用精确的代码块（如```python ... ```）匹配。这确保了送去执行的永远是格式良好且完整的代码，从源头上杜绝了边界不清带来的噪声。 * 屏蔽外部工具反馈：在计算模型的训练损失时，刻意将代码执行的结果（如output\\n44）屏蔽掉。这样做是为了让模型专注于学习“如何推理和写出正确的代码逻辑”，而不是去模仿工具返回结果的特定格式，从而避免了前文提到的分布偏移问题。 * 禁用熵增益（Entropy bonus disabling）：在强化学习中，“熵增益”通常用于鼓励模型探索更多可能性。但作者发现，在与工具交互时，过多的随机性反而会放大噪声，导致训练不稳定。因此，他们选择禁用这一项，追求更确定的学习路径。 在强化探索（Exploration enhancement）方面，他们也同样采取了精妙的策略： * 渐进式增加交互预算：训练初期，只允许模型调用少数几次工具。随着训练的进行，再逐步放宽限制。这鼓励模型在早期先学习简单的工具使用方法，后期再探索更复杂的、需要多次代码执行的解题路径。 * 移除KL散度惩罚项并采用非对称截断（Clip higher）：在传统的强化学习中，KL散度（KL divergence）惩罚项用于防止新模型偏离旧模型太远，以保证训练稳定。但在这里，作者认为这个限制反而束缚了模型学习使用工具的“新技能”。因此，他们大胆地移除了它，并借鉴了DAPO论文中的思想，允许模型在“好的方向”上进行更大幅度的探索和更新。 通过这套组合拳，论文中的模型（基于Qwen2.5-Math-7B）在多个高难度数学推理基准测试中取得了当前最佳性能。例如，在AIME2024测试集上，他们的模型CIR达到了42.3%的准确率，显著优于其他所有基线模型。更重要的是，论文不仅展示了“做什么”，还深入分析了“为什么有效”。通过大量的实验，他们证明了代码集成推理能够扩展模型的能力边界（capability boundaries），并且能以远比长思考链（Long-CoT）更简洁高效的方式解决问题。一个特别有趣的发现是，即使是无法成功执行的代码（non-executable code），有时也能通过返回的错误信息，促使模型反思和修正自身的逻辑，最终反而导向正确答案。这一发现揭示了模型与工具交互的复杂动态，极具启发性。 总而言之，这篇论文不仅仅是一次模型性能的提升，更是一份关于如何稳定、高效地训练“AI智能体（Agent）”的详细技术指南。它在理论分析和工程实践上都做出了坚实的贡献，为未来构建更强大、更可靠的、能够驾驭复杂工具的通用人工智能系统铺平了道路。 一、论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：本文的核心研究目标是提升代码集成推理（CIR）模型的训练效率和稳定性。它旨在系统性地解决在使用强化学习（RL）来教导大语言模型（LLM）使用代码解释器时所面临的一系列技术挑战。 解决的实际问题： LLM在精确计算上的短板：标准LLM本质上是文本预测模型，对于需要精确数学计算、逻辑推导和符号操作的任务（如高等数学、物理计算、金融分析等）常常力不从心，容易出现“一本正经地胡说八道”的情况。通过集成代码解释器，模型可以将这部分工作“外包”给最擅长它的工具。 工具增强型RL训练的不稳定性：虽然强化学习是训练模型使用工具的有效途径，但如前所述，其训练过程充满挑战，容易失败（即训练崩溃）。这篇论文要解决的，就是如何让这个训练过程变得更加稳健（robust）和可复现（reproducible），从“炼丹”变成“工程”。 行业意义： 推动AI从“聊天机器人”向“问题解决者”转变：一个能够稳定使用工具的LLM，其能力将发生质变。它不再局限于信息检索和内容生成，而是可以成为能够执行复杂任务的智能体（Agent），例如自动化数据分析、编写和调试软件、进行科学模拟等。这是通向通用人工智能（AGI）的关键一步。 提升AI应用的可靠性：在金融、医疗、工程等对准确性要求极高的领域，一个不可靠的AI是无法投入实际应用的。本文提出的稳定性增强策略，直接关系到能否构建出足够可靠、值得信赖的AI系统，对AI技术的商业化落地至关重要。 为多工具协同打下基础：虽然本文聚焦于代码解释器这一个工具，但其提出的关于稳定性的见解和方法论（如边界处理、反馈屏蔽等）具有很强的普适性，可以迁移到训练模型使用搜索引擎、数据库、API等多种工具的场景中，为构建更复杂的多智能体系统（Multi-Agent System）提供了宝贵的经验。 二、论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。 论文的核心创新在于提出了一套系统的、旨在平衡探索（Exploration）与稳定（Stability）的工具增强型RL训练框架。它不是一个全新的模型结构，而是一系列精巧的训练策略组合。 与之前的方法相比，其主要特点和优势在于系统性和针对性。之前的工作可能零散地注意到某个问题，但本文是第一个系统性地诊断并“对症下药”的。 原文引用 (Section 2.3.1): “Our empirical studies reveal that integrating external tools into reinforcement learning (RL) can introduce instability issues stemming from three primary factors: interaction boundary disruptions, distributional shifts between model reasoning and external feedback, and response homogenization due to fixed interaction budgets.” 这段话清晰地指出了本文所要解决的三个核心问题根源。 下面我们详细解析其创新方法： 稳定性维护策略 (Stability Maintenance) 精确的交互边界匹配 (Precise matching of interaction boundaries)： 特点：放弃了之前常用的、基于特定关键词（如output）的启发式方法。那种方法很脆弱，如果模型生成的代码里恰好包含了这个词，或者忘记生成这个词，就会导致交互失败。 优势：本文采用基于精确模式匹配的方式来识别完整的代码块（例如，```python ... ```）。如论文中的图1所示，这种方法确保了只有结构完整、语法正确的代码段才会被送去执行，极大地减少了因边界识别错误而引入的“噪声”。这是一个看似微小但极其关键的工程优化，是保证稳定性的第一道防线。 外部工具反馈屏蔽 (External too","date":"2025-07-26","objectID":"/towards-effective-code-integrated-reasoning/:0:0","tags":["Reading","Reasoning"],"title":"Towards Effective Code-Integrated Reasoning","uri":"/towards-effective-code-integrated-reasoning/"},{"categories":["Planning","reading"],"content":"论文深度解读：从“混沌”到“有序”，Routine框架如何驯服企业级AI智能体 当前，以大型语言模型（LLM）为核心的自主智能体（Autonomous Agents）正以前所未有的速度发展，展现出在数据分析、人机交互等领域的巨大潜力。然而，当我们将这些通用智能体置于规则严密、流程复杂的企业环境中时，往往会遭遇“水土不服”的窘境。论文开篇就指出了这一核心挑战： The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. （在企业环境中部署智能体系统常常受到几个挑战的阻碍：通用模型缺乏领域特定的流程知识，导致规划混乱、关键工具缺失以及执行稳定性差。） 通用大模型如同一个拥有渊博知识但缺乏特定公司工作经验的“实习生”，在面对企业内部复杂的工具链和业务逻辑时，常常会“手足无措”，无法稳定、准确地完成多步骤任务。 为了解决这一难题，论文作者提出了 Routine 框架，其本质是一个结构化的、面向任务的规划脚本。它不再让模型在运行时进行完全自主、不可预测的“自由发挥”，而是像一份详尽的“标准作业程序”（SOP），清晰地指导模型每一步应该做什么、调用哪个工具。这标志着一种从追求“完全自主”到强调“可靠可控”的范式转变，而后者恰恰是企业应用最看重的品质。 Routine的设计精妙之处在于它将一个复杂的智能体任务清晰地解耦为两个核心环节： 规划（Planning）：这个环节负责生成Routine。它可以由领域专家提供草稿，再由一个能力强大的大模型（如GPT-4o）进行优化和补全，生成包含明确步骤、工具和参数描述的结构化计划。这一过程可以离线完成，确保了计划的质量和周密性。 执行（Execution）：这个环节由一个相对轻量级、经过专门微调的模型负责。它的任务不再是复杂的推理和规划，而仅仅是严格遵循Routine的指令，一步步调用工具并传递参数。 这种“规划与执行分离”的架构带来了显而易见的优势：它允许我们用最强大的大脑（大模型+专家）来做最复杂的规划，同时用一个更经济、更高效的小模型来处理高频的执行任务，实现了能力与成本的最佳平衡。 为了支撑这套框架，论文还设计了一套完整的系统架构，如Figure 2所示，包括规划模块、执行模块、工具模块（MCP Server）和创新的双重内存模块（Memory Module）。其中，内存模块的设计尤其值得称道： * 程序内存（Procedure Memory）：用于存储和检索整个Routine库。当用户提出请求时，系统能快速匹配到最合适的Routine。 * 变量内存（Variable Memory）：这是一个巧妙的上下文管理机制。当工具调用返回过长的结果（如一整篇文章）时，系统会将其存入变量内存，并只在上下文中保留一个简短的键（如VAR_AI_ETHICS_TEXT_001）。这极大地压缩了上下文长度，降低了token消耗，并减少了长文本可能导致的模型“分心”或语法错误。 image.png 当然，仅有框架是不够的，如何让模型真正“学会”使用Routine是关键。为此，论文提出了两种互补的训练策略： 1. 通用Routine遵循能力训练：使用一个开源的多工具数据集（BUTTON），通过GPT-4o生成对应的Routine，然后用这些数据对模型进行微调。目的是让模型学会理解并遵循Routine这种结构化指令的“语法”。 2. 特定场景知识蒸馏（Knowledge Distillation）：这是论文的另一大亮点。针对具体场景（如文中的HR Agent），研究者让“教师模型”（GPT-4o）在Routine的指导下，生成大量高质量的、包含完整多步工具调用的“完美范例”。然后，用这些“蒸馏”出的数据去微调一个“学生模型”（如Qwen3-14B）。这一过程，本质上是将蕴含在Routine中的流程知识和教师模型的强大执行能力，“烘焙”进了轻量级模型的参数中。 实验结果是惊人的。论文在一个包含25个工具的真实HR Agent场景中进行了严谨的测试。关键数据显示（见Table 2），在没有Routine指导时，即便是强大的GPT-4o，其多步工具调用的端到端准确率也仅有41.1%。而引入Routine后，其准确率飙升至96.3%！对于开源模型Qwen3-14B，效果同样显著，准确率从32.6%提升到83.3%。 更具启发性的是，经过特定场景知识蒸馏后，轻量级的Qwen3-14B模型在没有Routine指导的情况下，准确率也能达到90.2%，当再次获得Routine指导时，准确率更是高达95.5%，几乎追平了GPT-4o。这雄辩地证明了，Routine框架不仅能作为一种外部“提示”来指导模型，还能作为一种有效的“知识载体”，通过蒸馏将领域知识内化到小模型中，从而构建出高稳定、低成本的企业级智能体。 总而言之，这篇论文为业界提供了一份极具操作性的蓝图，它巧妙地平衡了智能体的自主性与可靠性，通过结构化规划、执行解耦和知识蒸馏等一系列创新方法，为大模型在严肃、复杂的企业场景中大规模落地铺平了道路，真正向着“AI for Process”（AI赋能流程）的技术愿景迈出了坚实的一步。 接下来，我将按照您提出的六个问题，进行更详细的解读。 ### 1. 论文的研究目标是什么？想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标： 核心目标：设计并验证一个名为 Routine 的结构化规划框架，旨在显著提升LLM智能体在执行特定领域、多步骤工具调用任务时的稳定性、准确性和可靠性。 延伸目标：探索如何通过该框架，使得轻量级、低成本的本地化模型能够在特定企业场景中达到甚至逼近顶级闭源模型的性能水平。 解决的实际问题： 问题：当前通用大模型在企业环境中落地时面临的“最后一公里”难题。即便是最先进的模型，在面对企业内部特有的、复杂的业务流程和工具集时，也常常因为缺乏专门的领域知识而表现出“规划混乱、执行不稳”的问题。例如，模型可能会选择错误的工具、遗漏关键步骤（如权限验证），或在多步任务中途“迷失方向”，导致任务失败。 具体场景：想象一下一个企业HR智能体，需要处理“查询某员工上个季度的绩效、其所在部门的平均绩效、并生成一份对比报告”的请求。这涉及查询员工信息、查询部门数据、调用分析工具、调用报告生成工具等多个步骤，且步骤之间存在依赖关系。通用模型很可能在这个链条的某一步出错。 对行业发展的重要性： 推动AI在企业核心业务中的应用：目前，AI在企业的应用多集中在客服、内容生成等外围环节。要深入到ERP、CRM、SCM等核心业务流程，系统的可靠性是第一要求。Routine框架通过提供一种可控、可预测的执行模式，极大地增强了企业对AI智能体的信任，是AI从“玩具”走向“生产力工具”的关键一步。 降低企业部署AI的门槛和成本：顶级大模型的API调用费用高昂，且存在数据隐私风险。Routine框架通过知识蒸馏，使得企业可以部署性能媲美大模型、但成本更低、数据更安全的本地化小模型。这对于广大中小型企业或对数据安全要求极高的行业（如金融、医疗）意义重大。 催生新的技术生态和商业模式：围绕Routine的设计、管理、优化和分发，可能会诞生新的“智能体流程编排平台”、“领域知识蒸馏服务”等，为AI产业带来新的增长点。 ### 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 论文的核心创新在于提出了一整套系统性的方法论，而不仅仅是单一模型。 新的思路、方法与模型： Routine框架本身： 特点：它是一种人类可读、机器可执行的中间表示（Intermediate Representation）。它将模糊的自然语言任务意图，转化为结构化的、包含明确步骤、描述、工具和I/O的计划。 优势：与之前依赖模型在运行时进行“黑盒”规划的Plan-and-Act模式相比，Routine将“Plan”过程前置和显式化，使得整个工作流变得透明、可调试、可维护。开发者可以轻易地修改、增加或删除Routine中的步骤。 “规划-执行”解耦架构（Decoupling Planning and Execution）： 特点：系统在架构上明确分离了负责生成Routine的规划模块（高能力大模型）和负责执行Routine的执行模块（轻量级小模型）。 优势：这是对资源的最优化配置。复杂的、低频的规划任务由“最强大脑”完成，保证了计划的质量；而简单的、高频的执行任务由“经济小脑”完成，保证了效率和成本。这与传统软件工程中“控制面”与“数据面”分离的思想不谋而合。 AI辅助的Routine生成与优化流程： 特点：论文展示了一个半自动化的流程（见Figure 3），即领域专家只需提供一个简单的草稿（Routine Draft），强大的规划模型就能自动将其丰富、细化为一个完整的Routine。 优势：这大大降低了创建和维护Routine的门槛和工作量，使其具有更好的可扩展性，避免了完全依赖人工编写的瓶颈。 双重内存机制（Dual Memory）： 程序内存（Procedure Memory）：这类似于一个“流程库”，负责存储和检索Routines。 变量内存（Variable","date":"2025-07-25","objectID":"/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/:0:0","tags":["Reading","Planning"],"title":"Routine：A Structural Planning Framework for LLM Agent System in Enterprise","uri":"/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/"},{"categories":["Planning","reading"],"content":"### 5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 尽管这篇论文非常出色，但从批判性的学术角度审视，仍有一些可以探讨的局限性。 潜在的不足与缺失： 灵活性与泛化能力的权衡： 不足：Routine框架的核心优势是其结构性和稳定性，但这同时牺牲了智能体的灵活性和处理未知情况的能力。如果用户提出了一个偏离所有已知Routine的“边缘请求”，系统可能会束手无策或错误地匹配到一个不完全适用的Routine。论文承认了这是一个权衡，但未深入探讨如何优雅地处理“Routine Miss”的情况。 Routine创建与维护的隐性成本： 不足：虽然论文提出了AI辅助优化，但整个流程的起点仍然是“专家草稿”。对于拥有成百上千个业务流程的大型企业而言，创建和持续维护这个“Routine库”的真实人力成本和时间成本可能非常高昂。论文对此没有进行量化分析。 实验场景的单一性： 不足：尽管HR场景很典型，但它仍然是相对封闭和确定的。在一些更加开放和动态的领域，比如市场分析（需要实时调用外部API、处理非结构化新闻）、软件开发（代码库和需求频繁变更），Routine框架的有效性是否依然如此出色？需要更多样化的案例来验证其普适性。 对“程序内存”的检索精度要求过高： 存疑之处：论文的消融研究（Table 5）表明，即使只增加一个干扰Routine，模型的性能也会大幅下降。这反向说明，系统中的“程序内存”模块必须要有极高的检索精度，确保每次都能“又快又准”地找到唯一正确的Routine。然而，论文并未详细阐述这个高精度检索模块是如何实现的，这是一个非平凡的挑战。 ### 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？ 这篇论文是理论与实践结合的典范，充满了可以借鉴的智慧。 重点学习与启发： 拥抱务实主义：可靠性优于完全自主性。 启发：在企业级应用中，不要盲目追求一个无所不能、完全自主的“黑盒”AI。一个行为可预测、结果稳定、过程可追溯的系统远比一个时而惊艳、时而犯错的天才更有价值。将复杂任务流程化、结构化，是让AI落地的关键。 分而治之：解耦复杂问题。 启发：“规划与执行分离”的思想极具普适性。在设计任何复杂AI系统时，都可以思考：哪些是需要强大推理能力的、低频的“决策任务”？哪些是需要高效稳定执行的、高频的“操作任务”？尝试为它们匹配不同能力和成本的模型或模块。 知识蒸馏是实现“小模型、大能力”的利器。 启发：当你想让一个小模型掌握特定领域的复杂能力时，不要只想着喂给它原始数据。可以先用一个大模型（或者规则引擎、或者人类专家）在这个领域生成大量“高质量的解题过程”，然后让小模型去学习这些“过程”，而不是“结果”。这是一种高效的知识迁移模式。 中间表示（IR）的力量。 启发：Routine本质上是一种精心设计的IR。一个好的IR能够成为系统不同模块之间清晰的“沟通语言”，同时兼具人类可读性和机器可处理性。在你自己的项目中，思考一下是否也能设计一个类似的IR来解耦系统、降低复杂度。 需要补充的背景知识： LLM Agent主流框架：了解像 ReAct (Reason+Act)、Plan-and-Act 等基础的智能体工作模式，以便更好地理解Routine框架的创新之处。 模型微调技术：特别是 LoRA (Low-Rank Adaptation)，了解其原理，明白为什么可以用较低的成本对大模型进行有效微调。 知识蒸馏（Knowledge Distillation）：理解其基本概念，即“教师模型”如何将知识转移给“学生模型”。 企业业务流程管理（BPM）：了解企业中标准作业程序（SOP）、工作流等概念，你会发现Routine的思想与之有异曲同工之妙，它是AI时代的动态SOP。 ","date":"2025-07-25","objectID":"/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/:0:1","tags":["Reading","Planning"],"title":"Routine：A Structural Planning Framework for LLM Agent System in Enterprise","uri":"/routinea-structural-planning-framework-for-llm-agent-system-in-enterprise/"},{"categories":["verl","coding"],"content":"Verl 最近实现了 agent loop 功能，也就是多轮工具调用 RL ，弥补了 verl 中 vllm 无法使用多轮 rollout 的不足。整体流程大致如下（来自 https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/imgs/Multi-Turn_Rollout_Workflow.png） image.png 在官方实现中（目前在 verl/experimental/agent_loop 目录下），核心代码在 agent_loop.py中，single_turn_agent_loop.py和tool_agent_loop对应两种agent_loop，tool_parser.py定义了hermes工具解析类。所以重点就是在agent_loop.py中，各个类的协作流程如下图： image.png verl官网图如下（Agent Loop — verl documentation）： 按照上面的流程图进行逐步讲解。 ","date":"2025-07-24","objectID":"/agent_loop/:0:0","tags":["coding","verl"],"title":"agent_loop","uri":"/agent_loop/"},{"categories":["verl","coding"],"content":"AgentLoopManager AgentLoopManager是入口处，在ray_trainer.py中由actor_rollout_wg初始化： if self.config.actor_rollout_ref.rollout.mode == \"async\": from verl.experimental.agent_loop import AgentLoopManager self.async_rollout_mode = True self.async_rollout_manager = AgentLoopManager( config=self.config, worker_group=self.actor_rollout_wg, ) AgentLoopManager需要先启动多个推理引擎服务器，数量为dp_size（world_size/tp_size）。代码： class AgentLoopManager: \"\"\"Agent loop manager that manages a group of agent loop workers.\"\"\" def __init__(self, config: DictConfig, worker_group: RayWorkerGroup): \"\"\"Initialize agent loop manager. Args: config (DictConfig): trainer config. worker_group (RayWorkerGroup): ActorRolloutRef worker group. \"\"\" self.config = config self.worker_group = worker_group self._initialize_llm_servers() self._init_agent_loop_workers() def _initialize_llm_servers(self): # 这里也对应了一开始图中的最下面的部分 self.rollout_tp_size = self.config.actor_rollout_ref.rollout.tensor_model_parallel_size self.rollout_dp_size = self.worker_group.world_size // self.rollout_tp_size ... unready_dp_ranks = set(range(self.rollout_dp_size)) while len(unready_dp_ranks) \u003e 0: servers = { rollout_dp_rank: server_class.options( # make sure AsyncvLLMServer colocates with its corresponding workers scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy( node_id=workers_info[rollout_dp_rank * self.rollout_tp_size], soft=False, ), name=f\"async_llm_server_{rollout_dp_rank}\", ).remote(self.config, self.rollout_dp_size, rollout_dp_rank, self.worker_group.name_prefix) for rollout_dp_rank in unready_dp_ranks } for rollout_dp_rank, server in servers.items(): try: address = ray.get(server.get_server_address.remote()) self.server_addresses[rollout_dp_rank] = address self.async_llm_servers[rollout_dp_rank] = server unready_dp_ranks.remove(rollout_dp_rank) except Exception: ray.kill(server) print(f\"rollout server {rollout_dp_rank} failed, maybe address already in use, restarting...\") # All server instances are ready, init AsyncLLM engine. ray.get([server.init_engine.remote() for server in self.async_llm_servers]) AgentLoopManager负责管理多个AgentLoopWorker，个数由参数rollout.agent.num_workers确定。然后将prompt切分成num_worker个，让各个worker分别进行推理，这部分的代码如下： ## 初始化各个worker def _init_agent_loop_workers(self): self.agent_loop_workers = [] for i in range(self.config.actor_rollout_ref.rollout.agent.num_workers): self.agent_loop_workers.append( AgentLoopWorker.options( name=f\"agent_loop_worker_{i}\", ).remote(self.config, self.async_llm_servers) ) ... ## 根据worker的数量切分prompts，推理后再合并起来 def generate_sequences(self, prompts: DataProto) -\u003e DataProto: if self.config.actor_rollout_ref.rollout.free_cache_engine: self.wake_up() chunkes = prompts.chunk(len(self.agent_loop_workers)) outputs = ray.get( [ worker.generate_sequences.remote(chunk) for worker, chunk in zip(self.agent_loop_workers, chunkes, strict=True) ] ) output = DataProto.concat(outputs) 从初始化也说明了，我们在RayPPOTrainer中init_workers方法里面初始化好AgentLoopManager，同时也初始化好了推理服务器和各个workers。 ## AsyncLLMServerManager 在上面的代码中，可以看到我们在创建各个worker的时候传入了初始化好的servers，需要一个类来对这些server进行管理，即AsyncLLMServerManager类。具体的作用就是针对不同的request，来选择哪一个server进行推理，使用LRU算法来进行管理，具体如下： def __init__(self, config: DictConfig, server_handles: list[ray.actor.ActorHandle], max_cache_size: int = 10000): \"\"\"Initialize the AsyncLLMServerManager. Args: config (DictConfig): YAML config. server_handles (List[ray.actor.ActorHandle]): OpenAI compatible LLM server actor handles. max_cache_size (int, optional): max cache size for request_id to server mapping. Defaults to 10000. \"\"\" self.config = config self.server_handles = server_handles random.shuffle(self.server_handles) # Least requests load balancing self.weighted_serveres = [[0, (hash(server), server)] for server in server_handles] heapq.heapify(self.weighted_serveres) # LRU cache to map request_id to server self.request_id_to_server = LRUCache(maxsize=max_cache_size) def _choose_server(self, request_id: str) -\u003e ray.actor.ActorHandle: # TODO: implement server pressure a","date":"2025-07-24","objectID":"/agent_loop/:1:0","tags":["coding","verl"],"title":"agent_loop","uri":"/agent_loop/"},{"categories":["verl","coding"],"content":"AgentLoopWorker 现在回到AgentLoopWorker中，前面已经提到了，我们用AgentLoopManager管理多个Worker，每一个Worker都可以用所有创建的server进行推理，接下来就看worker的generate_sequcences方法做了什么： async def generate_sequences(self, batch: DataProto) -\u003e DataProto: \"\"\"Generate sequences from agent loop. Args: batch (DataProto): Input batch. Returns: DataProto: Output batch. - prompts: [bsz, prompt_length], prompt token ids from dataset. - responses: [bsz, response_length], output token ids include response tokens from LLM generation and observation tokens from tool_calls. - response_mask: [bsz, response_length], 1 for LLM generated tokens, 0 for observation/padding tokens. - input_ids: [bsz, prompt_length + response_length], whole sequence token ids, including prompt tokens and response tokens. - attention_mask: [bsz, prompt_length + response_length], 0 for padding tokens, 1 for other tokens. - position_ids: [bsz, prompt_length + response_length], incremental position ids. For multi-turn conversations: responses: |\u003c- LLM generation -\u003e|\u003c- tool_calls -\u003e|\u003c- LLM generation -\u003e|\u003c- padding -\u003e| response_mask: | 1, 1, 1, ..., 1, 1 | 0, 0, .., 0, 0 | 1, 1, 1, ..., 1, 1 | 0, 0, ..., 0| \"\"\" config = self.config.actor_rollout_ref.rollout sampling_params = dict( temperature=config.temperature, top_p=config.top_p, repetition_penalty=1.0, ) # override sampling params for validation if batch.meta_info.get(\"validate\", False): sampling_params[\"top_p\"] = config.val_kwargs.top_p sampling_params[\"temperature\"] = config.val_kwargs.temperature # by default, we assume it's a single turn agent if \"agent_name\" not in batch.non_tensor_batch: batch.non_tensor_batch[\"agent_name\"] = np.array([\"single_turn_agent\"] * len(batch), dtype=object) tasks = [] agent_names = batch.non_tensor_batch[\"agent_name\"] raw_prompts = batch.non_tensor_batch[\"raw_prompt\"] if \"index\" in batch.non_tensor_batch: index = batch.non_tensor_batch[\"index\"] else: index = np.arange(len(raw_prompts)) trajectory_info = await get_trajectory_info( batch.meta_info.get(\"global_steps\", -1), index, batch.meta_info.get(\"validate\", False) ) for agent_name, messages, trajectory in zip(agent_names, raw_prompts, trajectory_info, strict=True): tasks.append( asyncio.create_task(self._run_agent_loop(agent_name, messages.tolist(), sampling_params, trajectory)) ) outputs = await asyncio.gather(*tasks) output = self._postprocess(outputs) return output 根据注释的信息，输入为batch数据，输出为推理后的prompt、response、mask等。这里需要先判断使用的agent_loop的类型，前面提到这里实现了single_turn和tool两种agent_loop，此外用户可以根据AgentLoopBase抽象类来自定义agent_loop（init_class和run两个方法）。 这里先是有一个判断，如果数据集中没有agent_name字段，则默认就是single_turn类型，因此如果你想使用agent_loop，必须在数据预处理的时候加入agent_name字段为tool_agent。这里创建了一个叫trajectory_info的变量，目的是为了rollout_trace，来区分保存到文件中的各个prompt生成的response。接下来进入到_run_agent_loop方法中： async def _run_agent_loop( self, agent_name: str, messages: list[dict[str, Any]], sampling_params: dict[str, Any], trajectory: dict[str, Any], ) -\u003e AgentLoopOutput: with rollout_trace_attr( step=trajectory[\"step\"], sample_index=trajectory[\"sample_index\"], rollout_n=trajectory[\"rollout_n\"], validate=trajectory[\"validate\"], name=\"agent_loop\", ): assert agent_name in _agent_loop_registry, ( f\"Agent loop {agent_name} not registered, registered agent loops: {_agent_loop_registry.keys()}\" ) agent_loop_config = _agent_loop_registry[agent_name] agent_loop = hydra.utils.instantiate( config=agent_loop_config, trainer_config=_DummyConfig(config=self.config), server_manager=self.server_manager, tokenizer=self.tokenizer, ) output = await agent_loop.run(messages, sampling_params) return output 对于定义的不同的agent_loop，需要用register装饰器进行注册（设计模式中的工厂模式），register装饰器如下： def register(agent_name: str): \"\"\"Register agent loop class.\"\"\" def decorator(subclass: type[AgentLoopBase]) -\u003e type[AgentLoopBase]: fqdn = f\"{subclass.__module__}.{subclass.__qualname__}\" _agent_loop_registry[agent_name] = {\"_target_\": fqdn} return subclass return decorator 对于装饰的类，将这个信息存入一个名为 _agent_loop_registry 的全局字典中。 - 键 (Key)：在装饰器中提供的 agent_name 字符串 (e.g., “my_agent”)。 值 (Value)：一个特殊格式的字典 {“target”","date":"2025-07-24","objectID":"/agent_loop/:2:0","tags":["coding","verl"],"title":"agent_loop","uri":"/agent_loop/"},{"categories":["verl","coding"],"content":"ToolAgentLoop 首先看初始化： @register(\"tool_agent\") class ToolAgentLoop(AgentLoopBase): @classmethod def init_class(cls, config, tokenizer, **kwargs): if cls._class_initialized: return cls._class_initialized = True print(\"Performing class-level ToolAgentLoop initialization\") # Initialize tools from config file cls.tokenizer = tokenizer cls.max_user_turns = config.actor_rollout_ref.rollout.multi_turn.max_user_turns cls.max_assistant_turns = config.actor_rollout_ref.rollout.multi_turn.max_assistant_turns cls.max_parallel_calls = config.actor_rollout_ref.rollout.multi_turn.max_parallel_calls cls.max_tool_response_length = config.actor_rollout_ref.rollout.multi_turn.max_tool_response_length cls.tool_response_truncate_side = config.actor_rollout_ref.rollout.multi_turn.tool_response_truncate_side tool_config_path = config.actor_rollout_ref.rollout.multi_turn.tool_config_path tool_list = initialize_tools_from_config(tool_config_path) if tool_config_path else [] cls.tools = {tool.name: tool for tool in tool_list} cls.tool_schemas = [tool.tool_schema.model_dump(exclude_unset=True, exclude_none=True) for tool in tool_list] cls.tool_parser = ToolParser.get_tool_parser(config.actor_rollout_ref.rollout.multi_turn.format, cls.tokenizer) print(f\"Initialized tools: {cls.tools}\") cls.prompt_length = config.actor_rollout_ref.rollout.prompt_length cls.response_length = config.actor_rollout_ref.rollout.response_length cls.system_prompt = tokenizer.apply_chat_template([{}], add_generation_prompt=False, tokenize=True) 关于工具的初始化，来自于sglang团队，详细可以看博客：Awesome-ML-SYS-Tutorial/rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release_ZH.md at main · zhaochenyang20/Awesome-ML-SYS-Tutorial · GitHub 简单来说就是将工具的信息定义在一个yaml文件中，将文件路径传入到actor_rollout_ref.rollout.multi_turn.tool_config_path参数，然后获取tool_schemas用于后续传入到chat_template中，此外在tool_parser.py中定义了tool_parser。 这里还有一个值得注意的是system_prompt，这是因为进行chat_template的时候如果没有role为system的会自动加上，而在后面对工具返回结果进行单独chat_template的时候需要将自动添加的system prompt给删除，所以这里预存了一个system_prompt。 下面我们来看最核心的多轮rollout代码： @rollout_trace_op async def run(self, messages: list[dict[str, Any]], sampling_params: dict[str, Any]) -\u003e AgentLoopOutput: metrics = {} request_id = uuid4().hex prompt_ids = await self.loop.run_in_executor( None, lambda: self.tokenizer.apply_chat_template( messages, tools=self.tool_schemas, add_generation_prompt=True, tokenize=True ), ) response_mask = [] user_turns, assistant_turns = 0, 0 while True: with simple_timer(\"generate_sequences\", metrics): response_ids = await self.server_manager.generate( request_id=request_id, prompt_ids=prompt_ids, sampling_params=sampling_params ) prompt_ids += response_ids response_mask += [1] * len(response_ids) assistant_turns += 1 # reach max response length if len(response_mask) \u003e= self.response_length: break # reach max assistant turns if self.max_assistant_turns and assistant_turns \u003e= self.max_assistant_turns: break # reach max user turns if self.max_user_turns and user_turns \u003e= self.max_user_turns: break # no tool calls _, tool_calls = await self.tool_parser.extract_tool_calls(response_ids) if not tool_calls: break # call tools tasks = [] for tool_call in tool_calls[: self.max_parallel_calls]: tasks.append(self._call_tool(tool_call)) with simple_timer(\"tool_calls\", metrics): tool_responses = await asyncio.gather(*tasks) if any(isinstance(item, Exception) for item in tool_responses): break # append tool_response_ids tool_response_ids = await self.loop.run_in_executor( None, lambda messages=tool_responses: self.tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=True ), ) tool_response_ids = tool_response_ids[len(self.system_prompt) :] # NOTE: last turn should not be user turn, or the EOS token reward # can't be propagated to previous token in GAE. if len(response_mask) + len(tool_response_ids) \u003e= self.response_length: break prompt_ids += tool_response_ids response_mask += [0] * len(tool_response_ids) user_turns += 1 response_ids = prompt_ids[-len(response_mask) :] prompt_","date":"2025-07-24","objectID":"/agent_loop/:3:0","tags":["coding","verl"],"title":"agent_loop","uri":"/agent_loop/"},{"categories":["verl","coding"],"content":"返回 返回后，我们回到AgentLoopWorker的generate_sequences方法中，可以看到我们将返回的结果都放入了outputs变量中，而在最终输出前，需要进行后处理，也就是_postprocess方法。需要对prompt进行左pad，对response进行右pad，最后整合成DataProto返回。 def generate_sequences(self, prompts: DataProto) -\u003e DataProto: \"\"\"Split input batch and dispatch to agent loop workers. Args: prompts (DataProto): Input batch. Returns: DataProto: Output batch. \"\"\" if self.config.actor_rollout_ref.rollout.free_cache_engine: self.wake_up() chunkes = prompts.chunk(len(self.agent_loop_workers)) outputs = ray.get( [ worker.generate_sequences.remote(chunk) for worker, chunk in zip(self.agent_loop_workers, chunkes, strict=True) ] ) output = DataProto.concat(outputs) if self.config.actor_rollout_ref.rollout.free_cache_engine: self.sleep() # calculate performance metrics metrics = [output.meta_info[\"metrics\"] for output in outputs] # List[List[Dict[str, str]]] timing = self._performance_metrics(metrics, output) output.meta_info = {\"timing\": timing} return output 然后回到AgentLoopWorker的generate_sequences方法中，将各个worker推理的结果concat起来。并将一些metric放入meta_info来logger。 ","date":"2025-07-24","objectID":"/agent_loop/:4:0","tags":["coding","verl"],"title":"agent_loop","uri":"/agent_loop/"},{"categories":["大模型分布式","LLM"],"content":"如果想将模型训练扩展到大的批次，则很快就会达到在单个 GPU 上可以做的极限。具体来说，会发生 RuntimeError: CUDA out of memory。 梯度累计、Activation checkpointing 和 CPU offloading 都可以一定程度上减少显存的占用，为了_有效地_扩展到更大的模型大小和不断增长的数据集，同时仍然在合理的时间内训练模型，我们需要将计算分布在一组机器上。 3 D 并行即：数据并行、张量并行、流水线并行 后两者可以统一划分到模型并行，区别是一个是层内并行，一个是层间并行。 这里介绍数据并行。 ","date":"2025-07-23","objectID":"/data_parallel/:0:0","tags":["LLM","大模型分布式"],"title":"data_parallel","uri":"/data_parallel/"},{"categories":["大模型分布式","LLM"],"content":"Naive data parallel 一个很直觉的做法就是在 batch 维度上进行划分，各个卡上初始化完整的模型，然后将将划分的不同的 batch 发送到各个卡上进行前向传播和反向传播，再由一个卡整合梯度再下发给各 GPU，然后各 GPU 更新自己维护的模型参数。 image.png 但这种做法显然有很多问题，需要有一个 gpu 担任梯度聚合和下发的角色，如果这个 gpu 出问题了怎么办？每一个 gpu 都需要维护完整的模型参数、梯度和优化器，这部分的显存没有得到减少；此外这种方式通讯量很大，详见显存占用计算 ","date":"2025-07-23","objectID":"/data_parallel/:1:0","tags":["LLM","大模型分布式"],"title":"data_parallel","uri":"/data_parallel/"},{"categories":["大模型分布式","LLM"],"content":"DDP DDP 解决的问题就是将 Server 上的通讯压力均衡转移到各个 worker 上（Server 即担任梯度聚合和下发的角色，而 worker 就是各个 gpu），因此引入了 ring-all-reduce 算法来解决这个问题。需要把反向传播后的梯度切分成 N（world_size）份来进行 ring-all-reduce 算法。 ","date":"2025-07-23","objectID":"/data_parallel/:2:0","tags":["LLM","大模型分布式"],"title":"data_parallel","uri":"/data_parallel/"},{"categories":["大模型分布式","LLM"],"content":"zero zero ## fsdp fsdp ## 参考 The Ultra-Scale Playbook: Training LLMs on GPU Clusters 💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU \u0026 Distributed setups | by Thomas Wolf | HuggingFace | Medium Training extremely large neural networks across thousands of GPUs. # 图解大模型训练之：数据并行上篇(DP, DDP与ZeRO) ","date":"2025-07-23","objectID":"/data_parallel/:3:0","tags":["LLM","大模型分布式"],"title":"data_parallel","uri":"/data_parallel/"},{"categories":["大模型分布式","LLM"],"content":"参考 The Ultra-Scale Playbook: Training LLMs on GPU Clusters 💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU \u0026 Distributed setups | by Thomas Wolf | HuggingFace | Medium Training extremely large neural networks across thousands of GPUs. ","date":"2025-07-23","objectID":"/pipeline-parallelism/:1:0","tags":["LLM","大模型分布式"],"title":"pipeline parallelism","uri":"/pipeline-parallelism/"},{"categories":["大模型分布式","LLM"],"content":"参考 The Ultra-Scale Playbook: Training LLMs on GPU Clusters 💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU \u0026 Distributed setups | by Thomas Wolf | HuggingFace | Medium Training extremely large neural networks across thousands of GPUs. ","date":"2025-07-23","objectID":"/tensor_parallel/:1:0","tags":["LLM","大模型分布式"],"title":"tensor_parallel","uri":"/tensor_parallel/"},{"categories":["Agent","reading"],"content":"论文深度解读：从“思考时搜索”到“思考时搜索并提炼”的范式革命 这篇论文的核心贡献在于，它挑战了传统检索增强生成（RAG）系统中一个根深蒂固的、看似理所当然的流程，并提出了一种更为精细、鲁棒和智能的替代范式。传统的 RAG 模型通常遵循一种“思考时搜索”（search-during-think）的模式：当模型在生成答案的过程中意识到知识不足时，它会触发一次或多次搜索，获取外部文档，然后直接基于这些（可能混杂着大量噪声的）文档来生成最终答案。这种方法的致命弱点在于，它假设模型能够自行从混杂、冗长甚至可能错误的信息中精准地“淘金”，而现实是，这种“信息投喂”常常导致“垃圾进，垃圾出”（garbage in, garbage out）的困境。 论文作者敏锐地观察到了这一点，并提出了一个全新的范式——“思考时搜索并提炼”（search-and-refine-during-think）。这个新范式的灵魂在于引入了一个明确的、独立的“提炼”（Refine）步骤。具体来说，模型在执行 \u003csearch\u003e 操作并获取到 \u003cdocuments\u003e 后，不会立即生成 \u003canswer\u003e，而是会先进入一个 \u003crefine\u003e...\u003c/refine\u003e 的代码块。在这个阶段，模型的唯一任务就是阅读和理解取回的文档，然后从中提炼、过滤并整合出最关键、最相关的信息。这个过程完成后，模型再根据这些经过“预处理”的、高质量的信息精华来决定是继续进行下一轮搜索（如果发现还有知识盲区），还是生成最终答案。 正如论文图 1 (a) 所展示的生动例子，当被问及“‘The Umbrellas’是哪位法国印象派画家的作品？”时，传统方法可能会因为检索到的文档中提及了画作的捐赠者“Hugh Lane”（一位爱尔兰艺术品经销商）而错误地回答“Hugh Lane”。而 AutoRefine 则通过 \u003crefine\u003e 步骤，明确地提炼出“文档结论是，皮埃尔-奥古斯特·雷诺阿创作了‘The Umbrellas’”这样的核心事实，从而避免了干扰，给出了正确答案“Pierre-Auguste Renoir”。 为了让模型学会这种高级的“搜索-提炼”能力，作者们设计了一套巧妙的强化学习（Reinforcement Learning, RL） 框架。这里的第二个关键创新点在于其混合奖励机制（Hybrid Reward Mechanism）。传统方法通常只依赖于一个最终结果奖励（Outcome-Based Reward），即只有当最终答案完全正确时，模型才会得到正向激励。这种奖励机制非常稀疏，模型很难学会在复杂的中间步骤中做出正确的决策。 AutoRefine 则引入了一个额外的检索特定奖励（Retrieval-Specific Reward, R_Ret）。这个奖励的独特之处在于，它不关心最终答案是否正确，只关心在 \u003crefine\u003e 步骤中提炼出的内容是否包含了正确答案的关键信息。如论文公式 (2) 所示： R_Ret = I(a ∩ o_refine = a) 其中 I() 是指示函数，a 是标准答案，o_refine 是所有 \u003crefine\u003e 块中内容的集合。 这意味着，即使模型最终因为某些原因答错了，但只要它在中间的提炼步骤中成功地从文档里找到了正确的信息片段，它依然能获得一个正向的“部分学分”。如公式 (3) 所示，总奖励 R_overall 的设计非常精妙：如果最终答案正确 (R_ans \u003e 0)，则获得满分奖励；如果最终答案错误但提炼步骤正确 (R_ans = 0 and R_Ret \u003e 0)，则获得一个较小的固定奖励（0.1）；否则奖励为 0。这种“过程与结果并重”的奖励设计，极大地缓解了学习信号稀疏的问题，有效地引导模型学习如何在复杂的检索信息中“去粗取精”。 在训练算法上，论文采用了群体相对策略优化（Group Relative Policy Optimization, GRPO）。与需要大量人工标注的传统 RLHF（基于人类反馈的强化学习）不同，GRPO 通过在每次迭代中生成一组（Group）候选答案，并根据奖励函数来评估这组答案的相对好坏，从而计算策略梯度。这是一种更高效的、无需人工的 RL 训练方法，非常适合 AutoRefine 这种需要探索复杂行为空间的任务。 在实验验证部分，论文的设置非常全面。它在包括 Natural Questions、TriviaQA、HotpotQA 等在内的七个单跳及多跳问答基准上进行了测试。实验结果令人信服，如表 1 所示，AutoRefine 在所有七个数据集上的平均准确率达到了0.405，显著优于之前的 SOTA 模型如 Search-R 1（0.312），平均准确率提升了 6.9%。尤其是在需要多次推理的多跳问答（Multi-Hop QA） 任务上，AutoRefine 的优势更为明显，例如在 2 Wiki 数据集上，其准确率（0.393）相比 Search-R 1（0.274）提升了超过 10 个百分点，相对提升达到43%。 更重要的是，论文通过一系列的分析实验（Analytical Results）深入剖析了 AutoRefine 成功的原因。图 4 展示了其搜索行为，AutoRefine 能够根据问题难度自适应地调整搜索次数——在复杂的多跳问题上进行更多次搜索。图 5 则量化了“提炼”步骤的有效性，证明了 \u003crefine\u003e 块能将上下文长度压缩约 4 倍，同时保留了回答问题所需的关键信息。最后，关键的消融实验（Ablation Studies）（表 2）无可辩驳地证明了“提煉”步骤和“检索特定奖励”这两个核心组件都是不可或缺的。去掉任何一个，模型性能都会出现显著下滑。 总而言之，AutoRefine 不仅仅是一次模型性能的提升，更是一次关于 RAG 范式的深刻反思和创新。它提出的“search-and-refine-during-think”框架，以及与之配套的混合奖励机制，为构建更可靠、更智能、也更具可解释性（\u003crefine\u003e 的内容本身就是一种推理轨迹）的知识密集型 AI 应用，开辟了一条清晰且前景广阔的道路。 接下来，我将按照您提出的六个方面，对论文进行更详细的结构化解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是提升大型语言模型在处理需要外部知识的复杂问答任务时的准确性和鲁棒性。 要解决的实际问题： 信息噪声问题：当前的 RAG 系统在检索外部文档时，返回的内容往往是冗长、嘈杂、甚至包含错误或误导性信息的。模型直接基于这些原始文档进行推理，很容易被噪声干扰，导致生成错误的答案。 推理过程黑盒问题：传统 RAG 模型从检索到生成答案的过程不够透明。我们很难知道模型是根据文档中的哪部分信息得出结论的，这使得模型的行为难以解释和调试。 学习信号稀疏问题：在训练 RAG 模型（尤其是使用强化学习时），通常只有在最终答案完全正确时才会给予奖励。对于需要多步推理的复杂任务，这种“非黑即白”的奖励机制使得模型很难学会中间步骤的复杂操作。 行业意义： 提升 AI 系统可靠性：在金融、医疗、法律、科研等对信息准确性要求极高的领域，一个能够有效过滤噪声、精准利用知识的 AI 系统是至关重要的。AutoRefine 为构建这样的高可靠性系统提供了关键技术。 推动 Agent 智能体发展：未来的 AI 智能体需要具备自主规划、工具使用和信息处理的能力。AutoRefine 所展示的“识别知识缺口 -\u003e 搜索 -\u003e 提炼 -\u003e 决策”的闭环，是构建更高级、更自主的 AI Agent 的重要一步。 增强模型可解释性：通过 \u003crefine\u003e 步骤，模型被迫显式地写出它认为重要的信息摘要。这为我们提供了一个观察模型“思考过程”的窗口，极大地增强了系统的可解释性，有助于建立人机之间的信任。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 论文的核心创新在于AutoRefine 框架，它包含两个主要部分：新的推理范式和新的奖励机制。 新的思路和方法： “Search-and-Refine-During-Think”范式：这是对传统“Search-During-Think”范式的重大改进。 \u003e 论文在§2.1 中明确提出了这个范式，使用一个‘ \u003csearch\u003e...\u003c/search\u003e[documents]\u003crefine\u003e...\u003c/refine\u003e ’模板。 这个模板强制模型在每次搜索后插入一个显式的知识提炼环节。 混合奖励机制（Retrieval-Aware Signals）：它结合了两种奖励信号。 Outcome-Based Reward (R_ans): 评估最终答案的正确性，与传统方法类似。 Retrieval-Specific Reward (R_Ret): 评估中间 \u003crefine\u003e 步骤是否成功提取了答案的关键信息。这是该论文奖励设计的核心创新。 特点和优势： 抗噪声能力强：与之前直接处理原始文档的方法不同，AutoRefine 通过 refine 步骤主动过滤掉了无关信息，使得后续的决策和生成过程基于的是高质量、高信息密度的内容。 更高效的强化学习：混合奖励机制提供了更密集的学习信号。模型在探索过程中，即便最终没能答对，只要中间的“提炼”步骤做对了，也能得到鼓励。这就像一个好的老师，不仅看期末考试成绩，也为学生的每一次课堂正确发言而鼓掌，从而极大地加速了学习过程。 推理过程更具逻辑性和适应性：模型可以基于提炼后的信息，来决定是已经掌握足够信息可以回答问题，还是","date":"2025-07-20","objectID":"/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/:0:0","tags":["Reading","Agent"],"title":"Search and Refine During Think：Autonomous Retrieval - Augmented Reasoning of LLMs","uri":"/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/"},{"categories":["Agent","reading"],"content":"5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 尽管这篇论文非常出色，但从批判的角度看，仍存在一些可以进一步完善的地方。 模型规模的局限性（Model Scale）： \u003e 论文坦诚地在第 4 节“Limitations”中提到：“all experiments in this paper use 3 B-parameter language models.” 实验完全基于 3 B 模型，其结论能否无缝推广到更大、能力更强的模型（如 GPT-4、Llama-3 等）上尚不明确。有可能对于非常强大的基础模型，它们本身就有一定的抗噪声和信息提炼能力，AutoRefine 框架带来的增益可能会减小。 评估指标的单一性（Evaluation Metrics）： \u003e 论文也承认：“This work evaluates model performance solely on exact match accuracy, which may overlook semantically correct responses with minor textual variations.” 精确匹配（EM）过于严苛，会扼杀掉许多语义上正确但表述不同的答案。虽然这是该领域常用的指标，但它并不能完全反映模型的真实推理水平。 静态知识库的限制（Static Retrieval Corpus）： 如前所述，使用固定的维基百科快照，限制了其在真实世界动态场景中的应用价值。 效率与成本的权衡分析不足：AutoRefine 的迭代式“搜索-提炼”循环，相比于简单的 RAG 流程，无疑会增加推理的延迟（latency）和计算成本。论文没有详细分析这种准确率提升在多大程度上是以牺牲效率为代价的，以及在不同应用场景下如何权衡这两者。 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？有哪些启发？ 对于希望从中汲取创新想法的您来说，这篇论文提供了几个极具价值的“拿来即用”的启发。 重点学习的核心思想： 在流程中增加“净化”环节：最大的启发是，不要想当然地认为模型能处理好原始、嘈杂的输入。在任何一个复杂的 AI 工作流（不仅仅是 RAG）中，有意识地设计一个中间处理/净化/提炼步骤，都可能极大地提升系统的稳定性和最终效果。这是一个普适的系统设计哲学。 “过程奖励”的设计理念：在设计复杂的 Agent 或工作流时，不要只奖励最终结果。尝试将任务拆解，为关键的中间步骤设计独立的、易于评估的奖励，可以极大地改善模型的学习效率。AutoRefine 的混合奖励机制是这一理念的绝佳范例。 赋予模型“反思”的能力：AutoRefine 让模型在回答前先进行提炼，这本质上是一种“反思”。让模型有机会在决策前先整理和评估手头的信息，是通往更鲁棒 AI 的必经之路。 需要补充的背景知识： RAG（Retrieval-Augmented Generation）：需要深入理解 RAG 的基本工作原理，包括其中的检索器（Retriever）和生成器（Generator）是如何协同工作的。 强化学习（Reinforcement Learning）：至少需要了解 RL 的基本概念，如策略（Policy）、奖励（Reward）、状态（State）、动作（Action）。对策略梯度（Policy Gradient） 方法，特别是PPO (Proximal Policy Optimization) 有所了解，会更有助于理解论文中使用的GRPO。 Agentic AI / AI Agents：了解 AI Agent 的概念，以及一些经典的 Agent 框架，如ReAct (Reason+Act)，可以帮助您将 AutoRefine 放在一个更宏大的背景中去理解。 希望这份详尽的解读能够帮助您全面、深入地理解这篇优秀的论文。 ","date":"2025-07-20","objectID":"/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/:0:1","tags":["Reading","Agent"],"title":"Search and Refine During Think：Autonomous Retrieval - Augmented Reasoning of LLMs","uri":"/search-and-refine-during-thinkautonomous-retrieval-augmented-reasoning-of-llms/"},{"categories":["infra","LLM"],"content":" FLOPs, floating point operations, 表示浮点数运算次数，衡量了计算量的大小。 如何计算矩阵乘法的 FLOPs 呢？ 对于 \\(A\\in R^{1\\times n},B\\in R^{n\\times1}\\) ,计算 \\(AB\\) 需要进行 \\(n\\) 次乘法运算和 \\(n\\) 次加法运算，共计 \\(2n\\) 次浮点数运算，需要 \\(2n\\) 的 FLOPs。对于 \\(A\\in R^{m\\times n},B\\in R^{n\\times p}\\) ,计算 \\(AB\\) 需要的浮点数运算次数为 \\(2mnp\\) 。 在一次训练迭代中，假设输入数据的形状为 \\([b,s]\\) 。我们先分析 self-attention 块的计算，计 算公式如下： \\[Q=xW_Q,K=xW_K,V=xW_V\\] \\[x_{out}=softmax(\\frac{QK^T}{\\sqrt{h}})\\cdot V\\cdot W_o+x\\] 计算 \\(Q,K,V:\\) 矩阵乘法的输入和输出形状为 \\([b,s,h]\\times[h,h]\\to[b,s,h]\\) 。计算量为 \\(3*2bsh^2=6bsh^2\\) 。 \\(2.QK^T\\) 矩阵乘法的输入和输出形状为 \\([b,head\\_num,s,per\\_head\\_hidden\\_size]\\) \\(\\times[b,head\\_num,per\\_head\\_hidden\\_size,s]\\rightarrow[b,head\\_num,s,s]\\) 。计算量为 \\(2bs^2h\\) 。 3. 计算在 \\(V\\) 上的加权 \\(score\\cdot V\\), 矩阵乘法的输入和输出形状为 \\([b,head\\_num,s,s]\\times[b,head\\_num,s,per\\_head\\_hidden\\_size]\\) 。计算量为 \\(2bs^2h\\) 。 4. Attention 后的线性映射，矩阵乘法的输入和输出形状为 \\([b,s,h]\\times[h,h]\\to[b,s,h]\\) 。计 算量为 \\(2bsh^2\\) 。 接下来分析 MLP 块的计算，计算公式如下： \\[x=f_{gelu}(x_{out}W_1)W_2+x_{out}\\] 第一个线性层，矩阵乘法的输入和输出形状为 \\([b,s,h]\\times[h,4h]\\to[b,s,4h]\\)。计算量 为 \\(8bsh^2\\) 。 第二个线性层，矩阵乘法的输入和输出形状为 \\([b,s,4h]\\times[4h,h]\\to[b,s,h]\\)。计算量 为 \\(8bsh^2\\) 。 将上述计算量相加，得到每个 transformer 层的计算量大约为 \\(24bsh^2+4bs^2h\\) 。 此外，另一个计算量的大头是 logits 的计算，将隐藏向量映射为词表大小。矩阵乘法的输入和 输出形状为 \\([b,s,h]\\times[h,V]\\to[b,s,V]\\) ,计算量为 \\(2bshV\\) 。 因此，对于一个 \\(l\\) 层的 transformer 模型，输入数据形状为 \\([b,s]\\) 的情况下，一次训练迭代的 计算量为 \\(l*(24bsh^2+4bs^2h)+2bshV\\) 。 ","date":"2025-07-19","objectID":"/flops%E5%88%86%E6%9E%90/:0:0","tags":["LLM","infra"],"title":"flops分析","uri":"/flops%E5%88%86%E6%9E%90/"},{"categories":["infra","LLM"],"content":"计算量与参数量的关联 当隐藏维度 \\(h\\) 比较大，且远大于序列长度 \\(s\\) 时，我们可以忽略一次项，计算量可以近似为 \\(24bsh^2*l\\) 。前面提到当模型参数量为 \\(12lh^2\\), 输入的 tokens 数为 \\(bs\\) ,存在等式 \\(\\frac{24bsh^2l}{12h^2\\times bs}=2\\)。我们可以近似认为：在一次前向传递中，对于每个 token，每个模型参数，需要进行 2 次浮点数运算，即一次乘法法运算和一次加法运算。 一次训练迭代包含了前向传递和后向传递，后向传递的计算量是前向传递的 2 倍。因此，前向传递+后向传递的系数=1+2=3 。一次训练迭代中，对于每个 token，每个模型参数，需要进行 2*3=6 次浮点数运算。 接下来，我们可以估计训练 GPT 3-175 B 所需要的计算量。对于 GPT 3，每个 token，每个参数进行了 6 次浮点数运算，再乘以参数量和总 tokens 数就得到了总的计算量。GPT 3 的模型参数量为 174600 \\(M\\) ,训练数据量为 \\(300B\\) tokens。 \\[6\\times174600\\times10^6\\times300\\times10^9=3.1428\\times10^{23}flops\\] ","date":"2025-07-19","objectID":"/flops%E5%88%86%E6%9E%90/:1:0","tags":["LLM","infra"],"title":"flops分析","uri":"/flops%E5%88%86%E6%9E%90/"},{"categories":["infra","LLM"],"content":"训练时间 \\[ 训练时间=\\frac{8 * tokens数 *模型参数量}{GPU数 * GPU峰值flops*GPU利用率} \\] 也就是训练时的 flops / gpu 的 flops。8 是前向传播、后向传播、Activation checkpointing 。前向传播系数为 1，后向传播是前向传播计算量的 2 倍，再加上在反向传播时需要前向传播一次，因此总系数为 4，1 个参数需要 2 次浮点数运算，这就是 8 怎么来的。 ","date":"2025-07-19","objectID":"/flops%E5%88%86%E6%9E%90/:2:0","tags":["LLM","infra"],"title":"flops分析","uri":"/flops%E5%88%86%E6%9E%90/"},{"categories":["Transformer","reading"],"content":"好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇关于 Transformer 架构中归一化策略的优秀论文——《Peri-LN: Revisiting Normalization Layer in the Transformer Architecture》。 这篇论文系统地研究了大型语言模型（LLM）中一个长期存在但又至关重要的技术细节：层归一化（Layer Normalization, LN） 的放置位置。传统的 Post-LN 和 Pre-LN 策略各有优劣，前者梯度流不稳定，后者激活值容易爆炸，这给大模型的训练带来了诸多挑战。该论文通过详尽的理论分析和大规模实验，聚焦于一种被部分开源模型“悄然”采用但缺乏系统研究的“第三种”策略，并将其命名为 Peri-LN (Peripheral-LN)，即“外围层归一化”。 Peri-LN 的核心思想是在 Transformer 的每个子层（如自注意力或前馈网络）的输入和输出都进行归一化。这篇论文的卓越贡献在于，它不仅仅是介绍这种方法，更是从根本上阐明了其工作机理。论文指出，Pre-LN 之所以不稳定，是因为其残差连接是一条“高速公路”，模块计算产生的巨大激活值（文中称为 “massive activations”）会毫无阻碍地在网络中累积，导致方差呈指数级增长。正如论文在图 1 中所展示的，在训练过程中，Pre-LN 模型的隐状态方差（hidden-state variance）急剧攀升，而 Post-LN 则保持平稳。 Figure 1. Illustration of hidden-state variance… We observe the growth in hidden-state variance for both Pre-LN and Post-LN architectures. Peri-LN 通过在模块输出后、与残差相加前增加一个归一化层，巧妙地“净化”了这条高速公路。这使得模型的方差增长从 Pre-LN 的指数级（exponential growth）转变为更温和的线性增长（linear growth）。论文通过公式 Var(x_l+1) ≈ Var(x_l) + β_0 形象地描述了这一过程，其中 β_0 是一个接近常数的方差项。这意味着每一层只增加一个小的、可控的方差，从而保证了整个模型的稳定性。 更进一步，论文在 Proposition 3.1 中从理论上分析了这种稳定性。它证明了在 Pre-LN 架构中，巨大的激活值会直接导致梯度爆炸；而在 Peri-LN 中，新增的输出归一化层对梯度产生了一个“阻尼因子”（damping factor），即使出现巨大的中间激活值，最终的梯度范数也能保持有界，从而实现了“自正则化”（self-regularizing）。 在实验验证方面，论文做得非常扎实。他们训练了高达 32 亿参数的模型，在长达 300 亿个 token 的数据上进行了广泛测试。Table 1 的结果极具说服力，在所有模型尺寸（400 M, 1.5 B, 3.2 B）和多项基准测试（如 ARC, HellaSwag, PIQA）中，Peri-LN 的性能都稳定地、显著地优于 Pre-LN 和 Post-LN。例如，在 3.2 B 模型上，Peri-LN 的平均分达到了 58.56，而 Pre-LN 和 Post-LN 分别为 56.69 和 46.45。这不仅仅是性能的提升，更重要的是训练的稳定性。Figure 4 生动地展示了 Pre-LN 在训练早期频繁遭遇的损失尖峰（loss spikes）和发散（divergence）问题，而 Peri-LN 则表现出平滑稳定的训练曲线。 论文还有一个非常深刻的洞见，即激活值的大小与训练中使用的数值精度（如 FP 16 和 BF 16）息息相关。Figure 11 的实验表明，Pre-LN 中的激活值在训练早期（仅 5 亿 token）就轻易超过了 FP 16 能表示的最大范围 (65,504)，这解释了为什么使用 FP 16 训练 Pre-LN 模型（如早期的 OPT 模型）会如此不稳定。相比之下，Peri-LN 的激活值则始终保持在安全的范围内。这个发现为业界在硬件选择和训练策略制定上提供了宝贵的实践指导。 总而言之，这篇论文通过命名、理论化和系统验证“Peri-LN”这一策略，清晰地指出了大模型训练中一条更稳定、更高效的路径。它不仅解释了“为什么”一些成功的开源模型（如 Gemma, OLMo）会不约而同地采用类似设计，还为整个领域“应该”如何构建和训练未来的大模型提供了坚实的理论和实践依据。 接下来，我将按照您提出的六个问题，对论文进行更详细的解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：本文的核心研究目标是系统性地分析并验证一种更优的 Transformer 层归一化（LN）放置策略，即 Peri-LN，以解决现有主流策略（Post-LN 和 Pre-LN）在训练大型语言模型时面临的稳定性与性能权衡困境。 解决的实际问题： 训练不稳定性：Pre-LN 架构虽然解决了 Post-LN 的梯度消失问题，但引入了新的“巨量激活值”（massive activations）问题，导致训练过程非常脆弱，容易出现梯度尖峰、损失爆炸甚至训练发散，尤其是在模型规模增大时。这使得训练大型、高性能的模型变得像一场“赌博”，需要反复调试和祈祷。 性能瓶颈：Post-LN 虽然稳定，但其梯度信号随深度衰减，导致模型收敛缓慢且最终性能不佳，难以训练非常深的网络。Pre-LN 虽然理论上性能更好，但其不稳定性常常导致实际性能受损或无法完成训练。 资源浪费：不稳定的训练过程意味着大量的计算资源被浪费在失败的训练任务上。一次数十亿甚至百亿参数模型的训练失败，可能造成数十万甚至数百万美元的损失。 行业意义： 提高训练成功率和效率：提供一个像 Peri-LN 这样鲁棒的训练“配方”，可以显著降低大模型训练的失败风险，缩短研发周期，节约宝贵的计算资源。 推动模型规模化：一个更稳定的架构使得研究人员和工程师能更有信心地探索更大、更深的模型，而不必过分担心由架构本身带来的不确定性，从而推动 AI 能力的边界。 降低技术门槛：当训练大模型不再是一件高风险、高成本的“玄学”时，更多的中小型企业和研究机构也能参与其中，促进了技术的普及和生态的繁荣，这对于整个行业的健康发展至关重要。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 核心方法：Peri-LN (Peripheral Layer Normalization) 这篇论文系统化研究并命名了 Peri-LN。尽管作者坦言类似设计已在 Gemma、OLMo 等模型中被“悄然采用”，但本文的创新在于首次对其进行了系统的命名、深入的理论分析和全面的实验对比，从而将其从一种零散的实践提升为一种有理论支撑的、值得推广的范式。 方法对比与优势分析 我们可以通过论文中的 Figure 2 来直观理解三种策略的差异： \u003e \u003e \u003e \u003e Figure 2. Placement of normalization in Transformer sub-layer. Post-LN (后置归一化): y = Norm(x + Module(x))。LN 放在残差连接之后。 特点: 归一化作用于主干道，强制将每层的输出拉回到标准分布，使得方差保持恒定。 缺点: 在反向传播时，梯度会穿过 LN 层，导致梯度信号随层数加深而衰减，引发梯度消失问题，不利于深层模型训练。 Pre-LN (前置归一化): y = x + Module(Norm(x))。LN 放在残差连接之前，仅作用于模块输入。 特点: 残差连接的主干道 x 上没有 LN，梯度可以直接流过，解决了梯度消失问题，这是其被广泛采用的原因。 缺点: 正是因为主干道是“无障碍高速公路”，Module(Norm(x)) 产生的巨大激活值会直接累加到 x 上，导致模型方差指数级增长，引发训练不稳定和数值溢出（特别是对于 FP 16）。 Peri-LN (外围归一化): y = x + Norm(Module(Norm(x)))。在模块的输入和输出都进行归一化。 特点与优势: 兼收并蓄: 它既像 Pre-LN 一样，对模块输入 x 进行归一化（Norm(x)），保证了模块计算的稳定性；又通过对模块输出 Module(...) 进行归一化，切断了巨大激活值直接污染主干道的路径。 控制方差增长: 它将 Pre-LN 的方差指数增长 Var(x_l+1) ≈ exp(l) * Var(x_0) 变为线性增长 Var(x_l+1) ≈ Var(x_l) + β_0。这从根本上遏制了“巨量激活值”的累积效应，是其稳定性的关键。 保持梯度流: 与 Pre-LN 类似，它的主干道 x 上也没有 LN 层，因此保留了通畅的梯度流，避免了 Post-LN 的梯度消失问题。 自正则化梯度: 如 Proposition 3.1 所示，输出归一化层 Norm(a) 在反向传播中引入了一个分母 ||a||，对梯度起到了动态的、自适应的正则化效果，防止了梯度爆炸。 3. 论文通过什么实验来验证所提出方法的有效性？实验是如何设计的？实验数据和结果如何？ 论文设计了一系列缜密且全面的实验来验证 Peri-LN 的有效性，覆盖了从初始化分析到最终模型评测的整个流程。 实验设计: 模型与规模: 实验采用了 400 M、1.5 B 和 3.2","date":"2025-07-19","objectID":"/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/:0:0","tags":["Reading","Transformer"],"title":"Peri-LN：Revisiting Normalization Layer in the Transformer Architecture","uri":"/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/"},{"categories":["Transformer","reading"],"content":"5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 尽管这是一篇非常出色的论文，但从批判的角度看，仍有一些方面可以商榷或进一步完善： 原创性（Novelty）的边界: 论文最核心的观点——Peri-LN 架构本身并非作者首创。作者在文中也坦诚地指出，Gemma 和 OLMo 等模型已经采用了类似的设计。因此，这篇论文的贡献更多在于“发现”而非“发明”，即首次对这一“民间智慧”或“工程技巧”进行了系统的理论化、命名和验证。这一点在评价其科学贡献时需要客观看待。 实验范围的局限性: 架构多样性: 实验主要集中在标准的 Decoder-only 语言模型上。Peri-LN 在其他重要架构，如 Encoder-Decoder（用于翻译）、Vision Transformers (ViT) 或多模态模型上的表现如何，仍是未知数，需要进一步的实验验证。 数据和任务: 实验主要基于通用文本数据。在特定领域（如代码、生物、金融）的数据上进行预训练，Peri-LN 是否依然能保持同样的优势，值得探究。 理论分析的深度: 虽然 Proposition 3.1 给出了精彩的局部洞见，但整个理论分析相比于近年来信号传播理论的最高水平，仍有提升空间。一个更完整的、能预测整个训练过程动态的理论模型将更有价值。 对“巨量激活值”来源的探讨不足: 论文很好地解释了 Pre-LN 如何“传播”巨量激活值，以及 Peri-LN 如何“抑制”它。但对于这些巨量激活值最初在 Module 中是如何产生的，其深层原因（例如与特定 token、注意力模式或模型参数的关系）探讨相对较少。 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？有哪些启发？ 对于希望从中汲取实践经验和创新灵感的您来说，这篇论文提供了几个非常有价值的要点： 拿来即用的核心思想: “外围”归一化原则: 在设计任何深度残差网络时，都可以借鉴 Peri-LN 的核心思想：对进入计算模块的输入进行归一化，对模块的输出在汇入主干道（残差连接）之前再次进行归一化。这是一个简单、普适且极其有效的稳定性技巧。 优先采用 Peri-LN: 如果您正在从头开始构建或训练一个新的 Transformer 模型，将 Peri-LN 作为默认的架构选择，相比 Pre-LN，几乎可以肯定会带来更稳定、更可预测的训练过程和更优的最终性能。 关注数值精度与激活值的匹配: 在训练时，特别是使用 FP 16 时，要密切监控模型内部的激活值大小。如果发现激活值接近或超过精度上限，应考虑切换到 BF 16 或采用像 Peri-LN 这样能从根本上抑制激活值增长的架构。 启发与思维方式: 重新审视“标准实践”: Pre-LN 在很长一段时间里被认为是训练 Transformer 的“标配”。这篇论文告诉我们，即便是被广泛接受的行业标准，也可能存在被忽视的缺陷，值得我们用批判和探索的眼光去重新审视。 从“民间智慧”中挖掘金矿: 很多成功的开源模型中蕴含着未被言明的“工程智慧”。系统性地去分析、理解和理论化这些实践，本身就是一条重要的创新路径。 动态与全局视角: 不能只在初始化阶段分析模型，而必须在整个训练生命周期中去观察和理解模型的动态行为（如方差、梯度、激活值的演变）。很多问题只有在动态中才会暴露。 需要补充的背景知识: Transformer 基础: 深入理解 Transformer 的每一个组件，特别是残差连接（Residual Connection）、层归一化（Layer Normalization）和自注意力机制（Self-Attention）。 深度学习中的梯度问题: 了解梯度消失（Vanishing Gradients）和梯度爆炸（Exploding Gradients）的成因和影响。 归一化技术: 除了 LayerNorm，了解一下 BatchNorm、RMSNorm 等其他归一化方法的原理和区别。 信号传播理论（Signal Propagation Theory）: 如果想更深入地理解，可以阅读一些关于深度网络中信号（激活值和梯度）如何传播和演变的基础性论文。 希望这份详尽的解读能帮助您全面、深入地掌握这篇论文的精髓。 ","date":"2025-07-19","objectID":"/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/:0:1","tags":["Reading","Transformer"],"title":"Peri-LN：Revisiting Normalization Layer in the Transformer Architecture","uri":"/peri-lnrevisiting-normalization-layer-in-the-transformer-architecture/"},{"categories":["Agent","reading"],"content":"好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇富有启发性的论文《ZEROSEARCH: Incentivize the Search Capability of LLMs without Searching》。 这篇论文的核心思想极其巧妙，它直击了当前训练“搜索智能体（Search Agent）”LLM 时最头疼的两个问题：高昂的 API 调用成本和不可控的搜索结果质量。传统的做法是让 LLM 在训练时与真实的搜索引擎（如谷歌）进行交互，通过强化学习（RL）来学习何时搜索、搜索什么以及如何利用搜索结果。但这就像让一个新手司机直接在高峰期的纽约街头学开车，不仅成本高昂（每次“练习”都要花钱），而且路况复杂多变（搜索结果时好时坏），很容易让模型“学坏”或者干脆放弃学习。 而 ZEROSEARCH 提出了一种革命性的替代方案：我们能否创造一个“虚拟驾校”，用一个专门的 LLM 来模拟真实的搜索引擎？ 这个想法的精妙之处在于，它将一个不可控、高成本的外部环境问题，转化为了一个可控、低成本的内部模型问题。论文的作者们发现，大型语言模型本身就蕴含了海量的世界知识，完全有能力根据一个查询（Query）生成以假乱真的“搜索结果”。 为了实现这一点，他们首先通过监督微调（Supervised Fine-tuning, SFT），将一个 LLM 训练成一个“模拟器”。这个过程很有趣：他们先收集真实的搜索互动数据，然后让一个强大的 LLM（作为“法官”）来判断每一份搜索文档对于回答原始问题是“有用的（useful）”还是“有噪声的（noisy）”。然后，他们用这些标注好的数据来微调模拟器 LLM，让它能够根据指令（例如，在提示中加入“useful”或“noisy”的关键词）来生成特定质量的文档。正如论文在表格 2（Table 2）的模板中展示的，通过简单的关键词控制，就可以让模拟器生成不同质量的内容。 Template for Search Simulation You are the Google search engine. Given a query, you need to generate five [useful / noisy] documents for the query. The user is trying to answer the question: [question] whose answer is [ground truth]. … Query: [query] [Useful / Noisy] Output: 有了这个可控的模拟器，他们引入了课程学习（Curriculum Learning） 的策略。在训练初期，他们让模拟器多生成“有用”的文档，这就像在驾校里先在空旷的场地上练习，让智能体 LLM 先学会基本的搜索动作和格式。随着训练的进行，他们逐步提高生成“噪声”文档的概率（根据论文中的公式 1 进行调整），这就像逐步增加路上的障碍和干扰，逼迫智能体 LLM 学会分辨信息、深度思考，而不是盲目相信搜索结果。这种“从易到难”的训练方式，极大地稳定了强化学习过程，并激发了模型更强的推理能力。 论文最令人震撼的成果体现在实验数据上。在核心的性能对比表（Table 3）中，我们可以看到，无论是在 Qwen-2.5-7 B 还是 LLaMA-3.2-3 B 等不同模型上，ZEROSEARCH 的表现都稳定地超过了基线方法，包括直接使用真实搜索引擎进行 RL 训练的 Search-R1。例如，在使用 Qwen-2.5-7 B-Base 模型时，ZEROSEARCH 的平均分达到了40.93，而 Search-R1 为39.51。 更惊人的是，在表格 4（Table 4）中，论文探讨了不同模拟器的效果。结果显示，一个经过 SFT 微调的 14 B 模型（SFT-14 B）作为模拟器，其训练出的智能体 LLM 的最终表现，甚至超越了使用真实谷歌搜索（Google）作为训练环境的智能体（在多个问答数据集上的平均分为34.47 vs. 32.81）。这强有力地证明了，一个高质量的模拟环境不仅可以替代真实环境，甚至可能因为其“可控性”和“课程设计”的优势，带来更好的训练效果。 最后，成本优势是该方法最实际的贡献。表格 8（Table 8）的成本分析显示，使用谷歌 API 进行约 64,000 次搜索请求的训练成本估算为$586.7，而使用一个 14 B 模型的 SFT 模拟器，其 GPU 成本仅为$70.8，成本降低了近 8 倍。这使得原本对算力要求极高、只有少数大公司能负担得起的搜索智能体研究，变得更加普及和可行。 总而言之，ZEROSEARCH 通过“用 LLM 模拟搜索引擎”这一核心思想，并辅以“可控的 SFT 模拟器”和“课程学习”两大支柱，成功地构建了一个低成本、高效率、高稳定性的 LLM 搜索智能体训练框架，为整个领域的发展开辟了新的道路。 接下来，我将按照您提出的六个方面，对论文进行更详细的解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标: 论文的核心研究目标是，开发一个高效、低成本且可扩展的框架，用于训练大型语言模型（LLMs）掌握与外部搜索引擎交互以解决问题的能力。 解决的实际问题: 该研究旨在解决在使用强化学习（RL）训练 LLM 搜索智能体时面临的两个核心瓶颈： 高昂的 API 成本（Prohibitively High API Costs）: 训练过程需要数十万次的与真实搜索引擎（如 Google Search API）的交互，每次交互都需要付费，导致总成本极其高昂，限制了研究和应用的可扩展性。 不可控的文档质量（Uncontrolled Document Quality）: 真实搜索引擎返回的结果质量参差不齐，充满了噪声、广告和不相关信息。这种不可预测性会给 RL 训练带来巨大的不稳定性，可能误导模型的学习方向，使其难以建立有效的搜索和推理策略。 行业意义: 解决这个问题对于大模型和 AI Agent 领域的发展具有重大意义： 推动 Agent 研究的“民主化”: 它极大地降低了训练复杂 Agent 的门槛。之前，只有拥有雄厚资本和算力资源的大公司才能进行大规模的真实环境交互式训练。ZEROSEARCH 展示了一条更经济的路径，使得更多的中小企业、研究机构和个人开发者也能参与到高级 Agent 的研发中来。 加速 Agent 能力的迭代: 低成本和高效率意味着可以进行更多、更快的实验。研究人员可以快速验证新的想法、调整模型架构、优化奖励函数，从而加速整个领域的技术迭代速度。 开辟“世界模型”的新思路: 该研究是“用模型模拟世界”这一宏大构想（常被称为世界模型, World Models）的一次成功实践。它证明了我们可以为 Agent 创造出可控、安全的“模拟训练场”。这一思路可以从模拟搜索引擎，扩展到模拟代码解释器、API 调用、甚至复杂的物理或社会环境，为构建更通用、更强大的 AI Agent 奠定了基础。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 论文的核心创新在于其训练范式，而非全新的模型结构。其新颖之处体现在以下几个思路和方法的组合： 核心思路：用 LLM 模拟搜索引擎（LLM as a Simulator）: 特点: 这是与以往方法最根本的区别。之前的方法如 DeepResearcher 和 WebThinker 都强调与真实搜索引擎的“实时交互”。而 ZEROSEARCH 则反其道而行，主张在训练中完全脱离真实搜索引擎，转而使用一个经过特殊微调的 LLM 来扮演搜索引擎的角色。 优势: 成本归零: 消除了所有外部 API 调用费用。 控制在我: 可以精确控制生成文档的质量、风格和内容，为后续的课程学习提供了可能性。 速度和并行性: 模拟器部署在本地 GPU 上，通信延迟极低，并且可以轻松地进行大规模并行化，以支持多个 RL 训练任务，这是外部 API 难以做到的。 方法一：可控的搜索模拟器微调（Search Simulation Tuning, SFT）: 特点: 为了让模拟器更逼真、更可控，论文设计了一个精巧的 SFT 流程。它不是简单地让 LLM 去模仿搜索结果，而是通过“判断-学习”的模式，让模型学会生成两种特定类型的内容： “有用”文档: 包含能够直接或间接回答问题的关键信息。 “噪声”文档: 与问题相关，但没有提供实质性答案，甚至可能包含误导信息。 优势: 这种显式的质量区分和控制能力，是简单的模仿式微调所不具备的。它使得训练环境的“难度”变得可编程。 方法二：课程学习的 rollout 策略（Curriculum-based Rollout）: 特点: 将 RL 训练的难度从易到难动态调整。论文中给出的概率公式 p_i = p_s + (b^(i/m) - 1)/(b-1) * (p_e - p_s) 精确地描述了在训练的第 i 步生成“噪声”文档的概率。训练初期，p_i 很低（接近 p_s），智能体 LLM 主要看到高质量的“有用”文档，更容易学会基本的交互流程。随着训练步数 i 增加，p_i 指数级增长至 p_e，智能体被迫面对更复杂的、充满噪声的环境，从而锻炼出更强的分辨和推理能力。 优势: 相比于在整个训练过程中使用固定难度（例如，固定的噪声比例）或完全随机的真实环境，这种课程学习策略能显著提升训练的稳定性和最终性能。论文在表格 6（Table 6）中的消融实验也证明了这一点，课程学习策略（Cu","date":"2025-07-18","objectID":"/zerosearchincentivize-the-search-capability-of-llms-without-searching/:0:0","tags":["Reading","Agent"],"title":"ZEROSEARCH：Incentivize the Search Capability of LLMs without Searching","uri":"/zerosearchincentivize-the-search-capability-of-llms-without-searching/"},{"categories":["Agent","reading"],"content":"5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 尽管这篇论文非常出色，但从批判的角度审视，仍然存在一些潜在的局限和值得商榷之处： 模拟器的知识局限性与“分布外”问题: 模拟器 LLM 本身的知识受其预训练数据截止日期的限制。当智能体需要查询一个全新的、模拟器闻所未闻的事件或概念时（out-of-distribution），模拟器能否生成合理的结果？论文提到将问题和答案也输入模拟器以扩展其知识边界，但这可能不足以覆盖所有新知识。模拟器可能会“幻觉”出看似合理但完全错误的文档，这与真实搜索引擎返回“无结果”或低质量链接的行为模式不同。 真实世界交互的复杂性被简化: 真实的网页搜索体验远比返回结构化的“文档”要复杂。它包括广告、弹窗、动态加载内容、需要登录的网站、人机验证（CAPTCHA）等等。ZEROSEARCH 的模拟器目前似乎并未模拟这些复杂的交互挑战，这可能导致训练出的模型在面对真实世界的“脏数据”和复杂流程时表现下降。 “有用性”判断的主观性和潜在偏差: SFT 模拟器的训练数据质量，依赖于一个“法官”LLM 对文档“有用性”的判断。这个判断本身是主观的，且“法官”LLM 可能带有自身的偏见。一个有偏见的法官可能会训练出一个有偏见的模拟器，进而影响智能体 LLM 的学习，这种潜在的“偏见传递”链条需要被更深入地研究。 算力成本的相对性: 虽然与昂贵的 API 相比，GPU 成本显著降低，但部署和运行一个 14 B 甚至更大规模的 LLM 作为模拟器，本身仍然是一笔不小的开销。对于资源非常有限的个人研究者或初创公司来说，这依然是一个门槛。论文的成本分析可能过于乐观，没有完全考虑 GPU 的闲置成本和运维成本。 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？ 这篇论文是创新思想的宝库，您可以从中借鉴以下几点核心启发： 核心启发一：模拟范式（Simulation Paradigm） 学什么: 当与外部世界的交互是昂贵、缓慢、危险或不可控的时候，尝试用一个微调过的 LLM 来模拟它。 这是本文最核心、最普适的思想。 如何用: 如果您正在开发一个需要调用付费 API、操作数据库或与用户进行多轮对话的 Agent，您可以借鉴 ZEROSEARCH 的思路： 收集一批真实的交互数据。 定义交互结果的“好”与“坏”（例如，API 调用成功/失败，SQL 查询返回正确/错误结果，用户对话满意/不满意）。 微调一个 LLM 模拟器，让它能生成不同质量的交互结果。 在这个安全、廉价的模拟器中训练您的主 Agent。 核心启发二：课程学习的工程智慧 学什么: 不要让模型一开始就面对最困难的挑战。 采用从易到难的训练策略，是保证复杂系统（尤其是 RL 系统）训练成功的关键。 如何用: 在您的任何训练任务中，思考如何定义任务的“难度”。可以是数据噪声的比例、任务的复杂度、干扰信息的多少等。然后设计一个课程，让模型在训练初期处理简单的样本，逐步过渡到复杂的样本。 核心启发三：可控的数据生成（Controllable Data Generation） 学什么: 通过在 Prompt 中加入控制指令（如“useful”/“noisy”），可以精巧地控制 LLM 生成特定风格或质量的数据。 如何用: 这个技巧可以广泛用于数据增强。例如，您可以微调一个模型，让它能根据指令生成“带有攻击性的用户评论”和“礼貌的用户评论”，用于训练一个更鲁棒的客服机器人。或者生成“代码风格良好”和“代码风格糟糕”的示例，用于训练代码格式化工具。 需要补充的背景知识: 强化学习基础（Reinforcement Learning）: 至少要理解策略（Policy）、奖励（Reward）、智能体（Agent）、环境（Environment）等基本概念。对 PPO（Proximal Policy Optimization）和 REINFORCE 等经典算法有了解会更有帮助。 检索增强生成（RAG）: 理解 RAG 的基本工作原理，知道它是如何结合检索和生成来提升回答质量的。 LLM Agent 架构: 了解当前主流的 Agent 框架，如 ReAct（Reason+Act）等，理解它们是如何通过“思考-行动”循环来与工具交互的。这篇论文中的 \u003cthink\u003e、\u003csearch\u003e 标签就是 ReAct 思想的体现。 ","date":"2025-07-18","objectID":"/zerosearchincentivize-the-search-capability-of-llms-without-searching/:0:1","tags":["Reading","Agent"],"title":"ZEROSEARCH：Incentivize the Search Capability of LLMs without Searching","uri":"/zerosearchincentivize-the-search-capability-of-llms-without-searching/"},{"categories":["大模型分布式","LLM"],"content":"All-reduced=all-gather+reduce-scatter All-Gather ：将分布式数据汇总到所有节点，适用于需要全局数据同步的场景。 Reduce-Scatter：将分布式数据进行规约并分散到所有节点，适用于需要局部结果分发的场景。 All-Reduce ： Reduce-Scatter 和 All-Gather 的组合。 ","date":"2025-07-17","objectID":"/ring-all-reduce/:0:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"All-gather 核心功能：将每个节点的部分数据汇总到所有节点，最终所有节点拥有完整数据副本。 适用场景：模型并行中的参数同步、全局统计信息聚合。 image.png ","date":"2025-07-17","objectID":"/ring-all-reduce/:1:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"Reduce-Scatter 核心功能：先对多节点数据进行规约（如求和），再将结果分散到各节点，使每个节点仅保留部分规约结果。 适用场景：ZeRO显存优化、梯度分片更新。 image.png ","date":"2025-07-17","objectID":"/ring-all-reduce/:2:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"区别 All-Gather：只进行数据收集和分发，不进行任何计算或规约操作。每个节点拥有所有节点的数据副本。 Reduce-Scatter：先进行数据规约（reduce），然后再进行数据分散（scatter）。每个节点只拥有部分规约后的数据，而不是所有的数据 ","date":"2025-07-17","objectID":"/ring-all-reduce/:3:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"例子 ","date":"2025-07-17","objectID":"/ring-all-reduce/:4:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"通信量计算 假设模型参数大小为 \\(\\theta\\)，GPU 个数为 N，则每一个梯度块大小为 \\(\\frac{\\theta}{N}\\) 对于单卡而言： - Reduce-Scatter 阶段通讯量：\\((N-1) \\frac{\\theta}{N}\\) - All-Reduce 阶段通讯量：\\((N-1) \\frac{\\theta}{N}\\) 单卡通讯量为 \\(2(N-1) \\frac{\\theta}{N}\\)，所有卡的通讯量为 \\(2(N-1) \\theta\\) ","date":"2025-07-17","objectID":"/ring-all-reduce/:5:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["大模型分布式","LLM"],"content":"参考 # 分布式训练中All-Reduce、All-Gather、Reduce-Scatter原理介绍 ","date":"2025-07-17","objectID":"/ring-all-reduce/:6:0","tags":["LLM","大模型分布式"],"title":"ring-all-reduce","uri":"/ring-all-reduce/"},{"categories":["Reasoning","reading"],"content":"好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇富有洞见的论文《BRITE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning》。这篇论文确实触及了当前大模型领域一个核心且棘手的问题：如何让模型不仅能生成流畅的文本，更能进行可靠、严谨的逻辑推理。 首先，我将为您呈现一份对该论文的整体性深度解读。 ","date":"2025-07-16","objectID":"/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/:0:0","tags":["Reasoning"],"title":"BRiTE：Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning","uri":"/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/"},{"categories":["Reasoning","reading"],"content":"论文综合解读 大型语言模型（LLMs）在处理复杂推理任务时已展现出惊人的潜力，但这背后往往伴随着一个严峻的挑战——推理过程的可靠性。我们经常见到模型给出的答案看似合理，但其“思考”过程（即推理链，Chain-of-Thought）却充满了逻辑漏洞。这极大地限制了 LLM 在数学、编程、科学研究等高要求领域的实际应用。 《BRITE》这篇论文直面这一核心挑战，提出了一套名为BRITE (Bootstrapping Reinforced Thinking Process)的创新框架。其核心思想可以概括为：让模型“自我进化”，通过强化学习来学会如何“更好地思考”，然后用这些高质量的“思考过程”来反哺和提升自身。 这是一种“授人以渔”而非“授人以鱼”的思路，旨在从根本上增强模型的内在推理能力。 论文的精髓在于其优雅地融合了两种强大的机器学习范式：期望最大化（Expectation-Maximization, EM）算法的思想和强化学习（Reinforcement Learning, RL）的技术。 首先，作者构建了一个清晰的概率图模型（Probabilistic Graphical Model）来描述 LLM 的推理生成过程。这个模型包含四个关键变量：输入的问题（Prompt, X）、模型内部的潜在思考过程（Latent Rationale, Z）、最终的答案（Answer, Y）以及一个评估信号（Evaluation Signal, O，比如答案是否正确）。将“思考过程”Z 明确地作为一个潜在变量来建模，是整个框架的理论基石。它将一个复杂的“从 X 直接到 Y”的问题，分解为了“从 X 生成思考过程 Z”和“从 X 和 Z 生成答案 Y”两个更易于处理的步骤。 正如论文第 2 页所述：“引入 Z 有两个基本目的：首先，它将复杂的分布 P (Y|X) 分解为更易处理的边际分布 P (Z|X) 和 P (Y|X, Z)，这与思想链（CoT）方法一致；其次，引入评估信号 O 为生成期望的（正确的）推理过程提供了关键的质量信号。” 在此框架下，BRITE 算法通过一个两阶段的迭代过程来优化模型： ψ-更新阶段（类比于 EM 算法的 E-步）：这是 BRITE 最核心的创新。传统方法（如拒绝采样，Rejection Sampling）会生成大量推理路径，然后被动地筛选出能得到正确答案的路径。BRITE 则要主动得多。它将“生成高质量思考过程”这一任务构建为一个强化学习问题。模型（作为 RL 中的策略，Policy）的目标是生成一个推理链（一系列动作，Action），以最大化最终获得正确答案的奖励（Reward）。论文使用PPO (Proximal Policy Optimization)算法来训练这个策略，从而学习到一个专门用于生成高质量推理过程的“思考模型”Q (ψ)。这个过程本质上是在逼近一个理想但难以直接计算的后验分布 P (z | x, y, o)，即“在给定问题和正确答案的情况下，最优的思考过程是什么”。 θ-更新阶段（类比于 EM 算法的 M-步）：在获得高质量的“思考模型”Q (ψ) 后，此阶段就相对直接了。我们用 Q (ψ) 为训练集中的每个问题生成一个高质量的推理过程 z，然后将这些自动生成的、带有详细思考步骤的数据 (x, z, y*) 作为“黄金教材”，对基础的 LLM（由参数θ控制）进行监督微调（Supervised Fine-tuning, SFT）。通过学习这些优质的推理范例，基础 LLM 的内在推理能力得到“引导”和“强化”。 这个“生成更优的思考过程 -\u003e 训练出更强的模型 -\u003e 生成更优的思考过程”的循环，就是 BRITE“自举（Bootstrapping）”思想的体现。 实验结果非常有力地支撑了这一框架的有效性。在 GSM 8 K 和 MATH 等标准数学推理基准上，BRITE 不仅显著优于拒绝采样等基线方法，更令人瞩目的是，它在多个测试中达到甚至超越了使用人类专家标注的推理过程进行 SFT 的性能。例如，在 Gemma-2-9 B 模型上，BRITE 在 MATH 测试集上取得了 50.5%的准确率，而使用人类标注数据进行 SFT 的准确率仅为 41.5%（见论文 Table 1）。这揭示了一个惊人的可能性：机器通过强化学习自我探索生成的推理过程，其质量可以媲美甚至超过人类专家的示范。这对于降低 AI 训练成本、突破人类知识瓶颈具有重大意义。 总而言之，《BRITE》提供了一个理论完备且实践有效的框架，旨在通过 RL 驱动的自我进化，将 LLM 从一个简单的文本生成器，转变为一个更可靠的“思考者”。它不仅为提升 LLM 推理能力开辟了一条新路，也为我们理解和构建更高级的人工智能系统提供了深刻的启示。 接下来，我将按照您提出的六个问题，对论文进行更详细的剖析。 ### 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是提升大型语言模型的复杂推理能力，并解决其推理过程不可靠的问题。具体来说，它致力于开发一个能够自动化生成高质量、逻辑正确的思考过程（推理链）的框架，并利用这些生成的过程来增强 LLM 自身的推理性能。 解决的实际问题： 推理过程的“黑盒”与不可靠性：当前的 LLM 即使能给出正确答案，其推理步骤也常常是随意、甚至错误的。这使得我们无法信任它去执行金融分析、医疗诊断、法律咨询等高风险任务。 对人工标注数据的过度依赖：提升模型推理能力最直接的方法是使用大量由人类专家编写的“解题步骤”进行微调。但这不仅成本高昂、耗时费力，而且对于前沿的科学问题，人类专家自身也可能不存在或存在分歧。 现有自动化方法的局限性：像拒绝采样（Rejection Sampling, RS）这样的自动化方法，虽然无需人工数据，但效率低下。它们像是在“大海捞针”，只能从模型自己生成的众多（通常质量不高的）推理中筛选出碰巧能得到正确答案的，而无法主动地、有指导性地生成更优的推理路径。 对行业发展的重要意义： 推动 AI 从“生成”走向“推理”：这是 AI 发展的关键一步。可靠的推理能力是实现通用人工智能（AGI）的基石。解决了这个问题，AI 才能真正成为科学研究、软件开发和复杂决策中的得力助手。 大幅降低高质量 AI 的训练成本：BRITE 展示了模型“自我教育”和“自我提升”的潜力。如果模型能够自动化地为自己创造高质量的训练数据，将极大地减少对昂贵的人工标注的依赖，使顶尖 AI 技术的开发更加普惠和高效。 增强 AI 系统的可解释性和可信度：通过生成更清晰、更逻辑化的思考过程，我们能更好地理解模型“在想什么”，这对于调试模型、修复错误、以及在关键应用中建立信任至关重要。 ### 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 论文的核心贡献是提出了BRITE 框架，它包含了一系列环环相扣的新思路和方法。 新思路与方法： 统一的概率图模型：首次将 LLM 的推理过程形式化为一个包含问题 (X)、潜在思考过程 (Z)、答案 (Y) 和评估信号 (O)的概率图模型（见论文 Figure 1）。这个模型清晰地定义了问题，并将“思考过程”Z 作为核心的潜在变量进行优化，为后续算法设计提供了坚实的理论基础。 RL 驱动的推理生成（ψ-更新）：这是最具颠覆性的创新。BRITE 没有采用被动的筛选方式，而是将“生成最佳推理链”的任务转化为一个强化学习（RL）优化问题。 \u003e 论文第 8 页明确指出：“为了实现这个目标，我们旨在利用 RL 来训练一个 LLM，该 LLM 能够表征我们需要的分布……我们将这个具有挑战性的采样问题，转换为一个更易于处理的 RL 优化问题。” 在这个 RL 设定中，模型通过探索不同的“思考”步骤（动作），来学习如何最大化最终答案正确的概率（奖励）。这种主动探索的方式，使得模型有可能发现人类都未曾想到的、更优或更简洁的解题路径。 自举学习循环（Bootstrapping Loop）：BRITE 建立了一个“生成-微调”的自举循环。RL 阶段生成高质量的推理数据，SFT 阶段利用这些数据提升模型能力，而能力更强的模型又能反过来在下一轮迭代中生成更高质量的数据。这个正反馈循环是模型实现自我进化的关键。 框架的通用性：BRITE 不仅仅是一个单一算法，它是一个灵活的框架。论文展示了它可以与不同的训练范式结合，例如： BRITE-SFT：将 RL 生成的推理链用于标准的监督微调。 BRITE-DPO：将 RL 生成的推理链用于构建更高质量的偏好数据对 (z_win, y_win) vs (z_lose, y_lose)，从而显著增强直接偏好优化（DPO）的效果。 与之前方法的比较、特点和优势： 特点 BRITE 拒绝采样 (Rejection Sampling, e.g., STaR) 人工标注 SFT 数据来源 自动化，通过 RL 主动生成 自动化，通过模型自由生成后筛选 依赖人类专家 生成方式 主动探索与优化，寻找最优推理路径 被动筛选，在已有生成中“碰运气” 人类预设的固定路径 推理质量 高，可能发现超越人类的路径 中等，受限于模型原始生成能力 高，但可能存在偏见或非最优 成本 计算成本高，但无需人工 计算成本中等，无需人工 人工成本极高 可扩展性 强，可应用于任何有验证器的任务 强 弱，难以扩展到新领域 核心优势：相较于拒绝采样，BRITE 更高效且上限更高，因为它是在主动“学习如何思考”；相较于人工标注，BRITE成本更低、可扩展性更强，并且展现","date":"2025-07-16","objectID":"/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/:0:1","tags":["Reasoning"],"title":"BRiTE：Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning","uri":"/britebootstrapping-reinforced-thinking-process-to-enhance-language-model-reasoning/"},{"categories":["","LLM","attention","NLP"],"content":"Self-attention 首先介绍一下最主要的 self-attention，可以说是 self-attention 实现了上述的 token 之间交互的功能。 自注意力是模型的关键组成部分之一。注意和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。 形式上，这种直觉是通过查询键值注意来实现的。Self-attention 中的每个输入标记都会收到三种表示，对应于它可以扮演的角色： Query Key Value 进入正题： 作为我们想要翻译的输入语句“The animal didn’t cross the street because it was too tired”。句子中”it”指的是什么呢？“it”指的是”street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。 当模型处理单词“it”时，self-attention 允许将“it”和“animal”联系起来。当模型处理每个位置的词时，self-attention 允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。如果你对 RNN 熟悉，就能想到 RNN 的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer 使用 self-attention 来将相关词的理解编码到当前词中。 下面看一下 self-attention 是如何计算的： ","date":"2025-07-16","objectID":"/mha/:1:0","tags":["LLM","Attention","NLP"],"title":"MHA","uri":"/mha/"},{"categories":["","LLM","attention","NLP"],"content":"向量计算 第一步，根据编码器的输入向量，生成三个向量，比如，对每个词向量，生成 query-vec, key-vec, value-vec，生成方法为分别乘以三个矩阵，这些矩阵在训练过程中需要学习。【注意：不是每个词向量独享 3 个 matrix，而是所有输入共享 3 个转换矩阵；权重矩阵是基于输入位置的转换矩阵；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】 注意到这些新向量的维度比输入词向量的维度要小（512–\u003e64），并不是必须要小的，是为了让多头 attention 的计算更稳定。 第二步，计算 attention 就是计算一个分值。对“Thinking Matchines”这句话，对“Thinking”（pos #1 ）计算 attention 分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。 这个分，通过“Thing”对应 query-vector 与所有词的 key-vec 依次做点积得到。所以当我们处理位置 #1时 ，第一个分值是 q 1 和 k 1 的点积，第二个分值是 q 1 和 k 2 的点积。这也就是所谓的注意力得分. 第三步和第四步，除以 8 (\\(=\\sqrt{dim_{key}}\\))，这样梯度会更稳定。然后加上 softmax 操作，归一化分值使得全为正数且加和为 1。 Softmax 分值决定着在这个位置，每个词的表达程度（关注度）。很明显，这个位置的词应该有最高的归一化分数，但大部分时候总是有助于关注该词的相关的词。 第五步，将 softmax 分值与 value-vec 按位相乘。保留关注词的 value 值，削弱非相关词的 value 值。 第六步，将所有加权向量加和，产生该位置的 self-attention 的输出结果。 上述就是 self-attention 的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。 ","date":"2025-07-16","objectID":"/mha/:1:1","tags":["LLM","Attention","NLP"],"title":"MHA","uri":"/mha/"},{"categories":["","LLM","attention","NLP"],"content":"矩阵计算 第一步，计算 query/key/value matrix，将所有输入词向量合并成输入矩阵 \\(X\\)，并且将其分别乘以权重矩阵 \\(W^q, W^k,W^v\\) 最后，鉴于我们使用矩阵处理，将步骤 2~6 合并成一个计算 self-attention 层输出的公式。 ","date":"2025-07-16","objectID":"/mha/:1:2","tags":["LLM","Attention","NLP"],"title":"MHA","uri":"/mha/"},{"categories":["","LLM","attention","NLP"],"content":"多头注意力机制 论文进一步增加了 multi-headed 的机制到 self-attention 上，在如下两个方面提高了 attention 层的效果： 多头机制扩展了模型集中于不同位置的能力。在上面的例子中，z 1 只包含了其他词的很少信息，仅由实际自己词决定。在其他情况下，比如翻译 “The animal didn’t cross the street because it was too tired”时，我们想知道单词”it”指的是什么。 多头机制赋予 attention 多种子表达方式。像下面的例子所示，在多头下有多组 query/key/value-matrix，而非仅仅一组（论文中使用 8-heads）。每一组都是随机初始化，经过训练之后，输入向量可以被映射到不同的子表达空间中。 如果我们计算 multi-headed self-attention 的，分别有八组不同的 Q/K/V matrix，我们得到八个不同的矩阵。 这会带来点麻烦，前向网络并不能接收八个矩阵，而是希望输入是一个矩阵，所以要有种方式处理下八个矩阵合并成一个矩阵。 上述就是多头自注意机制的内容，我认为还仅是一部分矩阵，下面尝试着将它们放到一个图上可视化如下。 #### 代码 下面实现一下多头注意力机制，在原论文中，实现的方法如下： 也就是对每个 W 进行多头的设置，即为原维度/head，然后拼接后，再经过 \\(hd_v\\times d_{model}\\) 的转换又得到原来的维度，代码的实现不太一样，代码是 W 还是 \\(d_{model}\\times d_{model}\\) 的矩阵然后得到 q, k, v 之后再进行截断，实现如下。 class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1) -\u003e None: # h为head，这里为8，d_model为embedding的维度，这里为512 super().__init__() assert d_model % h == 0 self.d_k = d_model // h # 64 self.h = h self.Q_Linear = nn.Linear(d_model, d_model) self.K_Linear = nn.Linear(d_model, d_model) self.V_Linear = nn.Linear(d_model, d_model) self.res_Linear = nn.Linear(d_model, d_model) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) batch_size = query.size(0) query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64) query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64) key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k) # attn为(batch_size, h, seq_len1, seq_len2) x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (batch_size, h, seq_len, d_k) -\u003e (batch_size, seq_len, h, d_k) -\u003e (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512) return self.res_Linear(x) ","date":"2025-07-16","objectID":"/mha/:1:3","tags":["LLM","Attention","NLP"],"title":"MHA","uri":"/mha/"},{"categories":["","LLM","attention","NLP"],"content":"Masked self-attention 在训练的时候，主要是消除后面的信息对预测的影响，因为 decoder 输入的是整个句子，也就是我们所谓的参考答案，而实际预测的时候就是预测后面的 token，用不到后面的 token，如果不 mask 掉，当前的 token 将看到“未来”，这不是我们想要的，因此必须要 mask 掉。 其实 decoder 里的 sequence mask 与 encoder 里的 padding mask 异曲同工，padding mask 其实很简单，就是为了使句子长度一致进行了 padding，而为了避免关注 padding 的位置，进行了 mask，具体的做法就是将这些位置的值变成负无穷，这样 softmax 之后就接近于 0 了。 而 sequence mask 思想也差不多： 假设现在解码器的输入”\u003c s \u003e who am i \u003c e \u003e“在分别乘上一个矩阵进行线性变换后得到了 Q、K、V，且 Q 与 K 作用后得到了注意力权重矩阵（此时还未进行 softmax 操作），如图 17 所示。 此时已经计算得到了注意力权重矩阵。由第 1 行的权重向量可知，在解码第 1 个时刻时应该将 20%（严格来说应该是经过 softmax 后的值）的注意力放到’\u003c s \u003e’上，30%的注意力放到’who’上等等。不过此时有一个问题就是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer 中的 Decoder 通过加入注意力掩码机制来解决了这一问题。 当然还要进行 softmax 等计算。 在网上查了很多资料，说法都很不一样，不过我更倾向于这样的看法。而在预测的时候是用前面的输出结果作为输入的。 几张图帮助理解： 后面还有 padding mask，所有的 self attention 都要用这个，因为 pad 的位置没有任何意义。 实践一下加深理解： 首先我们来定义模型： # 词典数为10， 词向量维度为8 embedding = nn.Embedding(10, 8) # 定义Transformer，注意一定要改成eval模型，否则每次输出结果不一样 transformer = nn.Transformer(d_model=8, batch_first=True).eval() 接下来定义我们的 src 和 tgt： # Encoder的输入 src = torch.LongTensor([[0, 1, 2, 3, 4]]) # Decoder的输入 tgt = torch.LongTensor([[4, 3, 2, 1, 0]]) 然后我们将 [4] 送给 Transformer 进行预测，模拟推理时的第一步： transformer(embedding(src), embedding(tgt[:, :1]), # 这个就是用来生成阶梯式的mask的 tgt_mask=nn.Transformer.generate_square_subsequent_mask(1)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 然后我们将 [4, 3] 送给 Transformer，模拟推理时的第二步： transformer(embedding(src), embedding(tgt[:, :2]), tgt_mask=nn.Transformer.generate_square_subsequent_mask(2)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 出的第一个向量和上面那个一模一样。 最后我们再将 tgt 一次性送给 transformer，模拟训练过程： transformer(embedding(src), embedding(tgt), tgt_mask=nn.Transformer.generate_square_subsequent_mask(5)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396], [ 1.4799, -0.3575, 0.8310, 0.1642, 0.8811, -1.3140, -1.5643, -0.1204], [ 1.4359, -0.6524, 0.8377, 0.1742, 1.0521, -1.3222, -1.3799, -0.1454], [ 1.3465, -0.3771, 0.9107, 0.1636, 0.8627, -1.5061, -1.4732, 0.0729]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 可以看到使用 mask 后就可以保证前面的结果都是不变的，不然如果没有 mask 则计算 attention 时因为计算注意力变化所以结果都会变化，这就是 Mask self-attention 的意义。 到这里 self-attention 就介绍完了 ","date":"2025-07-16","objectID":"/mha/:1:4","tags":["LLM","Attention","NLP"],"title":"MHA","uri":"/mha/"},{"categories":[""],"content":" image.png 标准的 mha 中，KV heads 的数量和 Query heads 的数量相同，每一个 q head 对应一个独立的 kv head，但这样的开销比较大。 MQA (Multi Queries Attention): MQA 比较极端，只保留一个 KV Head，多个 Query Heads 共享相同的 KV Head。这相当于不同 Head 的 Attention 差异，全部都放在了 Query 上，需要模型仅从不同的 Query Heads 上就能够关注到输入 hidden states 不同方面的信息。这样做的好处是，极大地降低了 KV Cache 的需求，但是会导致模型效果有所下降。（层内共享） ","date":"2025-07-16","objectID":"/mqa/:0:0","tags":null,"title":"MQA","uri":"/mqa/"},{"categories":["Reasoning","reading"],"content":"论文深入解读 这篇名为《Process Reinforcement through Implicit Rewards》(通过隐式奖励进行过程强化) 的论文，由来自清华大学、上海人工智能实验室、UIUC 等顶尖机构的研究者共同完成，为大语言模型（LLM）的强化学习（RL）领域带来了一个极具价值和创新性的解决方案——PRIME 框架。其核心贡献在于，它成功地将过程监督 (Process Supervision) 的高效率与结果监督 (Outcome Supervision) 的低成本相结合，解决了在复杂推理任务（如数学和编程）中应用强化学习时面临的关键瓶颈。 在深入探讨 PRIME 之前，我们必须理解它试图解决的根本矛盾。在训练 LLM 进行多步推理时，我们有两种主要的奖励（Reward）机制： 稀疏结果奖励 (Sparse Outcome Rewards)：模型生成完整的解答后，我们只根据最终答案的正确与否给予一次性奖励。这种方法简单、成本低，因为我们只需要一个最终结果的验证器（例如，数学题的答案是否正确，代码是否通过所有测试用例）。但它的巨大缺陷在于“信用分配 (credit assignment)”难题：如果答案错误，模型无法知道是哪一步推理出了问题；即使答案正确，推理过程也可能存在瑕疵甚至逻辑跳跃（即“侥幸成功”）。这导致学习效率低下，尤其是在需要长链条推理的任务上。 稠密过程奖励 (Dense Process Rewards)：在模型生成过程中的每一步（或每一个 token），都给予一次奖励，以评价这一步推理的质量。这种方法能提供细粒度的指导，理论上学习效率和效果都远胜于稀疏奖励。但它的致命弱点在于成本高昂：为每一步推理都进行人工标注或验证，是“令人望而却步的昂贵 (prohibitively expensive)”。此外，如何定义中间步骤的“绝对正确性”本身也是一个模糊的问题。 现有的大模型 RL 训练（如 RLHF）大多依赖于结果奖励，因此难以在复杂推理上取得突破。而 PRIME 的精妙之处，就在于它找到了一条“鱼与熊掌兼得”的中间道路。 PRIME 的核心思想可以概括为：用结果监督的方式训练一个模型，却用它来产生过程级别的稠密奖励信号。这个看似矛盾的操作是通过一种名为隐式过程奖励 (Implicit Process Rewards) 的机制实现的。具体来说，它借鉴了 DPO (Direct Preference Optimization) 等方法的思想，将奖励模型 (Reward Model, RM) 本身也设计成一个自回归语言模型 πφ。这个奖励模型像一个标准的结果奖励模型 (Outcome Reward Model, ORM) 一样，只用最终的（正确/错误）结果标签进行训练。论文中提到，它的训练损失是标准的交叉熵损失： L_CE(φ) = -E(x,y,ro(y))~T [ro(y) · log σ(rφ(y)) + (1 - ro(y)) · log(1 - σ(rφ(y)))] 这里 ro(y) 就是最终的 0 或 1 的 outcome reward。然而，在推理和 RL 训练时，PRIME 并不只在最后使用这个模型。由于 πφ 是一个自回归模型，它能在生成过程中的任意一步 t，针对下一个 token yt 给出其概率 πφ(yt|y\u003ct)。PRIME 利用这个概率，与一个固定的参考模型 (reference model) π_ref 的概率进行比较，从而计算出每一步的“隐式奖励”： rφ(yt) := β log (πφ(yt|y\u003ct) / π_ref(yt|y\u003ct)) 这个公式是 PRIME 的灵魂。它意味着，尽管奖励模型 πφ 的训练信号是稀疏的最终结果，但我们可以在 RL 训练的每一步都从它身上“榨取”出一个稠密的、token 级别的奖励信号 rφ(yt)。这个奖励直观地衡量了当前策略模型生成的 token yt 在多大程度上“更像”一个能导向正确结果的模型会生成的 token。 基于这个核心机制，PRIME 框架展现出几个显著的优势： 解决了标注成本问题：它完全不需要过程级别的标注，只需要最终结果的标签，极大地降低了数据成本。 实现了奖励模型的在线更新：由于只需要结果标签，奖励模型 πφ 可以在 RL 训练过程中，用策略模型 πθ 新生成的样本（rollouts）进行实时更新。这至关重要，因为它能有效缓解“奖励黑客 (reward hacking)”问题——即策略模型找到奖励模型的漏洞并加以利用，而不是真正提升能力。如论文中的图 6 所示，在线更新的 PRM（蓝色和绿色线）其准确率能在训练中持续提升，而离线 PRM（橙色线）则会因为分布偏移而逐渐退化。 简化了开发流程：论文有一个惊人的发现，即 PRM无需专门预训练。可以直接用 SFT（监督微调）后的模型来初始化 PRM。这不仅消除了一个昂贵且耗时的训练阶段，而且实验表明（图 5），直接用 SFT 模型初始化的 PRM 效果甚至好于一个用额外 50 万数据专门训练的 PRM。这大大降低了技术门槛和开发开销。 在实验验证上，论文做得非常扎实。以 Qwen2.5-Math-7B-Base 为基础模型，训练出的 Eurus-2-7B-PRIME 在多个数学和编程基准测试中表现优异。最亮眼的数据是，相较于仅使用结果奖励的 RLOO 方法，PRIME 展现了2.5 倍的样本效率，并在最终性能上提升了6.9%（图 4）。更令人印象深刻的是，与强大的 Qwen2.5-Math-7B-Instruct 模型相比，Eurus-2-7B-PRIME仅用了 10%的训练数据，就在多个推理基准上超越了前者（表 1 和图 1）。例如，在 AIME 2024 测试中，PRIME 模型取得了 26.7%的 pass@1 准确率，远超 GPT-4 o（9.3%）和 Llama-3.1-70 B-Instruct（20.0%）等更大或更强的模型，充分证明了其方法的有效性。 总之，PRIME 论文不仅仅是提出了一个新模型，更是提出了一种高效、低成本、可扩展的 RL 训练范式。它通过“隐式奖励”这一精妙设计，优雅地化解了过程监督和结果监督之间的核心矛盾，为训练更强大的推理型大模型铺平了道路。 1. 论文的研究目标是什么？想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是为大语言模型的强化学习（RL）开发一种可扩展且高效的稠密奖励机制，特别是在需要复杂多步推理的任务（如数学、编程）上。 想要解决的实际问题： 高昂的标注成本：当前，要让模型学会正确的推理“过程”，需要对每一步进行人工标注，这种“过程监督”成本极高，限制了其在大规模 RL 中的应用。 稀疏奖励的低效性：仅依赖最终“结果”的正确性来提供奖励，信号过于稀疏，模型难以定位错误步骤，导致训练效率低下，性能提升困难。 奖励模型过拟合（Reward Hacking）：使用静态的、离线训练的奖励模型，策略模型很容易在训练中找到并利用其漏洞，从而获得高奖励分数，但实际能力并未提升。 对行业发展的重要意义： 大幅降低顶尖推理模型的训练成本：PRIME 证明了仅需结果标签（如代码测试用例、数学答案）就可以实现高效的过程级强化学习。这意味着，训练一个数学或代码高手模型的门槛，从需要大量昂贵的过程标注，降低到了只需要一个可靠的自动验证器。这将使更多中小型公司或研究机构有能力训练出专业领域的顶尖模型。 推动 RL 在 LLM 推理能力提升上的应用：此前，由于上述困难，业界更多采用 SFT（监督微调）来提升模型的推理能力。PRIME 范式为通过 RL 进行更深层次、更探索性的能力提升开辟了新道路，可能成为继 SFT 和 DPO 之后，训练推理智能体的又一核心技术。 迈向更自动化的 AI 训练流程：该方法减少了对人类专家的持续依赖，推动了“AI 训练 AI”的进程。只要能定义一个清晰的最终目标（outcome verifier），PRIME 就能自动地、细粒度地引导模型学习如何达成这个目标，这在科学发现、药物设计等领域具有巨大潜力。 ### 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 新思路/方法/模型：论文提出了 PRIME (Process Reinforcement through IMplicit rEwards) 框架。其核心是隐式过程奖励（Implicit Process Rewards） 的概念。 与之前方法的比较、特点和优势： 特点 传统方法 (结果奖励 RL) 传统方法 (过程奖励 RL) PRIME 方法 奖励信号 稀疏，仅在序列末尾 稠密，在每一步 稠密，在每一个 token 监督来源 结果标签 (e.g., 最终答案) 过程标签 (e.g., 每步对错) 仅需结果标签 奖励模型更新 通常离线训练，易被 hacking 在线更新成本极高 可在线更新，有效防止 hacking 开发成本 低 极高 低，无需额外标注和 RM 预训练 核心优势分析： 成本效益 (Cost-Effectiveness)：这是最大的优势。PRIME 绕过了对过程标注的依赖。论文在表 1 中明确对比了其模型 Eurus-2-7B-PRIME 和 Qwen2.5-Math-7B-Instruct 的资源需求，前者在 RL 阶段仅用了 150 k 查询，而后者用了 66 k 查询 x 32 个样本，总数据量远小于后者，且没有专门的 RM 训练数据（RM Data 为 0 vs 618k）。 \u003e We demons","date":"2025-07-16","objectID":"/process-reinforcement-through-implicit-rewards/:0:1","tags":["Reasoning"],"title":"PROCESS REINFORCEMENT THROUGH IMPLICIT REWARDS","uri":"/process-reinforcement-through-implicit-rewards/"},{"categories":["Reasoning","reading"],"content":"### 5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 隐式假设的强度：论文的核心假设是——一个仅基于最终结果训练的奖励模型 πφ，其在中间步骤给出的 token 概率 πφ(yt|y\u003ct) 能够有效指导推理过程。这个假设在逻辑性强的数理任务中可能成立，但在更宽泛的任务中可能很脆弱。例如，在写一个故事时，一个好的结局并不能保证每一个段落都是精彩的。论文缺乏对这一核心假设局限性的深入讨论。 对“好过程”的定义模糊：PRIME 优化的是“通往好结果”的过程，但这不完全等同于人类理解的“好的过程”（例如，优雅、可解释、简洁）。模型可能会学到一些人类难以理解但计算上有效的“捷径”，这在需要人机协作和信任的场景下可能是个问题。 实验领域的局限性：尽管实验很扎实，但它们都集中在数学和编程这两个具有确定性答案的领域。该方法在多轮对话、长文本摘要、知识问答等更开放领域的有效性没有得到验证，而这些是 LLM 应用更广泛的场景。 超参数的敏感性：PRIME 引入了至少一个关键超参数 β（公式 3 中的奖励缩放因子）。论文中设其为 0.05，但没有提供关于这个值如何选择、其敏感性如何的详细分析。在实践中，这类超参数的调整往往非常耗时耗力。 ### 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？有哪些启发？ 重点学习的创新想法： 核心启发：解耦监督信号的“形式”与“来源”。不要被监督数据的原始形态所束缚。PRIME 的精髓在于，它用一个稀疏的信号来源（最终结果）通过一个巧妙的模型设计，生成了稠密的信号形式（token 级奖励）。这个“转换”思想可以应用在很多领域。 自回归模型作为奖励函数：任何为了某个目标（如生成正确答案）而训练的自回归模型，都有潜力被用作一个 token 级的奖励函数。这个想法扩展了 DPO（模型即奖励模型）的概念，并将其应用到了过程级别。 实用主义至上：复用与简化。论文中“用 SFT 模型初始化 PRM”以及“在线更新”的做法，是典型的实用主义。它告诉我们，在构建复杂系统时，应优先考虑如何复用现有组件、简化流程，而不是默认一切都要从头构建。这在工程实践中极具价值。 需要补充的背景知识： 强化学习基础：需要扎实理解策略梯度 (Policy Gradient)、PPO、优势函数 (Advantage Function) 等概念，这是理解论文 RL 部分的基础。 LLM 的 RLHF 范式：特别是要深入理解 DPO (Direct Preference Optimization)。PRIME 的隐式奖励公式 log(πφ / π_ref) 在形式上与 DPO 的奖励思想一脉相承。理解 DPO 有助于你 grasp the core of PRIME. 过程监督与结果监督：深入了解这两种监督方式的优缺点，这是理解本论文动机和贡献的出发点。可以阅读论文中引用的相关工作，如 Uesato et al., 2022 和 Lightman et al., 2023。 ","date":"2025-07-16","objectID":"/process-reinforcement-through-implicit-rewards/:0:2","tags":["Reasoning"],"title":"PROCESS REINFORCEMENT THROUGH IMPLICIT REWARDS","uri":"/process-reinforcement-through-implicit-rewards/"},{"categories":["Agent","reading"],"content":"好的，作为大模型领域的学术专家，我非常乐于为您深入解读这篇具有重要意义的论文《SEARCH-R 1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning》。 这篇论文的核心，是探索如何让大型语言模型（LLM）学会像人类专家一样，在解决复杂问题时，主动、智能、且迭代地使用搜索引擎。它不仅仅是简单地把搜索结果“喂”给模型，而是通过强化学习（Reinforcement Learning, RL），训练模型形成一种内在的“研究”能力——知道什么时候需要信息，需要什么信息，以及如何整合这些信息来形成最终答案。 以下是我为您准备的详细解读。 ","date":"2025-07-16","objectID":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/:0:0","tags":["Agent"],"title":"Search-R1：Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning","uri":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/"},{"categories":["Agent","reading"],"content":"论文深度解读 SEARCH-R 1 的工作可以看作是连接“静态知识库”式 LLM 与“动态信息探索者”式 AI 智能体（Agent）的关键一步。传统的 LLM 像一本庞大但内容固定的百科全书，而 SEARCH-R 1 试图把它训练成一个懂得如何使用图书馆（搜索引擎）的研究员。 论文开篇就指出了当前 LLM 面临的核心困境：知识截止日期（Knowledge Cutoff）导致信息过时，以及在面对需要外部知识的复杂推理任务时容易产生“幻觉”（Hallucination）。为了解决这个问题，业界主流方案有两种： 检索增强生成（Retrieval-Augmented Generation, RAG）: 这是目前最流行的方法。它在处理用户问题时，首先根据问题检索相关文档，然后将这些文档作为上下文（Context）连同原始问题一起输入给 LLM，让其生成答案。这种方法的局限性在于，它通常是一次性的、前置的检索。对于需要多步推理的问题，比如“A 演员参演的、由 B 导演执导的电影是哪部？”，第一步检索可能只找到 A 演员的信息，无法一步到位。 工具使用（Tool-Use）: 这类方法将搜索引擎看作一个“工具”，通过提示工程（Prompting）或监督微调（Supervised Fine-Tuning, SFT）来教模型调用。例如，ReAct 框架通过交错的“思考-行动-观察”循环来使用工具。但这些方法要么依赖于复杂的提示词设计，泛化能力有限；要么需要大量高质量的、人工标注的“决策轨迹”数据进行微调，成本极高且难以规模化。 SEARCH-R 1 正是在这个背景下应运而生，它提出了一种更根本、更具扩展性的解决方案。其核心思想是：我们不直接教模型每一步怎么做，而是设定一个最终目标（答对问题），然后让模型在与环境（包括搜索引擎）的互动中自己学习如何达成这个目标。 这就是强化学习的精髓。 该框架的具体工作流程非常巧妙，它定义了一套特殊的“语言”让模型与系统交互： - 当模型需要思考和规划时，它会生成 \u003cthink\u003e...\u003c/think\u003e。 - 当它判断需要外部信息时，它会生成 \u003csearch\u003e搜索查询词\u003c/search\u003e。 - 系统捕捉到这个指令后，会执行搜索，并将结果封装在 \u003cinformation\u003e...\u003c/information\u003e 标签中返回给模型。 - 模型接收到新信息后，可以继续思考、再次搜索，或最终给出答案 \u003canswer\u003e...\u003c/answer\u003e。 这个过程是迭代和动态的。模型可以根据第一次搜索返回的信息，决定下一步是继续搜索（可能用新的关键词），还是已经信息充足，可以直接回答。例如在论文的案例研究（Table 9）中，为了回答“’Curious’香水的创始人出生在哪个城市？”，模型首先搜索“Curious fragrance information”，发现是布兰妮·斯皮尔斯（Britney Spears）创造的。然后，它生成了第二个搜索指令“Britney Spears birthplace”，找到了她出生在密西西比州的麦库姆（McComb, Mississippi）。这个多步查询和推理的过程，是传统 RAG 难以实现的。 案例分析：R 1 vs. SEARCH-R 1 (Table 9) 问题: “Curious”是哪位歌手推出的女士香水，这位歌手出生在哪个城市和州？ R 1 (无搜索的 RL): \u003cthink\u003e 这位歌手是碧昂丝，她出生在德州的休斯顿。\u003c/think\u003e \u003canswer\u003e 休斯顿 \u003c/answer\u003e (错误) SEARCH-R 1: 1. \u003cthink\u003e 我需要找出”Curious”香水的创造者是谁。\u003c/think\u003e 2. \u003csearch\u003e Curious fragrance information \u003c/search\u003e -\u003e \u003cinformation\u003e… 是 Britney Spears 推出的… \u003c/information\u003e 3. \u003cthink\u003e 我需要找出 Britney Spears 出生在哪个城市和州。\u003c/think\u003e 4. \u003csearch\u003e Britney Spears birthplace \u003c/search\u003e -\u003e \u003cinformation\u003e… 出生在 McComb, Mississippi… \u003c/information\u003e 5. \u003cthink\u003e 我已经找到了答案。\u003c/think\u003e 6. \u003canswer\u003e McComb, Mississippi \u003c/answer\u003e (正确) 为了让这个学习过程稳定有效，论文提出了一个关键的技术创新：检索内容损失掩码（Retrieved Token Loss Masking）。这是一个非常重要的细节。在 RL 训练中，模型生成的所有内容（token）都会计算损失，用于更新模型参数。但搜索返回的 \u003cinformation\u003e 内容是外部的、不可控的，我们不希望模型因为这些外部内容而受到“惩罚”或“奖励”，否则模型可能会学到一些奇怪的东西（比如试图去“修正”搜索结果）。因此，SEARCH-R 1 在计算损失时，会“屏蔽”掉所有从搜索引擎检索来的 token，只在模型自己生成的 \u003cthink\u003e, \u003csearch\u003e, \u003canswer\u003e 等部分计算损失。这确保了模型只专注于学习“如何思考和行动”，而不是去学习“知识内容本身”。实验证明，这个小小的改动带来了显著的性能提升，在 Qwen 2.5-7 B 模型上，使用掩码的版本平均分达到 0.431，而未使用时仅为 0.343（Table 4）。 最终，论文通过在 7 个不同的问答数据集上的大量实验，证明了 SEARCH-R 1 的有效性。与强大的 RAG 基线相比，它在 Qwen 2.5-7 B 模型上实现了41%的平均相对性能提升，在 Qwen 2.5-3 B 模型上提升了20%（Table 2）。这些坚实的数据证明，通过强化学习教会 LLM 如何使用搜索引擎，是一条非常有前途的道路。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标: 核心目标是创建一个框架（SEARCH-R 1），利用强化学习（RL）来训练大型语言模型（LLM），使其能够自主、有效地与搜索引擎进行多轮交互，以解决需要外部、实时知识的复杂问题。 解决的实际问题: 知识过时: LLM 的知识库是静态的，在预训练后就已固定。SEARCH-R 1 通过实时搜索，赋予了模型获取最新信息的能力。 幻觉问题: 当 LLM 缺乏特定知识时，它倾向于编造答案。SEARCH-R 1 通过提供事实依据，显著减少了幻觉。 复杂推理: 许多现实世界的问题需要分解成多个步骤，并逐步收集信息才能解决。SEARCH-R 1 通过支持迭代式、多轮的“思考-搜索”循环，增强了模型解决这类问题的能力，这远超传统单次检索的 RAG。 行业意义: 推动 AI Agent 的发展: 这是构建更强大、更可靠的 AI 智能体（Agent）的关键一步。未来的智能体需要能够主动探索、收集信息并执行任务，而不是被动地回答问题。SEARCH-R 1 为训练这种主动探索能力提供了有效范式。 提升 LLM 应用的可靠性: 在金融、医疗、法律等对事实准确性要求极高的领域，这项技术可以大大提升 LLM 应用的可靠性和实用性，使其从“玩具”变为真正的“生产力工具”。 降低对 SFT 的依赖: 传统上，让模型学会复杂行为需要昂贵的监督微调（SFT）。SEARCH-R 1 证明，仅通过简单的“结果导向”奖励（答案是否正确），就能让模型学会复杂的“过程行为”（如何搜索和推理），这为降低训练成本、提升模型能力开辟了新路径。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 新思路: 将搜索行为建模为 RL 问题: 论文的核心思路是将“与搜索引擎交互”这一过程，从一个简单的信息检索任务，重新定义为一个序列决策问题。模型在每一步都需要决策是继续思考、发起搜索还是给出答案。这个决策过程可以通过 RL 进行端到端的优化。 新方法/模型 (SEARCH-R 1 框架): 交错式推理与检索（Interleaved Reasoning and Retrieval）: 通过引入 \u003cthink\u003e, \u003csearch\u003e, \u003cinformation\u003e, \u003canswer\u003e 等特殊 token，创建了一个结构化的交互协议。这使得模型的推理过程变得透明且可控，并且能够灵活地在思考和信息收集之间切换。 结果导向的奖励机制（Outcome-based Reward）: 与需要标注完整行为轨迹的“过程监督”不同，SEARCH-R 1 采用了非常简单的奖励函数：只根据最终答案的正确性（如，精确匹配 Exact Match）来给予奖励。这种设计大大简化了训练过程，因为它不需要知道模型是如何得到答案的，只要结果对就行。 检索内容损失掩码（Retrieved Token Loss Masking）: 这是该方法的一个关键技术亮点。在 RL 训练更新模型时，它会忽略（mask out）所有从搜索引擎检索到的 \u003cinformation\u003e 部分产生的损失。 \u003e We introduce loss masking for retrieved tokens, ensuring the policy gradient objective is computed only over LLM-generated tokens, excluding retrieved content from the optimization process. ","date":"2025-07-16","objectID":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/:0:1","tags":["Agent"],"title":"Search-R1：Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning","uri":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/"},{"categories":["Agent","reading"],"content":"5. 退一步，从批判的视角看，这篇论文还存在哪些不足及缺失？ 对检索器的依赖性过强: 论文将搜索引擎 R 视为一个给定的、性能稳定的“黑盒”。但现实中，检索结果的质量波动很大。实验没有充分讨论当检索器性能下降时，SEARCH-R 1 框架的鲁棒性如何。 奖励函数的局限性: 正如前面提到的，EM 奖励函数限制了该方法的应用场景。它无法处理没有唯一正确答案或答案形式更复杂的任务。这使得它目前更像一个“解题专家”，而非一个“通用问题解决者”。 泛化能力的验证不足: 实验虽然在多个数据集上验证了方法的有效性，但这些数据集仍然局限于“问答”这一大类。它在更广泛的任务类型上（如内容创作、摘要、对话）的泛化能力仍有待验证。 对失败案例的分析不够深入: 论文在附录中给出了一些失败案例（Case Study），例如 Table 11 和 Table 14，显示模型有时会被不相关的检索结果误导或无法分解复杂问题。但文中缺乏对这些失败模式的系统性归因分析，以及相应的改进策略。 6. 我希望从这篇论文中找一些拿来即用的创新想法，我应该从这篇论文中重点学什么？ 重点学习的创新想法: 过程学习的范式转换: 核心启发是，对于复杂的行为，我们可以通过奖励最终结果来让模型自己学会中间过程。不要一开始就试图设计完美的“过程指令”，而是定义好目标，让模型在探索中学习。这个思想可以应用于各种需要模型学习多步操作的任务。 “内外有别”的损失设计: 损失掩码的思想极具价值和普适性。当你的模型需要整合一个外部的、不可微的、或你不希望模型去“学习”的信息源时，就应该在计算损失时将这部分信息屏蔽掉。这保证了模型“术业有专攻”，只学习它该学的部分。 用特殊 Token 定义交互协议: 通过设计一套简洁的、机器可读的特殊 Token（如 \u003csearch\u003e, \u003cthink\u003e），可以清晰地界定模型的不同行为阶段，使得整个系统更加模块化和可解释。这在设计任何需要 LLM 与外部系统交互的 Agent 时都是一个非常实用的技巧。 需要补充的背景知识: 强化学习（RL）基础: 尤其是策略梯度（Policy Gradient）方法，建议了解 REINFORCE、A 2 C/A 3 C，以及本篇论文使用的PPO (Proximal Policy Optimization)。理解 Actor-Critic 架构会对理解 PPO 有很大帮助。 检索增强生成（RAG）: 深入理解标准 RAG 的工作原理及其变种，这样才能体会到 SEARCH-R 1 在动态性和多轮交互上的突破。 思维链（Chain-of-Thought）与工具使用框架: 了解如ReAct (Reason+Act) 和 Toolformer 等论文，它们是 SEARCH-R 1 所处领域的重要前序工作，有助于理解整个技术演进的脉络。 ","date":"2025-07-16","objectID":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/:0:2","tags":["Agent"],"title":"Search-R1：Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning","uri":"/search-r1training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/"},{"categories":["Agent","reading"],"content":"好的，作为大模型领域的专家，我很乐意为您深入解读这篇富有洞见的论文《WebThinker: Empowering Large Reasoning Models with Deep Research Capability》。这篇论文确实触及了当前大模型研究的前沿核心——如何让模型从一个静态的“知识库”转变为一个动态的“研究员”。 首先，我将为您全面、深入地解读这篇论文的核心思想与贡献。 ","date":"2025-07-16","objectID":"/webthinkerempowering-large-reasoning-models-with-deep-research-capability/:0:0","tags":["Agent"],"title":"WebThinker：Empowering Large Reasoning Models with Deep Research Capability","uri":"/webthinkerempowering-large-reasoning-models-with-deep-research-capability/"},{"categories":["Agent","reading"],"content":"论文深度解读 这篇于 2025 年 4 月发表的论文《WebThinker》(arXiv:2504.21776v1)，旨在解决大型推理模型（Large Reasoning Models, LRMs）在处理复杂、知识密集型任务时面临的一个根本性瓶颈：它们对内部静态知识的过度依赖。尽管像 OpenAI-o 1 和 DeepSeek-R 1 这样的模型展现了强大的长链推理能力，但一旦任务需要最新的、多样化的或领域外的知识，它们的能力就会受到极大限制。传统的检索增强生成（Retrieval-Augmented Generation, RAG）技术虽然能从外部引入知识，但其工作流通常是预定义的、浅层的，无法模拟人类研究员那种“深入探索、动态调整、边想边写”的复杂研究过程。 WebThinker 的作者们敏锐地抓住了这一痛点，提出了一个全新的“深度研究代理”（Deep Research Agent）框架。其核心思想是，将大型推理模型（LRM）从一个被动的信息生成者，提升为一个能够自主规划、持续思考、深度探索、实时撰写的智能体。它不再是简单地“搜索-然后-回答”，而是实现了一个“思考-搜索-导航-整合-撰写”的动态闭环。 为了实现这一目标，论文提出了两个关键的核心组件： 深度网络探索器（Deep Web Explorer）：这不仅仅是一个简单的搜索引擎调用。它本身就是一个由 LRM 驱动的子流程，被赋予了两种关键能力：搜索（Search）和导航（Navigate）。当主推理流程遇到知识缺口时，它会调用这个探索器。探索器首先进行初步搜索，然后能像人一样，通过分析搜索结果，决定是否需要点击某个链接（Navigate）进入更深层次的网页，从中提取更精确、更深入的信息。这种递归式的探索能力，使得信息获取不再局限于搜索引擎返回的摘要，而是能够深入到网页内部，挖掘第一手资料。这极大地扩展了信息获取的深度和广度。 自主“思考-搜索-撰写”策略（Autonomous Think-Search-and-Draft）：这是 WebThinker 区别于传统 RAG 的又一革命性设计。它将报告的撰写过程与信息搜集过程深度融合。模型不是在所有信息搜集完毕后才开始写作，而是可以实时地、交错地进行推理、搜索和撰写。比如，模型可以先根据已有信息撰写报告的“引言”部分，然后在撰写过程中发现需要更多关于“方法论”的数据，于是它会暂停写作，启动“深度网络探索器”去搜集新信息，获得新知后再回来继续或修改报告。为了支持这一策略，论文为模型配备了三个专用工具：T_draft（撰写章节）、T_check（检查报告结构）、T_edit（编辑报告）。这种设计使得最终生成的报告不是零散信息的堆砌，而是随着研究深入而有机生长、不断完善的连贯产物。 为了让模型能够高效、准确地使用这些强大的工具，论文还引入了基于强化学习的训练策略——迭代式在线直接偏好优化（Iterative Online Direct Preference Optimization, DPO）。这是一个非常精妙的设计。DPO 是一种比传统 RLHF 更轻量、更高效的对齐技术，它通过构建“偏好对”数据（即一个好的行为轨迹比一个差的行为轨迹更好）来训练模型。WebThinker 的训练流程是： 1. 让当前模型针对一系列复杂任务生成多个不同的解决路径（轨迹）。 2. 根据预设的准则（如最终答案的正确性、工具使用的效率、思考过程的简洁性）自动构建偏好对 (Rw, Rl)，其中 Rw 是更优的轨迹。 3. 使用这些偏好对通过 DPO 损失函数来微调模型，使其更倾向于生成更优的轨迹。 4. 用微调后的新模型重复第一步，进行下一轮迭代。 这种“自我对弈”、“持续进化”的在线训练模式，使得 WebThinker 能够不断优化其复杂的工具使用策略，学会在恰当的时机，以恰当的方式调用恰当的工具。 在实验验证上，论文做得非常扎实。他们选取了横跨多个领域的极具挑战性的基准测试集，如考验专业科学知识的GPQA、评估通用 AI 能力的GAIA、专门测试网络遍历能力的WebWalkerQA，以及号称“人类最后考试”的超高难度推理任务HLE。实验结果令人印象深刻，以 32 B 模型为例，在 GAIA 任务上，WebThinker-32 B-RL 的平均分达到了48.5，显著高于标准迭代式 RAG 的35.0和之前的 SOTA 搜索代理 Search-o 1 的39.8（见 Table 1）。在 HLE 这个硬骨头上，WebThinker 的得分更是达到了15.8，而迭代 RAG 和 Search-o 1 分别只有9.6和10.8（见 Table 2）。这些数据强有力地证明了 WebThinker 框架在解决复杂问题上的优越性。此外，在科学报告生成任务（Glaive）上，WebThinker 同样取得了最高分，尤其在报告的“全面性”和“深入性”上表现突出（见 Figure 4），这直接验证了其“思考-搜索-撰写”策略的有效性。 总而言之，WebThinker 不仅仅是一次技术上的增量改进，它更代表了一种范式上的转变——推动大模型从“博学的对话者”向“能干的研究助理”迈出了坚实的一步。 接下来，我将按照您提出的六个问题，逐一进行详细解读。 1. 论文的研究目标是什么？想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是创建一个通用、灵活、开源的深度研究框架，以克服大型推理模型（LRMs）在处理需要实时、深度、多样化外部知识的复杂任务时的内在局限。 解决的实际问题： 知识静态性问题：现有大模型的知识截止于其训练数据，无法获取最新的信息，也无法处理训练语料库之外的“长尾”知识。 传统 RAG 的浅层性问题：标准的 RAG 通常只能利用搜索引擎返回的表层摘要信息，无法像人类一样点击链接深入挖掘信息，导致在需要多跳推理和信息整合的任务上表现不佳。 研究流程割裂问题：传统 AI 工作流通常将“信息检索”和“内容生成”作为两个独立的步骤，无法模拟人类研究员那种思考、探索和写作交织进行的动态、迭代过程。 行业意义： 提升 AI 能力上限：这个问题是当前限制 AI 从“通用问答”走向“专业问题解决”的关键瓶颈。解决它，意味着 AI 可以在科学研究、金融分析、市场调研、法律咨询等知识密集型行业中扮演更核心、更自主的角色。 降低专业研究门槛：一个强大的深度研究代理，可以极大地缩短研究人员在信息搜集和初步整合上花费的时间和精力，使他们能更专注于创新和决策，从而加速整个社会的知识生产和创新周期。 推动 Agentic AI 的发展：WebThinker 是智能体（Agent）技术在研究领域的一个完美范例。它的成功将激励更多研究者探索如何构建更强大、更自主的 AI 智能体，这对于通用人工智能（AGI）的最终实现至关重要。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。 论文的核心创新在于其独特的框架设计和训练方法，相较于之前的方法，其特点和优势非常鲜明。 正如论文在图 2 中所展示的，传统的 RAG（图 a）和迭代式 RAG（图 b）都是一种预定义的工作流，缺乏思考的深度和连贯性。而 WebThinker（图 c）将工具调用融入到一个连续的深度思考过程中，实现了端到端的自主执行。 新的思路与模型： Hierarchical Agent Architecture（分层代理架构）：WebThinker 实际上是一个两层架构。 顶层：主推理 LRM。它负责整体任务的规划和编排（Orchestration），决定何时搜索、何时写作、何时修改。 底层：专用功能模块。包括由另一个 LRM 驱动的Deep Web Explorer和由一个“助手 LLM”执行的Report Drafting Tools。这种“指挥官-士兵”的模式，让主模型专注于高层逻辑，而将繁琐的细节（如网页内容摘要、具体文本撰写）外包出去，提高了系统的效率和鲁棒性。 Autonomous Think-Search-and-Draft Strategy（自主“思考-搜索-撰写”策略）：这是对传统“检索后生成”范式的颠覆。 优势：这种交错进行的方式使得信息搜集更具目的性（按需索取，而非一次性泛泛检索），也使得最终报告的逻辑更连贯（信息和论点同步发展，而非生硬拼接）。 细节：论文中提到，该策略通过 T_check 工具，允许模型随时检查已生成的报告大纲，这有助于它动态调整后续的研究和写作计划，体现了高度的“元认知”（Metacognition）能力。 Recursive Deep Web Explorer（递归式深度网络探索器）：这是实现“深度”研究的关键。 特点：它不仅能调用搜索引擎 API（返回摘要），还能通过 T_n（导航工具）和网页爬虫（Crawl 4 AI）来“点击”链接，获取并理解完整的网页内容。 优势：这使得 WebThinker 能够执行多跳信息检索。例如，在 GAIA 任务中，它需要先找到一个地点（Fred Howard Park），然后从另一个信息源中找到这个公园对应的邮政编码。这是传统 RAG 难以完成的。 与之前方法的对比优势总结： 特性 传统 RAG WebThinker 优势分析 工作流 预定义、线性 动态、自主、闭环 更灵活，能适应复杂多变的任务需求 信息深度 浅层（搜索摘要） 深层（可点击链接导航） 能获取更精确、更底层的证据 核心过程 检索与生成分离 思考、检索、撰写深度融合 过程更像人类专家，产出质量更高 训练范式 依赖 ","date":"2025-07-16","objectID":"/webthinkerempowering-large-reasoning-models-with-deep-research-capability/:0:1","tags":["Agent"],"title":"WebThinker：Empowering Large Reasoning Models with Deep Research Capability","uri":"/webthinkerempowering-large-reasoning-models-with-deep-research-capability/"},{"categories":["RLHF","LLM"],"content":"DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling Policy Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2 个是主要亮点，是命名的来源） image.png ","date":"2025-07-15","objectID":"/dapo/:0:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["RLHF","LLM"],"content":"更高裁剪 clip的上边界可以放宽一些，即 clip_high 从 0.2 提高到了 0.28 ","date":"2025-07-15","objectID":"/dapo/:1:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["RLHF","LLM"],"content":"动态采样（Dynamic Sampling） image.png ","date":"2025-07-15","objectID":"/dapo/:2:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["RLHF","LLM"],"content":"Token 级策略梯度损失（Token-Level Policy Gradient Loss） image.png ","date":"2025-07-15","objectID":"/dapo/:3:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["RLHF","LLM"],"content":"超长奖励塑造（Overlong Reward Shaping） image.png ","date":"2025-07-15","objectID":"/dapo/:4:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["RLHF","LLM"],"content":"参考 # DAPO全是已有的小trick，为什么这么火? ","date":"2025-07-15","objectID":"/dapo/:5:0","tags":["LLM","RLHF"],"title":"dapo","uri":"/dapo/"},{"categories":["reading","Reasoning"],"content":"好的，作为大模型领域的学术专家，非常荣幸能与您一同深入探讨这篇富有启发性的论文——《First Return, Entropy-Eliciting Explore》。这篇由字节跳动、M-A-P 及曼彻斯特大学的研究者们共同完成的工作，直面了当前大模型在复杂推理任务中通过强化学习进行优化时的一个核心痛点。 首先，我将为您呈现一篇详尽的论文解读，然后逐一回答您提出的六个问题。 论文深度解读：在不确定性中寻找确定性的强化学习新范式 在大语言模型（LLM）能力飞速发展的今天，如何让模型像人一样进行复杂、多步骤的推理（如解决奥数题、编写复杂代码），已成为前沿研究的焦点。传统的监督微调（SFT）让模型学会了“模仿”，但要让模型真正“理解”并进行创造性、探索性的推理，强化学习（Reinforcement Learning, RL）被寄予厚望。然而，将 RL 应用于 LLM 的广阔“思想空间”中，如同在没有地图的汪洋中航行，充满了挑战。 这篇论文的核心贡献，在于提出了一种名为 FR 3 E (First Return, Entropy-Eliciting Explore) 的新型强化学习框架。它为 LLM 这艘大船在推理的汪洋中，提供了一套精巧的“声纳探测 + 局部精细探索”的导航系统，旨在解决一个根本性难题：稀疏奖励下的信用分配（Credit Assignment）。 想象一下，让模型解答一道复杂的数学题。它需要写下一连串的解题步骤，但只有最终答案的正确与否才能给出一个明确的奖励信号（reward=1 或 0）。这种奖励是稀疏（sparse）且延迟（delayed）的。如果最终答案错误，我们很难知道是哪一个推理步骤出了问题。传统方法，如论文中对比的GRPO (Group Relative Policy Optimization)，倾向于将最终的成败“平均”分摊到每一步，这显然是不合理的——可能仅仅是一个关键步骤的失误，导致了整个推理链的失败。而另一些方法，如PPO (Proximal Policy Optimization)，虽然试图通过一个“评论家”（Critic）网络来评估每一步的价值，但在 LLM 巨大的、离散的状态空间中，训练一个稳定而准确的评论家本身就极其困难，常常导致训练不稳定。 FR 3 E 的巧妙之处在于，它绕过了训练“评论家”的难题，而是直接从模型自身的行为中寻找探索的线索。它的哲学可以概括为：“与其在整片大海中盲目试探，不如先找到最可能出现风暴（或宝藏）的‘价值航区’，然后集中力量在该区域进行深入探索。” 这个过程被分为两个核心阶段： 第一阶段：First Return - 寻找高不确定性的关键决策点。FR 3 E 首先让模型生成一个完整的推理路径（称为“基础轨迹”）。然后，它并不急于评价这条路的好坏，而是用一个非常巧妙的指标——信息熵（Entropy）——来分析模型在生成这条路径时每一步的“犹豫程度”。 \u003e 通俗解释“熵”：在信息论中，熵衡量的是不确定性。当模型在生成下一个词时，如果对多个候选词的概率分布非常均匀（比如“A”有 30%可能，“B”有 28%可能，“C”有 25%可能），说明模型非常“纠结”，此时的熵就很高。反之，如果模型非常确定下一个词就是“A”（99%的可能），那么熵就很低。 FR 3 E 认为，这些高熵的节点，正是模型推理的“岔路口”，是思维最不确定的地方，也是最值得探索的“关键决策点”。论文通过计算每个生成词元（token）的熵，并选择熵值最高的 Top-K 个点作为“锚点”。 第二阶段：Entropy-Eliciting Explore - 从锚点出发进行多样化探索。找到这些“岔路口”后，FR 3 E 会从每一个锚点开始，生成多个不同的后续推理路径（称为“部分部署”，partial rollouts）。这就像是在一个关键决策后，尝试所有可能的平行宇宙。通过统计这些从同一“岔路口”出发的路径最终成功的比例，FR 3 E 能够非常有效地估计出这个“岔路口”决策的经验价值 V (Sj)。这种方式比从头开始生成完整路径要高效得多，并且提供了宝贵的、有语义基础的中间反馈。 更进一步，FR 3 E 还设计了自适应优势调制（Adaptive Advantage Modulation）机制。这是一个动态的反馈控制器，当模型在一个方向上的探索取得了进展（V (Sj) \u003e V (Sj-1)），它会适度“调低”学习信号，防止模型过早地陷入局部最优，鼓励继续探索；而当探索停滞不前时（V (Sj) ≤ V (Sj-1)），它会“放大”学习信号，促使模型更积极地摆脱困境。 论文的实验部分极其扎实，通过在 Qwen 2.5 系列模型上与 GRPO++（一个集成了多种优化技巧的强大基线）的对比，有力地证明了 FR 3 E 的有效性。例如，在极具挑战性的AIME 24数学竞赛基准测试中，对于 Qwen 2.5-32 B 模型，FR 3 E 取得了40.2%的准确率，显著优于 GRPO++的34.1%（见论文 Table 1）。 更有说服力的是对训练过程的深入分析。Figure 7 和 Figure 8 的图表显示，随着训练的进行，FR 3 E 能显著增加“完全正确”（All-Right）的轨迹数量，同时抑制“完全错误”（All-Wrong）的轨迹。这表明 FR 3 E 不仅仅是提升了平均分，更是教会了模型如何生成更稳定、更可靠的正确推理路径。 总而言之，FR 3 E 通过“熵”这一内生信号来识别不确定性，并以此为基点进行结构化探索，为解决 LLM 强化学习中的信用分配难题提供了一个优雅、高效且无需外部评论家的创新方案。它推动了 LLM 从“模仿者”向“思考者”的转变，为未来构建更强大、更可靠的 AI 推理系统铺平了道路。 ### 1. 论文的研究目标是什么？想要解决什么实际问题？这个问题对于行业发展有什么重要意义？ 研究目标：本文的核心研究目标是为基于强化学习的大语言模型（LLM）优化过程，设计一个更稳定、更高效的结构化探索（structured exploration）框架，以提升其在复杂推理任务（如数学解题）上的能力。 解决的实际问题： 信用分配难题（Credit Assignment Problem）：在多步骤的推理任务中，奖励信号往往是稀疏和延迟的（只有最终结果才有反馈）。当推理失败时，很难确定是哪一步的决策出了问题。这篇论文旨在解决如何将最终的成败功过，精确地归因到中间的关键步骤上。 \u003e 正如论文引言中所述：“A central challenge in these RLVR methodologies is the granular assignment of credit to intermediate steps within a reasoning trajectory.” 探索不稳定性（Unstable Exploration）：传统的 RL 方法要么因为依赖不稳定的价值网络（如 PPO 中的 Critic）而难以训练，要么因为探索方式过于简单（如 GRPO）而效率低下，导致模型训练过程波动大，甚至出现“熵崩溃”（entropy collapse），即模型变得过于自信，丧失了探索多样性的能力。 行业意义： 推动 AI 能力边界：解决这个问题，意味着我们可以更有效地训练 LLM 解决需要长链条、复杂逻辑推理的任务。这将直接提升 AI 在科学研究、软件工程、金融分析、法律咨询等专业领域的应用价值，使其从一个“知识问答机”进化为“问题解决伙伴”。 提升 AI 的可靠性与可解释性：通过 FR 3 E 这样的方法，我们不仅能提升模型的平均表现，还能使其生成更连贯、更稳定的推理过程。这有助于减少模型“胡说八道”的现象，并为我们理解模型的“思考”过程提供了新的窗口。 降低训练成本和门槛：FR 3 E 是一个无价值模型（value-model-free）的方法，它绕过了训练复杂 Critic 网络的需要，这在理论上可以降低对计算资源的要求和训练的复杂度，使得更多机构有能力去进行 LLM 的强化学习优化。 ### 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 核心新方法：论文提出了FR 3 E (First Return, Entropy-Eliciting Explore) 框架。其核心思路是“基于不确定性的结构化探索”。 与之前方法的对比及优势： 特性 PPO (Proximal Policy Optimization) GRPO (Group Relative Policy Optimization) FR 3 E (本文方法) 反馈粒度 词元级别（Token-level） 轨迹级别（Trajectory-level） 语义块级别（Semantic-block level） 核心机制 依赖一个需要额外训练的 Critic 网络来估计每一步的价值 比较多条完整轨迹的最终奖励，进行策略优化 通过计算熵识别不确定点，并从这些点进行局部探索 主要优势 反馈精细 实现简单，不依赖 Critic 无需 Critic，反馈信号比 GRPO 精细，探索效率高，训练更稳定 主要劣势 Critic 网络在 LLM 中训练不稳定，计算开销大 信用分配不精确，将奖励平均化 计算开销高于 GRPO，引入了新的超参数 FR 3 E 的具体创新点分析： 用熵作为内在不确定性信号：这是 FR 3 E 的基石。它不依赖外部的奖励模型或价值网络，而是利用模型自身在生成过程中固有的“犹豫”（高熵）来定位探索的起点。这是一种非常聪明的自监督信号。 \u003e","date":"2025-07-15","objectID":"/first-return-entropy-eliciting-explore/:0:0","tags":["reading","reasoning"],"title":"First Return, Entropy-Eliciting Explore","uri":"/first-return-entropy-eliciting-explore/"},{"categories":["Attention","LLM"],"content":"Safe softmax 并没有 1-pass 算法，那么 Attention 会不会有呢？有！这就是 FlashAttention！ 在使用 online attention 的情况下，从头开始计算 attention score 的过程如下： \\(\\operatorname{NOTATIONS}\\) \\(Q[k,:]:\\) the \\(k\\) -th row vector of \\(Q\\) matrix. \\(\\begin{aligned}O[k,:]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\\\\\mathbf{V}[i,i]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\end{aligned}\\) \\(V[i,:]{:\\text{ the }i\\text{-th row of }V\\text{ matrix}}.\\) \\(\\{\\boldsymbol{o}_i\\}{:}\\sum_{j=1}^ia_jV[j,:]\\), a row vector storing partial aggregation result \\(A[k,:i]\\times V[:i,:]\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}x_i\u0026\\leftarrow\\quad Q[k,:]\\:K^T[:,i]\\\\m_i\u0026\\leftarrow\\quad\\max\\left(m_{i-1},x_i\\right)\\\\d_i'\u0026\\leftarrow\\quad d_{i-1}'e^{m_{i-1}-m_i}+e^{x_i-m_i}\\end{aligned}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}\u0026a_i\\:\\leftarrow\\:\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026o_i\\:\\leftarrow\\:o_{i-1}+a_i\\:V[i,:\\:]\\end{aligned}\\] \\(\\mathbf{end}\\) \\[O[k,:]\\leftarrow\\boldsymbol{o}_N\\] 优化思路和 online attention 一样，将 \\(o_{i}\\) 的计算简化以便于可以写成迭代式。 原来的 \\(o_{i}\\) 使用以下方式计算，依赖于全局的 \\(m_{N}\\) 和 \\(d_{N}\\)。 \\[\\boldsymbol{o}_i:=\\sum_{j=1}^i\\left(\\frac{e^{x_j-m_N}}{d_N^{\\prime}}V[j,:]\\right)\\] 将其改写成如下形式： \\[\\boldsymbol{o}_i^{\\prime}:=\\left(\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:]\\right)\\] 这样按照上面的方式拓展下去，可以找到一个循环迭代式。 \\[\\begin{aligned} \\mathbf{o}_i^{\\prime}\u0026 =\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d'}V[j,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:] \\right)+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}}\\frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}}\\frac{d_{i-1}^{\\prime}}{d_i^{\\prime}}V[j,:]\\right)+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}}V[j,.]\\right)\\frac{d_{i-1}^{\\prime}}{d_i^{\\prime}}e^{m_{i-1}-m_i}+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,.] \\\\ \u0026= \\boldsymbol{o}_{i-1}'\\frac{d_{i-1}'e^{m_{i-1}-m_i}}{d_i'}+\\frac{e^{x_i-m_i}}{d_i'}V[i,:] \\end{aligned}\\] 这样就找到了 \\(o_{i}\\) 的递推表达式。 之后对 Q, K 进行 tiling 后计算，得到如下： \\[\\begin{aligned}\u0026\\textbf{for }i\\leftarrow1,\\#\\text{tiles do}\\\\\u0026\u0026\u0026\\boldsymbol{x}_i\\quad\\leftarrow\\quad Q[k;\\cdot] K^T[\\cdot,(i-1) b; i b]\\\\\u0026\u0026\u0026m_i^{(\\mathrm{local})}=\\begin{array}{c}\\overset{b}{\\operatorname*{max}}\\left(\\boldsymbol{x}_i[j]\\right)\\\\\\end{array}\\\\\u0026\u0026\u0026m_i \\leftarrow \\max\\left(m_{i-1},m_i^{(\\mathrm{local})}\\right)\\\\\u0026\u0026\u0026a_i^{\\prime} \\leftarrow d_{i-1}^{\\prime}e^{m_{i-1}-m_i}+\\sum_{j=1}^be^{\\boldsymbol{x}_i[j]-m_i}\\\\\u0026\u0026\u0026\\boldsymbol{o}_i^{\\prime} \\leftarrow \\boldsymbol{o}_{i-1}^{\\prime}\\frac{d_{i-1}^{\\prime}e^{m_{i-1}-m_i}}{d_i^{\\prime}}+\\sum_{j=1}^b\\frac{e^{\\boldsymbol{x}_i[j]-m_i}}{d_i^{\\prime}}V[(i-1) b+j,:]\\\\\u0026\\text{end}\\\\\u0026\u0026\u0026O[k,:]\\leftarrow\\boldsymbol{o}_{N/b}^{\\prime}\\end{aligned}\\] 对于 tiles，示意图如下： 可以理解成滑动窗口，\\(K^{T}\\) 从左向右滑动（按列读取），\\(V\\) 从上向下滑动（按行读取）。也可以直接理解成分块矩阵，具体为什么这么做，参考：Cuda 编程之 Tiling - 知乎 (zhihu.com) ","date":"2025-07-15","objectID":"/flash-attention/:0:0","tags":["Attention"],"title":"flash attention","uri":"/flash-attention/"},{"categories":["Attention","LLM"],"content":"参考 From Online Softmax to FlashAttention. ) ","date":"2025-07-15","objectID":"/flash-attention/:1:0","tags":["Attention"],"title":"flash attention","uri":"/flash-attention/"},{"categories":["Attention","LLM"],"content":" image.png 如上图所示，GQA 就是在 MHA 和 MQA 之间做了一个平衡。对 query heads 进行分组，分成几组就对应多少个 kv heads，然后每一组内的 query Heads 共享相同的 KV head。 GQA 可以在减少计算量和 KV Cache 同时确保模型效果不受到大的影响。 现在基本都使用 GQA，代码如下（核心是 repeat_kv 函数）： ```python def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: “““torch.repeat_interleave(x, dim=2, repeats=n_rep)”“” bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) class Attention(nn.Module): “““Multi-head attention module.”“” def __init__(self, args: ModelArgs): \"\"\" Initialize the Attention module. Args: args (ModelArgs): Model configuration parameters. Attributes: n_kv_heads (int): Number of key and value heads. n_local_heads (int): Number of local query heads. n_local_kv_heads (int): Number of local key and value heads. n_rep (int): Number of repetitions for local heads. head_dim (int): Dimension size of each attention head. wq (ColumnParallelLinear): Linear transformation for queries. wk (ColumnParallelLinear): Linear transformation for keys. wv (ColumnParallelLinear): Linear transformation for values. wo (RowParallelLinear): Linear transformation for output. cache_k (torch.Tensor): Cached keys for attention. cache_v (torch.Tensor): Cached values for attention. \"\"\" super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads // model_parallel_size self.n_local_kv_heads = self.n_kv_heads // model_parallel_size self.n_rep = self.n_local_heads // self.n_local_kv_heads self.head_dim = args.dim // args.n_heads self.wq = ColumnParallelLinear( args.dim, args.n_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wk = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wv = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wo = RowParallelLinear( args.n_heads * self.head_dim, args.dim, bias=False, input_is_parallel=True, init_method=lambda x: x, ) # kv_cache是缓存键值对，在训练过程中，我们只保存最近n个键值对 self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): \"\"\" Forward pass of the attention module. Args: x (torch.Tensor): Input tensor. start_pos (int): Starting position for caching. freqs_cis (torch.Tensor): Precomputed frequency tensor. mask (torch.Tensor, optional): Attention mask tensor. Returns: torch.Tensor: Output tensor after attention. \"\"\" # 假设当前x为(1, 1, dim)，也就是上一个预测的token # self-attention的输入，标准的(bs, seqlen, hidden_dim) bsz, seqlen, _ = x.shape # 计算当前token的qkv # q k v分别进行映射，注意这里key, value也需要先由输入进行映射再和kv_cache里面的key, value进行拼接 xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # 对当前输入的query和key进行RoPE，注意kv_cache里面的key已经做过了RoPE xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # 缓存当前token的kv self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos: start_pos + seqlen] = xk self.cache_v[:bsz, start_pos: start_pos + seqlen] = xv # 取出前seqlen个token的kv缓存 # 取出全部缓存的key和value（包括之前在cache里面的和本次输入的），作为最终的key和value keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # 将kv重复填充，使kv和q的头数个数相同 # repeat k/v heads if n_kv_heads \u003c n_heads，对齐头的数量 keys = repeat_kv(keys, self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values = repea","date":"2025-07-15","objectID":"/gqa/:0:0","tags":["Attention"],"title":"GQA","uri":"/gqa/"},{"categories":["RLHF","LLM"],"content":"GRPO (trl 库) ","date":"2025-07-15","objectID":"/grpo/:0:0","tags":["LLM","RLHF"],"title":"grpo","uri":"/grpo/"},{"categories":["RLHF","LLM"],"content":"重要参数 Num_generations: Number of generations to sample. The effective batch size (num_processes * per_device_batch_size * gradient_accumulation_steps) must be evenly divisible by this value. generation_batch_size: Batch size to use for generation. If None, it defaults to the effective training batch size: per_device_train_batch_size * num_processes * steps_per_generation. steps_per_generation: Number of optimization steps per generation. If None, it defaults to gradient_accumulation_steps. Num_iterations: Number of iterations per batch (denoted as μ in the algorithm). Per_device_train_batch_size Num_processes (world_size) trl 库的重要参数比较少。其中根据官方文档，generation_batch_size = `per_device_train_batch_size * num_processes * steps_per_generation Gradient_accumulation_steps 一般就是 steps_per_generation (对应 verl 中的 mini_batch_size / n_gpus / ppo_micro_batch_size_per_gpu)，可以理解为 per_device_train_bs (对应 verl 中的 ppo_micro_batch_size_per_gpu) 是使用梯度累计后的 bs，乘 gpu 数，再乘梯度累计的 steps 就是总的 batch_size（对应 verl 中的 train_batch_size * rollout. N）。所以注意，总的 batch_size (generation_batch_size) 是已经 rollout 采样后的 bs，除以 num_generations 才是针对 prompts 的 bs（verl 中的 train_batch_size）。 下面是_get_train_sampler 方法的注释，对每一个 prompt 重复 num_generations 是该方法实现的。 if dataset is None: dataset = self.train_dataset return RepeatSampler( data_source=dataset, mini_repeat_count=self.num_generations, # 每个 prompt 生成 self.num_generations 个 completions # 例如，如果 per_device_train_batch_size=8, num_generations=2, steps_per_generation=4, # 则 generation_batch_size = 8 (per_device_train_batch_size) * 4 (steps_per_generation) = 32 # 这里的 batch_size = 32 / 2 = 16，表示一个 \"generation block\" 中有16个不同的prompt。 batch_size=self.args.generation_batch_size // self.num_generations, # 每个 \"generation block\" (包含16个不同prompt，每个prompt有2个completion) 会被用于 num_iterations * steps_per_generation 次更新 # 例如 num_iterations=1, steps_per_generation=4, 则这个 block 会被重复 1*4=4 次，每次取出一个 per_device_train_batch_size 的数据进行训练 repeat_count=self.num_iterations * self.args.steps_per_generation, shuffle=self.shuffle_dataset, seed=self.args.seed, ) 结合下面的例子帮助理解，例子中梯度累计 steps 不等于 steps_per_generation 在 GRPO_trainer 中，最重要的方法是 _generate_and_score_completions 方法，输入为 input，输出为计算得到的优势值和 old_logp 用于计算 ratio。一些核心的部分和注释如下： with unwrap_model_for_generation( self.model_wrapped, self.accelerator, gather_deepspeed3_params=self.args.ds3_gather_for_generation ) as unwrapped_model: with ( FSDP.summon_full_params(self.model_wrapped, recurse=False) if self.is_fsdp_enabled else nullcontext() ): # prompt_ids: (B_gen_local, P_max) # prompt_mask: (B_gen_local, P_max) # prompt_completion_ids: torch.Tensor (B_gen_local, P_max + C_new), C_new 是 HF generate 生成的新 token 数量 (最大为 max_completion_length) prompt_completion_ids = unwrapped_model.generate( prompt_ids, attention_mask=prompt_mask, generation_config=self.generation_config ) # Compute prompt length and extract completion ids prompt_length = prompt_ids.size(1) # P_max # prompt_ids 保持不变: (B_gen_local, P_max) prompt_ids = prompt_completion_ids[:, :prompt_length] # completion_ids: torch.Tensor (B_gen_local, C_new_hf) completion_ids = prompt_completion_ids[:, prompt_length:] 上面为 generate 的过程，不过现在基本上使用 vllm 或者 sglang 加速推理。为了逻辑简单，这里展示了 HF generate 的过程。Trl 实现的时候，将一个 prompt 采样多次的逻辑实现在了 get_train_dataloader 方法中，即一开始就使用 get_train_sampler 方法对同一个 prompt repeat 了多次。因此这里不需要再进行 repeat。 之后得到补充部分的 mask: # Mask everything after the first EOS token # is_eos: torch.Tensor (B_gen_local, C_new), C_new 是 completion 的实际长度 (C_max_vllm 或 C_new_hf) is_eos = completion_ids == self.processing_class.eos_token_id # eos_idx: torch.Tensor (B_gen_local,), 存储每个 completion 中第一个 EOS token 的索引，如果没有EOS则为序列长度 eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device) eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)] sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1) # completion_mask: torch.Tensor (B_gen_local, C_new), 标记有效 token (EOS之前及EOS本身) completion_mask = (sequence_","date":"2025-07-15","objectID":"/grpo/:1:0","tags":["LLM","RLHF"],"title":"grpo","uri":"/grpo/"},{"categories":["Attention","LLM"],"content":"3-pass \\(\\mathsf{NO}\\) TATIONS \\(\\{m_i\\}{:}\\max_{j=1}^i\\left\\{x_j\\right\\}\\), with initial value \\(m_0=-\\infty.\\) \\(\\{d_i\\}{:}\\sum_{j=1}^ie^{x_j-m_N}\\), with initial value \\(d_0=0,d_N\\) is the denominator of safe softmax. \\(\\{a_i\\}{:\\text{ the final softmax value}}.\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[m_i\\leftarrow\\max\\left(m_{i-1},x_i\\right)\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[d_i\\leftarrow d_{i-1}+e^{x_i-m_N}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[a_i\\leftarrow\\frac{e^{x_i-m_N}}{d_N}\\] \\(\\mathbf{end}\\) 这是 3 step 计算 attention 的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于 sram 中没有足够的存储空间，因此需要多次访存。 ### Online attention \\[\\begin{aligned} d_i^{\\prime}\u0026 =\\sum_{j=1}^ie^{x_j-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_i}\\right)+e^{x_i-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_{i-1}}\\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\\\ \u0026= d_{i-1}' e^{m_{i-1}-m_i}+e^{x_i-m_i} \\end{aligned}\\] 找到迭代式之后就可以从 3 step 降到 2 step \\[\\begin{aligned}\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026m_i\u0026\u0026\\leftarrow\u0026\\max\\left(m_{i-1},x_i\\right)\\\\\u0026\u0026\u0026d_i^{\\prime}\u0026\u0026\\leftarrow\u0026d_{i-1}^{\\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\\\\u0026\\mathbf{end}\\\\\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026a_i\\leftarrow\u0026\u0026\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026\\mathbf{end}\\end{aligned}\\] 好像 FLOPs 计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的 scale X 值，也就是 pre-softmax logits，由于需要 O (N^2) 的显存无法放在 SRAM 中。因此： 1. 要么提前计算好 x，保存在全局显存中，需要 O (N^2) 的显存，容易爆显存。 2. 要么在算法中 online 计算，每次循环中去 load 一部分 Q，K 到片上内存，计算得到 x。 Attention 优化的目标就是避开第一种情况，尽可能节省显存，否则，LLM 根本无法处理类似 100 K 以上这种 long context 的情况。而对于第二种情况，我们不需要保存中间矩阵 x，节省了显存，但是计算没有节省，并且增加了 HBM IO Accesses（需要不断地 load Q, K）。此时，2-pass 算法相对于 3-pass 算法，可以减少一次整体的 load Q, K 以及减少一次对 xi 的 online recompute，因为在 2-pass 的第一个 pass 中， xi 是被两次计算共享的。类似 online-softmax 这种算法，对应到 Attention 中的应用，就是 Memory Efficient Attention（注意不是 FlashAttention）。 ","date":"2025-07-15","objectID":"/online-attention/:0:1","tags":["Attention"],"title":"online attention","uri":"/online-attention/"},{"categories":["RLHF","LLM"],"content":"PPO (openrlhf 库) 重点记录一下 experience 的采集过程。训练其实很简单。Actor 在 RLHF 会进行 auto-regressive decoding，而 critic, reward 和 reference 则只会 prefill，不会 decode。所以，我们将 actor 的推理特定称为 rollout，而其他模型的推理称为 inference。 获取 experience 的总体流程： #################### # 1. 调用Actor generate()方法获取Prompt的生成结果，把结果存储到Sample对象 #################### samples_list = self.generate_samples(all_prompts, **generate_kwargs) torch.distributed.barrier() #################### # 2. 调用make_experience 对每个Sample做处理，组装Experience部分字段（除了advantage和return） #################### experiences = [] for samples in samples_list: experiences.append(self.make_experience(samples).to_device(\"cpu\")) experiences, rewards = self.process_experiences(experiences) #################### # 3. 通过从后往前回溯计算的方式，获取advantage和return值 #################### for experience, reward in zip(experiences, rewards): num_actions = experience.info[\"num_actions\"] if self.advantage_estimator == \"gae\": experience.advantages, experience.returns = self.get_advantages_and_returns( experience.values, reward, experience.action_mask, generate_kwargs[\"gamma\"], generate_kwargs[\"lambd\"], ) if not getattr(self, \"packing_samples\", False): return_sums = reward.sum(dim=-1) else: return_sums = torch.tensor( [each_reward.sum() for each_reward in reward], device=torch.cuda.current_device() ) experience.info[\"return\"] = return_sums return experiences 关于句子序列中的 state 和 action 定义如下： ## Prompt -\u003e sample 首先对 batch 进行 pad, 注意推理时需要左 pad。 然后生成 sequences，attention_mask, action_mask： Sequences： Attention_mask： Action_mask： 至此 sample 数据就得到了。 ## Sample -\u003e experience 首先根据前面 generate 的 prompt+response 计算得到 response 部分的 logp： 同样的方式得到 reference_model 的 logp，然后就可以计算 kl 散度。 Critic 是预估状态的价值，看代码实现时，参考图 3，先理解 LLM 中状态的起始位置。最终状态序列长度是 num_actions (生成 token 的数量)，状态序列起始位置是 Prompt 的最后一个 token，结束位置是最后 eos token 前一个 token，所以计算出的 Critic 预估状态价值的数据为： 注意，图里 eos token 和 pad token 的位置输出应该并不是 0，是 regression head 的输出（小实数值），只是我们期望良好的价值函数在这些位置输出 0 在 RLHF 中，Reward Model 是一个 ORM（outcome Reward Model） 也就是对完整的生成 response 输出一个打分。代码实现上取每个 sequence eos token 位置的预估打分值。如图 11，图中”xx”也是会并行计算出的 Reward 值，单最终只取了序列最后 eos 位置的 score 作为完整序列的打分值。最后 reward 处理成[B, 1]格式，每个序列一个打分。 Gae (Generalized Advantage Estimation) 是 PPO 论文中实现的优势奖励值计算方法，可平衡优势预估的偏差和方差。结合公式和图片内容更容易理解： def get_advantages_and_returns(values: torch.Tensor, rewards: torch.Tensor,） Advantages looks like this: Adv1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... - V1 + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... Returns looks like this: Ret1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 计算 returns： image.png 此时我们就完成了 experience 的采集过程。 ","date":"2025-07-15","objectID":"/ppo/:1:0","tags":["LLM","RLHF"],"title":"ppo","uri":"/ppo/"},{"categories":["RLHF","LLM"],"content":"Clip 的一些细节 image.png image.png 上面这张图是很经典的一张图，来分析什么情况下 clip 项计算梯度。 ","date":"2025-07-15","objectID":"/ppo/:2:0","tags":["LLM","RLHF"],"title":"ppo","uri":"/ppo/"},{"categories":["RLHF","LLM"],"content":"参考 如何理解Q值和V值 (https://zhuanlan.zhihu.com/p/14569025663) ","date":"2025-07-15","objectID":"/ppo/:3:0","tags":["LLM","RLHF"],"title":"ppo","uri":"/ppo/"},{"categories":["","LLM","reading","RLHF"],"content":"一、论文的研究目标与意义 研究目标与待解决问题 论文的核心研究目标是：将基于强化学习（RL）的推理能力提升方法，从仅限于数学、编程等拥有明确验证规则的领域，扩展到更广泛的通用推理领域（如化学、法律、生物、商业等），同时摆脱对外部验证器（Verifier）的依赖。 要理解这一点，我们需要先了解一个背景概念：基于可验证奖励的强化学习 (RL with Verifiable Rewards, RLVR)。这是一种近年来非常成功的 LLM 训练范式，以 DeepSeek-R 1-Zero 为代表。其基本流程是： 模型针对一个问题，生成一个包含“思考过程”（Chain-of-Thought, CoT）和“最终答案”的完整回答。 一个验证器（Verifier），通常是一个基于规则的程序，会自动检查“最终答案”是否正确。 如果答案正确，模型获得+1 的奖励；如果错误，则获得 0 的奖励。 模型通过强化学习算法（如 PPO），根据这个非 0 即 1 的奖励信号来调整自身参数，以期生成更多能获得高奖励（即正确答案）的回答。 这种方法的瓶颈非常明显：它严重依赖一个可靠、廉价的自动验证器。这在数学（答案唯一）和编程（代码可执行）领域是可行的，但在绝大多数真实世界问题中，构建这样的验证器几乎不可能。例如，如何用程序自动判断一个法律分析或商业策略是否“正确”？ 为了解决这个问题，行业内出现了一种变通方法：用一个更强大的 LLM（如 GPT-4）作为模型验证器（Model-based Verifier）来判断答案的正确性。但这又引入了新的问题，正如论文摘要中提到的： Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. 这里的Reward Hacking是个很关键的概念，指的是模型可能会找到一些方法来“欺骗”验证器以获得奖励，但其推理过程本身可能是错误的。比如，模型可能学会了生成一些看起来很有说服力但实际上是错误的推理，来迎合验证器模型的“偏好”。同时，训练时需要额外加载和查询一个强大的验证器模型，这会带来巨大的计算和内存开销。 因此，这篇论文要解决的实际问题就是：如何在没有验证器（无论是基于规则还是基于模型）的情况下，依然能有效地通过强化学习提升 LLMs 在通用领域的推理能力？ 行业发展意义 这个问题的解决对于行业发展至关重要： 普及先进的训练技术：它将 RLVR 这一被证明在特定领域极为有效的技术，推广到了更广阔的通用场景，使得训练各行各业的“专家模型”成为可能。 降低训练成本和复杂性：通过移除验证器，该方法简化了训练流程，降低了对计算资源（特别是显存）的要求，让更多机构有能力进行类似的训练。 提升模型能力的“天花板”：相比于传统的监督微调（SFT），基于探索和奖励的强化学习能够让模型生成更多样化、更高质量的推理路径，从而可能达到比 SFT 更高的高度。这篇论文的方法为实现这一目标铺平了道路。 增强系统的鲁棒性：摆脱了对可能被“欺骗”的模型验证器的依赖，使得训练过程更稳定，更专注于提升模型内在的推理和答案生成能力的一致性。 二、核心思路与方法创新 论文提出的核心方法VeriFree (Verifier-Free)，其思路堪称精妙。它没有试图去判断模型生成的答案 y 是否正确，而是巧妙地改变了优化的目标。 核心思想：从“判断对错”到“评估置信度” 传统 RLVR 的优化目标可以理解为最大化“获得正确答案的期望奖励”。在只有一个正确答案 y* 的情况下，其目标函数如论文公式 (2) 所示： \u003e J_Verifier (θ; x, y*) = E_{z∼π_θ(·|x)} E_{y∼π_θ(·|x, z)} [ R_verifier (y; y*) ] 其中 z 是推理过程，y 是模型生成的答案，R_verifier 在 y 等于 y* 时为 1，否则为 0。 VeriFree 的核心洞察在于，上述期望值 E_{y∼π_θ(·|x,z)}[...] 其实可以直接计算出来。因为只有一个答案 y* 能获得奖励 1，其他所有答案奖励都为 0，所以这个期望值就等于模型在给定了问题 x 和推理过程 z 后，生成正确答案 y* 的概率 π_θ(y*|x, z)。 因此，VeriFree 将优化目标直接转化为了论文中的公式 (4)： \u003e J_VeriFree (θ; x, y*) = E_{z∼π_θ(·|x)} [ π_θ(y*|x, z) ] 这个转变的直观理解是：我们不再让模型生成一个完整的回答然后去判断对错，而是只让模型生成推理过程 z，然后直接评估模型对于“标准答案” y* 的置信度有多高。这个置信度（即概率 π_θ(y*|x, z)）本身就成了奖励信号。一个好的推理过程 z，自然应该让模型对正确答案 y* 有更高的置信度。 方法优势与创新点 与之前的方法相比，VeriFree 具有以下显著特点和优势： 更低的梯度方差（Lower Variance）：这是该方法在理论上的一大亮点。论文在定理 1（Theorem 1）中证明，VeriFree 的梯度估计器方差低于传统的验证器方法。这里涉及到一个重要的统计学概念叫Rao-Blackwellization。通俗地讲，传统方法需要随机采样一个答案 y 来获得一个充满随机性的 0 或 1 的奖励，这个过程方差很大。而 VeriFree 通过数学变换，用一个确定的期望值（即概率 π_θ(y*|x, z)）替代了这个随机采样过程，从而“分析性地边缘化”了 y 带来的随机性，使得训练信号更稳定，收敛更快。 理论上的等价性：在“单一正确答案”的假设下，VeriFree 的优化目标与原始的验证器方法在期望上是完全等价的。它不是一个近似，而是一个精确的数学重构。这使得它的理论基础非常坚实，不像其他一些方法（如 JLB、LaTRO）是在优化一个近似的下界。 对齐推理与答案：论文在 2.3 节中与 JLB、LaTRO 等方法的对比，揭示了 VeriFree 一个更深层次的优势。其他方法在更新答案生成部分时，权重通常是固定的（为 1），这意味着无论推理过程 z 有多糟糕，模型都会被强制要求在该推理下生成 y*。而 VeriFree 的更新权重是 π_θ(y*|x, z)，即奖励值本身。这会产生一个很好的效果： 如果推理过程 z 是高质量的，模型对 y* 的置信度 π_θ(y*|x, z) 会很高，那么对推理过程和答案生成的更新权重就大。 如果推理过程 z 是低质量的（例如胡言乱语），模型对 y* 的置信度会很低，更新权重就小，从而避免了让模型学习“从错误的推理得到正确的答案”这种有害的关联。 巧妙的实现细节：论文在 2.4 节中讨论了如何处理“拼接点”的 Tokenization 问题。这是一个非常实践的细节，展示了作者的严谨。简单地用文本分割推理和答案，可能会因为上下文变化导致 Tokenization 不一致而出错。他们提出的方法是，在生成推理时，将停止符设置为 \u003canswer（不含 \u003e），这确保了 Token 边界的干净对齐，是一个非常聪明且高效的工程技巧。 三、实验设计与结果验证 论文通过一系列周密设计的实验来验证 VeriFree 的有效性。 实验设计 基础模型：使用了不同参数规模的Qwen 3系列模型（1.7 B, 4 B, 8 B），这有助于检验方法在不同模型尺寸下的普适性。 训练数据：使用了一个名为WebData的通用推理数据集，该数据集源自 WebInstruct，经过了筛选和清洗，覆盖了多个领域，确保了训练的通用性。 评估基准： 通用推理：MMLU-Pro和SuperGPQA，这两个都是极具挑战性的、覆盖广泛学科的研究生水平的基准测试，能很好地评估模型的通用推理能力。 数学推理：同时也在 MATH、GSM 8 K 等一系列数学基准上进行了测试，以验证方法的泛化性。 核心对比对象 (Baselines)： Verifier：这是最重要的对比组。他们构建了一个基于模型的验证器，并使用与 VeriFree 相同的 RL 算法（Dr. GRPO）进行训练，确保了控制变量的公平性。 基础模型 (Base)：即未经任何微调的 Qwen 3 模型。 指令微调模型 (Instruct)：即官方发布的、经过 SFT 的 Qwen 3 模型。 实验数据与结果 实验结果非常有力地支持了论文的结论。 通用推理能力超越 Verifier 方法： 在 MMLU-Pro 和 SuperGPQA 这两个核心基准上，VeriFree 的表现稳定地优于或持平于需要额外验证器的 Verifier 方法。 模型 (Qwen 3-8 B) MMLU-Pro (Avg Acc %) SuperGPQA (Avg Acc %) Base (基础模型) 59.8 31.0 Base-Verifier 65.9 37.1 Base-VeriFree (Ours) 67.2 38.0 如上表所示（数据来自原文 Table 1 和 Table 2），在 8 B 规模的模型上，VeriFree 在两个基准上","date":"2025-07-15","objectID":"/reinforcing-general-reasoning-without-verifiers/:0:0","tags":["LLM","reading","RLHF"],"title":"Reinforcing General Reasoning without Verifiers","uri":"/reinforcing-general-reasoning-without-verifiers/"},{"categories":["","优化器"],"content":" AdaGrad 直接暴力累加平方梯度，这种做法的缺点就是累加的和会持续增长，会导致学习率变小最终变得无穷小，最后将无法获得额外信息。 ","date":"2025-07-12","objectID":"/adagrad/:0:0","tags":["优化器"],"title":"AdaGrad","uri":"/adagrad/"},{"categories":["","优化器","优化算法"],"content":"AdamW相对与Adam的改动十分简单，其将权重衰减项从梯度的计算中拿出来直接加在了最后的权重更新步骤上（图1，式12）。其提出的动机在于：原先Adam的实现中如果采用了 L2权重衰减，则相应的权重衰减项会被直接加在loss里，从而导致动量的一阶与二阶滑动平均均考虑了该权重衰减项，而这影响了Adam的优化效果，而将权重衰减与梯度的计算进行解耦能够显著提升Adam的效果。目前，AdamW现在已经成为transformer训练中的默认优化器了。 image.png 参考：# Adam和AdamW ","date":"2025-07-12","objectID":"/adamw/:0:0","tags":["优化算法","优化器","AdamW"],"title":"AdamW","uri":"/adamw/"},{"categories":["","优化器"],"content":" RMSProp 和 Adagrad 算法的最大区别就是在于更新累积梯度值 r 的时候 RMSProp 考虑加入了一个权重系数 ρ 。 它使用了一个梯度平方的滑动平均。其主要思路就是考虑历史的梯度，对于离得近的梯度重点考虑，而距离比较远的梯度则逐渐忽略。注意图中的是内积。 ","date":"2025-07-12","objectID":"/rmsprop/:0:0","tags":["优化器"],"title":"RMSProp","uri":"/rmsprop/"},{"categories":["优化器"],"content":"作为机器学习的初学者必然会接触梯度下降算法以及 SGD，基本上形式如下： \\[ \\theta_t = \\theta_{t-1} - \\alpha \\;g(\\theta) \\] 其中 \\(\\alpha\\) 为学习率，\\(g(\\theta)\\) 为梯度。 ","date":"2025-07-12","objectID":"/sgd/:0:0","tags":["优化器"],"title":"SGD","uri":"/sgd/"},{"categories":["","优化器"],"content":"带动量的随机梯度下降方法 它的思路就是计算前面梯度的该变量，每次迭代会考虑前面的计算结果。这样如果在某个维度上波动厉害的特征，会由于“momentum”的影响，而抵消波动的方向（因为波动剧烈的维度每次更新的方向是相反的，momentum 能抵消这种波动）。使得梯度下降更加的平滑，得到更快的收敛效率。而后续提出的 Adagrad，RMSProp 以及结合两者优点的 Adam 算法都考虑了这种“momentum”的思想。 前面求梯度的过程省略了，后面可以这样写： \\[ \\begin{align} \u0026 v_t = \\beta v_{t-1} + (1-\\beta)g_t \\\\\\\\ \u0026 \\theta = \\theta - \\alpha v_t \\end{align} \\] 其中 \\(\\alpha\\) 为学习率，一般的 \\(\\beta\\) 为 0.9。V 就是动量。 所以，SGD + Momentum 可以理解为，利用历史权重梯度矩阵 \\(W_{i} l(i\u003ct)\\) 和当前权重梯度矩阵 \\(W_{t} l\\) 的加权平均和，来更新权重矩阵 \\(W\\) 。由于 \\(\\beta \\in(0,1)\\) ，所以随着 \\(t\\) 的增大和 \\(i\\) 的减小， \\(\\beta^{t-i}\\) 会减小，历史权重梯度矩阵 \\(W_{i} l(i\u003ct)\\) 会逐渐减小。通俗来讲，会逐渐遗忘越旧的权重梯度矩阵。 ","date":"2025-07-12","objectID":"/sgd-momentum/:0:0","tags":["优化器"],"title":"SGD-Momentum","uri":"/sgd-momentum/"},{"categories":["","LLM","attention"],"content":"前置阅读：MHA 、MQA、GQA 和 rope 在标准的 Transformer中，多头注意力（Multi-Head Attention, MHA）机制通过并行计算多个注意力头来捕捉输入序列中的不同特征。每个注意力头都有自己的查询（Query, Q）、键（Key, K）和值（Value, V）矩阵，计算过程如下： 查询矩阵 Q：用于计算输入序列中每个位置的注意力权重。 键矩阵 K：用于与查询矩阵 Q 计算注意力分数。 值矩阵 V：用于根据注意力分数加权求和，得到最终的输出。 MLA 的核心思想是通过低秩联合压缩技术，减少训练时的激活占用和推理时的 kv cache 的占用，从而节省显存。 ","date":"2025-07-11","objectID":"/mla/:0:0","tags":["LLM","Attention"],"title":"MLA","uri":"/mla/"},{"categories":["","LLM","attention"],"content":"核心思想 当前我要存的K cache是4个k_head，但如果我能从这4个k_head中抽取出1份共有的信息，然后在做attn计算时，每个head都用这1份共有的信息做计算，那么我也只需存这1份共有信息作为K cache了。这样我就把K cache从原来num_heads = 4变成num_heads = 1，这不就能节省K cache了吗？ 但是等等，现在共有的k_head信息是抽取出来了，那么相异的k_head信息呢？（简单来说，就是由 \\(W_{K}\\) 不同head部分学习到的相异信息）。我们当然是希望k_head间相异的信息也能保留下来，那么该把它们保留至哪里呢？当你回顾attn_weights的计算公式时，一个想法在你脑中闪现：q部分不是也有heads吗！我可以把每个k_head独有的信息转移到对应的q_head上吗！写成公式解释就是： 原来 \\(attention\\_weights=(W_{Q}h_{i})^T * (W_{k}h_{j})\\) ，括号表示运算顺序，即先各自算2个括号内的，再做 * 计算 现在 \\(attention\\_weights=(h_{i}^TW_{Q}W_{k})^T * h_{j}\\)，同理括号表示运算顺序。也就是说，这里我们通过矩阵乘法的交换律，巧妙地把1个token上k_heads独有的信息转移到了对应的q_head上来，这样1个token上k_heads间共享的相同信息就能被我们当作K cache存储下来。 （在这里，你可以抽象地把 \\(h_{j}\\) 理解成是4个k_heads共享的信息，但最终K cache的形式还会在这基础上有所变化。我知道此时你脑海中一定有很多疑惑。但我们先不要纠结细节的问题，因为在后文会展示全部细节，这里我们要做的是从宏观上理解MLA设计的核心思想。） 上述叙述来自再读MLA，还有多少细节是你不知道的 因此 MLA 的核心思想就是找到一个压缩的 \\(h_{j}\\) 来表示所有 head 的共有信息，而将 head 之间相异信息让 q 来吸收，这样就大大压缩了 k 的 head_num，v 也是同理，不同是 \\(W_{V}\\) 和 \\(W_{O}\\) 吸收。因此 MLA 并没有损失信息，而是将信息转移到了 q_head 上。总维度并没有减少。 ","date":"2025-07-11","objectID":"/mla/:1:0","tags":["LLM","Attention"],"title":"MLA","uri":"/mla/"},{"categories":["","LLM","attention"],"content":"苏神思路 我认为先看苏神的博客再去看猛猿大佬的博客是比较容易理解的。 苏神认为 MLA 是在 GQA 的基础上将简单的线性变换（分割、复制）换成一般的线性变换来增强模型的能力。GQA 也可以看作是一种低秩投影。原因见原文： 其中 g 是组数。这里的 c 只是简单的拼接，MLA 的初始想法就是使用一种线性转换代替拼接。这个线性转换也是一个可以学习的矩阵 \\(W\\)。因此一个自然的想法就出来了： 然而这种方法在推理阶段并不会减少 kv cache 的显存占用，我们还是需要存储 k 和 v 而不是 c，并且 kv cache 大小反而和 MHA 一样大了。这时引入一个恒等变换就可以解决问题： 这样就可以把 c 取代原来的 k，而把两个 W 当作新的 Q，同理后面可以把 Wv 和 Wo 合并，v 也可以用 c 取代。这个 c 就是上面说的所有 head 的共享信息。而相异信息放入了新的 Q 中来学习。这样在推理阶段就可以只保存 c 来作为 kv cache 了。所以可以降低 c 的维度来进一步降低显存占用。 现在成功降低了显存占用，但有了一个新问题，如何在这里引入 rope 呢，众所周知 rope 是在计算注意力分数时加入了位置信息。如果强行加入则会产生下面的问题： image.png 也就是新的 Q 矩阵与 i 有关，而不是固定的一个矩阵了。 解决的办法也简单粗暴，就是在 q 和 k 的维度上增加 rope 的维度。也就是： image.png 其中前半部分是 nope，后半部分是 rope，这个在后面猛猿大佬讲解的源码中会有体现。 最后为了降低激活值，又对 q 进行了低秩投影，最后变成了： 至于为什么 q 的 rope 项是 c，而 k 的 rope 项是 x，这个我也不清楚。在训练阶段，MLA 只是在 MHA 的基础上加了低秩投影，以及为了 rope 的计算在原本 dk 的维度上增加了 dr 维度。 区别在于解码阶段，MLA 可以像 MQA 一样简单： 在解码阶段，只需要保存 c（注意 c 是 k 和 v 的集合体），这大大降低了显存占用。苏神博客写 prefill 阶段使用 (10)，generation 阶段使用 (12)，因为 prefill 是 compute-bound，需要降低计算量，而 generation 阶段是 memory-bound，需要降低显存占用，这样结合完美符合我们的需求。 ","date":"2025-07-11","objectID":"/mla/:2:0","tags":["LLM","Attention"],"title":"MLA","uri":"/mla/"},{"categories":["","LLM","attention"],"content":"猛猿大佬思路 一切从下面这张图说起，这是对源码实现逻辑的抽离： (图中的 kv_b_proj 维度标注错误，应该为(nope+v_head_dim)，这里的 v_head_dim=nope) 从图中可以看到，此时 q_len 为 1，kv_len 为 1024，q_lora_rank 大小为 1536，而 kv_lora_rank 大小为 512，我们可以观察到左路和右路的计算流程不太一样，这是因为上面公式 (10) 中 q 和 k 的 rope 部分使用的不一样，q 使用的是压缩后的 c，而 k 使用的是原始 x, 因此在 x 为输入的时候 kv 部分就可以将 rope 部分算出来，而 q 部分需要将压缩后的 q 得到后才可以计算。q 是将 PE 和 NoPE split 开，而 kv 是将 NoPE 的 k 和 v split 开。 需要注意的是，在 nope 中，head_dim 为 128，而这大于 hsz // num_heads，猛猿大佬猜测这是为了提高模型的复杂度，因为推理的时候通过只保存压缩后的 kv 来减少了 kv cache 占用，那么训练的时候就可以稍微提高复杂度。 下面的代码是上图中的一些定义。结合苏神的推导过程相信可以理解 MLA。 class DeepseekV2Attention(nn.Module): \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\" def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None): super().__init__() self.config = config self.layer_idx = layer_idx self.attention_dropout = config.attention_dropout self.hidden_size = config.hidden_size self.num_heads = config.num_attention_heads self.head_dim = config.head_dim self.max_position_embeddings = config.max_position_embeddings self.rope_theta = config.rope_theta self.q_lora_rank = config.q_lora_rank self.qk_rope_head_dim = config.qk_rope_head_dim self.kv_lora_rank = config.kv_lora_rank self.v_head_dim = config.v_head_dim self.qk_nope_head_dim = config.qk_nope_head_dim self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads self.is_causal = True if self.q_lora_rank is None: self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.qk_head_dim, bias=False) else: self.q_a_proj = nn.Linear(self.hidden_size, config.q_lora_rank, bias=config.attention_bias) self.q_a_layernorm = DeepseekV2RMSNorm(config.q_lora_rank) self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False) self.kv_a_proj_with_mqa = nn.Linear( self.hidden_size, config.kv_lora_rank + config.qk_rope_head_dim, bias=config.attention_bias, ) self.kv_a_layernorm = DeepseekV2RMSNorm(config.kv_lora_rank) self.kv_b_proj = nn.Linear( config.kv_lora_rank, self.num_heads * (self.qk_head_dim - self.qk_rope_head_dim + self.v_head_dim), bias=False, ) self.o_proj = nn.Linear( self.num_heads * self.v_head_dim, self.hidden_size, bias=config.attention_bias, ) self.scaling = self.qk_head_dim ** (-0.5) ","date":"2025-07-11","objectID":"/mla/:3:0","tags":["LLM","Attention"],"title":"MLA","uri":"/mla/"},{"categories":["","LLM","attention"],"content":"参考 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces # Multi-Head Latent Attention (MLA) 详细介绍（来自Deepseek V3的回答） 再读MLA，还有多少细节是你不知道的 modeling_deepseek.py · deepseek-ai/DeepSeek-V2 at main ","date":"2025-07-11","objectID":"/mla/:4:0","tags":["LLM","Attention"],"title":"MLA","uri":"/mla/"},{"categories":["","优化算法","优化器","Muon","LLM"],"content":"Muon 算法流程如下图所示： image.png 其中最主要的部分是 NewtonSchulz 5 算法，流程如下： def newtonschulz5(G, steps=5, eps=1e-7): assert G.ndim == 2 a, b, c = (3.4445, -4.7750, 2.0315) X = G.bfloat16() X /= (X.norm() + eps) if G.size(0) \u003e G.size(1): X = X.T for _ in range(steps): A = X @ X.T B = b * A + c * A @ A X = a * X + B @ X if G.size(0) \u003e G.size(1): X = X.T return X 这个算法的作用是将 G 近似为一个最接近他的半正交矩阵，即： image.png 对于经验动机，我们观察到，基于手动检查，SGD-momentum 和 Adam 对基于 Transformer 的神经网络中的 2D 参数产生的更新通常具有非常高的条件数。也就是说，它们几乎是低秩矩阵，所有神经元的更新仅由几个方向主导。我们推测正交化有效地增加了其他“罕见方向”的规模，这些方向在更新中幅度较小，但对学习仍然很重要。 ","date":"2025-07-11","objectID":"/muon/:0:0","tags":["优化算法","优化器","LLM"],"title":"Muon","uri":"/muon/"},{"categories":["","优化算法","优化器","Muon","LLM"],"content":"Muon in Moonlight ","date":"2025-07-11","objectID":"/muon/:1:0","tags":["优化算法","优化器","LLM"],"title":"Muon","uri":"/muon/"},{"categories":["","优化算法","优化器","Muon","LLM"],"content":"QK-clip ","date":"2025-07-11","objectID":"/muon/:2:0","tags":["优化算法","优化器","LLM"],"title":"Muon","uri":"/muon/"},{"categories":["","LLM","infra"],"content":"为什么存储激活值？ 首先回顾为什么要存储激活值。 简单来说，模型参数是根据导数更新的。为了有效地计算这些导数，必须缓存某些张量。激活内存是这些缓存张量的内存成本。 具体来说，以 \\(f\\) 是矩阵乘法运算： \\[y=f(x)=W\\cdot x\\] \\(W\\)是一个可学习的权重矩阵。假设我们有关于 早期反向传播阶段的手头输出，\\(\\frac{\\partial L}{\\partial y}\\),我们 需要计算 两个额外的梯度： 1.关于\\(W\\),以便我们可以更新此权重。 2.关于\\(x\\),这样我们就可以继续反向传播算法 到产生的任何作\\(x\\) 前导数是 \\[\\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial W}=\\frac{\\partial L}{\\partial y}\\times x\\] 而后者的导数是 \\[\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\\cdot W\\] 因此，如下图所示，我们需要缓存输入张量 \\(x\\) 为了能够计算我们关心的导数。节省的成本 \\(x\\) 是激活 memory。 我们发现只需要保存 x，而对于 y 不需要保存，也就是说我们只保存反向传播时绝对需要的激活值，其它的临时变量要立即释放。 ","date":"2025-07-10","objectID":"/activation-checkpointing/:1:0","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"MLP class MLP(nn.Module): \"\"\" Basic MLP (multi-layer perceptron) layer with optional Dropout. \"\"\" def __init__( self, d_model: int, act_fn: nn.Module, dropout_prob: Optional[float] = None, device: Optional[Union[str, torch.device]] = None, dtype: Optional[torch.dtype] = None, ) -\u003e None: super().__init__() self.d_model = d_model self.act_fn = act_fn self.dropout_prob = dropout_prob factory_kwargs = {\"device\": device, \"dtype\": dtype} self.lin_0 = nn.Linear(self.d_model, 4 * self.d_model, **factory_kwargs) self.lin_1 = nn.Linear(4 * self.d_model, self.d_model, **factory_kwargs) self.dropout = nn.Dropout(self.dropout_prob) if self.dropout_prob else None def forward(self, inputs: torch.Tensor) -\u003e torch.Tensor: x = self.lin_0(inputs) x = self.act_fn(x) x = self.lin_1(x) if self.dropout is not None: x = self.dropout(x) return x image.png Maybe saved 的是否 是根据激活函数的类型来的： 如果是 GELU 激活函数 \\[y=\\frac x2\\times\\tanh\\left(\\sqrt{\\frac2\\pi}\\left(x+.044715x^3\\right)\\right)\\] 这时候计算梯度的话还是需要输入 x，这是需要存储的情况 而如果是一些特殊的激活函数的话，比如 ReLU \\[y=\\mathsf{ReLU}(x)=\\begin{cases}x\u0026\\mathrm{if~}x\u003e0\\\\0\u0026\\mathrm{if~}x\u003c0\u0026\\end{cases}\\] 梯度为： \\[\\frac{dy}{dx}=\\frac{d\\text{ ReLU}(x)}{dx}=\\begin{cases}1\u0026\\mathrm{if~}x\u003e0\\\\0\u0026\\mathrm{if~}x\u003c0\u0026\\end{cases}\\] 就不需要存储。 Tanh 也是同理： \\[ \\frac{dy}{dx} = \\frac{d\\tanh (x)}{dx} = 1-\\tanh(x)^2 = 1-y^2 \\] 如果想要性能，选择 GELU，想要低显存，使用 ReLU。 ","date":"2025-07-10","objectID":"/activation-checkpointing/:2:0","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"激活值计算 根据博客 # 分析transformer模型的参数量、计算量、中间激活、KV cache 内容来得到激活值的计算公式。 大模型在训练过程中通常采用混合精度训练，中间激活值一般是float16或者bfloat16数据类型的。在分析中间激活的显存占用时，假设中间激活值是以float16或bfloat16数据格式来保存的，每个元素占了2个bytes。唯一例外的是，dropout操作的mask矩阵，每个元素只占1个bytes。在下面的分析中，单位是bytes，而不是元素个数。 先分析 self-attention 块的中间激活。Self-attention 块的计算公式如下： \\[Q=xW_Q,K=xW_K,V=xW_V\\] \\[x_{out}=softmax(\\frac{QK^T}{\\sqrt{h}})\\cdot V\\cdot W_o+x\\] 对于 \\(Q,K,V\\) ,需要保存它们共同的输入 \\(x\\) ,这就是中间激活。输入 \\(x\\) 的形状为 \\([b,s,h]\\) ,元素个数为 bsh ,占用显存大小为 \\(2*bsh=2bsh\\) 。 对于 \\(QK^T\\) 矩阵乘法，需要保存中间激活 \\(Q,K\\) ,两个张量的形状都是 \\([b,s,h]\\) ,占用显 存大小合计为 \\(2*2*bsh=4bsh\\) 。 对于 \\(softmax()\\) 函数，需要保存函数的输入 \\(QK^T\\) ,占用显存大小为 \\(2bs^2a\\) ,这里的 \\(a\\) 表示注意力头数。 \\[score=softmax(\\frac{QK^T}{\\sqrt{d_k}})\\] \\(Q\\) 的形状为：\\([ b, head\\_ num, s, per\\_ head\\_ hidden\\_ size]\\) \\(K^T\\) 的形状为：\\([b,head\\_num,per\\_head\\_hidden\\_size,s]\\) \\(QK^T\\) 的形状为：\\([b,head\\_num,s,s]\\) ,元素个数为 \\(bs^2a\\) ,占用显存大小为 \\(2bs^2a\\) 。 4. 计算完 \\(softmax()\\) 函数后，会进行 dropout 操作。需要保存一个 mask 矩阵，mask 矩阵的形状与 \\(QK^T\\) 相同，占用显存大小为 \\(bs^2a\\) 。 5. 计算在 \\(V\\) 上的 attention，即 \\(score\\cdot V\\), 需要保存 score ,大小为 \\(2bs^2a\\) ;以及 \\(V\\) ,大小为 \\(2bsh\\) 。二者占用显存大小合计为 \\(2bs^2a+2bsh\\) 。 6. 计算输出映射以及一个 dropout 操作。输入映射需要保存其输入，大小为 \\(2bsh\\) ;dropout 需要保存 mask 矩阵，大小为 bsh 。二者占用显存大小合计为 \\(3bsh\\) 。 因此，将上述中间激活相加得到，self-attention 块的中间激活占用显存大小为 \\(11bsh+5bs^2a\\) 。 接下来看 MLP 块的中间激活。MLP 块的计算公式如下： \\[x=f_{gelu}(x_{out}W_1)W_2+x_{out}\\] 第一个线性层需要保存其输入，占用显存大小为 \\(2bsh\\) 。 激活函数需要保存其输入，占用显存大小为 \\(8bsh\\) 。 第二个线性层需要保存其输入，占用显存大小为 \\(8bsh\\) 。 最后有一个 dropout 操作，需要保存 mask 矩阵，占用显存大小为 bsh 。 对于 MLP 块，需要保存的中间激活值为 19 bsh 。 另外，self-attention 块和 MLP 块分别对应了一个 layer normalization。每个 layer norm 需要保存其输入，大小为 \\(2bsh\\) （忽略了均值和方差的2 bs，如果不忽略应当为 2 bsh+4 bs，这里的单位都是 bytes）。2 个 layer norm 需要保存的中间激活为 \\(4bsh\\) 。 综上，每个 transformer 层需要保存的中间激活占用显存大小为 \\(34bsh+5bs^2a\\)。对于 \\(l\\) 层 transformer 模型，还有 embedding 层、最后的输出层。Embedding 层不需要中间激活。总的而言，当隐藏维度 \\(h\\) 比较大，层数 \\(l\\) 较深时，这部分的中间激活是很少的，可以忽略。因 此，对于 \\(l\\) 层 transformer 模型，中间激活占用的显存大小可以近似为 \\((34bsh+5bs^2a)*l\\) ","date":"2025-07-10","objectID":"/activation-checkpointing/:3:0","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"Activation checkpointing 当使用该方法时，我们只需要保存一些关键的激活值，可以舍弃一些激活值从而在反向传播的过程中重新计算，通常有两种策略： Full：我们在 Transformer 模型每一层之间的过渡点检查激活值。这通常被称为“完整”策略，因为它需要每层都进行一次前向传播，即在反向传播过程中增加了一次完整的前向传播。这种策略节省的内存最多，但在计算方面成本最高。它通常会使计算成本和时间增加高达 30-40%，这一影响非常显著。 Select：总体而言，我们可以比全面优化做得更好。那篇关于重新计算的论文的作者进行了详细分析，研究了哪些激活值的增长最大，并且以每秒浮点运算次数（FLOPS）为标准，其重新计算成本最低。结果表明，注意力计算属于这一类别，因此我们通常可以丢弃它们，而专注于对昂贵的前馈计算进行检查点设置。对于 GPT-3（1750 亿参数）模型而言，这意味着在计算成本仅增加 2.7%的情况下，激活值内存减少了 70%。 如今，大多数训练框架都使用 FlashAttention，它通过在反向传播中重新计算注意力得分和矩阵，而不是存储它们，将激活重新计算集成到其优化策略中。因此，大多数使用 FlashAttention 的人已经在使用 AC。 由于重新计算，激活重新计算略微增加了 FLOPS 的数量，同时显著降低了内存访问开销。 这种权衡在 GPU 等高速内存有限的硬件上特别有利，因为访问内存通常比执行计算慢。尽管涉及额外的操作，但总体效果通常是计算速度更快，内存占用更少。 ","date":"2025-07-10","objectID":"/activation-checkpointing/:4:0","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"1. Full Activation Checkpointing (全量激活值检查点) 这是最经典、最直接的检查点方法。 工作原理： 分段: 将整个模型的计算图（所有层）在逻辑上划分为若干个段 (Segment)。 保存: 在前向传播时，只保存每个段的输入张量 (Input Tensor)。这些被保存的张量就是“检查点”。段内所有中间层的激活值都会在计算后被立即丢弃，不占用显存。 重算: 在反向传播时，当需要某个段内的激活值来计算梯度时，它会找到该段的检查点（即该段的输入），然后重新执行该段的前向传播，以恢复所需要的激活值。一旦这个激活值被使用完毕，它会再次被丢弃。 图解： 假设一个模型有16层，我们每4层设置一个检查点： 标准训练 (无Checkpointing): [Input] -\u003e L1 -\u003e L2 -\u003e ... -\u003e L16 -\u003e [Output] - 前向传播: 计算并存储 L1 到 L16 的所有激活值。 - 内存占用: 极高。 Full Activation Checkpointing: [Input] -\u003e [Segment 1: L1-L4] -\u003e [Segment 2: L5-L8] -\u003e [Segment 3: L9-L12] -\u003e [Segment 4: L13-L16] -\u003e [Output] - 前向传播: - 执行 Segment 1 (L1-L4)，只保存 L5 的输入（即L4的输出）作为检查点，丢弃 L1-L3 的激活值。 - 执行 Segment 2 (L5-L8)，只保存 L9 的输入作为检查点，丢弃 L5-L7 的激活值。 - …以此类推。 - 反向传播: - 当需要计算 L12 的梯度时，发现它的激活值没有被保存。 - 系统加载最近的检查点（L9的输入）。 - 重新计算 L9 -\u003e L10 -\u003e L11 -\u003e L12 的前向传播，得到 L12 的激活值。 - 使用该激活值计算梯度，然后丢弃它。 优缺点： 优点: 效果显著: 可以大幅度降低显存占用，内存占用量与模型的层数基本无关，只与最长的那个段的计算复杂度有关。 实现简单: 逻辑清晰，易于在各种框架中实现（例如 PyTorch 的 torch.utils.checkpoint）。 缺点: 计算开销大: 每个被 checkpoint 的段（除了最后一个）都会被重新计算一次。如果模型很大，这会带来大约 30%-50% 甚至更高的训练时间开销。 ","date":"2025-07-10","objectID":"/activation-checkpointing/:4:1","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"2. Selective Activation Checkpointing (选择性激活值检查点) 这是对全量检查点方法的智能化升级，也是目前更受关注的重点。它认识到“全量丢弃、全量重算”的策略过于粗暴和低效。 核心思想： 在同一个计算段内，不同的操作（Op）或激活值，其存储成本和重算成本是不同的。 有些激活值占用显存大，但重算很快（例如 ReLU, Dropout 等元素级操作）。 有些激活值占用显存小，但重算很慢（例如 MatMul, Convolution 的输出）。 Selective Checkpointing 的目标就是：在每个段内，不再丢弃所有激活值，而是有选择性地保存那些“重算成本高、存储成本低”的激活值，从而在反向传播时，避免代价高昂的重计算。 工作原理： 成本分析: 它需要对计算图中的每个操作进行成本分析。 存储成本: 该操作输出的激活值张量占用的显存大小。 重算成本: 重新计算出这个激活值所需要的时间（通常用 FLOPs 衡量）。 智能决策: 基于成本分析，算法（通常是动态规划或启发式搜索）会做出决策。对于段内的每一个激活值，它会判断： 是直接保存它更划算？ 还是丢弃它，之后再通过重计算恢复更划算？ 选择性保存与重算: 在前向传播时，除了保存每个段的输入（主检查点）外，还会额外保存段内那些被判定为“值得保存”的少量关键激活值。 在反向传播时，当需要一个激活值时，如果它被保存了，就直接使用。如果没被保存，系统会从最近的一个检查点（无论是主检查点还是段内保存的次级检查点）开始重算，而不是必须从段的开头开始。 图解（续上例）： 在 Segment 3 (L9-L12) 中： L9 (MatMul) -\u003e L10 (LayerNorm) -\u003e L11 (ReLU) -\u003e L12 (MatMul) Full Checkpointing 会丢弃 L9, L10, L11 的所有输出。重算 L12 时需要从 L9 的输入开始，重新执行 MatMul, LayerNorm, ReLU。 Selective Checkpointing 可能会做出如下决策： L9 (MatMul) 的输出：重算成本极高，但存储成本可能相对可控。决策：保存。 L11 (ReLU) 的输出：占用显存和 L10 一样大，但重算成本极低（只需对 L10 的输出再做一次 ReLU）。决策：丢弃。 在反向传播时： - 当需要 L11 的激活值时，发现它没被保存。但系统发现 L10 的激活值（或者 L9 的）被保存了。 - 它只需从 L10 开始重算 L10 -\u003e L11 (ReLU)，而无需重算昂贵的 L9 (MatMul)。 优缺点： 优点: 最佳平衡: 在实现与 Full Checkpointing 几乎相同显存节省效果的同时，显著降低了重计算的开销，从而缩短了训练时间。它在“时间”和“空间”之间找到了一个更优的平衡点。 高效: 相比 Full Checkpointing，训练速度更快。 缺点: 实现复杂: 需要对模型的计算图进行深入分析，并建立准确的成本模型。这通常需要深度学习框架或特定库（如 DeepSpeed）的底层支持。 模型依赖: 优化的效果依赖于模型结构。对于包含大量昂贵操作的模型，效果会更明显。 ","date":"2025-07-10","objectID":"/activation-checkpointing/:4:2","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"总结对比 特性 Full Activation Checkpointing Selective Activation Checkpointing 核心策略 以时间换空间 在时间和空间之间寻找最优平衡 粒度 粗粒度 (段级别) 细粒度 (操作级别) 保存内容 仅保存每个段的输入 保存每个段的输入 + 段内部分关键激活值 重算方式 总是从段的开头重算整个段 从最近的可用检查点开始重算，路径更短 计算开销 高 较低 显存节省 非常高 非常高 (与 Full 类似) 实现复杂度 低 高 总而言之，Selective Activation Checkpointing 是对传统检查点技术的一次重大优化。它通过更精细化的资源管理，在不牺牲太多内存节省的前提下，大幅减少了因重计算带来的时间惩罚，是目前训练超大规模模型时更为先进和高效的主流技术之一。 ## 参考 Activation Memory: A Deep Dive using PyTorch | Determined AI ","date":"2025-07-10","objectID":"/activation-checkpointing/:4:3","tags":["LLM","infra"],"title":"Activation checkpointing","uri":"/activation-checkpointing/"},{"categories":["","LLM","infra"],"content":"CPU 卸载允许我们将一些状态传输到 CPU，因此我们不必将所有内容都保存在 GPU RAM 中。虽然 CPU作比 GPU作慢，但将不常访问的数据移动到 CPU 内存可以帮助我们保持在 GPU 内存限制范围内。 ","date":"2025-07-10","objectID":"/cpu-offloading/:0:0","tags":["LLM","infra"],"title":"CPU offloading","uri":"/cpu-offloading/"},{"categories":["","reading","RLHF"],"content":"好的，非常荣幸能以领域专家的身份，与您一同深入探讨这篇具有开创性意义的论文《Generalist Reward Models: Found Inside Large Language Models》。这篇论文提出了一种颠覆性的思想：我们或许不再需要耗费巨资去训练一个独立的奖励模型（Reward Model），因为一个强大的通用奖励模型，早已“内生”于任何一个通过标准方式（next-token prediction）训练的语言模型之中。 接下来，我将首先为您提供一个整体性的深度解读，然后严格按照您提出的六个问题，逐一进行剖析。 ","date":"2025-07-10","objectID":"/generalist-reward-modelsfound-inside-large-language-models/:0:0","tags":["reading","reward-model","RLHF"],"title":"GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS","uri":"/generalist-reward-modelsfound-inside-large-language-models/"},{"categories":["","reading","RLHF"],"content":"论文综合深度解读 这篇论文的核心贡献，在于它揭示并严谨地证明了一个深刻的“二象性”：大型语言模型（LLM）的训练过程，不仅是在学习“如何生成文本”（模仿），同时也是在隐式地学习“如何评价文本的好坏”（奖励）。作者将这种内在于 LLM 自身、无需额外训练即可提取的奖励信号，命名为“内生奖励”（Endogenous Reward）。这彻底改变了我们对 LLM 对齐（Alignment）的传统认知。 在当前主流的从人类反馈中强化学习（RLHF）流程中，研究者需要先收集大量的人类偏好数据（比如，对于同一个问题，标注哪个答案更好），然后用这些数据专门训练一个奖励模型（RM）。这个 RM 像一个“外部裁判”，负责给 LLM 生成的所有回答打分。最后，LLM 再根据这个“裁判”的打分，通过强化学习来调整自己的行为，让自己变得更“有用、无害、诚实”。这个过程虽然有效，但其最大的瓶颈在于“训练裁判”这一步——它极其依赖昂贵、耗时且难以规模化的人工标注数据。 为了解决这个问题，业界探索了从 AI 反馈中强化学习（RLAIF），即用一个更强大的闭源模型（如 GPT-4）来充当“AI 裁判”，代替人类进行标注。但这更像是一种“启发式”的工程实践，缺乏严谨的理论根基，并且容易让学生模型继承老师模型的偏见和风格。这就引出了一个根本性的问题：我们真的必须从外部寻找一个“裁判”吗？ 这篇论文给出了一个石破天惊的答案：不必，裁判就在模型自己心中。 作者的论证逻辑如同一场精彩的推理。他们首先将 LLM 的常规训练方式——“下一个词元预测”（next-token prediction），与一个经典的机器学习范式——“模仿学习”（Imitation Learning）联系起来。在模仿学习的视角下，LLM 其实是在模仿海量文本数据（专家示例）的行为。 紧接着，他们引入了另一个更深刻的理论工具——“逆向强化学习”（Inverse Reinforcement Learning, IRL）。IRL 的核心思想是，既然我们能观察到专家的行为，那么我们应该可以反推出专家内心遵循的那个“奖励函数”。也就是说，专家之所以会这么做，一定是因为他认为这么做能得到最高的奖励。 论文最关键的理论突破（命题 1, Proposition 1），就是证明了 LLM 的“下一个词元预测”训练目标，在数学上等价于一种名为“离线逆向软 Q 学习”（offline inverse soft Q-learning）的 IRL 问题的解。这个证明的直接推论是： 一个经过标准训练的 LLM 的 logits（即模型在输出每个词元之前的原始打分），其本身就是这个 IRL 问题所要寻找的“软 Q 函数”（soft Q-function）。 这个结论是整篇论文的基石。Q 函数在强化学习中代表了在某个状态下采取某个动作的长期价值。既然 LLM 的 logits 就是 Q 函数，那么根据强化学习的贝尔曼方程，我们就可以通过一个简单的数学变换（逆向软贝尔曼算子, inverse soft Bellman operator），直接从 logits 中“解码”出那个隐含的、逐词元的奖励值r。这就是内生奖励的由来。 R (sh, ah) := Q(sh, ah) – α log (∑ exp (Q(sh+1, ah+1)) / α) An+1∈V 这个公式（原文公式 7）展示了如何从当前状态-动作的 Q 值（即 logits）和下一步所有可能状态的 Q 值中计算出当前的奖励 r。简单来说，一个词元的奖励 = 模型选择这个词元的倾向性（logits） - 模型对生成这个词元后未来所有可能性的平均预期。这种方法完全是“训练-免费”（training-free）的，它不需要任何新的参数，也不需要任何偏好数据，仅仅是利用了模型已有的知识。 更令人信服的是，论文不仅仅停留在理论层面，还给出了严格的误差分析（定理 2, Theorem 2）。它证明了，使用这种内生奖励进行强化学习微调后的新模型，其最终的策略误差界是与生成长度 H 成线性关系 O (H)的。而传统的模仿学习，由于误差会逐词元累积，其误差界是与 H 成二次方关系 O (H²)的。这意味着，论文提出的方法能够从根本上缓解模仿学习中的“复合误差”问题，从而得到一个性能更优越的模型。 实验部分也充分验证了这一理论。在奖励模型的评测基准 RM-Bench 上，该方法（EndoRM）的平均准确率达到了70.2%，不仅远超其他所有“训练-免费”的基线方法，甚至可以媲美乃至超越那些使用大量数据专门训练的、最先进的奖励模型（例如，Skywork-Reward-Llama-3.1-8 B 的准确率为 70.1%）。 总而言之，这篇论文完成了一次华丽的“理论回归”。它将 LLM 对齐从依赖昂贵数据和工程技巧的“术”，升华到了一个有坚实理论基础、更高效、更可扩展的“道”的层面。它告诉我们，对齐的关键或许不是向外寻求一个“更聪明的老师”，而是向内挖掘模型自身已经蕴含的“评价智慧”。 接下来，我将针对您的六个问题进行详细解读。 ### 论文的研究目标与意义 研究目标：论文的核心研究目标是寻找一种无需额外训练、不依赖外部偏好数据，即可为大型语言模型提供高质量奖励信号的方法。 解决的实际问题： 高昂的对齐成本：解决当前 RLHF 流程中，为训练奖励模型而收集人类偏好数据集所带来的巨大时间、金钱和人力成本问题。 理论基础的缺失：解决 RLAIF 等方法虽然降低了成本，但本质上是启发式的，缺乏严格的理论证明，且容易引入“裁判”模型的偏见。 对齐的可扩展性瓶颈：传统的对齐方法在向多模态领域（如图像、视频）扩展时，收集偏好数据的难度呈指数级增长。论文试图提供一个更具可扩展性的对齐范式。 对行业发展的重要意义： 降低 AI 对齐门槛：如果该方法被广泛采纳，将极大降低中小企业和研究机构开发和对齐高性能 LLM 的成本和技术门槛，促进 AI 领域的创新和民主化。 加速模型迭代：它将传统的“预训练-SFT-RM 训练-RLHF”四阶段流程，简化为“预训练-SFT-RL”三阶段流程，省去了独立的 RM 训练阶段，从而缩短了模型的开发和部署周期。 推动对齐理论的发展：论文为 LLM 对齐提供了全新的、基于 IRL 的理论框架，将启发学界从更深层次的理论（如因果、世界模型）来理解和解决对齐问题，而不仅仅停留在工程层面。 ### 新思路、方法与优势 新思路：核心思路是范式转换，即从“构建一个外部奖励模型”转向“从模型内部发现一个内生奖励模型”。它假设 LLM 在学习生成文本的同时，已经隐式地学会了评价文本质量的准则。 新方法或模型： 理论连接：首次建立了下一个词元预测目标与离线逆向强化学习（IRL）之间的直接数学等价关系。 内生奖励提取（EndoRM）：提出了一种具体、无训练的算法来提取内生奖励。该方法将模型的logits直接视为强化学习中的软 Q 函数。 奖励计算：通过逆向软贝尔曼算子（见上文公式），从 Q 函数（logits）中计算出每个词元的奖励。 \u003e “We can simply take its logits, Q = f˜, and substitute them into the inverse soft Bellman operator from Eq. (7).” \u003e 简单来说，模型每一步生成一个词元的奖励，等于它对这个词元的原始偏好度（logit），减去生成这个词元后，对未来所有可能生成序列的价值期望。 特点与优势： 无训练（Training-Free）：与需要训练数百万参数的传统 RM 相比，该方法不引入任何新参数，也无需训练，极大地节省了计算资源。 数据高效：完全摆脱了对人类或 AI 偏好数据集（preference datasets）的依赖。 理论完备（Theoretically Grounded）：与 RLAIF 的启发式不同，该方法有严格的 IRL 理论作为支撑，并提供了误差分析，证明了其在纠正模仿学习复合误差上的优越性。 可控与可解释：如实验所示，内生奖励是可提示（promptable）的。通过在输入中提供不同的指令（system prompt），可以让同一个模型在评价时遵循不同的标准（例如，学术写作标准 vs. 娱乐写作标准），这为个性化对齐提供了极大的灵活性。 ### 实验验证与结果 论文设计了三个环环相扣的实验，以回答三个核心研究问题（Q 1, Q 2, Q 3）： 实验设计： Q 1: EndoRM 的性能如何？ 在权威的奖励模型评测基准 RM-Bench 和 Multifaceted-Bench 上，将 EndoRM 与多种基线进行比较，包括其他无训练方法（如 Generative Verifier）和需要专门训练的 SOTA 奖励模型。评价指标是分类准确率（即判断一对回答中哪个更优的能力）。 Q 2: EndoRM 是否具备指令遵循能力？ 使用 DSP 数据集，该数据集包含四个不同领域（学术、商业、文学、娱乐）的偏好。实验中，为 EndoRM 提供特定领域的指令，然后测试它在所有四个领域上的评价准确率，观察其是否在“被指令”的领域表现最好。 Q 3: 使用 EndoRM 进行 RL 微调能否提升模型性能？ 在数学推理任务上，以Qwen 2.5-Math-7 B为基础模型，首先测试其原始性能。然后，使用该模型自身的 EndoRM 作为奖励信号，进行强化学习微调（RLFT），再测试微调后模型的性能，看是否有提升。 实验数据与结果： Q 1 结果：EndoRM 表现出色。在 RM-Bench 上，其平均准确率达","date":"2025-07-10","objectID":"/generalist-reward-modelsfound-inside-large-language-models/:0:1","tags":["reading","reward-model","RLHF"],"title":"GENERALIST REWARD MODELS：FOUND INSIDE LARGE LANGUAGE MODELS","uri":"/generalist-reward-modelsfound-inside-large-language-models/"},{"categories":["","reading","LLM","RLHF"],"content":"好的，作为大模型领域的学术专家，我非常乐意为您深入解读这篇具有重要价值的论文《RLPR: Extrapolating RLVR to General Domains without Verifiers》。 这篇论文的核心贡献在于，它巧妙地绕开了现有强化学习方法在提升大模型通用推理能力时遇到的一个核心瓶颈——验证器（Verifier），从而为更广泛、更低成本地提升大模型能力开辟了一条新路径。 接下来，我将为您进行详细的剖析。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:0","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 一、论文的研究目标与意义 研究目标 论文的核心研究目标是解决基于可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR） 在扩展到通用领域时所面临的可扩展性瓶颈。 想要解决的实际问题 在 RLVR 框架下，为了训练模型，我们需要一个“裁判”或者说“验证器”（Verifier）来判断模型的输出是否正确，并据此给出奖励或惩罚。这个验证器在一些特定领域是比较容易实现的，例如： * 数学问题：验证器可以是一个程序，它能直接计算出正确答案并与模型的输出进行比对。 * 代码生成：验证器可以是一个沙箱环境，它能实际运行模型生成的代码，通过单元测试来判断代码是否正确。 然而，一旦我们想把这种强化学习方法应用到通用领域（General Domains），比如开放式问答、创意写作、逻辑推理等，问题就来了。为这些领域构建一个可靠的、自动化的验证器是极其困难甚至不可能的。正如论文在引言中提到的： \u003e For general-domain reasoning with free-form answers, it is even impossible to devise rule-based verifiers due to the high diversity and complexity of natural language. \u003e （对于具有自由形式答案的通用领域推理，由于自然语言的高度多样性和复杂性，设计基于规则的验证器几乎是不可能的。） 这种对领域特定验证器（domain-specific verifiers） 的重度依赖，导致了 RLVR 方法难以规模化，限制了它在提升大模型更广泛推理能力上的潜力。 对于行业发展的重要意义 这个问题的解决具有重大的行业意义。当前，提升大模型能力的主流方法之一是基于人类反馈的强化学习（RLHF），但它依赖大量昂贵的人工标注。RLVR 通过自动化验证器降低了成本，但应用范围受限。 如果能有一种方法，既能利用强化学习的强大威力，又不需要构建复杂的验证器，就能在通用领域内训练模型，这将意味着： 1. 极大降低训练成本：省去了为不同领域设计和维护验证器的巨额工程投入。 2. 极大拓宽应用范围：使得强化学习可以被用于提升模型在几乎所有文本任务上的能力，而不仅仅是数学和代码。 3. 加速模型迭代：开发者可以利用海量的、无标注的通用领域数据（只要有参考答案），通过该方法持续优化模型，实现更快的能力演进。 这篇论文提出的 RLPR 框架，正是朝着这个“鱼与熊掌兼得”的目标迈出的关键一步。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:1","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 二、论文的核心方法与创新 为了摆脱对外部验证器的依赖，论文提出了一个全新的框架——RLPR (Reinforcement Learning with Reference Probability Reward)，即基于参考答案概率的强化学习奖励。其核心思想是：不再依赖外部裁判，而是利用模型自身的“自信程度”作为奖励信号。 核心思路与流程 RLPR 的流程非常巧妙和简洁，具体可以分解为以下几步（可参考论文图 2）： 1. 模型生成：给定一个问题 Q，模型 πθ 首先生成一个包含思考过程 (reasoning chain, z) 和最终答案 (final answer, y) 的完整回复 o = z || y。 2. 奖励构建：RLPR 并不直接评判 y 的好坏。相反，它取出模型生成的思考过程 z，并与数据集中提供的标准参考答案 (reference answer, y*) 进行拼接，构成一个新的序列 o' = z || y*。 3. 概率计算：将这个新序列 o' 输入到模型 πθ 中，计算模型在给定了思考过程 z 的前提下，生成标准答案 y* 中每一个词（token）的概率。 4. 奖励信号：将这些概率值通过一个函数 f_seq 聚合成一个标量奖励 r。这个 r 就代表了奖励的大小。 这个设计的直觉是：如果模型的思考过程 z 是正确且高质量的，那么它理应更有信心地生成正确的答案 y*，从而在 y* 的每个词上赋予更高的概率。 主要创新点 RLPR 框架包含几个关键的技术创新，这些创新共同保证了方法的有效性和稳定性。 概率奖励的计算方式 (Probability Reward) 论文发现，简单地将所有 token 的概率相乘（即序列似然度）作为奖励，会引入很高的方差，且对个别低概率 token 过于敏感。例如，(0.01, 0.7, 0.9) 和 (0.05, 0.7, 0.9) 两个概率序列，虽然只是第一个 token 有微小差异，但乘积结果会相差 5 倍。 因此，论文提出使用平均概率 (mean probabilities) 作为奖励函数 f_seq = (1/N) * Σ p_i。这种方式更加稳健，能更好地反映整体的置信度，实验也证明其与答案质量的相关性更高（如图 4 所示）。 奖励去偏 (Reward Debiasing) 模型对 y* 的生成概率不仅受到思考过程 z 的影响，还可能受到问题 Q 本身或者 y* 格式的偏见影响。 为了分离出纯粹由思考过程 z 带来的“贡献”，论文引入了一个基线奖励 r'。这个 r' 是在没有任何思考过程 z 的情况下，模型直接生成 y* 的概率。 最终的奖励是 r_hat = clip(0, 1, r - r')。这衡量的是“经过思考后，模型信心的提升量”，这个信号更加纯粹和鲁棒。 标准差过滤 (Standard Deviation Filtering) 这是一种自适应课程学习 (adaptive curriculum learning) 机制。在训练过程中，模型对不同样本的反应是不同的。 如果对于一个问题，模型无论怎么回答（生成不同的 z），得到的奖励 r 都差不多（即奖励的标准差很低），这说明这个问题对当前的模型来说，要么“太简单”（都能得高分），要么“太难”（都得低分）。 RLPR 会动态地过滤掉这些低标准差的样本，让模型专注于那些“难度适中”、最能学到东西的样本上，从而稳定训练过程并提升最终性能。 与之前方法的比较优势 相比 RLVR：最大的优势是无验证器 (verifier-free)。这使得 RLPR 具备极强的领域无关性 (domain-agnostic) 和可扩展性 (scalability)，从复杂的、需要大量工程设计的验证器，变成了一个简单的、内置的概率计算。 相比其他无验证器方法（如 VeriFree）：RLPR 的设计更优越。例如，VeriFree 使用序列似然度作为奖励，并限制答案长度小于 7 个 token。而 RLPR 使用更稳健的平均概率，并且通过去偏和过滤机制，在性能上取得了显著的超越。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:2","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 三、实验设计与结果分析 论文通过一系列详尽的实验，在多个模型和基准测试上验证了 RLPR 的有效性。 实验设计 基础模型：实验覆盖了业界主流的多个模型家族，包括 Qwen 2.5 (7 B), Llama 3.1 (8 B), 和 Gemma 2 (2 B)，保证了结论的普适性。 训练数据：使用了 Ma et al. (2025) 发布的WebInstruct数据集，并特意移除了其中的数学相关问题，以专注于通用领域的推理能力。 评估基准：评估非常全面，涵盖了四大通用推理基准（MMLU-Pro, GPQA, TheoremQA, WebInstruct）和三大数学推理基准（MATH-500, Minerva, AIME 24）。 对比基线 (Baselines)：对比了多种方法，包括： 基础模型 (Base Models)：未经任何 RL 训练的原始模型。 指令微调模型 (Instruct Models)：官方的对话微调版本。 基于规则验证器的 RLVR：传统的 RLVR 方法。 基于模型验证器的 RLVR (General Reasoner)：使用一个专门训练的 1.5 B 模型作为验证器。 同期无验证器方法 (VeriFree)：一个与 RLPR 思路相似但实现不同的并发工作。 实验数据与结果 实验结果非常亮眼，充分证明了 RLPR 的优越性。以下是基于Qwen 2.5-7 B模型的一些关键数据（来自论文 Table 1）： 方法 (Verifier) MMLU-Pro GPQA TheoremQA WebInst. 通用平均分 总平均分 Qwen 2.5-7 B-Inst (Base) 54.5 34.2 47.3 72.6 52.2 49.0 RLVR (Rule-based) 55.1 36.2 52.2 75.3 54.7 52.6 General Reasoner (Model-based) 55.4 37.4 52.1 74.5 54.8 52.0 VeriFree (X, Verifier-free) 53.8 36.7 47.6 72.5 52.6 49.4 RLPR (X, Verifier-free) 56.0 37.6 55.4 75.5 56.1 53.6 从上表可以清晰地看到： 1. RLPR 显著超越所有基线：在通用平均分上，RLPR（56.1）不仅远超基础模型（52.2），也超过了依赖规则验证器的 RLVR（54.7）和依赖模型验证器的 General Reasoner（54.8）。这证明无验证器方法不仅可行，甚至可以比有验证器的方法做得更好。 2. RLPR 碾压同类方法：RLPR（56.1）的表现远优于同为无验证器方法的 VeriFree（52.6）。特别是在 TheoremQA 和 Minerva 这两个数据集上，RLPR 分别高出7.6 分和7.5 分，优势巨大。 3. 奖励质量的量化对比：论文图 4 显示，在通用领域数据上，RLPR 的奖励信号在区分正确与错误答案上的AUC 达到了 0.91，而传统的规则验证器只有0.61，基于模型的验证器也只有0.69。这直观地展示了 RLPR 奖励信号的高质量。 这些数据强有力地证明了 RLPR 框架的有效性和先进性。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:3","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 四、未来方向与潜在机会 值得探索的问题和挑战 多模态领域的扩展：论文主要集中在文本领域。未来一个重要的方向是将 RLPR 的思想扩展到多模态任务中，比如根据文本生成图片，或理解图文内容。如何定义多模态场景下的“概率奖励”将是一个有趣的挑战。 对“参考答案”依赖的优化：RLPR 依赖高质量的参考答案 y*。在一些没有唯一正确答案的、更具开放性的任务（如创意写作）中，如何应用 RLPR？或许可以探索使用多个参考答案、或者一个“评价模型”来提供概率目标。 与自洽性等方法的结合：可以将 RLPR 与自洽性（Self-Consistency） 等方法结合。例如，模型多次生成思考过程，选择使得参考答案概率最高的那个路径，或者对多个路径的奖励进行综合。 模型规模的影响：论文主要在 7 B/8 B 量级的模型上进行了实验。该方法在更大规模（如 70 B、数百 B）的模型上是否依然有效，以及其带来的性能提升是否会变化，值得进一步研究。 新的技术和投资机会 一站式 RL 优化平台：可以预见，未来会出现简化 RLPR 这类“无验证器 RL”流程的平台或工具。开发者只需提供基础模型和带有参考答案的数据集，平台即可自动完成 RL 优化，这将成为模型即服务（MaaS）的一个重要增值点。 高质量数据集的价值凸显：既然方法本身变得简单，那么竞争的焦点就会进一步向上游的数据转移。拥有大规模、高质量、多样化的带有“黄金参考答案”的数据集，将成为 AI 公司的核心资产。相关的数据采集、清洗、标注产业将迎来新的机会。 垂直领域模型的低成本定制：企业可以利用内部积累的业务数据（例如高质量的客服问答对、代码库、技术文档等），通过 RLPR 方法，低成本地在开源大模型的基础上训练出符合自身业务需求的、更高性能的垂直领域模型。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:4","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 五、论文的局限与批判性思考 从批判性的角度看，这篇论文虽然非常出色，但仍存在一些值得讨论的方面： 1. 对参考答案质量的强依赖：这是该方法最核心的假设，也是其潜在的弱点。如果参考答案 y* 本身存在错误或不是最优表述，RLPR 可能会“好心办坏事”，强化模型去学习一个次优的答案。 2. 对语义等价性的处理：模型生成的答案 y 可能在语义上与 y* 完全一致，但措辞不同。当前的概率计算方式是基于 token 的，可能会惩罚这种语义等价但字面不同的答案。虽然使用平均概率比序列似然度更稳健，但这个问题依然存在。 3. 计算开销问题：RLPR 在训练时，对于每个样本的每次生成，都需要一次额外的完整前向传播来计算奖励。如果每个 prompt 采样 k 个回答（论文中 k=8），则意味着计算开销会显著增加。虽然省去了验证器，但训练本身的成本变高了。 4. 去偏方法的普适性：r - r' 的去偏方式虽然直观，但可能无法完全消除所有潜在的偏见。例如，某些复杂的思考过程 z 可能与问题 Q 存在更深层的耦合关系，简单的相减可能无法完全解耦。 ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:5","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","reading","LLM","RLHF"],"content":"### 六、核心启发与知识补充 重点学习与启发 核心思想：将外部监督信号内部化：这篇论文最大的启发是“向内求”。当外部的、显式的奖励难以获得时，可以尝试从模型内部寻找代理信号。模型的“自信程度”（即概率）就是一个绝佳的例子。这个思想可以应用在很多其他机器学习问题上。 增量优化的理念：奖励去偏的设计（r - r'）体现了“优化增量”的思想。我们奖励的不是模型本身有多好，而是它“通过思考，进步了多少”。这在很多需要过程优化的场景中都极具借鉴意义。 课程学习的重要性：标准差过滤机制提醒我们，在训练模型时，数据的“喂给”方式至关重要。动态地为模型筛选难度适中的“教材”，可以让学习过程事半功倍。 大道至简：RLPR 的框架相比于构建复杂的验证器系统，显得异常简洁和优雅，但效果却更好。这启示我们在设计算法时，应追求简单而强大的解决方案。 需要补充的背景知识 为了完全理解这篇论文，建议您补充了解以下知识： * 强化学习基础：特别是策略梯度（Policy Gradient）方法，如PPO (Proximal Policy Optimization)。论文中提到的 GRPO 算法就是基于 PPO 的，理解 RL 的基本概念（Policy, Reward, Value Function）是必要的。 * RLHF (Reinforcement Learning from Human Feedback)：了解当前大模型主流的对齐技术，可以更好地理解 RLVR 和 RLPR 是在解决 RLHF 中的什么痛点。 * Transformer 与语言模型生成原理：需要知道大模型是如何基于前面的文本（context），逐个 token 生成后续内容，并为词汇表中的每个 token 赋予一个概率值的。这是 RLPR 核心机制的技术基础。 * 相关前置工作：如果想深入，可以阅读一些 RLVR 的代表性论文（如 PRIME）以及论文中提到的并发工作 VeriFree，这样能更清晰地看到 RLPR 的传承与创新。 希望这份详尽的解读能帮助您深入理解这篇优秀的论文！ ","date":"2025-07-10","objectID":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/:0:6","tags":["reading","LLM","RLHF"],"title":"RLPR：EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS","uri":"/rlprextrapolating-rlvr-to-general-domains-without-verifiers/"},{"categories":["","LLM","infra"],"content":"梯度累积使我们能够通过按顺序处理较小的批次来扩展到更大的有效批次。我们不是一次计算整个批次的梯度（这需要将所有激活存储在内存中），而是在更新模型参数之前将每个小批次的梯度相加。这减少了内存使用量，但需要更多的向前/向后传递。 ","date":"2025-07-10","objectID":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/:0:0","tags":["LLM","infra"],"title":"梯度累计","uri":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/"},{"categories":["","LLM","infra"],"content":"代码 loss = loss / gradient_accumulation_stepsloss.backward() if step% gradient_accumulation_steps == 0: optimizer.step() optimizer.zero_grad() step+=1 gradient_accumulation_steps 是梯度累积次数，累积几次，原本的 loss 就要除以几，这是为了对多个批次的数据的梯度做累积。 有人说，应该有这一行代码才算累加 losses += loss 这样理解是错误的。 要明白最重要的一点是，梯度累加，累加的并不是损失，而是根据损失得到的梯度。 ","date":"2025-07-10","objectID":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/:1:0","tags":["LLM","infra"],"title":"梯度累计","uri":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/"},{"categories":["","LLM","infra"],"content":"梯度累计如何节省显存 减少瞬时激活值显存：每次仅处理小批量的数据，激活值显存占用降低为原来的 1/k（例如 k=4 时，显存占用降至 25%）。 复用显存：每次小批量计算完成后，释放当前激活值显存，供下一次计算使用（显存占用峰值始终为小批量对应的量）。 梯度显存不变：模型参数和梯度的显存占用与批量大小无关，因此不受影响（但需额外存储累积梯度的变量，这部分开销极小）。 梯度累积两次，跟 batch size 增大 2 倍，在多数情况下，效果一样吗？（loss 的 3 次平均） 理论上，梯度累计在数学上应该等同于全批量训练，但实际发现 loss 并不匹配。( Gradient accumulation yields worse results than the equivalent batch size · Issue #2175 · huggingface/trl) 一般情况下，loss 计算会经历三次平均 micro batch 维度，分母是这个 micro batch 中的所有 label 不是 -100 的 token 数（不同 token 之间 loss 的平均） DP 维度，分母是 DP size （和 GPU 数量相关，不同机器之间 loss 的平均） 梯度累加维度，分母是梯度累加数。（不同 batch 之间的 loss 的平均） image.png ","date":"2025-07-10","objectID":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/:2:0","tags":["LLM","infra"],"title":"梯度累计","uri":"/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E8%AE%A1/"},{"categories":["coding","verl"],"content":" image.png 从上图中可以看到DataProto可以分为3个部分： - non_tensor_batch - batch - meta_info 其中non_tensor_batch和meta_info都是个字典，而batch是TensorDict类型的变量。 image.png DataProto支持的一些操作如下： - concat: Combines multiple DataProto objects along the batch dimension - chunk: Splits a DataProto into equal chunks (requires batch size to be divisible by chunks) - split: Splits a DataProto into chunks of specified size (handles uneven splits) - repeat: Repeats the entire batch a specified number of times - sample_level_repeat: Repeats each sample a variable number of times - union: Merges two DataProto objects, combining their tensors and non-tensors - select/pop: Filter or remove specific keys from the batch - rename: Rename keys in the batch ","date":"2025-07-09","objectID":"/dataproto/:0:0","tags":["coding","verl"],"title":"DataProto","uri":"/dataproto/"},{"categories":["coding","verl"],"content":"DataProtoConfig 这个设置主要用于管理auto_padding配置，用在dp中，即如果数据批次 / world_size不能整除的话，就需要padding。这部分体现在： def _split_args_kwargs_data_proto_with_auto_padding(chunks, *args, **kwargs): from verl.protocol import DataProto, DataProtoFuture data_proto_len = None padding_size = None def _padding_and_split_data(obj, chunks): nonlocal data_proto_len, padding_size assert isinstance(obj, DataProto | DataProtoFuture) if isinstance(obj, DataProto) and obj.is_padding_enabled(): # for padding, we only support DataProto with same length if data_proto_len is None: data_proto_len = len(obj) padding_size = (chunks - (data_proto_len % chunks)) if (data_proto_len % chunks \u003e 0) else 0 else: assert data_proto_len == len(obj), ( f\"expecting all arg share same length of {data_proto_len}, but got {len(obj)}\" ) obj.padding(padding_size=padding_size) return obj.chunk(chunks=chunks) splitted_args = [_padding_and_split_data(arg, chunks) for arg in args] splitted_kwargs = {key: _padding_and_split_data(val, chunks) for key, val in kwargs.items()} if padding_size is not None: splitted_kwargs[_padding_size_key] = padding_size return splitted_args, splitted_kwargs 计算需要多少个额外的项目才能使批量大小能被目标除数整除 从批次的开头获取项以用作填充 将原始批次与填充项连接起来 跟踪填充大小以供以后删除 ","date":"2025-07-09","objectID":"/dataproto/:1:0","tags":["coding","verl"],"title":"DataProto","uri":"/dataproto/"},{"categories":["coding","verl"],"content":"DataProtoFuture DataProtoFuture类用于veRL中的分布式计算 image.png 关于collect_fn和dispatch_fn详见init_workers详解 接下来是分布式dataproto的工作流： image.png 其中all_gather_data_proto函数为： def all_gather_data_proto(data: DataProto, process_group): # Note that this is an inplace operator just like torch.distributed.all_gather group_size = torch.distributed.get_world_size(group=process_group) assert isinstance(data, DataProto) prev_device = data.batch.device data.batch = data.batch.to(get_device_id()) data.batch = allgather_dict_tensors(data.batch.contiguous(), size=group_size, group=process_group, dim=0) data.batch = data.batch.to(prev_device) # all gather non_tensor_batch all_non_tensor_batch = [None for _ in range(group_size)] torch.distributed.all_gather_object(all_non_tensor_batch, data.non_tensor_batch, group=process_group) data.non_tensor_batch = {k: np.concatenate([d[k] for d in all_non_tensor_batch]) for k in data.non_tensor_batch} 也就是all_gather操作来收集data，主要用于对tp分组上的数据all_gather，因为tp分组需要相同的输入。 体现在ShardingManager中，即： @GPUMemoryLogger(role=\"fsdp vllm sharding_manager\", logger=logger) def preprocess_data(self, data: DataProto) -\u003e DataProto: \"\"\"All gather across tp group to make each rank has identical input.\"\"\" if self.tp_size == 1: return data # TODO: Current impl doesn't consider FSDP with torch micro-dp group = vllm_ps.get_tensor_model_parallel_group().device_group all_gather_data_proto(data=data, process_group=group) return data 可见这里获取了tp group，然后在group中进行all_gather操作。post_process_data就是再将数据按照tp分组dispatch出去。 with self.rollout_sharding_manager: log_gpu_memory_usage(\"After entering rollout sharding manager\", logger=logger) prompts = self.rollout_sharding_manager.preprocess_data(prompts) with simple_timer(\"generate_sequences\", timing_generate): output = self.rollout.generate_sequences(prompts=prompts) log_gpu_memory_usage(\"After rollout generation\", logger=logger) output = self.rollout_sharding_manager.postprocess_data(output) 总结：因为我们在进行dispatch的时候，是按照world_size进行的，但是对于tp分组需要相同的输入，所以要进行all_gather。 ## Sequence Balancing verl中对序列优化的做法有以下几点： - 序列长度平衡：在工作线程之间分配序列以平衡计算工作负载（本章节） - 动态批处理：按令牌计数而不是固定批次大小对序列进行分组（dynamic_bsz） - 序列打包：删除填充标记以提高内存效率remove_padding - 微批次优化：重新排列批次以实现最佳 GPU 利用率 image.png 平衡算法：对序列进行分组以最大限度地减少填充，同时尊重标记限制，从而减少填充标记上的计算浪费。 ","date":"2025-07-09","objectID":"/dataproto/:2:0","tags":["coding","verl"],"title":"DataProto","uri":"/dataproto/"},{"categories":["coding","verl"],"content":"数据处理流程 verl的数据处理流程大概如下图所示： image.png ","date":"2025-07-09","objectID":"/dataproto/:3:0","tags":["coding","verl"],"title":"DataProto","uri":"/dataproto/"},{"categories":["","infra","LLM"],"content":"即 packing，将不同长度的序列紧凑存储，避免填充，减少不必要的计算和存储，提升效率。 ","date":"2025-07-08","objectID":"/remove_padding/:0:0","tags":["LLM","infra"],"title":"remove_padding","uri":"/remove_padding/"},{"categories":["","infra","LLM"],"content":"动机 sft进行微调，因为gpu是并行计算的，所以如果一个batch里面的数据，每条数据长度不相等，就需要对数据进行truncation（截断）和padding（pad数据到相同的seq_length）。显然，如果使用了padding，那么一个batch里面，就会有很多的pad_token，这些pad_token输入进入到了模型，但是却没有样本训练，造成了计算量的浪费。 因此，对于这些长度不相等的样本，就可以使用packing（类似于打包），把这些样本拼接成长度相等的文本（比如20480, 4096, 8192）等长度。这样就能够是样本全部训练，增加了样本的计算效率。如图所示。每个样本之间不等长，但是可以使用eos_token进行拼接，达到加速训练的目的 image.png ","date":"2025-07-08","objectID":"/remove_padding/:1:0","tags":["LLM","infra"],"title":"remove_padding","uri":"/remove_padding/"},{"categories":["","infra","LLM"],"content":"带来的问题和解决方案（理论上） 如果使用了packing，需要考虑两个问题：attention和位置编码。相比于不使用packing，使用packing导致： atteniton有问题：本来我只需要和sample1的token计算attention，现在packing以后，我的attention不仅仅是sample1内部计算。现在是sample1，sample2，sample3，通通一起计算attention。这样是不是会有问题？ 位置编码：本来sample1的位置编码是从0开始的，现在我sample1,2,3一起packing，那sample2，3的位置编码就变了，无法和单条样本训练一致。 解决方案： - 将packing中的attention方式进行修改（每条样本只和自己内部做attention），如下图 - 将packing的位置编码，修改成和不使用packing一样的位置编码。 ","date":"2025-07-08","objectID":"/remove_padding/:2:0","tags":["LLM","infra"],"title":"remove_padding","uri":"/remove_padding/"},{"categories":["","infra","LLM"],"content":"代码做法 引用 verl 中 tests 的代码： def test_hf_casual_models(): batch_size = 4 seqlen = 128 response_length = 127 for config in test_configs: # config = AutoConfig.from_pretrained(test_case) with torch.device(\"cuda\"): model = AutoModelForCausalLM.from_config( config=config, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\" ) model = model.to(device=\"cuda\") breakpoint() input_ids = torch.randint(low=0, high=config.vocab_size, size=(batch_size, seqlen), device=\"cuda\") attention_mask = create_random_mask( input_ids=input_ids, max_ratio_of_left_padding=0.1, max_ratio_of_valid_token=0.8, min_ratio_of_valid_token=0.5, ) position_ids = compute_position_id_with_mask( attention_mask ) # TODO(sgm): we can construct the position_ids_rmpad here input_ids_rmpad, indices, *_ = unpad_input( input_ids.unsqueeze(-1), attention_mask ) # input_ids_rmpad (total_nnz, ...) input_ids_rmpad = input_ids_rmpad.transpose(0, 1) # (1, total_nnz) # unpad the position_ids to align the rotary position_ids_rmpad = index_first_axis( rearrange(position_ids.unsqueeze(-1), \"b s ... -\u003e (b s) ...\"), indices ).transpose(0, 1) # input with input_ids_rmpad and postition_ids to enable flash attention varlen logits_rmpad = model( input_ids_rmpad, position_ids=position_ids_rmpad, use_cache=False ).logits # (1, total_nnz, vocab_size) origin_logits = model( input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, use_cache=False ).logits origin_logits_rmpad, origin_logits_indices, *_ = unpad_input(origin_logits, attention_mask) logits_rmpad = logits_rmpad.squeeze(0) log_probs = log_probs_from_logits_all_rmpad( input_ids_rmpad=input_ids_rmpad, logits_rmpad=logits_rmpad, indices=indices, batch_size=batch_size, seqlen=seqlen, response_length=response_length, ) # (batch, seqlen) origin_log_probs = log_probs_from_logits_all_rmpad( input_ids_rmpad=input_ids_rmpad, logits_rmpad=origin_logits_rmpad, indices=origin_logits_indices, batch_size=batch_size, seqlen=seqlen, response_length=response_length, ) # (batch, seqlen) torch.testing.assert_close( masked_mean(log_probs, attention_mask[:, -response_length - 1 : -1]), masked_mean(origin_log_probs, attention_mask[:, -response_length - 1 : -1]), atol=1e-2, rtol=1e-5, ) print(\"Check pass\") 其中 unpad_input 函数简化逻辑的代码如下： def unpad_input(hidden_states, attention_mask): # 1. 找到所有有效 token 的位置 # seqlens_in_batch 是一个包含批次中每个序列实际长度的列表，例如 [3, 4] seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32) # indices 是一个一维张量，包含了所有值为1的 mask 元素的展平后索引 indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten() # 2. 从 hidden_states 中提取出所有有效的 token # 首先将 hidden_states 展平成 (batch_size * sequence_length, ...) flat_hidden_states = hidden_states.reshape(-1, hidden_states.shape[-1]) # 然后使用 indices 来挑选出所有有效的 token hidden_states_unpadded = flat_hidden_states[indices] # 3. 计算累积序列长度 (cu_seqlens) # 例如，如果 seqlens_in_batch 是 [3, 4]，cu_seqlens 会是 [0, 3, 7] cu_seqlens = torch.cat( [torch.zeros(1, dtype=torch.int32), seqlens_in_batch.cumsum(dim=0)], dim=0 ) max_seqlen_in_batch = seqlens_in_batch.max().item() return hidden_states_unpadded, indices, cu_seqlens, max_seqlen_in_batch 这里的 cu_seqlens 就是不需要传入 attention_mask 的原因，相当于取代了 mask 的功能。 调试输出一些张量的 shape： (Pdb) input_ids.shape torch.Size([4, 128]) (Pdb) attention_mask.shape torch.Size([4, 128]) (Pdb) position_ids.shape torch.Size([4, 128]) (Pdb) input_ids_rmpad.shape torch.Size([1, 359]) # 也就是说去掉pad后4个sample在一起的有效长度为359 简单来说，indices 是一个“索引地图”。它的核心作用是记录在原始的、带填充的、被展平（flattened）的批次数据中，所有有效（非填充）词元的位置。 当 unpad_input 处理 input_ids 时，它会丢掉所有的填充词元，只保留有效词元，并生成这个 indices 地图。这个地图至关重要，因为批次中的数据往往不止 input_ids，还有与之严格对齐的 position_ids、token_type_ids 等。 indices 的主要用途是：确保其他辅助张量（如 position_ids）能够以与 input_ids 完全相同的方式被“解填充”（unpad），从而保持数据的一致性和对齐。 如果 position_ids 的解填充方式与 input_ids 不一致，那么旋转位置编码（Rotary Position Embedding, RoPE）等依赖位置信息的操作就会完全错乱。 ","date":"2025-07-08","objectID":"/remove_padding/:3:0","tags":["LLM","infra"],"title":"remove_padding","uri":"/remove_padding/"},{"categories":["大模型分布式","LLM"],"content":"一句话：在sequence维度上进行切分 将输入序列 X (长度 N) 沿序列维度切分为 SP 块，每个 GPU 分配到 N/SP 长度的子序列。 对于非注意力层 (如 MLP)，计算是完全局部的，每个 GPU 处理自己的子序列即可。 token 之间独立，token-level projection Ulysses SP的核心复杂性在于Attention层。为了让每个token在计算注意力时能够考虑到全局序列信息（或者说，让每个head在计算时能看到完整的序列，即使这个head只在当前rank计算），Attention模块前后需要进行两次精密的all-to-all数据重排。MLP层则没有这样的需求，数据在进入MLP时已经是按序列分片好的，可以直接进行本地计算。 对于注意力层: 步骤 1 (计算 Q, K, V): 每个 GPU 基于其本地子序列计算出本地的 Q_local, K_local, V_local (维度约为 N/SP x d，d 是隐藏维度)。 步骤 2 (全局 K, V 收集 - 关键): 使用 All-to-All 通信操作（All-Gather??）。每个 GPU 将自己的 K_local, V_local 发送给所有其他 GPU，并接收来自所有其他 GPU 的 K, V 块。执行后，每个 GPU 拥有完整的全局 K 和 V 矩阵 (维度 N x d)，但仍然只拥有本地的 Q_local (维度 N/SP x d)。 https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html 步骤 3 (本地注意力计算): 每个 GPU 使用其 Q_local 和完整的全局 K, V 计算其负责的那部分注意力输出 O_local (维度 N/SP x d)。计算公式为 Attention(Q_local, K_global, V_global)。这一步的计算量是 (N/SP) * N * d，内存瓶颈在于存储临时的注意力分数矩阵，大小约为 (N/SP) * N。相比原始的 **N*N**，内存显著降低。 步骤 4 (可选的输出重组): 如果后续层需要按序列拼接的完整输出，可能需要另一次通信（如 All-Gather 或另一次 All-to-All 的变种）来组合 O_local。但在 DeepSpeed 实现中，通常保持分布式状态，直接输入到下一个同样按序列并行的层。 ","date":"2025-07-08","objectID":"/ulysses_sequence_parallel/:0:0","tags":["LLM","大模型分布式"],"title":"ulysses_sequence_parallel","uri":"/ulysses_sequence_parallel/"},{"categories":["大模型分布式","LLM"],"content":"verl中的序列并行 在verl中，一般与remove_padding一起使用，即 if config.actor_rollout_ref.actor.strategy in {\"fsdp\", \"fsdp2\"}: if ( config.actor_rollout_ref.actor.get(\"ulysses_sequence_parallel_size\", 1) \u003e 1 or config.actor_rollout_ref.ref.get(\"ulysses_sequence_parallel_size\", 1) \u003e 1 ): assert config.actor_rollout_ref.model.use_remove_padding, ( \"When using sequence parallelism for actor/ref policy, you must enable `use_remove_padding`.\" ) if self.use_critic and config.critic.strategy in {\"fsdp\", \"fsdp2\"}: if config.critic.get(\"ulysses_sequence_parallel_size\", 1) \u003e 1: assert config.critic.model.use_remove_padding, ( \"When using sequence parallelism for critic, you must enable `use_remove_padding`.\" ) 先进行remove padding操作 然后进行序列并行的pad和slice操作 ## 参考 pytorch_distribute_tutorials/tutorials/3D-parallel/SP-序列并行.ipynb at main · chunhuizhang/pytorch_distribute_tutorials · GitHub ","date":"2025-07-08","objectID":"/ulysses_sequence_parallel/:1:0","tags":["LLM","大模型分布式"],"title":"ulysses_sequence_parallel","uri":"/ulysses_sequence_parallel/"},{"categories":["reading","Planning"],"content":"论文深度解读 这篇发表于 2025 年 4 月的论文，直面了当前 LLM 智能体领域的一个核心痛点：虽然 LLM 在处理简单、单步的指令上表现出色，但在面对需要多步骤、长期规划和适应环境变化的长时域任务（long-horizon tasks）时，其能力显著下降。想象一下让一个 AI 助手“预订一张下周从北京到上海的靠窗经济舱机票”，这背后涉及到理解意图、访问网站、输入信息、处理动态页面、比较选项、完成支付等一系列复杂操作。传统的单一 LLM 方法，如著名的ReAct（Reasoning and Acting）框架，试图将思考（Reasoning）和行动（Acting）融合在一个模型中，但这给模型带来了巨大的“认知负-载”，常常导致其“顾此失彼”，在复杂的交互中迷失方向。 本文的核心贡献在于提出了一个名为 PLAN-AND-ACT 的双模块框架，旨在通过“分而治之”的思想来解决这一难题。其架构清晰地将任务分解为两个专业化的角色： 1. PLANNER（规划器）：一个高层次的“战略家”。它负责接收用户的原始指令（如“关注这个 GitHub 项目贡献最多的人”），并将其分解成一系列结构化的、高层次的步骤。例如，它会生成计划：“第一步，导航到‘贡献者’页面；第二步，识别出贡献最多的人并关注他”。这个规划器专注于“做什么”，而不关心“具体怎么做”。 2. EXECUTOR（执行器）：一个低层次的“操作手”。它接收 PLANNER 制定的高层次计划，并将其转化为在具体环境（如网页 HTML）中可以执行的精确动作。例如，它会根据“导航到‘贡献者’页面”这一计划，在 HTML 代码中找到对应的链接元素，并生成一个 do(action=\"Click\", element=\"13\") 的指令。 正如论文中所强调的：“这种分离允许每个组件专注于其核心任务。PLANNER 可以在不陷入实现细节的情况下进行高层战略推理，而 EXECUTOR 则可以专注于将抽象计划转化为具体行动。” image.png 然而，仅仅提出框架是不够的，训练这样专业的 PLANNER 和 EXECUTOR 需要大量高质量的、带有详细规划和行动标注的数据。在现实世界中，这类数据极其稀缺且标注成本高昂。这正是本文第二个，也是更具创新性的贡献所在：一个可扩展的合成数据生成流水线。这个流水线（如下图所示，源自论文图 3）巧妙地解决了“鸡生蛋，蛋生鸡”的困境，分为三个精巧的阶段： image.png 行动轨迹生成（Action Trajectory Generation）：首先，利用一个强大的“教师 LLM”（如 GPT-4 o），在真实或模拟环境（如 WebArena）中执行任务，收集成功的“行动轨迹”（即一系列成功的底层操作序列）。 接地规划生成（Grounded Plan Generation）：接着，让另一个“教师 LLM”扮演“事后诸葛亮”的角色，对上一步收集到的成功行动轨迹进行“逆向工程”。它会分析这些具体的行动序列，反推出一个合乎逻辑的高层计划，并将每个计划步骤与具体的行动序列进行关联。这一步至关重要，因为它确保了生成的计划是“接地的”（Grounded），即与真实世界的可行操作紧密相连，而非凭空想象。 合成计划扩展（Synthetic Plan Expansion）：最后，将上一步生成的“计划-行动”对作为“种子”，利用 LLM 强大的泛化能力，生成大量结构相似但内容多样的新查询和计划。例如，从一个关于“查找商品”的计划，可以衍生出关于“查找不同商品”、“比较价格”等多种新计划。 通过这个流水线，论文成功地为 PLANNER 和 EXECUTOR 的训练生成了海量的、高质量的、多样化的数据。此外，论文还引入了动态重规划（Dynamic Replanning）机制。在静态计划中，如果环境发生意外变化（如搜索结果为空），执行器可能会陷入困境。而在动态重规划中，每当 EXECUTOR 完成一个动作后，PLANNER 都会根据环境返回的新信息（新的 HTML 状态）来审视并更新后续的计划。例如，当最初的搜索“CMU 的图书馆”失败后，PLANNER 会将计划调整为更泛化的“搜索 CMU 附近的图书馆”。 “动态重规划允许规划器在演化的计划中保留关键信息… 这种方法使我们能够应对长时域任务中的记忆挑战，而无需明确的记忆模块。” 最终，实验结果雄辩地证明了该方法的有效性。在 WebArena-Lite 基准测试中，通过逐步应用其提出的各项技术，模型的成功率从9.85%（基础模型）飙升至57.58%。这个最终成绩不仅远超基线，也刷新了当时的最先进水平（SOTA），超过了之前 SOTA 模型 WebRL-Llama-3.1-70 B 的 49.1%。这一系列详尽的消融实验（Ablation Study）清晰地展示了框架中每个组成部分（如规划器、数据增强、动态重规划、思维链）的价值。 总而言之，PLAN-AND-ACT 框架及其合成数据生成流水线，为构建更强大、更鲁棒的 LLM 智能体提供了一套系统性且可扩展的解决方案。它不仅在理论上清晰地划分了智能体的“思考”与“行动”，更在实践中解决了训练数据匮乏的关键瓶颈，为自主 AI 在真实、动态环境中的应用铺平了道路。 接下来，我将按照您提出的六个问题，逐一进行更详细的解读。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:1","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 1. 论文的研究目标与意义 研究目标： 论文的核心研究目标是提升大型语言模型（LLM）智能体在处理复杂、多步骤、长时域任务时的规划和执行能力。 要解决的实际问题： 它旨在解决当前 LLM 智能体在面对真实世界任务（尤其是网页导航这类动态环境）时普遍存在的几个关键问题： * 规划难题：智能体难以将高层次的用户目标（如“帮我订一张去纽约的机票”）分解为具体、可执行的子任务（如“打开航空公司网站”、“输入旅行日期”等）。 * 上下文丢失：在长任务序列中，智能体容易“忘记”已经完成的步骤和最终目标，导致行为不连贯。 * 适应性差：真实环境（如网页）是动态和不可预测的，静态的、预先生成的计划往往会因为环境的微小变化而失败。 * 高质量训练数据稀缺：训练智能体进行有效规划需要大量的专家标注数据，而这类数据的获取成本极高，严重制约了模型性能的提升。 对行业发展的意义： 这个问题对于 AI 行业的发展具有至关重要的意义。能够可靠执行长时域任务的自主智能体是实现通用人工智能（AGI）的关键一步。 * 提升自动化水平：强大的智能体可以将人类从繁琐的数字化工作中解放出来，例如自动化的客户服务、数据录入、市场分析报告生成、复杂的软件测试等，极大地提高生产力。 * 革新用户交互方式：未来的用户与计算机的交互可能不再是点击和输入，而是通过自然语言下达复杂指令，由 AI 智能体自主完成所有中间步骤。这将彻底改变人机交互的范式。 * 催生新的商业模式：基于自主智能体的平台和服务（Agent-as-a-Service）将成为可能，企业可以雇佣“AI 员工”来完成特定的业务流程，这将催生出巨大的市场和投资机会。 解决好长时域任务规划问题，就如同为 AI 智能体装上了可靠的“大脑”和“手脚”，使其能从一个“玩具”真正走向“工具”，甚至成为“伙伴”。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:2","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 2. 论文的新思路、方法与模型 论文提出的核心方法是 PLAN-AND-ACT 框架，其创新之处在于双模块解耦设计与可扩展的合成数据生成流水线的有机结合。 新的思路与方法： PLANNER-EXECUTOR 架构： 思路：将智能体的“大脑”（战略规划）与“双手”（具体执行）进行解耦。这借鉴了软件工程中的模块化思想，让每个部分更简单、更健壮。 特点： PLANNER：生成高层次、结构化的计划，关注任务的逻辑流程。它使得整个任务流程更清晰，也更容易被人类理解和调试。 EXECUTOR：专注于将计划步骤转化为环境中的具体动作。它处理与环境交互的复杂细节，减轻了 PLANNER 的负担。 优势：相比于 ReAct 等单模型方法，这种解耦降低了单个模型的认知复杂性，避免了在规划和执行之间“精神分裂”。如论文所述，这有助于“弥合高层用户意图与低层行动之间的鸿沟”。 动态重规划（Dynamic Replanning）： 思路：规划不是一次性的，而是一个持续适应的过程。 特点：在 EXECUTOR 每执行一步后，PLANNER 都会接收环境的最新反馈，并对剩余的计划进行评估和调整。 优势：极大地增强了智能体对动态环境的适应能力。论文中给出了一个很好的例子：当搜索“CMU 的图书馆”失败时，PLANNER 能接收到“未找到结果”的 HTML 反馈，并主动将下一步计划调整为更宽泛的“搜索 CMU 附近的图书馆”，从而挽救了任务。这比静态计划要灵活和鲁棒得多。 三阶段合成数据生成流水线： 思路：既然没有现成的高质量数据，就利用 LLM 自身的能力来创造数据。 特点： 行动轨迹生成：利用强大的教师模型（文中是 WebRL-Llama-3.1-70 B）在环境中探索，生成成功的操作序列。 接地规划生成：这是最巧妙的一步。让教师模型（文中是 DeepSeek-R 1-Distill-Llama-70 B）从具体的成功轨迹中“反向推导”出高层计划。这确保了计划不是空想，而是与现实操作紧密绑定的。 合成计划扩展：利用 Alpaca 风格的指令生成技术，以少量高质量的“计划-查询”对为种子，生成数以万计的新数据。如论文提到，他们用 GPT-4 o 在不到一小时内生成了 10,000 个新的查询-计划对，极具效率和可扩展性。 优势：此方法系统性地解决了训练数据瓶颈。它比纯人工标注成本低、速度快，并且通过“接地”步骤保证了数据质量，比完全依赖 LLM 凭空想象的计划更可靠。 与之前的方法相比，PLAN-AND-ACT 的优势在于它的系统性和可扩展性。它不依赖于需要复杂提示工程（prompting）的闭源模型，而是通过为开源模型（如 LLaMA）生成高质量的微调数据来提升其能力，为构建更开放、更强大的自主智能体社区提供了切实可行的路径。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:3","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 3. 实验验证与结果分析 论文通过一系列设计严谨的实验，在多个公认的基准测试上验证了 PLAN-AND-ACT 框架的有效性。 实验设计： 基准测试（Benchmarks）： WebArena-Lite：这是一个 WebArena 的子集，包含 165 个跨越购物、论坛（Reddit）、代码托管（GitLab）等多种网站的测试用例。它被用作主要的消融实验平台。 WebArena (Full)：完整的基准测试，用于和其他 SOTA 模型进行公平比较。 WebVoyager：一个基于真实世界网站的动态基准，用于测试模型在非模拟环境中的泛化能力。 模型选择：主要使用 LLaMA-3.3-70 B-Instruct 作为 PLANNER 和 EXECUTOR 的基础模型，并通过微调进行训练。同时，也使用了其他模型（如 GPT-4 o, DeepSeek-R 1）作为数据生成过程中的“教师模型”。 消融实验（Ablation Study）：这是实验设计的核心亮点。作者在 WebArena-Lite 上，从一个最基础的系统开始，逐步增加 PLAN-AND-ACT 框架的各个组件，并记录每一步带来的性能提升。这清晰地展示了每个创新点的贡献。 实验数据与结果： 以下是论文核心的表 1（Table 1） 的简化版本，它直观地展示了消融实验的结果： PLANNER 设计 (逐步增强) EXECUTOR (Base) EXECUTOR (+Finetuning) EXECUTOR (+Synthetic Traj.) 无 PLANNER (ReAct-style) 9.85% 36.36% 36.97% … (其他基线) WebRL-3.1-70 B (先前 SOTA) - - 49.1%* Base PLANNER (zero-shot) 14.21% 17.16% 23.63% + Finetuning 22.42% 16.36% 20.60% + Plan Expansion (10 k) 27.10% 38.18% 39.40% + Targeted Augmentation (5 k) 29.63% 42.42% 43.63% + Dynamic Replanning 44.24% 48.48% 53.94% + CoT (最终模型) - - 57.58% 注：星号表示引用自原论文的报告结果。 关键数据解读： PLANNER 的价值：仅仅加入一个未经微调的 Base PLANNER，就能让最基础的 Base EXECUTOR 的成功率从 9.85%提升到 14.21%，证明了规划-执行解耦的初步有效性。 数据为王： + Plan Expansion 和 + Targeted Augmentation 这两步带来了显著的性能飞跃。例如，对于最强的 EXECUTOR，成功率从 20.60%一路提升到了 43.63%。这证明了论文提出的合成数据生成流水线是成功的关键。 论文中提到：“每次训练数据集的扩展都会带来性能提升，其中最显著的增长来自于增加了 10,000 个直接生成的计划，成功率提升了大约 10 个百分点。” 动态重规划的决定性作用：引入 Dynamic Replanning 后，最终模型的成功率从 43.63%跃升至53.94%，提升超过 10 个百分点。这说明在动态环境中，适应和调整的能力是不可或缺的。 最终成果：最终加入思维链（CoT）推理后，模型在 WebArena-Lite 上达到了57.58%的成功率，创造了新的 SOTA 记录，显著超越了之前 SOTA（49.1%）。 泛化能力：在更具挑战性的真实世界基准 WebVoyager 上，PLAN-AND-ACT（使用 QWQ-32 B 模型）同样取得了81.36%的成功率，超越了所有之前的模型，包括基于 GPT-4-Turbo 的方法（如 Wilbur 的 52.6%），展示了其强大的泛化能力。 这些详实的数据和严谨的实验设计，有力地证明了 PLAN-AND-ACT 框架及其核心技术在提升 LLM 智能体长时域任务能力方面的有效性和优越性。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:4","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 4. 未来研究方向、挑战与机遇 基于这篇论文的研究，未来在该方向上还有许多值得探索的问题和挑战，同时也孕育着巨大的技术和投资机会。 值得进一步探索的问题和挑战： 更高效的重规划机制： 挑战：论文中承认，“在每个动作之后都进行重规划可能是低效和缓慢的”。这在需要快速响应的实时应用中是致命的。 探索方向：可以研究让EXECUTOR 学会“自主判断”何时需要重规划。例如，只有当 EXECUTOR 遇到预期之外的错误，或者对下一步行动的置信度低于某个阈值时，才向 PLANNER 请求帮助。这可以演变成一种分层代理（Hierarchical Agents）或委托（Delegation）机制。 多模态信息的融合： 挑战：目前的框架主要依赖于 HTML 等文本信息，但现代网页包含大量视觉元素（图片、视频、复杂的 UI 布局），仅靠文本理解是不够的。 探索方向：将多模态大模型（如 GPT-4 V）的能力整合到 PLANNER 和 EXECUTOR 中。PLANNER 可以基于页面截图进行更高层次的视觉规划，而 EXECUTOR 则可以执行更精确的视觉定位操作（如“点击那个红色的购物车图标”）。 基于强化学习的计划优化： 挑战：目前的计划生成主要依赖监督学习，模型的创造性受限于训练数据。 探索方向：可以引入强化学习（RL），特别是基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）。让智能体在环境中试错，根据任务成功与否、执行效率等作为奖励信号，来持续优化其 PLANNER 的规划策略，从而发现比人类示范更优的解决方案。 工具使用与泛化： 挑战：智能体不仅要会浏览网页，还需要学会使用各种 API 工具（如日历 API、天气 API、计算器）。 探索方向：将工具学习（Tool-use）框架与 PLAN-AND-ACT 结合，让 PLANNER 能够规划何时以及如何调用外部工具，并将工具返回的结果整合到后续的计划中。 新的技术和投资机会： Agentic AI 平台：开发允许用户低代码或无代码构建、训练和部署自定义 AI 智能体的平台。这类平台可以集成 PLAN-AND-ACT 的思想，提供模块化的 PLANNER 和 EXECUTOR 组件，以及强大的合成数据生成工具。 垂直领域的“AI 员工”：在电商、金融、法律、医疗等特定行业，训练专门的 AI 智能体来执行复杂的行业流程（如自动化的索赔处理、合同审查、病历分析）。这些是短期内最可能商业化的方向。 新一代操作系统和硬件：未来的操作系统可能会以 AI 智能体为核心，用户通过自然语言与整个系统交互。这也可能催生对专为 Agentic 计算优化的 AI 芯片（如加速并行函数调用和推理）的需求。 AI 安全与对齐：随着智能体变得越来越自主，如何确保其行为符合人类的意图和价值观（即对齐问题）变得至关重要。针对自主智能体的安全、监控和可解释性技术将成为一个重要的研发和投资领域。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:5","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 5. 论文的不足与存疑之处 从批判的视角来看，尽管这篇论文非常出色，但仍存在一些固有的局限性和需要进一步验证的地方。 对强大“教师模型”的依赖（自举问题）： 不足：论文的合成数据生成流水线始于“行动轨迹生成”阶段，该阶段依赖一个已经很强大的模型（如 WebRL-Llama-3.1-70 B 或 GPT-4 o）来成功完成任务。 存疑：这带来了一个“自举（bootstrapping）”问题：如果你想进入一个全新的领域，而这个领域没有任何强大的基础模型，那么这个数据生成流程就无法启动。正如论文在“局限性”一节中坦承：“对于没有任何训练数据的任务（如 WebVoyager），流水线将依赖于有一个基础模型来收集轨迹。” 这在一定程度上限制了该方法的普适性。 效率和延迟问题： 不足：如前所述，每步都进行动态重规划的计算成本很高，会导致显著的延迟。论文虽然承认了这一点，但并未提供解决方案或对延迟进行量化分析。 存疑：在实际应用中，用户是否能接受一个反应缓慢的智能体？在需要与动态环境进行高频交互的场景（如在线游戏）中，这种方法可能并不可行。 成功率的绝对值仍有提升空间： 不足：尽管 57.58%的成功率在 WebArena-Lite 上是 SOTA，但这也意味着在超过 40%的情况下，智能体仍然会失败。对于许多要求高可靠性的商业应用来说，这个失败率是不可接受的。 存疑：这表明长时域网页任务的内在复杂性极高，即便有了先进的框架，距离实现人类水平的鲁棒性还有很长的路要走。是什么导致了剩余的失败？是规划的错误，执行的偏差，还是对环境理解的不足？论文对此缺乏深入的错误分析（error analysis）。 泛化领域的局限性： 不足：实验主要集中在网页导航任务上。虽然这是一个极具代表性的领域，但该框架能否无缝迁移到其他需要长时域规划的领域（如复杂的软件开发、科学研究、物理机器人控制）仍有待验证。 存疑：不同领域的“环境”和“行动空间”差异巨大。例如，在代码编写中，“行动”是编辑文本，“环境”是代码库和编译器反馈。该框架的 PLANNER 和 EXECUTOR 是否需要针对每个新领域进行大量重新设计和训练？其跨领域泛化的能力尚不明确。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:6","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["reading","Planning"],"content":"### 6. 对读者的启发与学习建议 从这篇论文中，我们可以汲取许多宝贵的、可直接应用的创新思想。 重点学习与启发： 分治思想（Divide and Conquer）： 学什么：核心启发是将一个复杂问题分解为几个更简单、更专业的子问题。在构建任何复杂的 AI 系统时，都可以思考是否能将“战略规划”与“战术执行”分离。 怎么用：例如，在开发一个自动生成报告的 AI 时，可以设计一个 PLANNER 来规划报告的大纲和结构（引言、数据分析、结论），再设计一个 EXECUTOR 来填充每个部分的具体文字和图表。 “逆向工程”创造高质量数据： 学什么：从“结果”反推“过程”是生成高质量标注数据的绝佳思路。论文中从成功的行动轨迹反推高层计划的“接地规划生成”是点睛之笔。 怎么用：如果你想训练一个 AI 来解决数学题，可以先用求解器（Solver）找到答案和解题步骤（结果），然后让 LLM 学习如何从问题生成这些解题步骤（过程），而不是让它凭空想象。这能保证训练数据的正确性和逻辑性。 迭代式的数据增强策略： 学什么：数据生成不是一蹴而就的。论文展示了一个从“生成-接地-扩展”的完整流程，特别是其中“靶向增强（Targeted Augmentation）”的思想——分析模型的失败案例，并针对性地生成更多相关的训练数据来弥补短板。 怎么用：在你的 AI 项目进入测试阶段后，系统性地收集和分类失败案例。然后，让 LLM 围绕这些失败案例生成更多样的、更具挑战性的训练数据，进行模型的迭代优化。这是一个持续改进模型的强大闭环。 需要补充的背景知识： 为了完全理解和应用这篇论文的思想，建议您补充了解以下背景知识： * LLM 智能体（LLM-based Agents）基础：特别是 ReAct (Reasoning and Acting) 框架（arXiv:2210.03629），这是理解当前智能体研究的基石。 * 指令微调（Instruction Fine-tuning）：了解如 Alpaca、FLAN 等工作，这对于理解如何通过微调使 LLM 遵循特定格式和指令至关重要。 * Web Agent 相关基准测试：可以查阅 WebArena (arXiv:2307.13854) 和 WebVoyager (arXiv:2401.13919) 的原始论文，以了解评测环境的具体细节和挑战。 * 思维链（Chain-of-Thought, CoT）：了解 CoT prompting 技术（arXiv:2201.11903）如何通过引导 LLM 生成中间推理步骤来提升其在复杂任务上的表现。 希望以上详尽的解读能帮助您深入理解这篇优秀的论文，并从中获得有价值的启发。如果您还有任何问题，随时可以提出。 ","date":"2025-07-07","objectID":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/:0:7","tags":["Reading","Planning"],"title":"LAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks","uri":"/plan-and-actimproving-planning-of-agents-for-long-horizon-tasks/"},{"categories":["","LLM","大模型分布式"],"content":"参考 RL 系统深思：FSDP 训练后端 PyTorch FSDP 设计解读 ","date":"2025-07-06","objectID":"/fsdp/:1:0","tags":["LLM","大模型分布式"],"title":"fsdp","uri":"/fsdp/"},{"categories":["","LLM","infra"],"content":"进入大模型时代，基本上所有大模型都使用 decoder 部分，因此本文只分析 decoder 部分的参数量。 Transformer 的 decoder 每一层由 attention 和 mlp 组成，一般有 l 层。 ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:0:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"Self-attention Self-attention 层由 \\(W_{Q}\\) 、\\(W_{K}\\)、\\(W_{V}\\) 和输出矩阵 \\(W_{O}\\) 和它们的偏置组成，权重矩阵的形状为 \\([h,h]\\)，偏置形状为 \\([h]\\)，则 self-attention 部分的参数量为 \\(4h^2+4h\\) ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:1:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"MLP MLP 由 2 个线性层构成，第一个线性层将维度从 h 变为 4 h，第二个将维度由 4 h 变为 h，第一个权重矩阵形状为 \\([h,4h]\\)，偏置为 \\([4h]\\)，第二个形状为 \\([4h,h]\\)，偏置为 \\([h]\\)，则参数量为 \\(8h^2+5h\\) ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:2:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"Layer norm 在 self-attention 和 mlp 中都存在 layer norm，有 2 个可训练参数：缩放参数 \\(\\gamma\\) 和平移参数 \\(\\beta\\)，形状都是 \\([h]\\)，2 个 layer norm 的参数量为 4 h。 ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:3:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"词嵌入 词嵌入矩阵的参数量和词表大小 V 有关，而且输入和输出一般公用一个矩阵，因此参数量为 \\(Vh\\) ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:4:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"位置编码 如果是可训练式的位置编码，则占据一定的参数量，否则不占参数量 ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:5:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"总参数 综上所述，transformer 一个层的参数量为 \\(12h^2+13h\\)，l 层就是 \\(l(12h^2+13h)\\)，再加上词嵌入矩阵，总参数量为 \\(l(12h^2+13h)+Vh\\)，h 较大时，可以忽略一次项，近似为 \\(12lh^2\\) 实际参数量 隐藏维度h 层数l 12lh^2 6.7B 4096 32 6,442,450,944 13.0B 5120 40 12,582,912,000 32.5B 6656 60 31,897,681,920 65.2B 8192 80 64,424,509,440 表来自 分析 transformer 模型的参数量、计算量、中间激活、KV cache ## 参考 # 分析transformer模型的参数量、计算量、中间激活、KV cache Transformer Math (Part 1) - Counting Model Parameters ","date":"2025-07-06","objectID":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/:6:0","tags":["LLM","infra"],"title":"transformer参数量分析","uri":"/transformer%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90/"},{"categories":["","LLM","infra"],"content":"训练时 模型参数：我们模型的可学习权重。 Optimizer states（优化器状态）：您需要跟踪的确切状态取决于您使用的优化器;例如，如果您使用的是 AdamW，则除了模型参数之外，您还需要跟踪第一和第二动量估计值。 模型激活值：这将根据您的网络架构和批处理大小而有所不同，但会显著影响内存使用。反向传播需要此信息，以便我们能够有效地计算梯度。 梯度：为模型的每个参数存储，与模型参数相同的内存占用。 Input data：要传递给模型的 Importing 数据批次，内存占用取决于正在建模的数据的大小和类型。 图示： 具体数值： 对于一个 transformer 来说，参数量可以由以下公式给出（详见 transformer参数量分析）： \\[N=h*v+L*(12*h^2+13*h)+2*h\\] In that equation, \\(h\\) is the hidden dimension, \\(v\\) the vocabulary size, and \\(L\\) the number of layers in the model. Note that looking at the equation we can see that the term that will dominate with large hidden dimensions is the \\(h^{2}\\) term, since it’s the only one growing quadratically As we scale the parameters. 在全精度训练中（所有的存储单位都是 fp 32），优化器使用 adam 的情况下，模型部分我们需要存储： \\[\\begin{aligned}\u0026m_{params}=4*N\\\\\u0026m_{grad}=4*N\\\\\u0026m_{opt}=(4+4)*N\\end{aligned}\\] 而在使用混合精度的情况下，模型的参数和梯度使用 bf 16，为了稳定性，优化器还需要存储 fp 32 的模型参数，即： \\[\\begin{aligned}\u0026m_{params}=2*N\\\\\u0026m_{grad}=2*N\\\\\u0026m_{params\\_fp32}=4*N\\\\\u0026m_{opt} =(4+4)*N\\end{aligned}\\] Some libraries store grads in FP32, which would require an additional mparams_fp32=4∗Nmparams_fp32​=4∗N memory. This is done, for example, in Nanotron, because BF16 is lossy for smaller values and we always prioritize stability. See this DeepSpeed issue for more information. 也就是说有的库还实现了存储 fp 32 的梯度，考虑稳定性。 The FP32 copy of the parameters (mparams_fp32mparams_fp32​) is sometimes called the “master weights” in the literature and codebases. 保存 fp 32 模型参数的原因是 bf 16 的精度不足以支持高效参数更新，fp 32 可以避免误差累计，保证优化器的数值稳定性和训练效果。具体分析如下（来自为什么LLM一般使用较大的权重衰减系数？）： 从浮点数的存储格式建立了「计算机浮点数的数值绝对值越大，则精度越低」的结论，对于深度学习训练过程（前向-反向-更新）来说： 如果使用低精度浮点数保存和更新模型参数时，如果模型参数绝对值比较大，而更新的步幅比较小，那么更新会由于舍入误差而失效，这就是为什么要维护一个 fp 32 的模型参数的原因。 并且从一个高精度的模型转化为低精度模型的时候，参数的绝对值越大，则丢失的精度越多。在模型更新了fp32的备份之后，还需要将fp32的权重转化为低精度的版本，参与后续的forward过程。由于浮点数的精度随着绝对值的增加而降低，因此参数的绝对值越大，在精度的转化中损失的精度也越多。此外，在前向和反向计算的过程中，激活值也会存在类似的精度损失问题。如果我们在训练过程中引入权重衰减，那么模型的权重的绝对值就可以得到一定的控制。除了提供一定的正则化效应之外，也能够降低由于模型的参数范数增长而导致的精度损失的风险。 混合精度训练示意图： 1. 参数以FP32存储；这主要是因为，在基于梯度更新权重的时候，往往公式:权重 = 旧权重 + lr * 梯度，而在深度模型中，lr * 梯度这个值往往是非常小的，如果利用 fp16 来进行相加的话， 则很可能出现精度的问题，导致模型无法更新。因此参数以FP32的形式存储 2. 正向计算过程中，遇到FP16算子，需要把算子输入和参数从FP32 cast成FP16进行计算； 3. 将Loss层设置为FP32进行计算； 4. 反向计算过程中，首先乘以Loss Scale值，避免反向梯度过小而产生下溢； 5. FP16参数参与梯度计算，其结果将被cast回FP32； 6. 除以Loss scale值，还原被放大的梯度； 7. 判断梯度是否存在溢出，如果溢出则跳过更新，否则优化器以FP32对原始参数进行更新。 根据上述公式可以快速得到一些模型训练时占用显存： image.png 此外激活值也是显存的巨大杀手，随着句子长度的增加而增加，有以下的计算公式： \\[m_{act}=L\\cdot seq\\cdot bs\\cdot h\\cdot(34+\\frac{5\\cdot n_{heads}\\cdot seq}h)\\] Here, \\(L\\) is the number of layers, seq the sequence length, \\(bs\\) the batch size in samples, \\(h\\) the hidden dimension of the model, and \\(n_{heads}\\) the number of heads. image.png 可见在长上下文的训练中，激活值才是显存最大的杀手。这就需要Activation checkpointing 来降低这部分的显存占用，详见 Activation checkpointing ","date":"2025-07-06","objectID":"/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/:1:0","tags":["LLM","infra"],"title":"显存占用计算","uri":"/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/"},{"categories":["","LLM","infra"],"content":"推理时 一个经验法则是：推理时的峰值显存大致是模型参数显存的 1.5 - 2.5 倍（尤其在处理长序列或大批次时）。更精确的估计需要结合具体模型和输入 输入/输出的 Token 存储：需要显存存储输入的 Token 嵌入（embedding）和生成的输出 Token。 Key-Value 缓存（KV Cache）：自回归生成时，为避免重复计算历史 Token 的 Key/Value，需缓存这些中间结果（显存占用与输入+输出长度成正比） 关于 kv cache 占用显存的计算，详见 KV cache ## 参考 The Ultra-Scale Playbook - a Hugging Face Space by nanotron ","date":"2025-07-06","objectID":"/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/:2:0","tags":["LLM","infra"],"title":"显存占用计算","uri":"/%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/"},{"categories":["reading"],"content":"这篇名为《Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation》（https://arxiv.org/abs/2410.13232）的论文，由延世大学的研究团队撰写，并计划在 ICLR 2025 会议上发表。它直面了当前大型语言模型（LLM）在构建自主网页代理（Web Agents）时遇到的一个核心瓶颈：缺乏对环境动态的理解，即没有“世界模型”（World Model）。 通俗地说，当前的 AI 代理在执行复杂的网页任务时，就像一个蒙眼走路的人，只能通过不断试错来摸索前进，这导致它们效率低下，并且容易犯下一些无法挽回的错误，比如在购物网站上重复购买不可退款的商品。人类之所以能避免这类错误，是因为我们的大脑中有一个“世界模型”，能让我们在行动之前“预演”或“想象”出这个行动可能带来的后果。 这篇论文的核心贡献，就是为 LLM 代理引入了这样一个“世界模型”的概念，并提出了一套完整、高效的框架来实现它。他们首先通过初步实验有力地证明了问题的存在性：即便是最先进的 LLM（如 GPT-4 o, Claude-3.5-Sonnet），在预测一个网页操作（如点击按钮）会带来什么页面变化时，其表现也近乎随机猜测。然而，如果把操作的“后果”（即下一个页面的状态）直接告诉 LLM，它们选择正确操作的能力就会大幅提升。 正如论文在初步分析部分（Preliminary Analyses）中展示的那样，在预测下一个网页状态的任务中，LLMs 的表现非常糟糕（平均准确率仅为 54.75%），而人类则高达 83%（图 1）。但当为 LLM 提供每个候选动作的后果时，其选择正确动作的准确率则出现了巨大飞跃，例如 GPT-4 o 的准确率从 53%提升到了 73%（图 2）。 这一发现直接催生了论文的核心方法：世界模型增强的网页代理（World-Model-Augmented Web Agent, WMA）。该方法并不试图让一个庞大的模型包办一切，而是设计了一个精巧的分工体系： 1. 策略模型（Policy Model）：一个强大的 LLM（如 GPT-4 o），负责提出可能的行动选项。 2. 世界模型（World Model）：一个经过专门训练的、更小巧的模型（如 Llama-3.1-8 B），负责预测每个行动选项将导致的网页变化。 3. 价值函数（Value Function）：一个评估器，负责判断哪个预测结果最有利于实现最终目标，从而选出最佳行动。 其中最巧妙的创新在于世界模型的训练方式。研究者意识到，让模型预测整个网页的 HTML 代码是极其低效且困难的。因此，他们提出了“聚焦于状态转换的观测抽象”（Transition-focused Observation Abstraction）。这个技术不要求模型生成一整个新页面，而是只学习和预测两次页面状态之间的“差异”。 他们通过匈牙利算法（Hungarian algorithm）来比较前后两个网页的元素，精确地找出哪些元素是“新增的”（ADDED）、“删除的”（DELETED）或“更新的”（UPDATED）。然后，再用一个 LLM 将这些结构化的“差异”信息转换成一段简洁的自然语言描述。例如，不是预测整个购物车页面，而是预测出“商品‘A’已被添加到购物车，总价更新为‘$59’”。 这种方法极大地降低了世界模型的学习难度和计算开销，使得用一个小型模型来完成高质量的未来预测成为可能。最终，论文通过在 WebArena 和 Mind 2 Web 这两个极具挑战性的基准测试上的大量实验，证明了 WMA 代理不仅显著提升了任务成功率，并且在推理时间和 API 成本上，远优于其他需要进行实际环境探索的先进方法（如树搜索）。例如，WMA 代理的成本仅为树搜索代理的1/6.8，速度则快了5.3 倍。 总而言之，这篇论文不仅仅是提出了一种新模型，更是为自主代理领域引入了一种更接近人类思考方式的“深思熟虑”的决策范式。它将复杂的任务分解为“提出可能 -\u003e 模拟后果 -\u003e 评估选择”，并通过巧妙的抽象技术解决了模拟过程中的效率难题，为构建更智能、更高效、更可靠的 AI 代理指明了一个极具潜力的方向。 接下来，我将按照您提出的六个方面，对论文进行更详细的解读。 一、论文的研究目标是什么？想要解决什么实际问题？ 研究目标：核心目标是提升基于大型语言模型的自主网页代理在复杂、长时程任务（long-horizon tasks）中的性能和可靠性。 解决的实际问题： 决策短视（Myopia）：现有的 LLM 代理在决策时是“反应式”的，它们根据当前所见的页面内容直接生成下一步操作，缺乏对未来的规划和预见。这导致它们在需要多步、有逻辑关联操作的任务中表现不佳。 不可逆的错误：由于缺乏预见能力，代理容易犯下一些代价高昂且无法撤销的错误。论文中反复提及的例子是“重复购买一张不可退款的机票”，这生动地说明了问题的严重性。 低效率和高成本：依赖“试错”的代理需要与真实网页环境进行大量交互。每一次交互都可能耗费时间（页面加载）和金钱（API 调用成本），尤其是在探索多种可能性时（如树搜索方法），成本会急剧增加。 行业意义： 解决这个问题对于推动 AI 从“聊天机器人”向真正的“自主智能体”转变至关重要。一个能够预见后果、深思熟虑的 AI 代理，可以被可靠地应用于自动化各种复杂的日常和商业流程，例如： 个人助理：自动完成在线购物比价、预订旅行、管理复杂的在线账户等。 企业自动化：自动完成数据录入、跨平台信息整合、软件测试、客户支持等。 可访问性：为残障人士开发能够自主操作复杂网页界面的辅助工具。 因此，这项研究为构建更实用、更值得信赖的下一代 AI 应用奠定了坚实的技术基础。 二、论文提出了哪些新的思路、方法或模型？ 论文的核心是提出了世界模型增强的网页代理（WMA）框架。相较于之前端到端（end-to-end）或基于简单试错的方法，其特点和优势在于其模块化的、基于模拟的决策流程。 具体来说，该框架包含三个关键组件和一项核心技术： 模块化设计： 策略模型 (Policy Model θ)：负责生成候选动作。这通常是一个强大的通用 LLM（如 GPT-4 o）。它保持冻结，无需训练，这使得整个框架易于集成和升级。 世界模型 (World Model Φ)：负责模拟未来。这是一个经过专门训练的小型 LLM（如 Llama-3.1-8 B），输入当前状态和候选动作，输出对下一状态的预测。 价值函数 (Value Function V)：负责评估。它也是一个 LLM，用于评估每个“预测出的未来”对于完成最终目标的价值，并给出一个分数。 核心技术：聚焦于状态转换的观测抽象 (Transition-focused Observation Abstraction) 这是论文方法论的精髓所在。它解决了直接用 LLM 模拟网页环境的两个核心痛点： 低信息增益：网页上的大部分内容在一次操作后保持不变。让模型重复生成这些不变内容是在浪费计算资源。 序列过长：完整的 HTML 或 DOM 树作为模型的输出，会产生极长的文本序列，训练和推理成本高昂。如下图（源自论文图 4）所示，原始的观测（axtree）平均长度达 4 K token，而论文提出的抽象描述（description）则极短。 图注: 论文图 4 的可视化，展示了不同观测表示的序列长度分布。Õt+1 description（蓝色）代表论文提出的抽象方法，其长度远小于其他方法。 该抽象过程如论文图 5 所示，分为两步： 步骤一：机械化差异提取。利用匈牙利算法对前后两个页面的 DOM 元素进行匹配，自动识别出 UPDATED, DELETED, ADDED 的元素列表。 步骤二：自然语言总结。将这些结构化的差异信息输入给一个 LLM，让它生成一段简洁、流畅的自然语言描述，作为世界模型的训练目标。 优势： 高效性：用一个小型模型进行内部“模拟”，远比让一个大型模型与实际浏览器进行多次交互要快得多、便宜得多。 有效性：通过“预见未来”，代理可以避开那些会导致任务失败或陷入死循环的“陷阱”动作。 模块化与灵活性：世界模型可以独立训练和优化，并且可以即插即用地增强任何现有的策略模型，无需对策略模型本身进行微调。 三、论文通过什么实验来验证所提出方法的有效性？ 论文的实验设计非常全面、严谨，通过在两个公认的基准上进行的多维度对比，充分验证了方法的有效性。 实验平台： WebArena：一个包含购物、社交论坛（Reddit）、代码协作（Gitlab）等多种真实场景的模拟网页环境，任务复杂，极具挑战性。 Mind 2 Web：一个大规模的、涵盖了来自 137 个网站的真实世界任务的数据集，注重泛化能力。 实验设计与关键结果： 与基准模型的性能对比： 在 WebArena 上，WMA 代理全面超越了传统的 CoT（思维链）方法。如下表（简化自论文表 1）所示，使用 GPT-4 o 作为策略模型时，WMA 将成功率（Success Rate, SR）从12.8%提升至16.6%。更亮眼的是，当使用能力稍弱的 GPT-4 o-mini 时，性能提升幅度更大，从9.4%跃升至13.5%，相对提升了43.6%。这证明该方法能有效“赋能”能力较弱的模型。 Policy LLM Method SR (w/o Action Selection) SR (w/ Action Selection) Relative Gain GPT-4 o","date":"2025-07-05","objectID":"/web-agents-with-world-models-learning-and-leveraging-environment-dynamics-in-web-navigation/:0:0","tags":["reading"],"title":"WEB AGENTS WITH WORLD MODELS ：LEARNING AND LEVERAGING ENVIRONMENT DYNAMICS IN WEB NAVIGATION","uri":"/web-agents-with-world-models-learning-and-leveraging-environment-dynamics-in-web-navigation/"},{"categories":["reading"],"content":"这篇论文的核心目标是解决当前自主网页智能体（Web Agent） 在通过自我改进（Self-Improvement） 学习时遇到的一个关键瓶颈：性能停滞（stagnation point）。传统的自我改进方法，即智能体通过与真实网络环境交互，收集成功经验（轨迹），然后用这些经验来微调自身的语言模型。这种模式在初期有效，但很快就会因为探索范围有限而“碰壁”——智能体倾向于在已经熟悉的任务和路径上反复练习，难以发现更优或全新的解决方案，从而无法持续进步。 WebEvolver 的作者们敏锐地洞察到，这个问题的根源在于“对真实世界交互的过度依赖”和“对模型内部知识的利用不足”。真实世界的交互既慢又昂贵，而且充满了重复。为了打破这一僵局，他们引入了一个在强化学习和认知科学中非常经典但在此领域应用新颖的概念——世界模型（World Model）。 这篇论文的最大创新，在于提出了一个“与智能体共同进化的世界模型”（Co-evolving World Model）框架，并赋予了这个世界模型双重关键角色： 作为“虚拟网络服务器”（Virtual Web Server）：在训练阶段，世界模型扮演了一个模拟器的角色。它学习理解网页的动态变化规律，即在给定当前网页状态（用可访问性树 Accessibility Tree 表示）和一个操作（如点击、输入）后，预测出下一个网页状态应该是什么样子。有了这个能力，它就可以凭空“想象”出大量的、多样化的网页交互轨迹（synthetic trajectories）。这极大地丰富了训练数据，让智能体可以在一个低成本、高效率的虚拟环境中进行“沙盘推演”，接触到在真实世界中可能永远不会或很少遇到的情况，从而打破探索瓶颈。如下图 1 所示，这是整个框架的核心循环。 Our framework co-trains a world model alongside the agent by predicting next-step observations from current states and actions, using trajectory data collected during sampling. The world model then serves as a virtual web engine, enabling synthetic multi-step trajectories generation for training the policy. (我们的框架通过使用采样收集的轨迹数据，在训练智能体的同时，共同训练一个世界模型来预测基于当前状态和动作的下一步观察。该世界模型随后充当一个虚拟网络引擎，为策略训练生成合成的多步轨迹。) 作为“想象力引擎”（Imagination Engine）：在推理（即实际执行任务）阶段，世界模型的能力被用来进行前瞻性规划（Look-ahead Planning）。当智能体面临多个可能的下一步操作时，它不再是盲目选择一个。而是利用世界模型，对每个候选操作进行“脑内模拟”，想象出执行该操作后可能会发生什么。论文中将此机制称为 世界模型前瞻（World-Model Look-Ahead, WMLA）。通过模拟未来几步的走向，并由一个评分器（LLM Scorer）评估哪条路径最有可能导向成功，智能体可以做出远比之前更明智、更具前瞻性的决策。例如，在下图 2 的推理演示中，智能体在考虑 Click [13] 这个低效选项和 Click [4] 这个高效选项时，WMLA 可以模拟出后者的光明前景，从而引导智能体做出正确选择。 图 1：论文核心框架图（修改自原论文 Fig 1 \u0026 2），清晰展示了世界模型的双重角色 论文通过在 WebVoyager 和 Mind 2 Web-Live 等高难度、真实的网页导航基准测试集上的实验，有力地证明了该方法的有效性。实验数据显示，相比于基线自改进方法，WebEvolver 带来了显著的性能提升。例如，在 WebVoyager 测试集上，仅使用 WebEvolver 进行训练，成功率就从 38.68%提升到了 42.49%。而当在推理时结合 WMLA（d=2，即向前看两步），成功率更是飙升至 51.37%，相较于迭代一次的自改进模型，提升了超过 12 个百分点，这在自主智能体领域是一个非常巨大的飞跃。 Experiments in real-world web environments (Mind 2 Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents… 这篇论文不仅仅是提出一个有效的技术方案，更重要的是，它为自主智能体的持续学习和适应性进化指明了一条极具潜力的道路。它巧妙地将基于模型的强化学习思想（Model-Based RL）与大语言模型的强大先验知识相结合，证明了让智能体“学会想象”，是解锁其更高层能力的“金钥匙”。 接下来，我将根据您提出的六个问题，进行更详细的逐一解读。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是提升 LLM 驱动的网页智能体在自我改进过程中的学习效率和最终性能上限。 解决的实际问题： 性能停滞：如前所述，基于纯粹真实环境交互的自我改进方法，智能体的能力会很快达到一个平台期。它们会陷入“认知固化”，无法发现新的、更高效的策略。 探索与利用困境 (Exploration-Exploitation Dilemma)：智能体在真实环境中进行探索（尝试新操作）的成本非常高，可能会导致任务失败、耗时过长。因此，它们倾向于利用（exploit）已知的成功路径，但这又限制了学习。 数据效率低下：完全依赖真实交互来收集高质量的训练数据，是一个缓慢且昂贵的过程。 行业意义： 通向通用人工智能助理 (General AI Assistants)：解决这个问题是创造能够自主学习、适应不断变化的网页环境的通用 AI 助理的关键一步。一个能持续自我进化的智能体，才能真正成为我们可靠的“数字雇员”，处理复杂的线上任务，如自动预订、比价购物、信息整合等。 降低训练成本和门槛：通过引入世界模型生成合成数据，可以大幅减少对昂贵的真实世界交互和人工标注的依赖，使得训练高性能智能体的成本更低、速度更快，有助于技术的普及。 提升智能体的鲁棒性和泛化能力：在多样化的虚拟环境中训练，能让智能体学会应对更多样的网站布局和任务流程，从而在遇到从未见过的网站时表现得更好，泛化能力更强。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？ 新思路/方法/模型： 核心是 WebEvolver 框架，其精髓在于引入了一个与智能体策略模型（Agent Policy Model）共同学习、共同进化（Co-learning / Co-evolving） 的 世界模型（World Model）。 与之前方法的比较、特点和优势： 特性 之前的方法 (如 OpenWebVoyager) WebEvolver 学习数据来源 仅依赖真实环境交互。智能体在真实网站上操作，收集成功轨迹用于微调。 真实数据 + 合成数据。除了真实轨迹，还利用世界模型生成大量虚拟交互轨迹，数据来源更丰富、多样。 学习范式 纯粹的无模型 (Model-Free) 学习。智能体直接学习“状态-动作”的映射，不理解环境的动态。 模型辅助 (Model-Based) 学习。引入世界模型来学习环境的动态变化规律 T(s, a) -\u003e s'，让智能体具备了“预测未来”的能力。 推理决策方式 反应式 (Reactive)。根据当前观察直接生成一个动作。 规划式 (Planning-based)。利用 WMLA 机制，在决策前“想象”多个动作的后果，进行多步前瞻规划，选择最优动作。 解决停滞问题 效果有限，容易陷入局部最优，导致性能停滞。 通过合成数据拓宽探索边界，通过前瞻规划优化决策质量，有效打破性能瓶颈。 细节分析： 共同进化是关键。世界模型并非一个固定的模拟器，它和智能体一样，在每一轮自我改进中都会被更新。这意味着，随着智能体能力的提升，它能探索更复杂的场景，收集到更高质量的真实轨迹；这些高质量的轨迹又可以把世界模型训练得更精准；更精准的世界模型又能生成更高质量的合成数据，并为智能体提供更可靠的规划支持。这是一个正向的飞轮效应。 论文中提到，即使世界模型会产生“幻觉”（hallucinated web states），在训练中也不是致命问题，甚至可能是有益的。 \u003e Importantly, while the World Model may produce hallucinated (i.e., non-realistic) web states, this is not a critical issue during training, as the agent’s objective is to learn flexible action prediction rather than perfect state prediction. \u003e (重要的是，尽管世界模型可能会产生幻觉（即不真实的）网页状态，但这在训练中并非关键问题，因为智能体的目标是学习灵活的动作预测，而非完美的状态预测。) 这种“不完美但多样”的模拟环境，反而可能增强智能体的泛化能力和想象力。 3. 论文通过什么实验来验证所提","date":"2025-07-05","objectID":"/webevolverenhancing-web-agent-self-improvement-with-coevolving-world-model/:0:0","tags":null,"title":"WebEvolver：Enhancing Web Agent Self-Improvement with Coevolving World Model","uri":"/webevolverenhancing-web-agent-self-improvement-with-coevolving-world-model/"},{"categories":["","面经"],"content":"常见问题总结 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:0:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"多轮对话sft样本怎么构造？ [大模型微调样本构造 trick]（https://zhuanlan.zhihu.com/p/641562439) 多轮对话的传统组织方式：将多轮对话拆分为多条独立的训练样本，如 Q1A1/Q2A2/Q3A3 可拆分为 Q1—\u003eA1， Q1A1Q2-\u003eA2， Q1A1Q2A2Q3-\u003eA3 三条样本。 将整个 session 的对话内容拼接成一个长文本序列，例如：Q1 A1 Q2 A2 Q3 A3。这样，整个 session 被表示为一个连续的文本序列，而不是多条独立的样本。 构造时计算损失有一些坑，详见 https://zhuanlan.zhihu.com/p/721652210 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:1:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"SFT 是否可以注入知识？ continue pretrain 注入知识。 sft 对齐输出格式。（sft 在一些特定的场景下确实是可以注入知识的） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:2:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"如何解决灾难性遗忘？ 保留通用数据：在进行领域数据训练时，仍然需要保留一部分通用数据用于模型训练。这样可以确保模型仍然能够学习到通用的语言和知识，从而保持一定的通用能力。 增量学习：使用增量学习（Incremental Learning）的方法，将领域数据与通用数据逐步交替进行训练。这样可以在学习新领域的同时，保持对通用知识的记忆。 数据重采样：在进行领域数据训练时，可以使用数据重采样的方法，使得模型在训练过程中能够更多地接触到通用数据，从而缓解遗忘通用能力的问题。 强化学习：使用强化学习的方法，通过给模型设置奖励机制，鼓励模型在领域任务上表现好，同时保持一定的通用能力。 领域适应技术：使用领域适应技术，如领域自适应（Domain Adaptation）和领域对抗训练（Domain Adversarial Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。 SDFT：微调前，让大模型将任务数据集重写一遍。这样的话，重写后的任务数据集的分布和大模型的差异就小了很多。在这样的数据集上微调对大模型分布上的改变会小很多，对大模型通用能力的损害也会降低。 Llama-Pro:在原始模型中每个Transformer块或者某几个Transformer块后增加一个Transformer块，但为了保持扩展后的模型输出保持不变，需要增加的块为恒等块（输入输出相同） 为什么大模型都是 Decoder-Only 架构？（https://www.zhihu.com/question/588325646/answer/3357252612） 泛化性能更好：ICML 22的What language model architecture and pretraining objective works best for zero-shot generalization?. 在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在各种下游任务上zero-shot泛化性能最好 苏神强调的注意力满秩的问题，双向attention的注意力矩阵容易退化为低秩状态，而 causal attention的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强； @yili大佬强调的预训练任务难度问题，纯粹的decoder-only架构+next token predicition预训练，每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高； @mimimumu 大佬强调，上下文学习为decoder-only架构带来的更好的few-shot性能：prompt 和demonstration的信息可以视为对模型参数的隐式微调，decoder-only的架构相比encoder-decoder在in-context learning上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，微调的信号更强； 多位大佬强调了一个很容易被忽视的属性，causal attention（就是decoder-only的单向 attention）具有隐式的位置编码功能，打破了transformer的位置不变性，而带有双向 attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。 decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:3:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Transfomer attention计算为什么除以根号 d？ 为什么不在rollout阶段保存logp而是再计算一遍logp？ 因为forward的时候和generate的时候logprob由于推理引擎（vllm）和训练引擎（fsdp）的优化目标不一样，会造成两者对不上，因此需要做两次。 batch 算子的细微差异，都会造成这两个 log_prob 不完全一致。推理引擎要的是快速出 token id，训练引擎需要保证一定的log_prob 精度。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:4:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"为什么需要 Server 来完成 rollout？ 为了配合 agentic LLM 的训练，在现有的PPO/GRPO 算法的基础上，从 single turn rollout 改动为 environment interactive multi-turn rollout 的需求非常强烈。 这一过程中，policy 与 environment 的交互存在绝对不可忽视的延迟，turn 之间的等待时间很长。一直用 Engine 做 rollout 的话（ engine.generate），可能连 continuous batching 都组不起来。所以，改用 server 来通过 https 做 rollout的需求就呼之欲出了。实际上，这也是 最自然的工作方式。除此之外，environment 的交互往往也是通过 https 请求来完成的。譬如，众多 coding sandbox 都是 environment 自己启动一个 server 暴露一个 port，然后往里面发请求来实现交互的。 总之，为了在 training engine,rollout 和 environment 三个子进程中保持良好的通讯和交互，选择 server 势在必行。 MCP和fuction call的区别？（https://zhuanlan.zhihu.com/p/1898326676087223572） 函数调用是一种机制，它允许 LLM 根据用户的输入识别它需要什么工具以及何时调用它。 MCP（即模型上下文协议,Model Context Protocol）试图标准化此过程。 MCP (Model Context Protocol):是一个开放协议和标准，旨在标准化AI 应用（MCP 客户端）如何发现、连接和与外部工具/数据源（实现为 MCP 服务器）进行交互。它关注的是系统间的通信和集成，解决 Function Calling 指令生成后，如何高效、安全、可扩展地执行这些调用。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:5:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"为什么R1不使用模型reward？ reward hacking 训练资源问题，成本过高。 为什么PRM+MCTS这条路走不通？（https://zhuanlan.zhihu.com/p/19623772462） 在reasoning任务中如何显式定义step，比如以\\n 还是以推理逻辑来划分step？ 如何定义step正确性，将影响step labeler来高效标注 PRM容易reward hacking LLM比象棋搜索空间大太多 MCTS价值影响模型生成质量（不如纯CoT采样） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:6:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"GRPO中可以去掉KL项吗？ 可以。 去除KL项, 意味着不需要ref-model, 减少一个模型的显存，减少一次前向ref_policy的计算。 没有KL的约束，那么可以将过大的梯度进行裁剪(max_grad_norm)，避免优化的不稳定性(这也是另一种层面的clip)。 没有KL的约束，参数的优化更加自由，更容易探索到好的回答 GRPO 的损失为什么会为负（参考 https://zhuanlan.zhihu.com/p/28326620566） 总结：组奖励变化对 loss 上升影响不直观。优化策略远离 ref，KL 变化大，导致 Loss 上升明显。 Qwen模型为什么随机奖励也能work？（https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f） 模型依赖性：研究发现，RLVR的有效性更多地依赖于模型的预训练能力，而不是监督信号的质量。Qwen模型在预训练期间学会了特定的推理策略，这些策略可以通过RLVR轻易地被激发出来，而其他模型则不具备这些策略。 代码推理策略：Qwen-Math模型在预训练阶段就频繁地使用Python代码来解决数学问题，即使在没有代码执行器的情况下，也能生成正确的代码输出和答案。RLVR训练（无论奖励质量如何）进一步增加了这种代码推理的频率，从而提高了性能。 奖励信号的作用：不可靠奖励通过放大模型在预训练期间学到的有用推理表示来发挥作用。这些奖励信号并没有教会模型任务质量，而是触发了一种集中效应，使模型专注于其现有的推理模式分布。 为什么DPO里Chosen和Rejected概率会同时下降?（https://zhuanlan.zhihu.com/p/6327313416） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:7:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"reward hacking如何解决？ 梯度累积两次，跟 batch size 增大 2 倍，在多数情况下，效果一样吗？（loss 的 3 次平均） 理论上，梯度累计在数学上应该等同于全批量训练，但实际发现 loss 并不匹配。( Gradient accumulation yields worse results than the equivalent batch size · Issue #2175 · huggingface/trl) 一般情况下，loss 计算会经历三次平均 micro batch 维度，分母是这个 micro batch 中的所有 label 不是 -100 的 token 数（不同 token 之间 loss 的平均） DP 维度，分母是 DP size （和 GPU 数量相关，不同机器之间 loss 的平均） 梯度累加维度，分母是梯度累加数。（不同 batch 之间的 loss 的平均） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:8:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"为什么不用大模型做 embedding？ 大模型主要训练的预测 next token 的能力，而非判断整个句子 embedding 的好坏。因此使用 LLM 做嵌入效果不理想。 部署成本高。 基础知识 Post-Training总结（https://mp.weixin.qq.com/s/VLWU3YnJa1SZRCySZQc4Hw） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:9:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Flash Attention ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:10:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"MoE ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:11:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"KV cache ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:12:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"从 MHA 到 MLA ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:13:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Transformer 相关 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:14:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"手撕各模块代码 free-running mode 和 teacher forcing mode（https://zhuanlan.zhihu.com/p/630356292) ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:14:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Transformer 参数量计算 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:14:2","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"overthinking 怎么解决 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:15:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"相关文献 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:15:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"方法 推理链压缩：在保持推理质量的同时，减少生成的 token 数量。 长度预算控制：在推理过程中动态调整生成内容的长度，以提高效率。 系统切换与模型切换：根据任务需求灵活选择不同的模型或推理路径。 并行搜索：利用并行计算加速推理过程。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:15:2","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"DeepSpeed（zero 显存优化） 模型显存占用（bf16/fp16）（https://zhuanlan.zhihu.com/p/665172400） 训练时 可见激活值占大头。 优化方法如下： 推理时 一个经验法则是：推理时的峰值显存大致是模型参数显存的 1.5 - 2.5 倍（尤其在处理长序列或大批次时）。更精确的估计需要结合具体模型和输入 输入/输出的 Token 存储：需要显存存储输入的 Token 嵌入（embedding）和生成的输出 Token。 中间激活值（Intermediate Activations）：前向传播过程中每一层的输出（如 Attention 的 Key/Value 缓存、FFN 的中间结果等）。 Key-Value 缓存（KV Cache）：自回归生成时，为避免重复计算历史 Token 的 Key/Value，需缓存这些中间结果（显存占用与输入+输出长度成正比） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:16:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"zero3 这是inter-layer + broadcast + reduce-scatter的实现方法，是很久之前官方的实现，现在官方的真正做法是：intra-layer + all-gather+ reduce-scatter（与fsdp一致） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:16:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"位置编码（RoPE） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:17:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"LoRA（手撕） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:18:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"R1 相关 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"训练整体流程 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"aha monment 在训练过程中，研究团队使用数学问题来训练和评估模型的逻辑推理能力。在观察模型输出时，研究人员正是在下面这个数学方程的解题过程中捕捉到了一个引人注目的”顿悟时刻”，充分展现了模型通过强化学习自然获得的自主反思能力： ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:2","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"k1 k2 k3 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:3","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"R1-zero qwen2.5-0.5B 模型，num_generations 为 2，gsm8k 数据集准确率 0.45489006823351025。num_generations 为 4，准确率为 0.47763457164518575 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:4","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Rule-based reward 准确性奖励（Accuracy Rewards）：答案是否正确 格式奖励（Format Rewards）：是否有思考标签 语言混合问题：对语言一致性进行打分 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:19:5","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"梯度累计 代码（https://zhuanlan.zhihu.com/p/423359955) 暂时无法在飞书文档外展示此内容 gradient_accumulation_steps 是梯度累积次数，累积几次，原本的 loss 就要除以几，这是为了对多个批次的数据的梯度做累积。 有人说，应该有这一行代码才算累加 暂时无法在飞书文档外展示此内容 这样理解是错误的。 要明白最重要的一点是，梯度累加，累加的并不是损失，而是根据损失得到的梯度。 梯度累计如何节省显存 减少瞬时激活值显存：每次仅处理小批量的数据，激活值显存占用降低为原来的 1/k（例如 k=4 时，显存占用降至 25%）。 复用显存：每次小批量计算完成后，释放当前激活值显存，供下一次计算使用（显存占用峰值始终为小批量对应的量）。 梯度显存不变：模型参数和梯度的显存占用与批量大小无关，因此不受影响（但需额外存储累积梯度的变量，这部分开销极小）。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:20:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Adam 和 AdamW ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:21:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"RLHF 相关（PPO、RLOO、REINFORCE++、ReMax、GRPO、SAC） on-policy 和 off-policy（参考 https://zhuanlan.zhihu.com/p/346433931） Off-policy: the learning is from the data off the target policy（引自《Reinforcement Learning An Introduction》）。也就是说 RL 算法中，数据来源于一个单独的用于探索的策略（不是最终要求的策略）。（Off-policy 方法中不一定非要采用重要性采样，要根据实际情况采用（比如，需要精确估计值函数时需要采用重要性采样；若是用于使值函数靠近最优值函数则不一定）） On-policy: the target and the behavior polices are the same. 也就是说 on-policy 里面只有一种策略，它既为目标策略又为行为策略。 总结：PPO 算法，虽然有 2 个 policy，用\\(\\pi_{old}\\)采样去更新 pi，但是由于 pi_old 的参数是从 pi 复制的，本质上还是属于同一个策略。所以 PPO 是一个看起来很像 off-policy 的 on-policy 算法。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"online 和 offline 更新的数据是否是最新 agent 的模型生成的数据 Online: 更新的数据为最新模型采样得到的（采样的数据一次性用完） Offline: 更新的数据为 x 次更新前模型采样得到的（采样的数据更新多次） online、offline、on-policy、off-policy online 学习中可以是 on/off policy 的。而 offline 学习中除了第一次更新模型的学习可能是 on policy 的，之后的所有学习只有仍是 offline 的则一定是 off policy。 critic 和 reward 区别（参考 https://www.zhihu.com/question/1900547615495545054/answer/1901411039406457541） reward model 评估整个 response 质量，给出整体奖励信号，无法直接映射到每个 token 的贡献。 critic model 估计价值函数，预测未来可能获得的累积奖励，为策略更新提供稳定的 advantage 信号 reward 扮演的是环境的角色，而 critic 属于 llm 这个智能体的一部分，就好比在考试中，你自己检查卷子和老师给你打分的区别。 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"clip 细节 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:2","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"PPO 和 GRPO 的区别 优势值区别 kl 散度区别 PPO 的 KL 计算是在每个 token 的生成过程中发生的，不断计算当前 token 和 ref_model 的 KL 散度。 GRPO 的 KL 计算是在一个回答生成结束后发生的，一次性对句子中的每个 token 计算 KL 散度，并参与最终 loss 的计算； PPO 的 KL 塞到 reward 里（reward shaping） GRPO 的 KL 是独立的损失项。 advantage 角度来看，PPO 的 KL 惩罚是 token-level 的 GRPO 的 KL 惩罚是 sentence-level（但是也是逐个 token 算 kl 再取 mean）的 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:3","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"REINFORECE++ ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:4","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"RLOO（REINFORCE leave-one-out） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:5","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"ReMAX（REINFORCE argmax） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:6","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"DPO a变大，b变小，理想情况，chosen response概率提升，rejected response概率下降 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:22:7","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"针对GRPO的改进 DAPO（参考 https://www.zhihu.com/question/1895273986537014226/answer/1899582779408245950） DAPO 是对 GRPO 的改进。DAPO（Decoupled Clip and Dynamic sAmpling Policy Optimization，即解耦裁剪和动态采样策略优化）的优化点有四个（其中前 2 个是主要亮点，是命名的来源）： 更高裁剪 动态采样（Dynamic Sampling） Token 级策略梯度损失（Token-Level Policy Gradient Loss） 超长奖励塑造（Overlong Reward Shaping） ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:23:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"DR.GRPO ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:23:1","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"GPG GPG彻底使用policy-based的方法，去除了其他的PPO小trick ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:23:2","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"FP16 和 BF16 的区别 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:24:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"Tokenize 相关 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:25:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","面经"],"content":"面经 大模型微调经验 ai 八股 ","date":"2025-07-02","objectID":"/%E9%9D%A2%E7%BB%8F2025/:26:0","tags":["面经"],"title":"面经2025","uri":"/%E9%9D%A2%E7%BB%8F2025/"},{"categories":["","NLP","agent","world-model"],"content":"我理解的agent中的world model即可以预测采取某个action之后state的变化，这样做的好处是可以降低试错带来的时间成本或者是其它潜在的成本、风险。 目前agent中常用的模块为reflexion，即反思模块。作用是评价行动轨迹的结果是成功还是失败，如果评估为失败，则接受失败的轨迹和评估结果，生成一段失败总结文本。 这当然是可行的，但问题是这需要不断试错才有可能纠错从而执行正确的action，这就会造成时间成本，甚至如果agent负责的是一个购物任务，错误的action甚至有可能是下单错误的商品。 所以我认为world model的作用就是预测action之后state会如何变化，这相当于走一步，看一步，来避免出错。 reflexion是必要的，这是对整个轨迹的评估，如果评估结果为失败的话，还是要进行反思，与world model结合的作用是降低评估失败的概率，使试错的次数降低。我认为这在实际应用当中是有必要的，试错的时间成本要远远大于每一步调用world model所带来的开销。 ","date":"2025-06-30","objectID":"/world_model/:0:0","tags":["NLP","agent","world-model"],"title":"world_model","uri":"/world_model/"},{"categories":["","coding","verl"],"content":"前置知识，ray前置知识 我将用此文来详细介绍veRL中有关single_controller和SPMD的相关内容。本文不涉及ppo训练相关，只是记录一下理解veRL架构实现的核心。 实现single_controller方法的核心有以下方法： Worker RayResourcePool RayWorkerGroup RayClassWithInitArgs 实现SPMD的有以下方法： register dispatch_fn和collect_fn execute_fn func_generator 推荐阅读：verl 解读 - 源码阅读(part3) ","date":"2025-06-26","objectID":"/init_workers%E8%AF%A6%E8%A7%A3/:0:0","tags":["coding","verl"],"title":"init_workers详解","uri":"/init_workers%E8%AF%A6%E8%A7%A3/"},{"categories":["","coding","verl"],"content":"原生Ray 代码改编自# verl 解读 - ray 相关前置知识 (part1) ray分配资源的单位是bundle，一个bundle一般由1个cpu和1个gpu构成。 而一个placement_group由多个bundle组成，当参数设置为pack通常为同1个node上的bundle构成的。参数设置为spread为不同node的bundle组成。如下图所示： 在python中，定义placement_group如下： pg = placement_group([ {\"CPU\": 1, \"GPU\": 1} for _ in range(total_devices) ], strategy=\"STRICT_PACK\", name=\"ray_multi_group_comm\") ray.get(pg.ready()) print(f\"=\u003e Placement group is ready, total_devices: {total_devices}\") ray中的基本单位有task和actor，一个无状态，一个有状态（可以简单理解为一个是函数，一个是面向对象的类）。都需要用worker来代替运行。因此我们需要定义一个worker来完成各种任务。这个worker都是上述资源的分配单位，它只会看到ray分配给它的资源，而看不到其它的资源，对于分布式训练而言，可以在这个worker中定义分布式训练的各种东西、加载模型、前向训练等。一个worker定义如下： class WorkerBase: \"\"\" 通用的 Worker 基类。 每个 Worker 实例是一个 Ray Actor，它会在一个独立的进程中运行，并持有一个模型副本。 \"\"\" def __init__(self, model_cls, temp_init: bool = False): # 获取当前 Ray Actor 所在的节点 ID 和 Actor ID self._node_id = ray.get_runtime_context().get_node_id() self._actor_id = ray.get_runtime_context().get_actor_id() # 获取当前主机的 IP 地址 self._ip_address = socket.gethostbyname(socket.gethostname()) if temp_init: # 如果是临时初始化，则直接返回，不进行模型和分布式环境的设置 return # 设置随机种子 self._set_seed(42) # 初始化 PyTorch 分布式环境 if not dist.is_initialized(): # 检查是否已经初始化 dist.init_process_group( backend=\"nccl\", # 使用 NCCL 作为后端，适用于 NVIDIA GPU world_size=int(os.getenv(\"WORLD_SIZE\", \"1\")), # 从环境变量获取 world_size rank=int(os.getenv(\"RANK\", \"0\")), # 从环境变量获取当前进程的 rank # 从环境变量获取主节点的地址和端口，用于建立连接 init_method=f\"tcp://{os.getenv('MASTER_ADDR')}:{os.getenv('MASTER_PORT')}\" ) self._rank = dist.get_rank() # 获取当前进程的排名 self._world_size = dist.get_world_size() # 获取分布式组的大小 self.model = model_cls() # 实例化传入的模型类 self.model.to(\"cuda\") # 将模型移动到 GPU # 打印初始化信息 print(f\"=\u003e Rank {self._rank}/{self._world_size} in group '{os.getenv('GROUP_NAME')}' initialized model: {self.model.__class__.__name__}\") def get_actor_info(self): \"\"\"返回该 actor 的网络信息。\"\"\" return { \"ip_address\": self._ip_address } def _set_seed(self, seed: int = 42): \"\"\"设置随机种子。\"\"\" set_random_seed(seed) def train_step(self, data): \"\"\"执行一个训练步骤。\"\"\" x = data.to(\"cuda\") # 将输入数据移动到 GPU y = self.model(x) # 模型前向传播 loss = y.sum() # 计算一个简单的损失（所有输出的和） loss.backward() # 反向传播，计算梯度 return loss.cpu() # 返回在 CPU 上的 loss 值 def sample_grads(self): \"\"\"采样一个参数的梯度用于验证。\"\"\" for name, p in self.model.named_parameters(): # 遍历所有带名字的参数 if p.requires_grad is True: # 找到第一个需要梯度的参数 return name, p.grad.cpu() # 返回参数名和它在 CPU 上的梯度 其中model_cls就是定义的各种模型。 然后就可以给各种worker绑定资源。这里我定义了两种模型，node上有4张gpu，我实现的是两种模型分别占2张不同的卡： def setup_and_create_workers(pg, model_cls, group_name, start_bundle_index, num_workers_in_group): \"\"\" 一个辅助函数，用于设置一个分布式组并为其创建 workers。 \"\"\" print(f\"\\\\n========== 正在设置组: {group_name} ==========\") # 将 WorkerBase 类包装成 Ray Remote Actor Worker = ray.remote(WorkerBase) # 创建一个临时 worker 来获取该组的网络信息 # 这个 worker 会被放置在指定的 placement group bundle 上 temp_worker = Worker.options( scheduling_strategy=PlacementGroupSchedulingStrategy( placement_group=pg, placement_group_bundle_index=start_bundle_index, ), num_gpus=1, ).remote(model_cls=model_cls, temp_init=True) # 使用 temp_init=True，避免完全初始化 # 获取临时 worker 的网络信息，特别是 IP 地址，用作该组的 master 地址 network_info = ray.get(temp_worker.get_actor_info.remote()) master_addr = network_info[\"ip_address\"] master_port = str(find_free_port()) # 为该组找到一个空闲端口 ray.kill(temp_worker) # 销毁临时 worker print(f\"为组 {group_name} 使用的主节点地址: {master_addr}:{master_port}\") workers = [] # 存储创建的 worker # 循环创建指定数量的 worker for i in range(num_workers_in_group): rank = i # 当前 worker 在其组内的 rank bundle_index = start_bundle_index + i # 该 worker 在 placement group 中的 bundle 索引 # 设置分布式环境所需的环境变量 env_vars = { \"WORLD_SIZE\": str(num_workers_in_group), \"RANK\": str(rank), \"MASTER_ADDR\": master_addr, \"MASTER_PORT\": master_port, \"GROUP_NAME\": group_name } # 创建一个正式的 worker actor workers.append( Worker.options( name=f\"{group_name}_rank_{rank}\", # 为 actor 命名，方便调试 scheduling_strategy=PlacementGroupSchedulingStrategy( placement_group=pg, placement_group_bundle_index=bundle_index, ), runtime_env={\"env_vars\": env_vars}, # 将环境变量传递给 actor num_gpus=1, # 为每个 actor 分配一个 GPU ).remote(model_cls=model_cls)","date":"2025-06-26","objectID":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:1:0","tags":["coding","verl"],"title":"ray前置知识","uri":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["","coding","verl"],"content":"veRL中的Ray 使用# 【AI Infra】【RLHF框架】二、VeRL中colocate实现源码解析中的代码： import ray from verl.single_controller.base import Worker from verl.single_controller.base.decorator import register, Dispatch from verl.single_controller.ray.base import RayResourcePool, RayClassWithInitArgs, RayWorkerGroup, create_colocated_worker_cls from verl import DataProto @ray.remote class Actor(Worker): def __init__(self) -\u003e None: super().__init__() @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO) def add(self, data: DataProto): data.batch['a'] = data.batch['a'].to(\"cuda\") data.batch['a'] += self.rank return data @ray.remote class Critic(Worker): def __init__(self, config) -\u003e None: super().__init__() self.config = config @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO) def sub(self, data: DataProto): data.batch['a'] = data.batch['a'].to(\"cuda\") data.batch['a'] -= self.config['b'] return data def test_colocated_workers(): ray.init() import torch # 构建一个DataProto，其中属性a是维度为10的零向量。 data = DataProto.from_dict({'a': torch.zeros(10)}) print(data.batch[\"a\"]) # 利用RayClassWithInitArgs将自定义的worker和参数封装起来 actor_cls = RayClassWithInitArgs(cls=Actor) critic_cls = RayClassWithInitArgs(cls=Critic, config={'b': 10}) # 定义资源池，仅包含一个2GPU的节点 resource_pool = RayResourcePool(process_on_nodes=[2]) # 利用create_colocated_worker_cls将自定义的两个worker绑定到WorkerDict上 cls_dict = {'actor': actor_cls, 'critic': critic_cls} ray_cls_with_init = create_colocated_worker_cls(cls_dict) # 启动WorkerDict wg_dict = RayWorkerGroup(resource_pool=resource_pool, ray_cls_with_init=ray_cls_with_init) # 分别获取actor和critic的workergroup spawn_wg = wg_dict.spawn(prefix_set=cls_dict.keys()) colocated_actor_wg = spawn_wg['actor'] colocated_critic_wg = spawn_wg['critic'] # actor执行add、critic执行sub actor_output = colocated_actor_wg.add(data) critic_output = colocated_critic_wg.sub(data) # actor_output.batch[\"a\"]==[0, 0, 0, 0, 0, 1, 1, 1, 1, 1] # critic_output.batch[\"a\"]==[-10, -10, -10, -10, -10, -10, -10, -10, -10, -10] print(actor_output.batch[\"a\"]) print(critic_output.batch[\"a\"]) ray.shutdown() if __name__ == '__main__': test_colocated_workers() 具体的操作我会在init_workers章节进行讲解，这里是对veRL中的single_controller的一个初探。 ","date":"2025-06-26","objectID":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:2:0","tags":["coding","verl"],"title":"ray前置知识","uri":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["","coding","verl"],"content":"问题 有一个常见的问题是为什么在placement_group中分配了一个bundle为1个gpu，为什么还要指定num_gpus=1，这个参数有什么含义吗? 简单来说，两者都需要，因为它们在 Ray 的调度系统中扮演着不同但互补的角色： placement_group 中的 {\"GPU\": 1}：这是在做资源“预留” (Reservation)。 它的作用是告诉 Ray 集群：“我需要你为我准备好一个资源包（bundle），这个包里必须包含 1 个 CPU 和 1 个 GPU。” placement_group 的主要目的是确保一组相关的资源能够被原子性地、协同地调度。比如，strategy=\"STRICT_PACK\" 确保了所有这些 bundles 都会被放置在同一个节点上，这对于需要低延迟通信的分布式训练至关重要。 这就像是为一场宴会预订了一个能容纳4位客人的包间，并确保这4个座位都在同一张桌子上。它只是圈占了资源，但还没有指定谁来使用这些资源。 Worker.options(num_gpus=1)：这是在为 Actor 提出具体的“资源请求” (Request)。 它的作用是告诉 Ray：“我即将创建的这个 Worker Actor，它本身在运行时需要消耗 1 个 GPU。” Ray 的调度器需要根据这个明确的请求来为 Actor 分配具体的物理设备。没有这个声明，Ray 调度器会认为该 Actor 不需要 GPU。 这就像是告诉宴会的主管：“这位客人需要一个座位。” 为什么缺一不可？ PlacementGroupSchedulingStrategy 将这两者联系起来。它告诉 Ray：“请将这个需要1个GPU的Actor (num_gpus=1)，安排到我们之前预留的那个包含GPU的bundle ({\"GPU\": 1}) 上去。” 如果只有 placement_group 的预留而没有 Actor 的 num_gpus=1 请求，Ray 的调度器会看到一个矛盾：你试图将一个声称不需要 GPU 的 Actor 安排在一个为 GPU 使用者保留的“席位”上。这可能会导致调度失败或资源分配混乱。 反之，如果只有 Actor 的 num_gpus=1 请求而没有 placement_group，Ray 会在整个集群中寻找任何一个可用的 GPU 来满足这个 Actor，但无法保证它会和其他相关的 Actor 运行在同一个节点上，从而失去了分布式训练的性能优势。 总结： placement_group 是一个宏观的、用于资源预留和协同定位的机制，而 num_gpus=1 是一个微观的、用于声明单个 Actor 实际资源消耗的机制。两者必须同时使用并保持一致，才能确保 Ray 能够精确、高效地将需要特定资源的 Actor 调度到你为它们预留的、具有特定拓扑结构的资源包中。这种显式的设计让复杂的分布式资源管理变得更加清晰和可控。 ","date":"2025-06-26","objectID":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:3:0","tags":["coding","verl"],"title":"ray前置知识","uri":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["","coding","verl"],"content":"参考 ","date":"2025-06-26","objectID":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:4:0","tags":["coding","verl"],"title":"ray前置知识","uri":"/ray%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["world-model","reading","Planning"],"content":"这篇论文的核心思想，一言以蔽之，就是通过一种名为 RLVR（Reinforcement Learning with Verifiable Rewards，可验证奖励的强化学习） 的技术，对 世界模型 (World Models) 进行“二次打磨”或“精加工”，从而使其更精准地服务于特定任务。这解决了传统训练方法（如最大似然估计 MLE）与最终应用目标之间存在的“貌合神离”的问题。 让我们先通俗地理解一下什么是 世界模型。您可以把它想象成一个学习了特定环境“物理规律”的模拟器。比如，在一个游戏中，世界模型知道“你推一下箱子，箱子会向前移动”；在一个视频里，它知道“球被抛出后，会沿着抛物线落下”。它通过预测“在当前状态下，执行某个动作后，世界会变成什么样子”来工作。 然而，传统的训练方式存在一个巨大痛点。它们通常使用 最大似然估计 (MLE)，目标是让模型预测的下一个词元（token）或像素（pixel）与真实数据尽可能一致。这种方法虽然能让模型学到数据的大致分布，但往往与我们真正关心的“任务目标”有所偏差。论文中提到： However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. 例如，在视频预测任务中，仅用像素级的均方误差 (MSE) 作为目标，会导致生成的视频模糊不清；在语言任务中，则容易导致模型产生重复、无意义的“车轱辘话”或事实性错误（幻觉）。 为了解决这一“对不齐”的难题，作者们提出了 RLVR-World 框架。其精髓在于，不再使用模糊的、学习来的人类偏好（如 RLHF 中那样），而是将一个 可验证的、基于规则的奖励函数 作为强化学习的明确信号。这个奖励函数直接衡量模型输出的好坏，例如，预测的游戏状态是否完全准确，或者生成的视频帧在观感上是否清晰真实 (使用 LPIPS 这类感知度量)。这个直接、清晰的信号，能更有效地指导模型“进步”。 该框架的一大亮点是其通用性。它将语言、视频等不同模态的任务统一到了一个自回归的序列建模范式下。无论是文本描述的状态，还是视频画面的像素，都被编码成一串串的 词元 (tokens)。世界模型的工作，就是像一个语言模型一样，逐个预测代表“未来世界”的词元。 为了证明其有效性，论文在两个截然不同的领域——语言和视频——进行了详尽的实验，并取得了令人瞩目的成果。 在 语言世界模型 的实验中，以“文本游戏状态预测”为例，任务要求模型根据玩家的动作，预测游戏世界的状态变化（以 JSON 格式表示）。如论文 表1 所示，经过 RLVR 微调后，模型的整体准确率从 32.87% 飙升至 63.24%，几乎翻了一倍，并且达到了与更强大的 GPT-4 (64.76%) 相媲美的水平。这充分证明了 RLVR 在精确理解和推理任务上的巨大潜力。同样，在“网页状态预测”任务中（表2），使用 RLVR 训练的世界模型，让网页导航智能体的任务成功率相对提升了 18.4%，展现了其在实际应用中的价值。 在 视频世界模型 的探索中，作者们更是扮演了“开创者”的角色。他们将其应用于机器人操作轨迹预测。实验结果（表3 和 图3）揭示了一个惊人的事实：RLVR 仅需 几百次 的梯度更新，就能在各项指标上取得显著优于传统 MLE 方法 数十万次 训练的效果。这不仅仅是“更快”，更是“质”的飞跃。尤其是在多步预测中，模型生成重复画面的问题（Repetition Rate）从惊人的 48.6% 大幅降低到了 9.9%，这意味着生成的视频序列更加连贯和真实。 图1: 视频模型训练效率对比（示意图，改编自原论文图3）。RLVR（橙线）仅用极少的训练步数就达到了远超长时间 MLE 预训练（蓝线）的性能水平。 总而言之，这篇论文为我们揭示并验证了一个强大而通用的后训练（Post-training）范式。RLVR-World 通过将强化学习与直接、可量化的任务指标相结合，有效地弥合了传统训练目标与真实世界需求之间的鸿沟，为提升各类生成模型（尤其是世界模型）的实用性和可靠性开辟了一条崭新的道路。 接下来，我将按照您提出的六个问题，逐一进行更详细的解读。 一、研究目标与实际意义 研究目标：本文的核心目标是解决在训练世界模型时普遍存在的 “目标函数错位” (Objective Misalignment) 问题。 实际问题：传统的训练方法（如 MLE）旨在优化代理指标（如预测下一个词元的概率），但这并不等同于优化模型在真实任务中的表现（如预测的准确性、生成内容的质量）。这种错位导致模型“学会了皮毛，却未得精髓”，限制了其在复杂决策和模拟任务中的可靠性。 行业意义：高保真、高精度的世界模型是实现更高级别人工智能的关键基石。 推动自主智能体发展：对于机器人、自动驾驶汽车、软件智能体（如自动化网页操作）而言，一个强大的世界模型意味着它们可以在“脑中”对行为的后果进行更准确的预演和规划，从而做出更优决策，减少在现实世界中进行昂贵且危险的试错。 革新模拟与规划领域：精准的世界模型本身就是强大的模拟器。这可以彻底改变从产品设计、药物研发到供应链管理的各个行业，使得大规模、低成本、高效率的模拟测试成为可能。 提升生成式AI的可靠性：该方法论不仅限于世界模型，它提供了一种通用的思路，用于提升所有生成式模型（包括大型语言模型）在特定任务上的实用性与可控性，例如减少幻觉、提升代码生成正确率等。 ","date":"2025-06-16","objectID":"/rlvr-world/:0:0","tags":["task_planning","world-model"],"title":"RLVR-World","uri":"/rlvr-world/"},{"categories":["world-model","reading","Planning"],"content":"二、新思路与方法优势 论文提出的核心方法是 RLVR-World 框架，其创新之处体现在以下几个方面： 核心思路：放弃间接的、基于学习的奖励模型（如 RLHF），转而采用 直接、可量化的任务指标作为强化学习的奖励信号。这是对齐（Alignment）技术路线的一次重要探索和补充。 方法特点： 统一的序列建模框架：将不同模态（文本、视频、机器人动作）统一视为词元序列，使得强大的 Transformer 架构可以被直接应用，极具通用性和扩展性。 可验证奖励 (Verifiable Rewards)：这是整个方法论的基石。奖励函数直接根据任务的“金标准”来计算。如论文公式 (4) 所示： \u003e \\[ R_i = \\text{sign}(D) \\cdot D(\\hat{s}'_i, s') \\] 这里的 \\(D\\) 就是一个可量化的评价指标，例如，对于语言任务可以是 F1 分数或准确率，对于视频任务可以是 LPIPS (Learned Perceptual Image Patch Similarity) 这种更符合人类视觉感知的指标。这种奖励信号是客观、稳定且可靠的。 高效的强化学习算法：采用了 GRPO (Group Relative Policy Optimization)。该算法通过对一组生成样本进行“内部比较”和“相对排序”来计算优势，避免了传统 RL 算法中对复杂值函数的依赖，使得训练过程更稳定、更高效。 相比优势： 直接性：直接面向最终目标进行优化，避免了代理目标的“中间商差价”，效果更显著。 效率高：如视频模型实验所示，RLVR 只需极少的微调成本（几百步）就能带来巨大性能提升，相比于动辄需要数周甚至数月的预训练，性价比极高。 问题修复能力：能有效缓解 MLE 训练带来的固有顽疾，如在视频生成中将重复率从 48.6% 降至 9.9%，显著提升了生成质量。 通用性强：该框架在语言和视频两个差异巨大的模态上都取得了成功，证明了其作为一种通用后训练范式的潜力。 三、实验设计与结果分析 论文通过一系列精心设计的实验，充分验证了 RLVR-World 框架的有效性。 实验设计： 两大模态验证：选取了 语言 和 视频 作为代表，验证框架的通用性。 多层次任务评估： 核心能力测试：在孤立环境中测试世界模型本身的能力（文本游戏预测、视频轨迹预测）。 下游应用测试：将训练好的世界模型嵌入到一个更大的系统中（网页导航智能体），检验其对整个系统性能的提升效果。 严格的基线对比：所有 RLVR 模型都与仅经过监督微调 (SFT) 的模型进行对比，清晰地展示了 RLVR 带来的增益。同时，在部分任务中还与 GPT-4 等更强大的模型进行了比较。 关键数据与结果： 语言模型 - 文本游戏 (表1)： 任务：预测游戏状态的 JSON 对象变化。 结果：在预测“状态发生改变”的困难样本上，RLVR 模型准确率达到 33.80%，而 SFT 模型仅有 24.21%，基座模型更是只有 0.08%。这表明 RLVR 极大地增强了模型的精确推理能力。 语言模型 - 网页导航 (表2)： 任务：预测网页状态变化，并作为世界模型提升导航智能体。 结果：RLVR 将世界模型的 F1 分数从 49.94% 提升至 65.11%（相对提升 +30.3%），并最终将整个智能体的任务成功率从 12.06% 提升至 14.29%（相对提升 +18.4%）。 视频模型 - 机器人操作 (表3)： 任务：预测机器人手臂操作的未来视频帧。 结果：在多步预测中，RLVR 使得衡量感知质量的 LPIPS 指标提升了 9.2%，同时将衡量图像相似度的 SSIM 提升了 1.9%。这说明生成的视频不仅更准确，而且看起来更真实。 ","date":"2025-06-16","objectID":"/rlvr-world/:0:1","tags":["task_planning","world-model"],"title":"RLVR-World","uri":"/rlvr-world/"},{"categories":["world-model","reading","Planning"],"content":"四、未来探索与机遇 这篇论文为该领域指明了方向，同时也留下了许多值得探索的课题和潜在机遇。 值得探索的问题与挑战 (论文第7节也进行了讨论): 更智能的奖励函数：当前的“可验证奖励”虽然有效，但仍相对初级。如何设计能捕捉更复杂、更抽象概念（如物理规则的遵守、长期任务的一致性、美学价值）的奖励函数，是一个巨大的挑战。 突破性能瓶颈：RLVR 的训练收敛得非常快，这既是优点也是一个挑战。如何让模型在快速收敛后还能持续学习和提升，可能需要对模型架构、数据多样性和 RL 算法本身进行更深入的研究。 分布外 (OOD) 泛化能力：经过 RLVR“精调”的模型，在面对训练时未曾见过的、甚至是反事实的场景时，其表现如何？提升模型在开放世界中的泛化能力，是其走向实用化的关键。 可能催生的技术和投资机会: 企业级数字孪生 (Digital Twin)：高保真世界模型是构建数字孪生的核心技术。在制造业、城市管理、物流等领域，通过精准模拟来优化流程、预测故障，拥有巨大的商业价值。 AI驱动的科学发现 (AI for Science)：利用世界模型来模拟分子动力学、气候变化、材料科学等复杂系统，有望加速科学研究的进程。 下一代机器人技术：能够精准预见未来的机器人将拥有更强的学习能力和安全性，这将极大地推动机器人在家庭、医疗、工业等场景的普及。 模型对齐与安全工具链：围绕 RLVR 这类技术，开发一套方便研究者和开发者定义奖励函数、执行对齐训练的平台和工具，本身就是一个重要的技术和商业方向。 五、批判性视角与局限性 从批判的角度看，这篇论文也存在一些值得注意的局限性： 对基座模型的依赖：论文坦言，RLVR 的效果上限受制于基座模型的能力。它更像是一个“优化器”而非“创造者”。如果基座模型本身能力很弱，RLVR 也难以“点石成金”。例如，在文本游戏任务中，其 1.5B 模型在处理复杂变化时的能力仍显著落后于 GPT-4。 可验证奖励的适用范围：RLVR 的成功依赖于一个易于计算的、明确的奖励函数。这在代码生成（单元测试）、游戏（胜负判断）等任务中是可行的。但对于许多目标模糊、主观性强的任务（如“写一首优美的诗”），定义这样的函数极其困难，这也是 RLHF 这类范式仍然不可或缺的原因。 探索与利用的权衡：论文在图4a中发现一个有趣的现象，在测试时，虽然 RLVR 模型的单次生成质量更高，但传统的 MLE 模型通过大量采样（生成100个样本后选最优）最终能反超。这可能暗示 RLVR 在优化过程中牺牲了一部分生成的多样性，过于集中在奖励高的区域。如何平衡优化与多样性是一个待解的问题。 评估方法的局限：在最终的智能体策略评估中，成功与否依赖于单个人类标注员的判断。虽然这样做保证了标准的一致性，但缺乏大规模评估的鲁棒性，可能存在个人偏见。 六、核心启发与学习路径 对于希望从中汲取创新想法的您来说，这篇论文提供了宝贵的启发： 核心启发： 目标驱动的优化范式：“预训练 + 任务导向的精调”是一个极其强大的范式。其核心思想是，不要满足于训练一个通用的预测器，而要致力于打磨一个卓越的任务执行者。找到衡量最终任务成功的“那把尺子”，并用它来直接指导模型的学习。 创新的通用配方：您可以将这个思路应用到自己的领域。首先，识别出一个使用生成式模型，并且其产出可以用一个自动化脚本来评估好坏的场景。然后，应用 RLVR 框架：用模型生成结果，用评估脚本计算得分，再将此得分作为奖励信号反馈给模型进行微调。这是一个实现模型性能定点优化的“万能公式”。 需要补充的背景知识： 强化学习基础 (Reinforcement Learning)：建议深入理解策略梯度（Policy Gradient）方法，特别是 PPO 算法。这将帮助您理解本文所用的 GRPO 算法的动机和原理。经典的教材是 Sutton 和 Barto 的《强化学习导论》。 世界模型概念：可以阅读 David Ha 和 Jürgen Schmidhuber 的开创性论文 “Recurrent World Models Facilitate Policy Evolution” (参考文献 [16])，来理解世界模型的基本思想。 模型对齐技术：了解 RLHF（来自人类反馈的强化学习）的原理，并与本文的 RLVR 进行对比。这能让您更深刻地理解，为什么用“可验证”的机器反馈替代“学习来的”人类反馈，是一种重要且有效的思路。 ","date":"2025-06-16","objectID":"/rlvr-world/:0:2","tags":["task_planning","world-model"],"title":"RLVR-World","uri":"/rlvr-world/"},{"categories":["","NLP","agent"],"content":"根据Anthropic的定义，agent定义如下： At Anthropic, we categorize all these variations as agentic systems, but draw an important architectural distinction between workflows and agents: Workflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. 简单来说就是Workflows是预先定义好的一个路径，而Agents是让其自主完成各种流程，而无需预先定义。 ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:0:0","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"workflows ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:0","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"Prompt chaining image.png ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:1","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"Routing image.png ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:2","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"Parallelization image.png ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:3","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"Orchestrator-workers image.png ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:4","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"Evaluator-optimizer image.png 最近热门的Gemini 2.5 Pro Capable of Winning Gold at IMO 2025论文就是使用的这个工作流。 ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:1:5","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"agents image.png ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:2:0","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","NLP","agent"],"content":"参考 Agents Zero to One: Learning Agentic Patterns Building Effective AI Agents \\ Anthropic How we built our multi-agent research system \\ Anthropic ","date":"2025-06-14","objectID":"/agent%E6%A6%82%E8%A7%88/:3:0","tags":["NLP","agent"],"title":"agent概览","uri":"/agent%E6%A6%82%E8%A7%88/"},{"categories":["","coding","verl"],"content":"Hybrid Engine 在 RLHF 流程中，actor model 的 generation 和 rollout 占据了绝大多数运行时间（在 veRL 是 58.9%）。并且，由于 PPO 是 online 算法，经验（experiences）必须来自于被 train 的模型本身，因此，rollout 和 training 是必须串行的。如果这两者使用不同的资源组，比如 rollout 用 2 张卡，而 training 用 4 张卡，rollout 的时候 training 的资源闲置，training 的时候 rollout 的资源闲置，无论如何都会浪费大量的计算资源。由此，veRL 将 training 和 rollout engine 放置在同一个资源组中串行执行。training 时，将 rollout engine 的显存回收（offload 到 CPU 上 或者直接析构掉），rollout 时，再将 training engine 的显存释放掉。这种将 actor model 的不同 engine 放置在同一个资源组上的方案，就称为 hybrid engine。 image.png ","date":"2025-06-05","objectID":"/verl%E6%80%BB%E4%BD%93%E6%A6%82%E8%A7%88/:1:0","tags":["coding","verl"],"title":"verl总体概览","uri":"/verl%E6%80%BB%E4%BD%93%E6%A6%82%E8%A7%88/"},{"categories":["","coding","verl"],"content":"PPO流程 从图中可以得到的是ppo_mini_batch_size是全局的prompt batch，而ppo_micro_batch_per_gpu是每一个gpu上的prompt+response batch，所以由此可以得到梯度累计的steps为 ppo_mini_batch_size * n // (gpu数) // micro_batch_per_gpu。 ","date":"2025-06-05","objectID":"/verl%E6%80%BB%E4%BD%93%E6%A6%82%E8%A7%88/:2:0","tags":["coding","verl"],"title":"verl总体概览","uri":"/verl%E6%80%BB%E4%BD%93%E6%A6%82%E8%A7%88/"},{"categories":["","coding","verl"],"content":"最原生的reward_mananger: class NaiveRewardManager: \"\"\"The reward manager.\"\"\" def __init__(self, tokenizer, num_examine, compute_score=None, reward_fn_key=\"data_source\") -\u003e None: self.tokenizer = tokenizer self.num_examine = num_examine # the number of batches of decoded responses to print to the console self.compute_score = compute_score or default_compute_score self.reward_fn_key = reward_fn_key def __call__(self, data: DataProto, return_dict=False): \"\"\"We will expand this function gradually based on the available datasets\"\"\" # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn if \"rm_scores\" in data.batch.keys(): if return_dict: return {\"reward_tensor\": data.batch[\"rm_scores\"]} else: return data.batch[\"rm_scores\"] reward_tensor = torch.zeros_like(data.batch[\"responses\"], dtype=torch.float32) reward_extra_info = defaultdict(list) already_print_data_sources = {} for i in range(len(data)): data_item = data[i] # DataProtoItem prompt_ids = data_item.batch[\"prompts\"] prompt_length = prompt_ids.shape[-1] valid_prompt_length = data_item.batch[\"attention_mask\"][:prompt_length].sum() valid_prompt_ids = prompt_ids[-valid_prompt_length:] response_ids = data_item.batch[\"responses\"] valid_response_length = data_item.batch[\"attention_mask\"][prompt_length:].sum() valid_response_ids = response_ids[:valid_response_length] # decode prompt_str = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=True) response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=True) ground_truth = data_item.non_tensor_batch[\"reward_model\"][\"ground_truth\"] data_source = data_item.non_tensor_batch[self.reward_fn_key] extra_info = data_item.non_tensor_batch.get(\"extra_info\", None) score = self.compute_score( data_source=data_source, solution_str=response_str, ground_truth=ground_truth, extra_info=extra_info, ) if isinstance(score, dict): reward = score[\"score\"] # Store the information including original reward for key, value in score.items(): reward_extra_info[key].append(value) else: reward = score reward_tensor[i, valid_response_length - 1] = reward if data_source not in already_print_data_sources: already_print_data_sources[data_source] = 0 if already_print_data_sources[data_source] \u003c self.num_examine: already_print_data_sources[data_source] += 1 print(\"[prompt]\", prompt_str) print(\"[response]\", response_str) print(\"[ground_truth]\", ground_truth) if isinstance(score, dict): for key, value in score.items(): print(f\"[{key}]\", value) else: print(\"[score]\", score) if return_dict: return { \"reward_tensor\": reward_tensor, \"reward_extra_info\": reward_extra_info, } else: return reward_tensor 逻辑很简单，就是通过compute_score函数来计算score。 ","date":"2025-06-02","objectID":"/reward_mananger/:0:0","tags":["coding","verl"],"title":"reward_mananger","uri":"/reward_mananger/"},{"categories":["LLM","NLP","reasoning"],"content":"核心总结 PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1\u003e2的效果。例如， 单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。 单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。 PRM 和 MCTS 既可以应用于优化训练数据，也可以用来预测用 用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward model训练数据。 用于推理：很简单，推理用MCTS的方式把 test-scaling 做上来，再结合PRM的方式从众多路径中挑选最佳答案。 PRM和MCTS的缺点 这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。 Process Reward Model(PRM) 在实际应用中有三大局限： 第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。 第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。 第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy model，但反而更多时间去优化 reward model 去了。 对MCTS的看法： 文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。 MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。 ","date":"2025-04-04","objectID":"/mcts%E5%92%8Cprm/:1:0","tags":["LLM","NLP","reasoning"],"title":"MCTS和PRM","uri":"/mcts%E5%92%8Cprm/"},{"categories":["LLM","NLP","reasoning"],"content":"参考 https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking ","date":"2025-04-04","objectID":"/mcts%E5%92%8Cprm/:2:0","tags":["LLM","NLP","reasoning"],"title":"MCTS和PRM","uri":"/mcts%E5%92%8Cprm/"},{"categories":["","python","coding"],"content":"a = 'global' def outer(): # def len(in_var): # print('called my len() function: ', end=\"\") # l = 0 # for i in in_var: # l += 1 # return l a = 'local' def inner(): nonlocal a a += ' variable' inner() print('a is', a) # print(len(a)) outer() # print(len(a)) print('a is', a) 此时为nonlocal a，会按照local-闭包-global的顺序找到闭包变量a。a的值为local variable a = 'global' def outer(): # def len(in_var): # print('called my len() function: ', end=\"\") # l = 0 # for i in in_var: # l += 1 # return l a = 'local' def inner(): global a a += ' variable' inner() print('a is', a) # print(len(a)) outer() # print(len(a)) print('a is', a) 此时为global，会从全局变量中寻找a，a的值为global variable ","date":"2025-03-24","objectID":"/legb/:0:0","tags":["python","coding","LEGB"],"title":"LEGB","uri":"/legb/"},{"categories":["","coding","python"],"content":"python调试工具，类似于vscode的调试工具，使用命令行进行调试。 ","date":"2025-03-23","objectID":"/debugger/:0:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"使用方法 ","date":"2025-03-23","objectID":"/debugger/:1:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"插入式 import pdb; pdb.set_trace() 或者 breakpoint() ","date":"2025-03-23","objectID":"/debugger/:1:1","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"非插入式 python -m pdb [-c command] (-m module | pyfile) [args ...] ","date":"2025-03-23","objectID":"/debugger/:1:2","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"常用命令 ","date":"2025-03-23","objectID":"/debugger/:2:0","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"h 即help，可用命令如下 ","date":"2025-03-23","objectID":"/debugger/:2:1","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"p p x 即print(x)，用于打印变量。 pp x，使用pprint打印 ","date":"2025-03-23","objectID":"/debugger/:2:2","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"w 即where，查看当前调用栈。 ","date":"2025-03-23","objectID":"/debugger/:2:3","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"u和d u即up，回到上一个frame d即down，到下一个frame ","date":"2025-03-23","objectID":"/debugger/:2:4","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"l 即lst l 查看前后12行代码 ll查看当前函数全部代码 ","date":"2025-03-23","objectID":"/debugger/:2:5","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"b 即break，进行打断点 b x，在第x行打断点。 b 查看所有断点。 相同的有tbreak，与break的区别是第一次到该断点后会自动移除断点。即temporary breakpoint ","date":"2025-03-23","objectID":"/debugger/:2:6","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"n 即next，执行下一条语句，但忽视函数调用内部细节 ","date":"2025-03-23","objectID":"/debugger/:2:7","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"s 即step，执行下一条语句，如果有函数调用，则调用新frame，进入函数内部。 ","date":"2025-03-23","objectID":"/debugger/:2:8","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"c 即continue，继续程序的执行直到下一个断点 image.png ","date":"2025-03-23","objectID":"/debugger/:2:9","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"r 即return，直接跳转到当前函数return语句 ","date":"2025-03-23","objectID":"/debugger/:2:10","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"until until n，使程序继续执行直到执行到行数为n的语句。 ","date":"2025-03-23","objectID":"/debugger/:2:11","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"cl 即clear clear n，清除编号为n的断点 clear，清除所有断点。 ### j 即jump，向前或向后跳转，与until区别是，jump不会执行中间的语句，而是忽略他们。 ","date":"2025-03-23","objectID":"/debugger/:2:12","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["","coding","python"],"content":"display 相当于一个监视器，用于监视变量的变化 ### retval 打印当前函数最后一次返回的返回值 ### q 即quit，退出pdb调试。 ","date":"2025-03-23","objectID":"/debugger/:2:13","tags":["coding","python"],"title":"debugger","uri":"/debugger/"},{"categories":["coding","LLM","generate"],"content":"理论部分在这：generate相关 ## generate参数 def generate( self, inputs: Optional[torch.Tensor] = None, generation_config: Optional[GenerationConfig] = None, logits_processor: Optional[LogitsProcessorList] = None, stopping_criteria: Optional[StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[\"PreTrainedModel\"] = None, streamer: Optional[\"BaseStreamer\"] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs, ) -\u003e Union[GenerateOutput, torch.LongTensor]: 在代码中可以看到在函数入口显式的定义了很多参数。他们的具体含义如下 inputs：tensor 形式的 token_id，通常先准备文本形式的提示词和输入，使用tokenizer转化为对应 id，这里维度通常为 [batch_size, seq_len] generation_config：一个用 GenerationConfig 类创建的对象，存储着模型生成的超参数，可以提前创建该对象并传入 .generate() logits_processor：高级功能，logits_processor 可以在每个 step 的输出概率计算完成后，对分数进行进一步的干预，改变输出的概率分布，从而影响生成的结果，例如最常见的，重复惩罚，就是使用 logits_processor 完成的。（不懂的话可以看后面如何具体实现的） stopping_criteria：高级功能，允许用户通过 stopping_criteria 自定义生成停止条件（不懂的话可以看后面如何具体实现的） prefix_allowed_tokens_fn：解码策略的一个超参数，用于前缀 token 约束（感觉没必要放在这里） synced_gpus： DeepSpeed ZeRO Stage-3 多GPU时使用（ZeRO-3包括优化器状态+梯度+权重并行优化，而推理阶段只使用权重并行），此时需要将 synced_gpus 设置成 Ture。. 否则，如果一个 GPU 在另一个 GPU 之前完成生成，整个系统就会挂起，因为其余 GPU 尚未从最先完成的 GPU 接收到权重分片。 transformers\u003e=4.28 在生成时检测到多个 GPU 会自动设置 synced_gpus=True，transformers\u003c4.28 需要手动设置，本文代码环境transformers=4.41.1 assistant_model：高级功能，辅助生成模型，另一个词表完全相同的小模型，有些token使用辅助模型生成更快 streamer：流式输出控制器，现在的大模型平台都是一个字一个字显示出来的，这就是流式输出，否则的话会等所有生成完成再显示出来。这个可以自定义流式输出的方式 negative_prompt_ids：负面提示，一些前沿研究会用到，不用管 negative_prompt_attention_mask：负面提示的 attention_mask **kwargs 以上输入都太高大上了，只有 inputs 会每次传入，其他的对于常规输出根本用不到（其实 inputs 也可以不用输入，通过tokenizer()得到model_inputs后，使用**model_inputs方式也可以传入） 回想一下别人的代码，会看到这里经常传入 temperature=0.7, top_k=20, max_new_tokens=512等参数，都是通过**kwargs传入进来的 其实传入的这些都是输入参数 generation_config 的属性（可以进入对应类中看一下有哪些属性，from transformers.generation.configuration_utils import GenerationConfig），你可以创建该对象并覆盖某些参数，也可以通过参数形式在调用.generate()时传进来 在后面会将传入的这些参数覆盖掉generation_config中对应的属性 ","date":"2025-03-09","objectID":"/generate/:0:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["coding","LLM","generate"],"content":"inputs处理 def _prepare_model_inputs( self, inputs: Optional[torch.Tensor] = None, bos_token_id: Optional[torch.Tensor] = None, model_kwargs: Optional[Dict[str, torch.Tensor]] = None, ) -\u003e Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]: \"\"\" This function extracts the model-specific `inputs` for generation. \"\"\" # 1.有一些 encoder-decoder 模型的输入有不同的名称，这里首先确认名称 if ( self.config.is_encoder_decoder and hasattr(self, \"encoder\") and self.encoder.main_input_name != self.main_input_name ): input_name = self.encoder.main_input_name else: input_name = self.main_input_name # 从 model_kwargs 中去掉 input_name: None 的键值对 model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name} # 2.这里确保 model.generate() 输入参数中的 inputs 和 kwargs 中的 input_name 只输入一个 inputs_kwarg = model_kwargs.pop(input_name, None) if inputs_kwarg is not None and inputs is not None: raise ValueError( f\"`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. \" f\"Make sure to either pass {inputs} or {input_name}=...\" ) elif inputs_kwarg is not None: inputs = inputs_kwarg # 3.如果 input_name != inputs_embeds， 这里确保 input_name 和 inputs_embeds 只输入一个 if input_name == \"input_ids\" and \"inputs_embeds\" in model_kwargs: # 如果是 decoder-only 模型，先看看模型 .forward() 函数的参数中，是否包含 inputs_embeds，如果不包含就弹出异常 if not self.config.is_encoder_decoder: has_inputs_embeds_forwarding = \"inputs_embeds\" in set( inspect.signature(self.prepare_inputs_for_generation).parameters.keys() ) if not has_inputs_embeds_forwarding: raise ValueError( f\"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} \" \"doesn't have its forwarding implemented. See the GPT2 implementation for an example \" \"(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!\" ) # In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of # the attention mask) can rely on the actual model input. model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation( inputs, bos_token_id, model_kwargs=model_kwargs ) else: if inputs is not None: raise ValueError(\"You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.\") inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\" # 4. 如果 `inputs` 还是 None，尝试用 BOS token 创建 `input_ids` inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs) return inputs, input_name, model_kwargs 若传入了 inputs，就不要在 kwargs 中再次定义 input_ids 若 inputs 为 None，且 model_kwargs 不包含 input_ids 或 input_ids 也为 None，则创建一个 [batch_size, 1] 大小的tensor，里面的值都为 bos_token_id ","date":"2025-03-09","objectID":"/generate/:1:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["coding","LLM","generate"],"content":"参考 https://blog.csdn.net/qq_41496421/article/details/142346738?spm=1001.2014.3001.5502 https://blog.csdn.net/qq_41496421/article/details/142580960?spm=1001.2014.3001.5501 ","date":"2025-03-09","objectID":"/generate/:2:0","tags":["coding","LLM"],"title":"generate","uri":"/generate/"},{"categories":["","einops"],"content":" einops.einsum calls einsum operations with einops-style named axes indexing, computing tensor products with an arbitrary number of tensors. Unlike typical einsum syntax, here you must pass tensors first, and then the pattern. Also, note that rearrange operations such as \"(batch chan) out\", or singleton axes (), are not currently supported. 爱因斯坦求和 ","date":"2025-01-11","objectID":"/einsum/:0:0","tags":["einops"],"title":"einsum","uri":"/einsum/"},{"categories":["einops"],"content":"pack Packs several tensors into one. See einops tutorial for introduction into packing (and how it replaces stack and concatenation). ## unpack \u003eUnpacks a single tensor into several by splitting over a selected axes. See einops tutorial for introduction into packing (and how it replaces stack and concatenation). image.png ","date":"2025-01-11","objectID":"/pack-and-unpack/:1:0","tags":["einops"],"title":"pack and unpack","uri":"/pack-and-unpack/"},{"categories":["","einops"],"content":" einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. 代替reshape，给维度命名。可以用…代表不想动的维度。 ","date":"2025-01-11","objectID":"/rearrange/:0:0","tags":["einops"],"title":"rearrange","uri":"/rearrange/"},{"categories":["","einops"],"content":" einops.reduce combines rearrangement and reduction using reader-friendly notation. reduce会使维度减少。 ","date":"2025-01-11","objectID":"/reduce/:0:0","tags":["einops"],"title":"reduce","uri":"/reduce/"},{"categories":["","einops"],"content":" einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, and broadcast functions. repeat是使维度增加，与reduce相反。 ## 应用 比如说repeat_kv函数就可以用einops.repeat很方便的实现 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\" bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) 等价于 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: einops.repeat(x, 'bs slen kvheads dim-\u003e bs slen (kvheads n_rep) dim', n_rep=n_rep).shape ","date":"2025-01-11","objectID":"/repeat/:0:0","tags":["einops"],"title":"repeat","uri":"/repeat/"},{"categories":["","einops"],"content":" Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/jax/etc.) to numpy.ndarray image.png ","date":"2025-01-08","objectID":"/asnumpy/:0:0","tags":["einops"],"title":"asnumpy","uri":"/asnumpy/"},{"categories":["einops"],"content":" Parse a tensor shape to dictionary mapping axes names to their lengths. # Use underscore to skip the dimension in parsing. \u003e\u003e\u003e x = np.zeros([2, 3, 5, 7]) \u003e\u003e\u003e parse_shape(x, 'batch _ h w') {'batch': 2, 'h': 5, 'w': 7} # `parse_shape` output can be used to specify axes_lengths for other operations: \u003e\u003e\u003e y = np.zeros([700]) \u003e\u003e\u003e rearrange(y, '(b c h w) -\u003e b c h w', **parse_shape(x, 'b _ h w')).shape (2, 10, 5, 7) 也就是把维度的维数映射到对应的命名。与数据无关，只看得到维度。 ","date":"2025-01-08","objectID":"/parse_shape/:0:0","tags":["einops"],"title":"parse_shape","uri":"/parse_shape/"},{"categories":["survey","reading","Planning"],"content":"该工作主要梳理了LLM-based Agent 中的规划（planning）能力，原文链接： Understanding the planning of LLM agents: A survey 文章中，作者将planning能力进一步细分为了五个维度： 任务分解（Task Decomposition） 规划选择（Plan Selection） 外部辅助规划（External Planner） 反馈和改进（Reflection and Refinement） 记忆（Memory） ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:0:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"引言 (Introduction) 自主智能代理：被定义为能够完成特定任务的智能实体。它们通过感知环境、规划和执行动作来实现目标。 规划的重要性：规划是代理最关键的能力之一，它要求代理进行复杂的理解、推理和决策过程。 传统方法的局限性：以往的工作主要依赖于符号方法或基于强化学习的方法，如规划领域定义语言（PDDL）或策略学习。这些传统方法有其局限性，例如符号方法需要将自然语言描述的问题转换为符号建模，这可能需要人类专家的努力，而且缺乏容错性。强化学习方法通常需要与环境的大量样本（交互）来学习有效策略，这在数据收集耗时或成本高昂的场景中可能不切实际。 LLM的潜力：近年来，大型语言模型（LLM）的出现标志着一个范式的转变。LLM在多个领域取得了显著的成功，展示了在推理、工具使用、规划和指令跟随方面的重要智能。这种智能为将LLM作为代理的认知核心提供了可能性，从而有潜力提高规划能力。 本文工作：尽管已有调查尝试总结LLM的技术，但文献中往往缺乏对规划能力的详细分析。本调查旨在分析最新的研究工作，讨论优势和局限性，并提供对基于LLM的代理规划能力的系统性视角。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:1:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"方法 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:2:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"任务分解 (Task Decomposition) image.png 现实世界中的任务通常是复杂和多步骤的，直接通过单步规划过程来解决复杂任务是一项巨大挑战。任务分解通过将复杂任务分解为多个简单子任务，使得规划过程更加可行。 分解方法分类：任务分解方法主要分为两类： - 分解优先方法（Decomposition-First Methods）：首先将任务分解为子目标，然后依次为每个子目标制定计划。 - 交错分解方法（Interleaved Decomposition Methods）：在任务分解和子任务规划之间进行交错，每次只揭示当前状态的一个或两个子任务。 分解优先方法的代表工作 HuggingGPT：LLM作为控制器，负责将人类输入的任务分解为子任务，选择模型，并生成最终响应。 Plan-and-Solve：将原始的“让我们逐步思考”转变为两步提示指令：“我们首先制定计划”和“我们执行计划”。 ProgPrompt：将自然语言描述的任务转化为编码问题，将每个动作形式化为函数，每个对象表示为变量。 #### 交错分解方法的代表工作： Chain-of-Thought (CoT)：通过构建的轨迹指导LLM对复杂问题进行推理，利用LLM的推理能力进行任务分解。 Zero-shot CoT：使用“让我们逐步思考”的指令，解锁LLM的零样本推理能力。 ReAct：将推理和规划解耦，交替进行推理（思考步骤）和规划（行动步骤）。 #### 讨论 分解优先方法的优势在于创建了子任务与原始任务之间的强关联，降低了任务遗忘和幻觉的风险。但需要额外的调整机制，以避免某个步骤的错误导致整体失败。 交错分解方法可以根据环境反馈动态调整分解，提高了容错性。但对于复杂任务，过长的轨迹可能导致LLM产生幻觉，偏离原始目标。 挑战：尽管任务分解显著提高了LLM代理解决复杂任务的能力，但仍存在挑战，包括任务分解引入的额外开销、时间成本和计算成本，以及LLM的上下文长度限制。 ### 多计划选择 (Multi-Plan Selection) 由于任务的复杂性和LLM固有的不确定性，对于给定任务，LLM代理可能会生成多种不同的计划。多计划生成涉及利用生成模型解码过程中的不确定性，通过不同的采样策略来产生多个候选计划。 - Self-consistency：采用简单的直觉，即复杂问题的解决方案很少是唯一的。通过温度采样、top-k采样等策略，获得多个不同的推理路径。 - Tree-of-Thought (ToT)：提出“采样”和“提议”两种策略来生成计划。LLM在解码过程中会采样多个计划，并通过少量示例提示生成各种计划。 - Graph-of-Thought (GoT)：在ToT的基础上增加了思想的转换，支持任意思想的聚合。 - LLM-MCTS 和 RAP：利用LLM作为启发式策略函数，通过蒙特卡洛树搜索（MCTS）算法来获取多个潜在动作。 最优计划选择：在候选计划中选择最优计划时，采用了多种启发式搜索算法。 - Self-consistency：使用简单的多数投票策略，将得票最多的计划视为最优选择。 - Tree-of-Thought (ToT)：支持树搜索算法，如广度优先搜索（BFS）和深度优先搜索（DFS），使用LLM评估多个动作并选择最优动作。 - LLM-MCTS 和 RAP：也使用树结构辅助多计划搜索，但它们采用MCTS算法进行搜索。 - LLM A：利用经典的A算法协助LLM搜索，使用当前位置到目标位置的切比雪夫距离作为启发式成本函数来选择最优路径。 #### 讨论 多计划选择的可扩展性显著优势在于提供了在广阔搜索空间中更广泛探索潜在解决方案的能力。 然而，这种优势伴随着计算需求的增加，尤其是对于具有大量token计数或计算的模型，这在资源受限的情况下尤为重要。 LLM在计划评估中的作用引入了新的挑战，因为LLM在任务排名方面的表现仍在审查中，需要进一步验证和微调其在此特定情境下的能力。 LLM的随机性质为选择过程增加了随机性，可能影响所选计划的一致性和可靠性。 外部规划器辅助规划 (External Planner-Aided Planning) 尽管LLM在推理和任务分解方面展现出了强大的能力，但在面对具有复杂约束的环境时，例如数学问题求解或生成可执行动作，仍然面临挑战。 方法分类：根据引入的规划器类型，这些方法可以分为两类： - 符号规划器（Symbolic Planner）：基于形式化模型，如PDDL，使用符号推理来找到从初始状态到目标状态的最优路径。 - 神经规划器（Neural Planner）：通过强化学习或模仿学习技术训练的深度模型，针对特定领域展现出有效的规划能力。 #### 符号规划器的代表工作 - LLM+P：通过结合基于PDDL的符号规划器，使用LLM将问题组织成PDDL语言格式，并利用Fast Downward solver进行规划。 - LLM-DP：特别为动态交互环境设计，将环境反馈信息形式化为PDDL语言，并使用BFS solver生成计划。 - LLM+PDDL：在LLM生成的PDDL模型中增加手动验证步骤，并提出使用LLM生成的计划作为局部搜索规划器的初始启发式解。 - LLM+ASP：将自然语言描述的任务转换为ASP问题，然后使用ASP求解器CLINGO生成计划。 #### 神经规划器的代表工作 - CALM：结合了语言模型和基于RL的神经规划器，使用语言模型生成候选动作，然后通过DRRN策略网络重新排序以选择最优动作。 - SwiftSage：将规划过程分为快速思考和慢速思考，快速思考通过模仿学习训练的DT模型实现，慢速思考则涉及LLM基于当前状态的推理和规划。 #### 讨论 在这些策略中，LLM主要扮演支持角色，其主要功能包括解析文本反馈并提供额外的推理信息以协助规划，特别是在解决复杂问题时。 传统的符号AI系统在构建符号模型时复杂且依赖于人类专家，而LLM可以加速这一过程，有助于更快更优地建立符号模型。 符号系统的优势包括理论完备性、稳定性和可解释性。将统计AI与LLM结合，有望成为未来人工智能发展的主要趋势。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:2:1","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"反思和精炼 (Reflection and Refinement) 反思和精炼是规划过程中不可或缺的组成部分，它们增强了LLM代理规划的容错能力和错误纠正能力。由于LLM在规划过程中可能产生幻觉或在复杂问题上推理能力不足，导致错误或陷入“思维循环”，反思和总结失败有助于代理纠正错误并在后续尝试中打破循环。 Self-refine： 利用迭代过程，包括生成、反馈和精炼。在每次生成后，LLM为计划产生反馈，促进基于反馈的调整。 Reflexion： 扩展了ReAct方法，通过引入评估器来评估轨迹。LLM在检测到错误时生成自我反思，帮助纠正错误。 CRITIC： 使用外部工具，如知识库和搜索引擎，来验证LLM生成的动作。然后利用外部知识进行自我纠正，显著减少事实错误。 InteRecAgent： 使用称为ReChain的自我纠正机制。LLM用于评估由交互推荐代理生成的响应和工具使用计划，总结错误反馈，并决定是否重新规划。 LEMA： 首先收集错误的规划样本，然后使用更强大的GPT-4进行纠正。这些纠正后的样本随后用于微调LLM代理，从而在各种规模的LLaMA模型上实现显著的性能提升。 #### 讨论 自我反思策略类似于强化学习的原则，其中代理作为决策者，环境反馈触发策略网络的更新。然而，与深度强化学习通过修改模型参数来更新不同，在LLM代理中，这种更新是通过LLM自身的自我反思来实现的，最终形成文本形式的反馈。 -这些文本反馈可以作为长期和短期记忆，通过提示影响代理后续的规划输出。然而，目前还没有确凿的证据证明这种文本形式的更新最终能够使LLM代理达到特定目标。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:2:2","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"记忆增强规划 (Memory-Augmented Planning) 记忆是提升代理规划能力的关键途径，可以帮助代理从经验中学习并适应新的情境。 #### RAG-based Memory（基于RAG的记忆) - 概念：使用检索增强生成（Retrieval-Augmented Generation, RAG）技术，将记忆以文本形式存储，并在需要时检索出来辅助规划。 - 方法：如MemoryBank、TiM 和 RecMind，这些方法通过文本编码模型将记忆编码为向量，并建立索引结构，以便在规划时检索与当前任务相关的经验。 #### Embodied Memory（体现记忆）： - 概念：通过微调（fine-tuning）LLM，将代理的历史经验样本嵌入到模型参数中，从而增强记忆能力。 - 方法：如CALM 和 TDT，这些方法使用从代理与环境交互中收集的数据来微调模型，使其能够记住与规划相关的信息，并在规划任务中表现更好。 #### 记忆更新方式： - RAG-based：提供了实时、低成本的外部记忆更新，但依赖于检索算法的准确性。 - Finetuning：提供了更大的记忆容量，但记忆更新成本较高，并且在保留细节方面存在挑战。 讨论： 记忆增强的LLM代理在规划中表现出更强的增长潜力和容错能力，但记忆生成在很大程度上依赖于LLM自身的生成能力。 通过自我生成的记忆来提升较弱LLM代理的能力仍然是一个具有挑战性的领域。 #### 挑战 尽管记忆增强LLM代理在规划方面表现出优势，但它们在记忆生成上仍然面临挑战，特别是在自我生成记忆方面。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:2:3","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"评估 (Evaluation) 评估代理的规划能力是研究领域中的一个关键问题。作者调查了几种主流的基准测试方法，并将它们分为以下几类： 交互式游戏环境（Interactive Gaming Environments）： 提供基于代理动作的实时多模态反馈，如文本和视觉反馈。 例如Minecraft，代理需要收集材料制作工具以获得更多奖励，常用评价指标是代理创建的工具数量。 ### 基于文本的交互环境（Text-based interactive environments）： 代理位于用自然语言描述的环境中，动作和位置有限。 常用评价指标是成功率或获得的奖励，例如ALFWorld和ScienceWorld。 ### 交互式检索环境（Interactive Retrieval Environments）： 模拟人类在现实生活信息检索和推理的过程。 代理可以与搜索引擎和其他网络服务交互，通过搜索关键词或执行点击、前进、后退操作来获取更多信息，完成问答任务或信息检索任务。 ### 交互式编程环境（Interactive Programming Environments）： 模拟程序员与计算机之间的交互，测试代理解决计算机相关问题的规划能力。 代理需要与计算机交互，通过编写代码或指令来解决问题。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:3:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"实验 作者在四个基准测试上进行了实验，以验证代表性方法的性能。这些基准测试包括==ALFWorld==、==ScienceWorld==、==HotPotQA==和==FEVER==，涵盖了交互式游戏和问答基准测试。 实验结果显示，性能随着成本的增加而提高，表明更详细的思考（即消耗更多的token）可以带来性能上的提升。 另外，对于复杂任务，示例（例如Zero-shot CoT和Few-shot CoT）对于LLM进一步理解任务至关重要。 反思（Reﬂexion）在提高成功率方面发挥了关键作用，尤其是在复杂任务中，显示了LLM具备错误纠正能力。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:4:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"讨论： 现有的基准测试大多依赖于任务的最终完成状态，缺乏细粒度的逐步评估。 环境反馈通常是规则化的、简化的，并且与现实世界场景有距离。 未来的发展方向可能包括利用高智能模型如LLM来设计更现实的评估环境。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:4:1","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["survey","reading","Planning"],"content":"结论和未来方向 (Conclusions and Future Directions) 进展总结：自LLM展现出智能以来，使用LLM增强代理规划能力的研究受到了越来越多的关注。作者概述了主要的研究方向，并在前文中对各种方法进行了详细比较和分析。 实验结果：作者在四个基准测试上进行了实验，比较了几种代表性方法的有效性，并指出随着投入成本的增加，性能也随之提高。 ### 挑战 幻觉问题（Hallucinations）：LLM在规划过程中可能会产生幻觉，导致非理性的计划或无法遵循复杂指令。 生成计划的可行性：与基于符号的人工智能相比，LLM在优化过程中可能难以遵守复杂约束，导致生成的计划缺乏可行性。 计划的效率：现有LLM代理的规划过程可能没有考虑生成计划的效率，未来的发展可能需要引入额外的效率评估模块。 ### 未来方向： 多模态环境反馈：考虑集成多模态大型模型的发展，并重新审视相关的规划策略，以处理包括图像、音频等在内的多模态反馈。 细粒度评估：利用高智能模型如LLM设计更现实的评估环境，提供更细致的逐步评估，以更好地模拟现实世界场景。 ","date":"2024-12-20","objectID":"/agent-planning%E7%BB%BC%E8%BF%B0/:5:0","tags":["task_planning","survey"],"title":"agent planning综述","uri":"/agent-planning%E7%BB%BC%E8%BF%B0/"},{"categories":["","coding","torch"],"content":"gather 参数： input (Tensor) – the source tensor dim (int) – the axis along which to index index (LongTensor) – the indices of elements to gather out (Tensor_,__optional_) – the destination tensor sparse_grad (bool,optional) – If True, gradient w.r.t. input will be a sparse tensor. \u003e gather操作是scatter操作的逆操作，如果说scatter是根据index和src求self(input)，那么gather操作是根据self(input)和index求src。具体来说gather操作是根据index指出的索引，沿dim指定的轴收集input的值。 out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 对于gather操作来说，有三个约束需要满足： （1）对于所有的维度d != dim，有input.size(d) == index.size(d)，对于维度dim来说，有index.size(d) \u003e= 1； （2）张量out的维度大小必须和index相同； （3）和scatter一样，index中的索引值必须在input.size(dim)范围内。 ### example ### code example import torch t = torch.Tensor([[1, 2], [3, 4]]) # t = 1 2 # 3 4 index = torch.LongTensor([[0, 0], [1, 0]]) # index = 0 0 # 1 0 # dim = 0 : [[1,2],[3,2]] # dim = 1 : [[1,1],[4,3]] # index = 0 # 1 # dim = 0 : [[1],[3]] # dim = 1 : [[1],[4]] # index = 0 1 # dim = 0 : [[1, 4]] # dim = 1 : [[1, 2]] ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:1:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"scatter Writes all values from the tensor into at the indices specified in the tensor. For each value in , its output index is specified by its index in for and by the corresponding value in for .`src``self``index``src``src``dimension != dim``index``dimension = dim` For a 3-D tensor, is updated as:`self` 参数： - dim (int) – the axis along which to index - index (LongTensor) – the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity - src (Tensor) – the source element(s) to scatter, incase value is not specified - value (float) – the source element(s) to scatter, incase src is not specified self[index[i][j][k]][j][k] = src[i][j][k] # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k] # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k] # if dim == 2 看了上面这个操作就理解了。 由此可以得出以下约束： 1. 张量self，张量index和张量src的维度数量必须相同（即三者的.dim()必须相等，注意不是维度大小）； 2. 对于每一个维度d，有index.size(d)\u003c=src.size(d)； 3. 对于每一个维度d，如果d!=dim，有index.size(d)\u003c=self.size(d)； 对于index也有一些约束： 1. 张量index中的每一个值大小必须在[0, self.size(dim)-1]之间； 2. 张量index沿dim维的那一行中所有值都必须是唯一的（弱约束，违反不会报错，但是会造成没有意义的操作）。 ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:2:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"example ### code example import torch a = torch.arange(10).reshape(2,5).float() print(f\"a: \\n{a}\") b = torch.zeros(3, 5) print(f\"b: \\n{b}\") b_= b.scatter(dim=0, index=torch.LongTensor([[1, 2, 1, 1, 2], [2, 0, 2, 1, 0]]), src=a) print(f\"b_: \\n{b_}\") # tensor([[0, 6, 0, 0, 9], # [0, 0, 2, 8, 0], # [5, 1, 7, 0, 4]]) ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:2:1","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"scatter_add_ 这个函数和scatter基本上没有任何区别，区别在于上图中的对于self中同一位置的填入是随机的，self[3,0]不确定是7还是9，self[0,1]不确定是8还是10，但是使用scatter_add就将所有即将填入同一位置的数相加，例子如下： ### example ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:3:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","coding","torch"],"content":"参考 https://zhuanlan.zhihu.com/p/158993858 ","date":"2024-12-20","objectID":"/gather%E5%92%8Cscatter/:4:0","tags":["coding","gather","scatter"],"title":"gather和scatter","uri":"/gather%E5%92%8Cscatter/"},{"categories":["","LLM","NLP"],"content":"LLaMA介绍 LLaMA 是目前为止，效果最好的开源 LLM 之一。 论文的核心思想：相比于GPT，更小的模型+更多的训练数据**也可以获得可比的效果 基于更多 tokens 的训练集，在各种推理预算下，训练出性能最佳的一系列语言模型，称为 LLaMA，参数范围从 7B 到 65B 不等，与现有最佳 LLM 相比，其性能是有竞争力的。比如，LLaMA-13B 在大多数基准测试中优于 GPT-3，尽管其尺寸只有 GPT-3 的十分之一。作者相信，LLaMA 将有助于使 LLM 的使用和研究平民化，因为它可以在单个 GPU 上运行！在规模较大的情况下，LLaMA-65B 也具有与最佳大型语言模型（如 Chinchilla 或 PaLM-540B）相竞争的能力。 LLaMA1、2的主要差别在训练上下文长度、训练token数、注意力机制以及对齐方法上。 模型 训练长度 分词器 词表大小 位置编码 激活层 标准化 训练token数 链接 精度 注意力机制 有无chat版本 Alignment LLaMA 2,048 BPE（Sentence-Piece） 32k ROPE SwiGLU 基于 RMSNorm 的 Pre-Norm 1万亿(6.7B,13B) 1.4万亿（32.5B,65.2B） http://arxiv.org/abs/2302.13971 fp16 MHA 0 LLaMA2 4,096 同上 32k ROPE 同上 同上 2万亿 https://arxiv.org/abs/2307.09288 bf16 34B,70B GQA, 其他MHA 1 SFT+RLHF(拒绝采样+PPO) （表来自LLaMA家族） LLaMA1 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:0:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"训练数据 image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:1:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"训练参数 image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:2:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"RMSnorm 与 Layer Norm 相比，RMS Norm的主要区别在于去掉了减去均值的部分，计算公式为： \\[\\overline{a}_{i}=\\frac{a_{i}}{RMS(a)}\\] 其中 \\[RMS(a)=\\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}a_{i}^{2}} \\\\ \\] 此外RMSNorm 还可以引入可学习的缩放因子g，从而得到 \\[\\overline{a}_i=\\frac{a_i}{RMS(\\boldsymbol{a})}g_i\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"Pre-norm和Post-norm 注意其使用的是Pre-norm结构，与Post-norm结构差异如下： 关于Pre Norm的效果和Post Norm效果差异，相关分析在这两篇文章中： 模型优化漫谈：BERT的初始标准差为什么是0.02？ 为什么Pre Norm的效果不如Post Norm？ 总结来说就是Pre-norm加深的是模型的宽度，而不是深度，从而导致训练效果不如Post-norm，但可以缓解Post-norm的梯度消失。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaRMSNorm(nn.Module): def __init__(self, hidden_size, eps=1e-6): \"\"\" LlamaRMSNorm is equivalent to T5LayerNorm \"\"\" super().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.variance_epsilon = eps # eps 防止取倒数之后分母为0 def forward(self, hidden_states): input_dtype = hidden_states.dtype variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True) hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) # rsqrt 即sqrt后取倒数 # weight 是末尾乘的可训练参数, 即g_i return (self.weight * hidden_states).to(input_dtype) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:3:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"RoPE RoPE 的核心思想是“通过绝对位置编码的方式实现相对位置编码”，可以说是具备了绝对位置编码的方便性，同时可以表示不同 token 之间的相对位置关系。RoPE 是将位置编码和 query或者key进行相乘。 \\[\\begin{bmatrix}\\cos m\\theta_0\u0026-\\sin m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\\\sin m\\theta_0\u0026\\cos m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\cos m\\theta_1\u0026-\\sin m\\theta_1\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\sin m\\theta_1\u0026\\cos m\\theta_1\u0026\\cdots\u00260\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\cos m\\theta_{d/2-1}\u0026-\\sin m\\theta_{d/2-1}\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\sin m\\theta_{d/2-1}\u0026\\cos m\\theta_{d/2-1}\\end{bmatrix}\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\] 由于矩阵太稀疏，会造成浪费，因此计算时是这么做的： \\[\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix}\\] 此外，角度的计算方式如下： \\[\\theta_j=10000^{-2j/d},j\\in[1,2,\\dots,d/2]\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:4:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaRotaryEmbedding(torch.nn.Module): def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None): super().__init__() inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim)) self.register_buffer(\"inv_freq\", inv_freq) # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. # Keep the logic here just in case. if seq_len \u003e self.max_seq_len_cached: self.max_seq_len_cached = seq_len t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False) return ( self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype), self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype), ) def rotate_half(x): \"\"\"Rotates half the hidden dims of the input.\"\"\" x1 = x[..., : x.shape[-1] // 2] x2 = x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) def apply_rotary_pos_emb(q, k, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:4:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"SwiGLU \\[\\begin{aligned} \\mathrm{FFN}_{\\mathrm{SwiGLU}}(x,W,V,W_{2})\u0026=\\mathrm{SwiGLU}(x,W,V)W_{2}\\\\\\mathrm{SwiGLU}(x,W,V)\u0026=\\mathrm{Swish}_{\\beta}(xW)\\otimes xV\\\\\\mathrm{Swish}_{\\beta}(x)\u0026=x\\sigma(\\beta x) \\end{aligned}\\] ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:5:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class LlamaMLP(nn.Module): def __init__( self, hidden_size: int, intermediate_size: int, hidden_act: str, ): super().__init__() self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False) self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False) self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False) # config 中 hidden_act = 'silu' # 'silu' 和 'swish' 对应的激活函数均为：SiLUActivation # https://github.com/huggingface/transformers/blob/717dadc6f36be9f50abc66adfd918f9b0e6e3502/src/transformers/activations.py#L229 self.act_fn = ACT2FN[hidden_act] def forward(self, x): # 对应上述公式的 SwiGLU return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:5:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"实验结果 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"常识推理任务 image.png LLaMA - 13B模型虽然比GPT - 3小10倍，但在大多数基准上也优于GPT - 3。 除BoolQ外，LLaMA - 65B在所有报告的基准上都优于Chinchilla-70B。 除了在BoolQ和WinoGrande上，LLaMA-65B在所有地方都超过了PaLM540B。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"阅读理解任务 image.png 可以看到，LLaMA-13B比GPT-3高出了几个百分点。 LLaMA-65B的表现已经接近甚至超越PaLM-540B的表现。 LLaMA2 Llama1只做了预训练，Llama2做了预训练+SFT+RLHF ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:6:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"KV Cache image.png LLM推理过程分为Prefill和Decode两个阶段。 Prefill阶段会对Prompt中所有的token做并行计算，得到Prompt中所有Tokens的KV Cache以及计算得到生成的第一个Token。Prompt阶段Token计算得到的KV Cache会保存下来，留给Decode阶段复用。 Decode阶段是一个自回归过程，每decode一个新的Token，都需要用到所有之前计算得到的KV Cache来计算当前query token的Attention。因此，当输出长度越来越大或者context很长时，KV Cache将会占用大量的显存。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"使用KV cache时位置信息怎么注入？ 初次学习KV cache时，虽然原理比较简单易懂，但是对于后续的输入只有一个token这里产生了些许困惑，后续只输入一个token的话，位置编码该怎么办呢？于是我比较简单粗暴地猜测位置index随着推理不断更新，当时翻了各种资料也没有得到解释，后面翻了翻llama的源码，发现我的猜测还真是正确的。 def forward(self, tokens: torch.Tensor, start_pos: int): \"\"\" Perform a forward pass through the Transformer model. Args: tokens (torch.Tensor): Input token indices. start_pos (int): Starting position for attention caching. Returns: torch.Tensor: Output logits after applying the Transformer model. \"\"\" _bsz, seqlen = tokens.shape h = self.tok_embeddings(tokens) self.freqs_cis = self.freqs_cis.to(h.device) freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] mask = None if seqlen \u003e 1: mask = torch.full( (seqlen, seqlen), float(\"-inf\"), device=tokens.device ) mask = torch.triu(mask, diagonal=1) # When performing key-value caching, we compute the attention scores # only for the new sequence. Thus, the matrix of scores is of size # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for # j \u003e cache_len + i, since row i corresponds to token cache_len + i. mask = torch.hstack([ torch.zeros((seqlen, start_pos), device=tokens.device), mask ]).type_as(h) for layer in self.layers: h = layer(h, start_pos, freqs_cis, mask) h = self.norm(h) output = self.output(h).float() return output 可以看到forward函数中的start_pos参数代表着位置信息，freqs_cis是实现RoPE位置编码需要用到的。 注意 freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]这一行，即是实现了rope相对位置编码的kv cache的核心。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"代码 class Attention(nn.Module): # ... self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], ): # 假设当前x为(1, 1, dim)，也就是上一个预测的token # self-attention的输入，标准的(bs, seqlen, hidden_dim) bsz, seqlen, _ = x.shape # 计算当前token的qkv # q k v分别进行映射，注意这里key, value也需要先由输入进行映射再和kv_cache里面的key, value进行拼接 xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim) # 对当前输入的query和key进行RoPE，注意kv_cache里面的key已经做过了RoPE xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) # 缓存当前token的kv self.cache_k = self.cache_k.to(xq) self.cache_v = self.cache_v.to(xq) self.cache_k[:bsz, start_pos: start_pos + seqlen] = xk self.cache_v[:bsz, start_pos: start_pos + seqlen] = xv # 取出前seqlen个token的kv缓存 # 取出全部缓存的key和value（包括之前在cache里面的和本次输入的），作为最终的key和value keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] # 将kv重复填充，使kv和q的头数个数相同 # repeat k/v heads if n_kv_heads \u003c n_heads，对齐头的数量 keys = repeat_kv(keys, self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) values = repeat_kv(values, self.n_rep) # (bs, cache_len + seqlen, n_local_heads, head_dim) # 计算当前token的attention score，，注意mask需要加上，另外维度要对应上 xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim) keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim) scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_local_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1).type_as(xq) output = torch.matmul(scores, values) # (bs, n_local_heads, seqlen, head_dim) output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) return self.wo(output) ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:7:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"MQA\u0026GQA image.png ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"为什么不继续使用MHA？ 标准的mha中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"MQA 标准的MHA中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 MQA比较极端，只保留一个KV Head，多个Query Heads共享相同的KV Head。这相当于不同Head的Attention差异，全部都放在了Query上，需要模型仅从不同的Query Heads上就能够关注到输入hidden states不同方面的信息。这样做的好处是，极大地降低了KV Cache的需求，但是会导致模型效果有所下降。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:2","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"GQA GQA就是在MHA和MQA之间做了一个平衡。对query heads进行分组，分成几组就对应多少个kv heads，然后每一组内的query Heads共享相同的KV head。 GQA可以在减少计算量和KV Cache同时确保模型效果不受到大的影响。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:8:3","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"SFT 监督微调（Supervised Fine-Tuning, SFT）是对已经预训练的模型进行特定任务的训练，以提高其在该任务上的表现。预训练模型通常在大量通用数据上进行训练，学到广泛的语言知识和特征。在SFT过程中，利用特定任务的数据，对模型进行进一步调整，使其更适合该任务。 SFT数据一般就是\u003cprompt, response\u003e数据对。在训练方式上和pretrain没有任何区别，即得到当前token对应的logit，以next token作为标签计算交叉熵损失。 pretrain 是在背书，纯粹的学习知识；sft 则是在做题，学习的是指令 follow 能力。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:9:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"一些要点 少量高质量数据集训练模型的效果，要好于大量低质量数据集的训练效果。分析数据和清洗数据就是 sft 阶段 90% 的工作量。 sft 会让模型见到最重要的 eos_token，pretrain 模型因为没见过该 token 而无法停止生成。 sft 的 prompt 不做 loss，但这并不是说它不能做 loss。主要原因是 prompt 的同质化比较严重，不做 loss_mask 的话，同样的一句话会被翻来覆去的学，但如果你能保证你的每条 prompt 都是独一无二的，就完全可以省去 prompt 的 loss_mask 环节。 为了提高模型训练效率，将多组数据进行拼接，尽量填满4096。但对于分类任务会出现问题，详见https://zhuanlan.zhihu.com/p/809229182。 经过一通分析后，我们发现，新的训练方式改变了短 answer 数据的 loss 占比，毕竟模型在计算 loss 的时候，是先算一个句子内每个 token 的 平均 loss，再算一个 batch_size 内的平均 loss。 分类任务的 answer 通常只有 1 个 token：不 concat 的时候，它的 loss 贡献就是 1 / batch_size；concat 的时候，它就需要先和别的 answer 的 token 算平均 loss，再贡献 1 / batch_size。 这也就是说，采用 llama2 提到的 先 concat 语料再做 sft 训练，会对短 answer 数据很不公平，也就更容易造成短 answer 数据的欠拟合，pretrain 由于所有 token 都算 loss 则没有这个现象。最终，我们通过上采样短 answer 数据，成功的避免了分类任务的效果下滑。 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:9:1","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","NLP"],"content":"实验结果 - Llama 2模型优于Llama 1模型。 - Llama 2-70B比Llama 1-65B在MMLU和BBH上的结果分别提高了≈5和≈8个点。 - Llama 2-7B和30B模型在除代码基准以外的所有类别上都优于相应大小的MPT模型。 - Llama 2-7B和34B在所有类别的基准测试集上都优于Falcon-7B和40B模型。 参考 [KV Cache优化]🔥MQA/GQA/YOCO/CLA/MLKV笔记: 层内和层间KV Cache共享 - 知乎 (zhihu.com) Transformers KV Caching Explained | by João Lages | Medium https://zhuanlan.zhihu.com/p/679640407 LLaMA家族 https://zhuanlan.zhihu.com/p/809229182 ","date":"2024-09-26","objectID":"/llama%E7%B3%BB%E5%88%97/:10:0","tags":["LLM","NLP","llama"],"title":"llama系列","uri":"/llama%E7%B3%BB%E5%88%97/"},{"categories":["","LLM","generate"],"content":"wLLM解码时采用的自回归采样，其过程如下： 小模型使用前缀作为输入，将输出结果处理+归一化成概率分布后，采样生成下一个token。 将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。 将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、top-k和top-n等 # 贪心搜索 直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复 # 随机采样 按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。 # beam search 维护一个大小为 k 的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k 个单词，然后保留总概率最高的 k 个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。 # top-k 选取前k个token，然后再重新生成概率分布，再进行抽样 它可以与其他解码策略结合使用，例如温度调节（Temperature Scaling）、重复惩罚（Repetition Penalty）、长度惩罚（Length Penalty）等，来进一步优化生成的效果。 代码: import torch from labml_nn.sampling import Sampler # Top-k Sampler class TopKSampler(Sampler): # k is the number of tokens to pick # sampler is the sampler to use for the top-k tokens # sampler can be any sampler that takes a logits tensor as input and returns a token tensor; e.g. `TemperatureSampler`. def __init__(self, k: int, sampler: Sampler): self.k = k self.sampler = sampler # Sample from logits def __call__(self, logits: torch.Tensor): # New logits filled with −∞; i.e. zero probability zeros = logits.new_ones(logits.shape) * float('-inf') # Pick the largest k logits and their indices values, indices = torch.topk(logits, self.k, dim=-1) # Set the values of the top-k selected indices to actual logits. # Logits of other tokens remain −∞ zeros.scatter_(-1, indices, values) # Sample from the top-k logits with the specified sampler. return self.sampler(zeros) top-p top-k 有一个缺陷，那就是“k 值取多少是最优的？”非常难确定。于是出现了动态设置 token 候选列表大小策略——即核采样（Nucleus Sampling）。 top-p 采样的思路是，在每一步，只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的单词。这种方法也被称为核采样（nucleus sampling），因为它只关注概率分布的核心部分，而忽略了尾部部分。例如，如果 p=0.9，那么我们只从累积概率达到 0.9 的最小单词集合中选择一个单词，而不考虑其他累积概率小于 0.9 的单词。这样可以避免采样到一些不合适或不相关的单词，同时也可以保留一些有趣或有创意的单词。 import torch from torch import nn from labml_nn.sampling import Sampler class NucleusSampler(Sampler): \"\"\" ## Nucleus Sampler \"\"\" def __init__(self, p: float, sampler: Sampler): \"\"\" :param p: is the sum of probabilities of tokens to pick $p$ :param sampler: is the sampler to use for the selected tokens \"\"\" self.p = p self.sampler = sampler # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits self.softmax = nn.Softmax(dim=-1) def __call__(self, logits: torch.Tensor): \"\"\" Sample from logits with Nucleus Sampling \"\"\" # Get probabilities $P(x_i | x_{1:i-1})$ probs = self.softmax(logits) # Sort probabilities in descending order sorted_probs, indices = torch.sort(probs, dim=-1, descending=True) # Get the cumulative sum of probabilities in the sorted order cum_sum_probs = torch.cumsum(sorted_probs, dim=-1) # Find the cumulative sums less than $p$. nucleus = cum_sum_probs \u003c self.p # Prepend ones so that we add one token after the minimum number # of tokens with cumulative probability less that $p$. nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1) # Get log probabilities and mask out the non-nucleus sorted_log_probs = torch.log(sorted_probs) sorted_log_probs[~nucleus] = float('-inf') # Sample from the sampler sampled_sorted_indexes = self.sampler(sorted_log_probs) # Get the actual indexes res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1)) # return res.squeeze(-1) Temperature采样 详见温度超参数 speculative decoding 大型语言模型（LLM）的推理通常需要使用自回归采样。它们的推理过程相当缓慢，需要逐个token地进行串行解码。因此，大型模型的推理过程往往受制于访存速度，生成每个标记都需要将所有参数从存储单元传输到计算单元，因此内存访问带宽成为严重的瓶颈。 为了解决推理速度慢的问题，已经进行了许多针对推理的工程优化，例如改进的计算核心实现、多卡并行计算、批处理策略等等。然而，这些方法并没有从根本上解决LLM解码过程是受制于访存带宽的问题。 投机采样是一种可以从根本上解码计算访存比的方法，保证和使用原始模型的采样分布完全相同。它使用两个模型：一个是原始目标模型，另一个是比原始模型小得多的近似模型。近似模型用于进行自回归串行采样，而大型模型则用于评估采样结果。解码过程中，某些token的解码相对容易，某些token的解码则很困难。因此，简单的token生成可以交给小型模型处理，而困难的token则交给大型模型处理。这里的小型模型可以采用与原始模型相同的结构，但参数更少，或者干脆使用n-gram模型。小型模型不仅计算量较小，更重要的是减少了内存访问的需求。 ## 采样过程 投机采样过程如下： 用小模型Mq做自回归采样连续生成 γ 个tokens。 把生成的γ个tokens和前缀拼接一起送进大模Mp执行一次forwards。 使用大、小模型logits结果做比对，如果发现某个token小模型生成的不好，重新采样这个token。重复步骤1。 如果小模型生成结果都满意，则用大模型采样下一个token。重复步骤1。 第2步，将γ个tokens和前缀拼成一起作为大模型输入，和自回归相比，尽管计算量一样，但是γ个tokens可以同时参与计算，计算访存比显著提升。 第3步，如何评价一个token生成的不好？如果q(x) \u003e p(x)（p，q表示在大小模型采样概率，也就是logits归一化后的概率分布","date":"2024-09-05","objectID":"/generate%E7%9B%B8%E5%85%B3/:0:0","tags":["LLM","generate"],"title":"frequency_penalty\u0026presence_penalty","uri":"/generate%E7%9B%B8%E5%85%B3/"},{"categories":["","LLM"],"content":"线性Transformer \\[V_i'=\\frac{\\sum_{j=1}^N sim(Q_i,K_j)V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\] 注意下标i。 其中 \\[sim(Q_{i},K_{j})=\\phi(Q_{i},K_{j})\\] 此时有： \\[V_{i}^{\\prime}=\\frac{\\phi(Q_{i})\\sum_{j=1}^{i}\\phi(K_{j})^{T}V_{j}}{\\phi(Q_{i})\\sum_{j=1}^{i}\\phi(K_{j})^{T}}\\] 注意可以将\\(\\phi(Q_{i})\\)提出来。 原始Transformer的计算复杂度随序列长N呈二次方增长，这是因为attention的计算包含两层for循环，外层是对于每一个Query，我们需要计算它对应token的新表征；内层for循环是为了计算每一个Query对应的新表征，需要让该Query与每一个Key进行计算。 所以外层是 for q in Queries，内层是 for k in Keys。Queries数量和Keys数量都是N，所以复杂度是 O(N^2) 。而Linear Transformer，它只有外层for q in Queries这个循环了。因为求和项的计算与i无关，所以所有的 Qi 可以共享求和项的值。换言之，求和项的值可以只计算一次，然后存在内存中供所有 Qi 去使用。所以Linear Transformer的计算复杂度是O(N) 。 Attention Free Transformer \\[V_i'=\\sigma(Q_i)\\odot\\frac{\\sum_{j-1}^iexp(K_j+w_{i,j})\\odot V_j}{\\sum_{j=1}^iexp(K_j+w_{i,j})}\\] 其中σ是sigmoid函数\\(^{+};\\odot\\)是逐元素相乘 (element-wise product); wi,j是待训练的参数。 AFT采用的形式和上面的Linear Transformer不一样。首先是attention score, Linear Transformer仍然是同Transformer一样，为每一个Value赋予一个weight。而AFT会为每个 dimension\\(^{+}\\)赋予weight。换言之，在Linear Transformer中，同一个Value中不同dimension的weight是一致的；而AFT同一Value中不同dimension的weight不同(\\(w_{i,j}\\))。此外，attention score的计算也变得格外简单，用K去加一个可训练的bias\\(^{+}\\)。Q的用法很像一个gate。 可以很容易仿照公式(5)把AFT也写成递归形式，这样容易看出，AFT也可以像Linear Transformer,在inference阶段复用前面时刻的计算结果，表现如RNN形式，从而相比于 Transformer变得更加高效。 RWKV RWKV的特点如下： 改造AFT，通过Liner Transformer变换将self-attention复杂度由O(N^2)降为 O(N) 。 保留AFT简单的“attention”形式和Sequential Decoding，具有RNN表现形式。 ","date":"2024-09-04","objectID":"/rwkv/:0:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"Time-Mixing image.png def time_mixing(x, last_x, last_num, last_den, decay, bonus, mix_k, mix_v, mix_r, Wk, Wv, Wr, Wout): k = Wk @ ( x * mix_k + last_x * (1 - mix_k) ) v = Wv @ ( x * mix_v + last_x * (1 - mix_v) ) r = Wr @ ( x * mix_r + last_x * (1 - mix_r) ) wkv = (last_num + exp(bonus + k) * v) / \\ (last_den + exp(bonus + k)) rwkv = sigmoid(r) * wkv num = exp(-exp(decay)) * last_num + exp(k) * v den = exp(-exp(decay)) * last_den + exp(k) return Wout @ rwkv, (x,num,den) ","date":"2024-09-04","objectID":"/rwkv/:1:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"Channel-Mixing image.png def channel_mixing(x, last_x, mix_k, mix_r, Wk, Wr, Wv): k = Wk @ ( x * mix_k + last_x * (1 - mix_k) ) r = Wr @ ( x * mix_r + last_x * (1 - mix_r) ) vk = Wv @ np.maximum(k, 0)**2 return sigmoid(r) * vk, x 参考 How the RWKV language model works | The Good Minima (johanwind.github.io) ","date":"2024-09-04","objectID":"/rwkv/:2:0","tags":["LLM","rwkv"],"title":"rwkv","uri":"/rwkv/"},{"categories":["","LLM"],"content":"证明 核心思想就是找到一个转换，可以通过点积操作将位置信息注入，即： \\[\u003cf_q\\left(x_m,m\\right),f_k\\left(x_n,n\\right)\u003e=g\\left(x_m,x_n,m-n\\right)\\] 而通过复数的一些性质，找到了满足上述操作的转换： \\[\\begin{aligned} \u0026f_{q}\\left(\\boldsymbol{x}_{m},m\\right)=\\left(\\boldsymbol{W}_{q}\\boldsymbol{x}_{m}\\right)e^{im\\theta} \\\\ \u0026f_{k}\\left(\\boldsymbol{x}_{n},n\\right)=\\left(\\boldsymbol{W}_{k}\\boldsymbol{x}_{n}\\right)e^{in\\theta} \\\\ \u0026g\\left(\\boldsymbol{x}_{m},\\boldsymbol{x}_{n},m-n\\right)=\\mathrm{Re}\\left[\\left(\\boldsymbol{W}_{q}\\boldsymbol{x}_{m}\\right)\\left(\\boldsymbol{W}_{k}\\boldsymbol{x}_{n}\\right)^{*}e^{i(m-n)\\theta}\\right] \\end{aligned}\\] 可以发现g函数中存在相对位置信息。 欧拉公式：\\(e^{ix}=\\cos x+i\\sin x\\) \\[\\begin{aligned}\u0026\\text{基于上面面1点结论,可知}\\\\\u0026f_{q}\\left(x_{m},m\\right)=\\left(W_{q}x_{m}\\right)e^{im\\theta}=q_{m}e^{im\\theta}\\\\\u0026\\text{然后将}q_{m\\text{表示成复数形式（torch.view\\_as\\_complex）,可得}}\\\\\u0026q_{m}=\\left[q_{m}^{(1)},q_{m}^{(2)}\\right]=\\left[q_{m}^{(1)}+iq_{m}^{(2)}\\right]\\\\\u0026\\text{从而有}\\\\\u0026f_{q}\\left(x_{m},m\\right)=q_{m}e^{im\\theta}=\\left[q_{m}^{(1)}+iq_{m}^{(2)}\\right]e^{im\\theta}\\\\\u0026\\text{基于欧拉公式,可知}f_{q}\\left(x_{m},m\\right)_{\\text{即是两个复数相乘}}\\\\\u0026f_{q}\\left(x_{m},m\\right)=q_{m}e^{im\\theta}=\\left(q_{m}^{(1)}+iq_{m}^{(2)}\\right)*\\left(\\cos(m\\theta)+i\\sin(m\\theta)\\right)\\end{aligned}\\] 根据复数的计算，可得： \\[\\begin{aligned}q_{m}e^{im\\theta}=\\left(q_{m}^{(1)}+iq_{m}^{(2)}\\right)*(\\cos(m\\theta)+i\\sin(m\\theta))\\\\=\\left(q_{m}^{(1)}\\cos(m\\theta) -q_{m}^{(2)}\\sin(m\\theta)\\right)+i\\left(q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right)\\end{aligned}\\] 再将结果写成向量的形式，即： \\[q_{m}e^{im\\theta}=\\left[q_{m}^{(1)}\\cos(m\\theta)-q_{m}^{(2)}\\sin(m\\theta),q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right]\\] 即是query向量乘了一个旋转矩阵： \\[\\begin{gathered} f_{q}\\left(x_{m},m\\right)=\\left(W_{q}x_{m}\\right)e^{im\\theta}=q_{m}e^{im\\theta} \\\\ =\\left|q_{m}^{(1)}\\cos(m\\theta)-q_{m}^{(2)}\\sin(m\\theta),q_{m}^{(2)}\\cos(m\\theta)+q_{m}^{(1)}\\sin(m\\theta)\\right| \\\\ =\\left(\\begin{array}{cc}{\\cos(m\\theta)}\u0026{-\\sin(m\\theta)}\\\\{\\sin(m\\theta)}\u0026{\\cos(m\\theta)}\\end{array}\\right)\\left(\\begin{array}{c}{q_{m}^{(1)}}\\\\{q_{m}^{(2)}}\\end{array}\\right) \\end{gathered}\\] 后续的证明看一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)-CSDN博客 将二维推广，有： \\[\\boldsymbol{R}_{\\Theta,m}^{d}=\\underbrace{\\left(\\begin{array}{ccccccc}{\\cos m\\theta_{0}}\u0026{-\\sin m\\theta_{0}}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{\\sin m\\theta_{0}}\u0026{\\cos m\\theta_{0}}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{0}\u0026{0}\u0026{\\cos m\\theta_{1}}\u0026{-\\sin m\\theta_{1}}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{0}\u0026{0}\u0026{\\sin m\\theta_{1}}\u0026{\\cos m\\theta_{1}}\u0026{\\cdots}\u0026{0}\u0026{0}\\\\{\\vdots}\u0026{\\vdots}\u0026{\\vdots}\u0026{\\vdots}\u0026{\\ddots}\u0026{\\vdots}\u0026{\\vdots}\\\\{0}\u0026{0}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{\\cos m\\theta_{d/2-1}}\u0026{-\\sin m\\theta_{d/2-1}}\\\\{0}\u0026{0}\u0026{0}\u0026{0}\u0026{0}\u0026{\\cdots}\u0026{\\sin m\\theta_{d/2-1}}\u0026{\\cos m\\theta_{d/2-1}}\\end{array}\\right)}\\] 则计算旋转编码，即有： \\[\\begin{bmatrix}\\cos m\\theta_0\u0026-\\sin m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\\\sin m\\theta_0\u0026\\cos m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\cos m\\theta_1\u0026-\\sin m\\theta_1\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\sin m\\theta_1\u0026\\cos m\\theta_1\u0026\\cdots\u00260\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\cos m\\theta_{d/2-1}\u0026-\\sin m\\theta_{d/2-1}\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\sin m\\theta_{d/2-1}\u0026\\cos m\\theta_{d/2-1}\\end{bmatrix}\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\] 由于矩阵太稀疏，会造成浪费，因此计算时是这么做的： \\[\\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix}\\] 此外，角度的计算方式如下： \\[\\theta_j=10000^{-2j/d},j\\in[1,2,\\dots,d/2]\\] 代码 ","date":"2024-08-31","objectID":"/rope/:0:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["","LLM"],"content":"llama实现 llama实现比较简单，但是一开始很不容易理解，实现如下： def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0): \"\"\" Precompute the frequency tensor for complex exponentials (cis) with given dimensions. This function calculates a frequency tensor with complex exponentials using the given dimension 'dim' and the end index 'end'. The 'theta' parameter scales the frequencies. The returned tensor contains complex values in complex64 data type. Args: dim (int): Dimension of the frequency tensor. end (int): End index for precomputing frequencies. theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0. Returns: torch.Tensor: Precomputed frequency tensor with complex exponentials. \"\"\" # dim = 128 # end = 4096 # torch.arange(0, dim, 2) [0, 2, 4, 6, 8, 10,..., 124, 126] 共64个 # torch.arange(0, dim, 2)[: (dim // 2)] 保证是64个 freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) # rope中的角度 # freqs = [1/10000.0^(0/128), 1/10000.0^(2/128), 1/10000.0^(4/128), ..., 1/10000.0^(126/128)] t = torch.arange(end, device=freqs.device) # postition idx # t = [0, 1, 2, ..., 4095] freqs = torch.outer(t, freqs).float() # type: ignore # freqs 得到 freqs和t的笛卡尔积，维度为（4096，64） # freqs = [[0, 0, 0,..., 0], # [1/10000.0^(0/128), 1/10000.0^(2/128), 1/10000.0^(4/128), ..., 1/10000.0^(126/128)], # [2/10000.0^(0/128), 2/10000.0^(2/128), 2/10000.0^(4/128), ..., 2/10000.0^(126/128)], # ..., # [4095/10000.0^(0/128), 4095/10000.0^(2/128), 4095/10000.0^(4/128), ..., 4095/10000.0^(126/128)]] freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # complex64 # freqs_cis的维度为(4096,64)，相当于半径为1，角度为freqs的极坐标的复数表示，如公式6所示。 return freqs_cis def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor): # 将除了position和dim其他的维度变为1 \"\"\" Reshape frequency tensor for broadcasting it with another tensor. This function reshapes the frequency tensor to have the same shape as the target tensor 'x' for the purpose of broadcasting the frequency tensor during element-wise operations. Args: freqs_cis (torch.Tensor): Frequency tensor to be reshaped. x (torch.Tensor): Target tensor for broadcasting compatibility. Returns: torch.Tensor: Reshaped frequency tensor. Raises: AssertionError: If the frequency tensor doesn't match the expected shape. AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions. \"\"\" # freqs_cis.shape = [1024, 64] # x.shape = [2, 1024, 32, 64] ndim = x.ndim assert 0 \u003c= 1 \u003c ndim assert freqs_cis.shape == (x.shape[1], x.shape[-1]) # 将freqs_cis.shape变为[1, 1024, 1, 64] shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)] return freqs_cis.view(*shape) def apply_rotary_emb( xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, ): \"\"\" Apply rotary embeddings to input tensors using the given frequency tensor. This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are returned as real tensors. Args: xq (torch.Tensor): Query tensor to apply rotary embeddings. xk (torch.Tensor): Key tensor to apply rotary embeddings. freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials. Returns: Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings. \"\"\" # 将xq和xk的最后一个维度进行复数运算，得到新的xq和xk # 为了进行复数运算，需要将xq和xk的最后一个维度展开为2维 # 例如，xq的形状为[2, seq_len, 32, 128], reshape后为[2, seq_len, 32 , 64, 2] # view_as_complex函数可以将张量中的最后一维的两个元素作为实部和虚部合成一个复数xq的形状变为[2, seq_len, 32, 64] xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)) xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) # 将freqs_cis广播到xq和xk的最后一个维度 freqs_cis = reshape_for_broadcast(freqs_cis, xq_) # freqs_cis.shape = [1, 1024, 1, 64] # view_as_real和view_as_complex相反，可以将张量中最后一维的复数拆出实部和虚部 # (xq_ ","date":"2024-08-31","objectID":"/rope/:1:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["","LLM"],"content":"另一种实现 另一种实现(transformers)利用了下面这个式子： \\[ \\begin{bmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{bmatrix}\\otimes\\begin{bmatrix}\\cos m\\theta_0\\\\\\cos m\\theta_0\\\\\\cos m\\theta_1\\\\\\cos m\\theta_1\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{bmatrix}+\\begin{bmatrix}-q_1\\\\q_0\\\\-q_3\\\\q_2\\\\\\vdots\\\\-q_{d-1}\\\\q_{d-2}\\end{bmatrix}\\otimes\\begin{bmatrix}\\sin m\\theta_0\\\\\\sin m\\theta_0\\\\\\sin m\\theta_1\\\\\\sin m\\theta_1\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{bmatrix} \\] class LlamaRotaryEmbedding(torch.nn.Module): def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None): super().__init__() inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim)) self.register_buffer(\"inv_freq\", inv_freq) # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. # Keep the logic here just in case. if seq_len \u003e self.max_seq_len_cached: self.max_seq_len_cached = seq_len t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype) freqs = torch.einsum(\"i,j-\u003eij\", t, self.inv_freq) # Different from paper, but it uses a different permutation # in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False) self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False) return ( self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype), self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype), ) def rotate_half(x): \"\"\"Rotates half the hidden dims of the input.\"\"\" x1 = x[..., : x.shape[-1] // 2] x2 = x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) def apply_rotary_pos_emb(q, k, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed 相对于llama的版本比较容易理解。 Long-term decay of RoPE 公式不看了，结论就是RoPE有长距离衰减的特性，相对距离越远的token之间的关注度也会降低，表现为attention score减小，这是个很好的特性。”This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection.“ # 参考 一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long(含NTK-aware简介)-CSDN博客 LLM—llama2结构和源码解读 - 知乎 (zhihu.com) ","date":"2024-08-31","objectID":"/rope/:2:0","tags":["LLM"],"title":"rope","uri":"/rope/"},{"categories":["reading"],"content":"好的，非常荣幸能以专家的身份，与您一同深入探讨这篇在长上下文（Long Context）领域具有重要影响力的论文——《Data Engineering for Scaling Language Models to 128K Context》。 这篇文章的核心价值在于，它为如何经济高效地将现有大语言模型（LLMs）的上下文窗口扩展到128K甚至更长，提供了一套清晰、可复现且极为有效的数据工程（Data Engineering）实践方案。在业界普遍认为扩展上下文窗口需要巨大的算力投入和复杂的模型结构改造时，这篇论文如同一股清流，指出问题的关键或许不在于模型架构的“大手术”，而在于如何通过“精细喂养”——即高质量的数据策略——来“解锁”模型早已在预训练阶段就潜移默化中习得的长距离信息处理能力。 接下来，我将首先为您全面深入地解读这篇论文的精髓，然后按照您提出的六个问题，逐一进行剖析。 ","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:0:0","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":["reading"],"content":"论文深度解读 在长上下文技术成为大模型核心竞争力的今天，无论是处理长篇文档、进行多文档问答，还是实现代码仓库级别的理解，拥有一个超长且有效的上下文窗口都至关重要。然而，在很长一段时间里，开源社区的模型尽管在理论上支持数万甚至十万以上的上下文长度（例如通过修改RoPE位置编码的base），但在实际应用中，尤其是在著名的“大海捞针”（Needle-in-a-Haystack, NIAH）测试中表现不佳。这个测试通过将一个特定的信息（“针”）隐藏在一段冗长的文本（“草堆”）中的不同位置，来检验模型是否真的能“看到”并利用整个上下文的信息。许多模型在“草堆”的中间部分会出现信息提取失败的情况，形成一个“U”形性能曲线，即只对开头和结尾的信息敏感。 本文作者正是针对这一痛点展开研究。他们提出了一个颠覆性的假设： \u003e We hypothesize that the capability to utilize information at arbitrary locations within long context length is (mostly) already acquired during pretraining, even for models pretrained on substantially shorter 4K contexts. … a small amount of long-context data, in our case, 1-5B tokens, can “unlock” a 7B model’s capability… \u003e \u003e 我们假设，在任意位置利用长上下文信息的能力，（大部分）是在大规模预训练阶段就已经获得的，即使模型是在短得多的4K上下文上预训练的。… 少量长上下文数据，在我们的案例中是10亿到50亿个token，就足以“解锁”一个7B模型的这种能力… 基于这个假设，他们将研究重心从复杂的模型算法创新彻底转向了数据工程。他们没有设计新的注意力机制或模型结构，而是沿用了标准的Transformer架构，并使用了FlashAttention-2等现有技术来优化效率，其核心贡献是一套被称为“按源长度上采样”（Per-source Length Upsampling）的数据策略。 这个策略的精妙之处在于它解决了数据工程中的一个核心矛盾：为了让模型学习长依赖，我们需要喂给它更多长文档；但长文档（如书籍、代码）在通用语料库中占比较低，如果直接大量增加长文档的比例，就会破坏预训练时的数据领域分布，可能导致模型在网页、对话等短文本领域的性能下降，即所谓的“知识遗忘”或“领域偏移”。 该论文提出的解决方案如下： 1. 维持领域平衡：他们使用了SlimPajama数据集，这是一个LLaMA预训练数据的开源复现版本。他们严格维持SlimPajama中各个领域的原始比例，例如CommonCrawl占67%，Github占4.5%，Books占4.5%等。 2. 域内长度上采样：在保持上述领域比例不变的前提下，他们在每个领域内部，提高了长文档被采样的概率。例如，在处理Books领域的数据时，他们会更倾向于选择那些完整的、长篇的章节，而不是短小的片段。 通过这种方式，模型既接触到了足够多的长文本样本来学习长距离依赖，又不会因为“偏食”某一类数据而损害其通用能力。 为了验证这一方法的有效性，论文进行了一系列详尽的实验。最引人注目的结果体现在大海捞针测试中（见下图）。他们基于LLaMA-2 7B模型，使用其数据配方持续预训练了50亿（5B）个token后，模型在长达128K的上下文长度测试中，信息检索的准确率达到了惊人的88.0%。这一结果不仅远超当时其他的开源长上下文模型（如YaRN Mistral的57.4%），甚至在特定测试配置下略高于被视为业界标杆的GPT-4 128K（87.1%）。 图1：论文核心成果展示。绿色代表模型成功在“草堆”中找到了“针”，红色代表失败。可见，“Ours LLaMA 7B”模型在100K长度内基本实现了全绿色覆盖，表现与GPT-4 128K相当，远优于其他开源模型。 更重要的是，他们证明了这种性能提升的“性价比”极高。整个持续预训练过程仅需要10亿到50亿（1B-5B）个token的数据量，这与从头预训练动辄数万亿（trillion）token相比，成本降低了几个数量级。如论文Table 2所示，在8块A100（80G）GPU上，训练一个7B模型仅需约5天时间，这使得顶尖的长上下文模型技术不再是少数巨头的专利，普通的学术实验室和中小型企业也完全有能力负担。 同时，论文还通过消融实验（Ablation Study）详细对比了不同数据策略的优劣（见下表）。实验清晰地表明，那些破坏了领域平衡的策略，例如只用书籍数据进行上采样（Upsample Book），虽然在书籍相关的长文本任务上表现优异，但在代码等其他领域的性能上却造成了损害。这强有力地证明了“按源长度上采样”策略的均衡性和优越性。 表5：不同数据工程方法对模型在各领域损失（loss）的影响。红色表示性能下降，绿色表示提升。可见，“Per-source”策略在提升长文本性能（4K-128K Context Length部分多为绿色）的同时，对短文本性能（0-4K部分）的负面影响最小，是最均衡的方案。 综上所述，这篇论文以严谨的实验和清晰的逻辑，为大模型社区贡献了一个极其宝贵的“食谱”：它告诉我们，通往高性能长上下文模型的道路，或许就隐藏在对数据的深刻理解和精巧处理之中。 一、论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义? 研究目标：论文的核心研究目标是，探索并验证一套经济高效（affordable）且效果卓越（effective）的数据工程方法，用于将现有的大语言模型（如LLaMA-2）的上下文处理能力从标准的4K扩展到128K。 解决的实际问题： 开源模型长上下文能力不足：在当时，许多开源模型虽然声称支持长上下文，但在实际应用（如NIAH测试）中表现不佳，无法在整个上下文窗口内可靠地检索信息，存在明显的性能短板。 扩展上下文的成本高昂：传统观念认为，要让模型具备强大的长上下文能力，要么需要从头开始用海量长文本数据进行预训练，要么需要进行大规模的持续预训练（如之前的工作动辄使用400B token），这两种方式都耗资巨大，非普通机构所能承受。 对行业发展的重要意义： 技术的民主化：该论文提出的方法大幅降低了构建高性能长上下文模型的门槛。它证明了只需“学术级别”的资源（几天到几周的GPU时间），就能将一个普通的开源模型提升到接近甚至超越“前沿模型”（frontier models）的水平。这极大地推动了长上下文技术的普及，使得中小型企业、创业公司和学术研究者都能参与到这一领域的创新中。 催生新的应用场景：一个可靠的128K上下文窗口，意味着模型可以一次性处理整本书、一个完整的代码仓库、一份长篇的法律或财务报告。这直接赋能了如代码智能体（Repo-level Code Agents）、多文档复杂问答系统、法律/金融文档分析等过去难以实现的复杂应用，为行业开辟了全新的想象空间。 指明了新的研究方向：它将社区的注意力从“卷”模型架构和算法，部分转移到了数据本身。这启发了更多关于数据质量、数据配比、数据工程对模型能力影响的深入研究，强调了“数据为王”在LLM时代依然是黄金法则。 二、论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。 新的思路： “解锁”而非“注入”：这是本文最核心的思路转变。之前的方法（如LLaMA Long、XVERSE）倾向于认为长上下文是一种需要通过大规模训练“注入”到模型中的新能力。而本文认为这种能力在标准预训练后已经“潜伏”在模型内部，只需要通过小规模、有针对性的持续预训练来“解锁”。这一思路直接导致了方法论上的巨大差异，即从“重投入”转向“轻量化”。 新的方法： 轻量化持续预训练（Lightweight Continual Pretraining）：在仅使用1B到5B token的数据量上进行训练，相比之前工作使用的400B token，数据量减少了近两个数量级。 按源长度上采样（Per-source Length Upsampling）：这是其数据工程配方的核心技术。具体操作细节如下： 基准数据集：采用SlimPajama，这是一个复刻LLaMA预训练数据的公开语料库，包含了网页、代码、书籍、论文等多个领域。 维持领域分布：在采样时，严格保持SlimPajama中各领域的原始比例（如67% CommonCrawl, 15% C4, 4.5% Github, 4.5% Books等）。 域内上采样：在每个领域内部，提高长文档被选中的概率。论文中提到，他们将长度超过4K的序列占比从约30%提升到了约70%。 特点和优势： 性能卓越：如Table 3所示，其7B模型在NIAH测试上达到88.0%的准确率，13B模型达到90.0%，效果媲美甚至超越了GPT-4。 成本极低：训练成本大幅降低，使得技术普及成为可能。 保持通用性：与那些“偏科”的方法形成鲜明对比，该方法在提升长上下文能力的同时，几乎没有损害模型在短上下文任务上的通用能力。Table 3中的MMLU分数（一个衡量模型通用知识和推理能力的主流基准）可以证明这一点。例如，Ours LLaMA-2 7B的MMLU分数为43.3，与基础模型（Together LLaMA-2 7B的44.8）相比，仅有微小下降，","date":"2024-08-08","objectID":"/data-engineering-for-scaling-language-models-to-128k-context/:0:1","tags":["文献","LLM"],"title":"Data Engineering for Scaling Language Models to 128K Context","uri":"/data-engineering-for-scaling-language-models-to-128k-context/"},{"categories":[""],"content":"KV cache LLM推理过程分为Prefill和Decode两个阶段，其中Prefill阶段会对Prompt中所有的token做并行计算，得到Prompt中所有Tokens的KV Cache以及计算得到首Token。Prompt阶段Token计算得到的KV Cache会保存下来，留给Decode阶段复用，Decode阶段是一个自回归过程，每decode一个新的Token，都需要用到所有之前计算得到的KV Cache来计算当前query token的Attention。因此，当输出长度越来越大或者context很长时，KV Cache将会占用大量的显存。如何优化KV Cache的显存占用，一直都是LLM推理的核心主题之一。 之前一直疑惑kv cache既然每次只输入生成token就可以，那么位置信息该怎么注入呢？翻了翻llama的源码，找到了答案： def forward(self, tokens: torch.Tensor, start_pos: int): \"\"\" Perform a forward pass through the Transformer model. Args: tokens (torch.Tensor): Input token indices. start_pos (int): Starting position for attention caching. Returns: torch.Tensor: Output logits after applying the Transformer model. \"\"\" _bsz, seqlen = tokens.shape h = self.tok_embeddings(tokens) self.freqs_cis = self.freqs_cis.to(h.device) freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen] mask = None if seqlen \u003e 1: mask = torch.full( (seqlen, seqlen), float(\"-inf\"), device=tokens.device ) mask = torch.triu(mask, diagonal=1) # When performing key-value caching, we compute the attention scores # only for the new sequence. Thus, the matrix of scores is of size # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for # j \u003e cache_len + i, since row i corresponds to token cache_len + i. mask = torch.hstack([ torch.zeros((seqlen, start_pos), device=tokens.device), mask ]).type_as(h) for layer in self.layers: h = layer(h, start_pos, freqs_cis, mask) h = self.norm(h) output = self.output(h).float() return output 注意freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]这一行，即是实现了rope相对位置编码的kv cache的核心。 kv cache代码 def repeat_kv(x: torch.Tensor, n_rep: int) -\u003e torch.Tensor: \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\" bs, slen, n_kv_heads, head_dim = x.shape if n_rep == 1: return x return ( x[:, :, :, None, :] .expand(bs, slen, n_kv_heads, n_rep, head_dim) .reshape(bs, slen, n_kv_heads * n_rep, head_dim) ) class Attention(nn.Module): \"\"\"Multi-head attention module.\"\"\" def __init__(self, args: ModelArgs): \"\"\" Initialize the Attention module. Args: args (ModelArgs): Model configuration parameters. Attributes: n_kv_heads (int): Number of key and value heads. n_local_heads (int): Number of local query heads. n_local_kv_heads (int): Number of local key and value heads. n_rep (int): Number of repetitions for local heads. head_dim (int): Dimension size of each attention head. wq (ColumnParallelLinear): Linear transformation for queries. wk (ColumnParallelLinear): Linear transformation for keys. wv (ColumnParallelLinear): Linear transformation for values. wo (RowParallelLinear): Linear transformation for output. cache_k (torch.Tensor): Cached keys for attention. cache_v (torch.Tensor): Cached values for attention. \"\"\" super().__init__() self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads model_parallel_size = fs_init.get_model_parallel_world_size() self.n_local_heads = args.n_heads // model_parallel_size self.n_local_kv_heads = self.n_kv_heads // model_parallel_size self.n_rep = self.n_local_heads // self.n_local_kv_heads self.head_dim = args.dim // args.n_heads self.wq = ColumnParallelLinear( args.dim, args.n_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wk = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wv = ColumnParallelLinear( args.dim, self.n_kv_heads * self.head_dim, bias=False, gather_output=False, init_method=lambda x: x, ) self.wo = RowParallelLinear( args.n_heads * self.head_dim, args.dim, bias=False, input_is_parallel=True, init_method=lambda x: x, ) # kv_cache是缓存键值对，在训练过程中，我们只保存最近n个键值对 self.cache_k = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() self.cache_v = torch.zeros( ( args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim, ) ).cuda() def forward( self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[","date":"2024-08-07","objectID":"/kv-cache/:0:0","tags":null,"title":"KV cache","uri":"/kv-cache/"},{"categories":[""],"content":"prefill和decode分离 在传统的 LLM 推理框架中，Prefill 和 Decode 阶段通常由同一块 GPU 执行。推理引擎的调度器会根据显存使用情况及请求队列状态，在 Prefill 和 Decode 之间切换，完成整个推理过程。 而在 Prefill-Decode 分离式架构（以下简称 PD 分离式架构）中，这两个阶段被拆分到不同的 GPU 实例上独立运行。如下图所示，这是 DistServe 提供的一张架构图： 在大模型推理中，常用以下两项指标评估性能： TTFT（Time-To-First-Token）：首 token 的生成时间，主要衡量 Prefill 阶段性能。 TPOT（Time-Per-Output-Token）：生成每个 token 的时间，主要衡量 Decode 阶段性能。 当 Prefill 和 Decode 在同一块 GPU 上运行时，由于两阶段的计算特性差异（Prefill 是计算密集型，而 Decode 是存储密集型），资源争抢会导致 TTFT 和 TPOT 之间的权衡。例如： 若优先处理 Prefill 阶段以降低 TTFT，Decode 阶段的性能（TPOT）可能下降。 若尽量提升 TPOT，则会增加 Prefill 请求的等待时间，导致 TTFT 上升。 PD 分离式架构的提出正是为了打破这一矛盾。通过将 Prefill 和 Decode 分离运行，可以针对不同阶段的特性独立优化资源分配，从而在降低首 token 延迟的同时提高整体吞吐量。 在 PD 分离架构中，Prefill 和 Decode 阶段的资源需求不同，分别体现为： Prefill 阶段：计算密集型（compute-bound）。在流量较大或用户提示长度较长时，Prefill 的计算压力更大。完成 KV Cache 的生成后，Prefill 阶段本身无需继续保留这些缓存。 Decode 阶段：存储密集型（memory-bound）。由于逐 token 生成的特性，Decode 阶段需频繁访问 KV Cache，因此需要尽可能多地保留缓存数据以保障推理效率。 Batching 策略对两阶段的性能影响显著，但趋势相反： Prefill 阶段：吞吐量随 batch size 增加逐渐趋于平稳。这是因为 Prefill 的计算受限特性（compute-bound），当 batch 中的总 token 数超过某个阈值时，计算资源成为瓶颈。 Decode 阶段：吞吐量随 batch size 增加显著提升。由于 Decode 阶段的存储受限特性（memory-bound），增大 batch size 可提高计算效率，从而显著增加吞吐量。 ","date":"2024-08-07","objectID":"/kv-cache/:1:0","tags":null,"title":"KV cache","uri":"/kv-cache/"},{"categories":[""],"content":"Chunked prefills image.png 常见问题 ","date":"2024-08-07","objectID":"/kv-cache/:2:0","tags":null,"title":"KV cache","uri":"/kv-cache/"},{"categories":[""],"content":"128k token输入需要多少显存存kv cache? image.png 超参数如上，如果我们采用int8精度，也就是每个参数占据一个字节，每个token占据的kv cache大小就是 2 * K * H * L = 2 (k 和 v) * 8 (n kv heads) * 128 (d_qkv) * 80 (n layers) * 1 (byte)= 160kB （这里的 2 是因为每个 token 需要存储k和v） 那么128k的kv cache就是： 162e3 * 128 * 1024 = 21.2GB 需要注意的是 llama 3 使用的是 gqa，所以 N 为 64，K 为 8. 套公式就是 （使用 bf 16 即 2 bytes）： \\[ b(s+n)h*l*2*2 \\] 其中 b 为 batch_size，s 为输入序列长度，n 为输出序列长度，h 为 k 或者 v 的总维度，不同的 attention 改变的是这个东西来降低显存。 参考 LLM—llama2结构和源码解读 - 知乎 (zhihu.com) # LLM推理优化 - Chunked prefills # 图解大模型计算加速系列：分离式推理架构1，从DistServe谈起 ","date":"2024-08-07","objectID":"/kv-cache/:3:0","tags":null,"title":"KV cache","uri":"/kv-cache/"},{"categories":[""],"content":"Low-Rank Adaption (LoRA)，即“低秩适配”，实现了预训练模型的参数高效微调，且不会增加模型的推理延迟。 ## 内在维度 2020年，A. Aghajanyan等人研究了这一现象，发现预训练模型存在一个较低的”内在维度”,使用少量样本微调时，实际上是在更新低维空间中的参数。把预训练模型的全部参数看成一个D维参数向量，记为\\(\\Theta^\\mathrm{(D)}\\),模型的原始参数为\\(\\Theta_0^\\mathrm{(D)}\\),设\\(\\Theta^{(d)}\\)是d维子空间中的一个向量，d\u003cD,利用一个固定的D*d映射矩阵P 把d维空间中的向量映射到D维空间，\\(\\Theta^{(\\mathrm{D})}\\)可写为： \\[\\mathrm{\\theta^{(D)}=\\theta_0^{(D)}+P\\theta^{(d)}}\\] 下图中，以D=3,d=2为例： 左图直接在3维空间中训练模型，直接优化原始模型参数\\(\\Theta_{0}^{(\\mathrm{D})}\\),把它更新为\\(\\Theta^{(\\mathrm{D})}\\)。右图冻结\\(\\Theta_{0}^{(\\mathrm{D})}\\),转而在2维空间中寻找一个\\(\\Theta^{(\\mathrm{d})}\\),再用矩阵P把\\(\\Theta^{(\\mathrm{d})}\\)映射到3维空间。如果用右图的方式可以把模型优化到良好的效果，例如，达到了全量参数微调效果的90%,则该模型的内在维度\\(\\mathrm{d}_{90}=2\\)。 实验表明，仅训练200个参数，就可以使RoBERTa-large在MRPC数据集上的效果达到全量参数微调效果的90%。 ## 低秩适配 预训练模型的权重矩阵通常具有满秩，这意味着权重矩阵的各个列向量之间线性无关，这样的矩阵没有冗余信息，是无法被压缩的。但是，“内在维度”现象表明，微调模型时只需更新少量参数，这启发我们微调时产生的权重增量矩阵\\(\\Delta\\)W可能包含大量冗余参数，\\(\\Delta\\)W很可能不是满秩的。对低秩矩阵做分解，可以利用较少的参数重建或近似原矩阵。这就是LoRA的核心思想。 设输入为x，微调时得到增量\\(\\Delta W\\),与原始权重\\(\\mathcal{W}_{0}\\)相加得到更新后的权重，输出h= ( \\(W_0\\)+ \\(\\Delta\\)W) x。根据矩阵的乘法分配律，有h= \\(W_0\\)x+ \\(\\Delta\\)Wx,这意味着微调时可以保持\\(W_0\\)不变，分别将\\(W_0\\)、\\(\\Delta\\)W与x相乘，最后把两个乘积相加即可得到输出h。 设\\(\\mathcal{W}_0\\in\\mathbb{R}^{\\mathrm{dxk}}\\),\\(\\Delta \\mathcal{W} _{\\mathrm{f} }\\)的秩为r。\\(\\Delta \\mathcal{W}= \\mathcal{B} \\mathcal{A}\\)是\\(\\Delta\\mathcal{W}\\)的一个满秩分解，其中 \\(\\mathcal{B} \\in \\mathbb{R} ^{\\mathrm{dxr}}, \\mathcal{A} \\in \\mathbb{R} ^{\\mathrm{rxk}}, \\mathcal{r} \\ll \\min ( \\mathcal{d} , \\mathcal{k} )\\)。训练时，分别用随机高斯和零矩阵初始化A和B，确保初始化时 BA是零矩阵，对模型效果没有影响。训练过程中冻结\\(\\mathcal{W}_0\\),只更新矩阵B和A，共r(d+k)个参数，从而实 现“参数高效”微调。推理时，分别计算\\(\\mathbf{W_{\\mathrm{n} }x}\\)和 BAx并相加，得到输出h，如下图所示： 实际上，r是一个超参，训练时可任意设定，\\(\\Delta\\)W真正的秩未必等于r。如果r恰好等于\\(\\Delta\\)W的秩，甚至大于\\(\\Delta\\)的秩(例如等于预训练权重矩阵\\(W_{0}\\)的秩),利用学到的B和A可以完全重建\\(\\Delta\\)W,这时，LoRA的效果近似于全量微调。如果r小于\\(\\Delta\\)W的秩，BA就是\\(\\Delta\\)W的一个低秩近似，利用矩阵B和A可以恢复矩阵\\(\\Delta W\\)中的部分信息。 还有一个超参数为lora_alpha: image.png ","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:0:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":[""],"content":"降低了哪部分显存需求 image.png image.png ","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:1:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":[""],"content":"代码 class LoraModel(torch.nn.Module): \"\"\" Creates Low Rank Adapter (Lora) model from a pretrained transformers model. Args: model ([`transformers.PreTrainedModel`]): The model to be adapted. config ([`LoraConfig`]): The configuration of the Lora model. Returns: `torch.nn.Module`: The Lora model. Example:: \u003e\u003e\u003e from transformers import AutoModelForSeq2SeqLM, LoraConfig \u003e\u003e\u003e from peft import LoraModel, LoraConfig \u003e\u003e\u003e config = LoraConfig( peft_type=\"LORA\", task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.01, ) \u003e\u003e\u003e model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") \u003e\u003e\u003e lora_model = LoraModel(config, model) **Attributes**: - **model** ([`transformers.PreTrainedModel`]) -- The model to be adapted. - **peft_config** ([`LoraConfig`]): The configuration of the Lora model. \"\"\" def __init__(self, config, model): super().__init__() self.peft_config = config self.model = model self._find_and_replace() mark_only_lora_as_trainable(self.model, self.peft_config.bias) def _find_and_replace(self): kwargs = { \"r\": self.peft_config.r, \"lora_alpha\": self.peft_config.lora_alpha, \"lora_dropout\": self.peft_config.lora_dropout, \"fan_in_fan_out\": self.peft_config.fan_in_fan_out, \"merge_weights\": self.peft_config.merge_weights, } key_list = [key for key, _ in self.model.named_modules()] for key in key_list: if any(key.endswith(target_key) for target_key in self.peft_config.target_modules): # 对特定的层插入lora层 parent, target, target_name = self._get_submodules(key) bias = target.bias is not None if isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None: new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs) elif self.peft_config.enable_lora is not None: kwargs.update({\"enable_lora\": self.peft_config.enable_lora}) if isinstance(target, Conv1D): in_features, out_features = target.weight.shape else: in_features, out_features = target.in_features, target.out_features if kwargs[\"fan_in_fan_out\"]: warnings.warn( \"fan_in_fan_out is set to True but the target module is not a Conv1D. \" \"Setting fan_in_fan_out to False.\" ) kwargs[\"fan_in_fan_out\"] = False new_module = MergedLinear(in_features, out_features, bias=bias, **kwargs) self._replace_module(parent, target_name, new_module, target) def _get_submodules(self, key): parent = self.model.get_submodule(\".\".join(key.split(\".\")[:-1])) target_name = key.split(\".\")[-1] target = self.model.get_submodule(key) return parent, target, target_name def _replace_module(self, parent_module, child_name, new_module, old_module): setattr(parent_module, child_name, new_module) new_module.weight = old_module.weight if old_module.bias is not None: new_module.bias = old_module.bias def forward(self, *args, **kwargs): return self.model(*args, **kwargs) def __getattr__(self, name: str): \"\"\"Forward missing attributes to the wrapped module.\"\"\" try: return super().__getattr__(name) # defer to nn.Module's logic except AttributeError: return getattr(self.model, name) @property def modules_to_save(self): return None def get_peft_config_as_dict(self, inference: bool = False): config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(self.peft_config).items()} if inference: config[\"inference_mode\"] = True return config 插入lora层的核心代码如下： class LoraLayer: def __init__( self, r: int, lora_alpha: int, lora_dropout: float, merge_weights: bool, ): self.r = r self.lora_alpha = lora_alpha # Optional dropout if lora_dropout \u003e 0.0: self.lora_dropout = nn.Dropout(p=lora_dropout) else: self.lora_dropout = lambda x: x # Mark the weight as unmerged self.merged = False self.merge_weights = merge_weights class Linear(nn.Linear, LoraLayer): # Lora implemented in a dense layer def __init__( self, in_features: int, out_features: int, r: int = 0, lora_alpha: int = 1, lora_dropout: float = 0.0, fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out) merge_weights: bool = True, **kwargs, ): nn.Linear.__init__(self, in","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:2:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":[""],"content":"参考 https://snailcoder.github.io/2023/08/06/parameter-efficient-llm-fine-tuning-lora.html # 当红炸子鸡 LoRA，是当代微调 LLMs 的正确姿势？ ","date":"2024-08-07","objectID":"/lora%E5%BE%AE%E8%B0%83/:3:0","tags":null,"title":"Lora微调","uri":"/lora%E5%BE%AE%E8%B0%83/"},{"categories":["","NLP","LLM","MoE"],"content":"MoE 的思想类似于集成学习中的 Ensemble Learning。MoE 作用于原本 transformer 模型的 MLP 层，即： 图片来自于Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity 论文。 总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个路由器（或者叫门控网络）和若干数量的专家。 用多个 FeedForward 块替换单个 FeedForward 块 （如在 MoE 设置中所做的那样）会大大增加模型的总参数数。然而，关键的诀窍是，我们不会对每个token使用（“激活”）所有专家。相反，路由器仅为每个token选择一小部分专家。 image.png ","date":"2024-08-07","objectID":"/moe/:0:0","tags":["NLP","LLM","MoE"],"title":"MoE","uri":"/moe/"},{"categories":["","NLP","LLM","MoE"],"content":"技术选择 Expert Choice 路由（容量为 2，即每个专家选两个 token）在 NLP 模型效果好，Top-K 路由则在视觉模型效果好。 在 NLP 任务中，专家越多越好，而视觉任务中存在饱和点。 增加 MoE 层数也可以增加模型容量，但视觉任务中同样存在饱和点。 恢复优化器状态（一些统计量）和路由权重归一化可以提高视觉 MoE 模型的性能，对 NLP 任务无效。 ","date":"2024-08-07","objectID":"/moe/:1:0","tags":["NLP","LLM","MoE"],"title":"MoE","uri":"/moe/"},{"categories":["reading"],"content":"这篇论文的核心贡献，在于它为Transformer模型中占据了约三分之二参数量、但长期以来其功能被严重忽视的前馈神经网络（Feed-Forward Network, FFN）层，提供了一个简洁而深刻的解释框架。在此之前，学术界的目光大多聚焦于自注意力（Self-Attention）机制，而FFN层则像一个神秘的“黑箱”。Geva等人的这项工作，通过一系列精巧的实验，令人信服地论证了：FFN层在功能上等同于一个键值记忆（Key-Value Memory）系统。 具体来说，论文揭示了FFN内部的运作机制。FFN层包含两个线性变换矩阵，论文将它们分别类比为“键（Keys）”和“值（Values）”。当一个输入向量（代表某个词元或文本片段的含义）进入FFN层时，它会首先与所有的“键”进行匹配计算。这个匹配度，论文称之为记忆系数（memory coefficient），决定了每个“键”所对应的“值”被激活的强度。每个“值”本身并不直接是一个词，而是一个向量，它蕴含了一个关于“下一个可能出现的词”的概率分布。最终，FFN层的输出是所有被激活的“值”向量的加权和。这个加权和的输出，实际上是对模型下一步应该生成什么内容的一次“提议”或“修正”。 论文的一个惊人发现是，这些“记忆”是具有层次性和可解释性的。 Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. 我们的实验表明，学习到的模式是人类可解释的，并且底层网络倾向于捕获浅层模式，而上层网络则学习更具语义的模式。 例如，在模型的底层（如1-9层），FFN的“键”主要响应一些浅层模式（shallow patterns），比如特定的n-gram短语（例如，以“substitutes”结尾的句子）。而到了模型的高层（如10-16层），“键”则演变为响应更抽象的语义模式（semantic patterns），比如某个特定的话题（例如，关于“电视剧”的讨论）。这一点在论文的 图2 中得到了清晰的展示，图中显示了随着层数的增加，语义模式（绿色部分）的占比显著提升，而浅层模式（蓝色部分）则相应减少。 图2：论文中的关键图表，展示了不同层级的FFN记忆中，浅层模式与语义模式的比例变化。 更进一步，论文发现高层FFN中的“值”向量与对应的“键”模式高度相关。当一个高层的“键”被某个输入模式激活时，其对应的“值”所编码的词汇分布，确实能很大概率地预测出该模式之后最可能出现的词。例如，如果一个“键”专门识别“关于二战轰炸机基地”的语境，那么它对应的“值”向量，在被激活后，就会倾向于生成“任务（missions）”、“攻击（attacked）”等相关词汇。论文在 图4 中通过“一致率（agreement rate）”这个指标量化了这一发现，在高层（11-16层），这种一致率达到了约3.5%，虽然看似不高，但论文强调这比随机猜测（约0.0004%）高出几个数量级，证明了其非凡的预测能力。 最后，论文还探讨了这些记忆是如何被聚合（Aggregating）的。在一个FFN层内部，最终的输出通常不是由单个最强的记忆决定的，而是由数百个被激活的记忆组合（composition）而成，这在 图8 中有明确数据支撑，超过68%的情况下，层的输出与任何单个记忆的预测都不同。在层与层之间，FFN的输出通过残差连接（residual connections）对主干信息流进行精细的修正（refinement）。模型并非在最后一层才做出最终决定，而是在逐层传递的过程中，不断地利用FFN的“记忆”来调整和优化对下一个词的预测。 总而言之，这篇论文将FFN从一个难以理解的计算模块，变成了一个结构清晰、功能明确的分布式记忆系统。它不仅为我们理解Transformer的内部工作原理打开了一扇窗，也为后续的模型编辑、可信AI和模型压缩等研究方向奠定了坚实的基础。 1. 论文的研究目标是什么？ 想要解决什么实际问题？这个问题对于行业发展有什么重要意义？ 研究目标：论文的核心研究目标是揭开Transformer模型中前馈神经网络（FFN）层的神秘面纱，并为其功能提供一个清晰、可验证的解释。具体来说，它试图回答一个核心问题：占模型三分之二参数的FFN层，究竟在做什么？ 解决的实际问题： 模型可解释性缺失：在当时，自注意力机制的研究已经比较深入，但FFN的功能却是一个巨大的知识盲区。这使得我们对大模型的决策过程理解不完整，难以信任其输出，也难以在出错时进行调试。 模型行为的不可预测性：我们不知道模型为何会产生“幻觉”、输出有害内容或泄露隐私数据。如果能理解FFN的功能，就可能从机理上找到这些问题的原因。 对行业发展的重要意义： 推动可信AI（Trustworthy AI）：理解是信任的前提。通过将FFN解释为键值记忆，这篇论文极大地增强了我们对大模型内部工作原理的理解，是迈向构建更安全、更可控、更值得信赖的AI系统的关键一步。 催生新的技术方向：该研究直接启发了模型编辑（Model Editing）领域。如果我们知道某个事实性错误（比如“埃菲尔铁塔在罗马”）存储在哪个或哪些FFN“记忆”中，我们就有可能直接修改这些“值”向量来纠正错误，而无需重新训练整个庞大的模型。 提升模型效率和安全性：通过识别FFN中存储的“记忆”，我们可以分析哪些是冗余的，从而进行模型压缩；也可以识别哪些“记忆”可能存储了训练数据中的敏感信息，为隐私保护和数据安全审计提供新的工具。 2. 论文提出了哪些新的思路、方法或模型？跟之前的方法相比有什么特点和优势？请尽可能参考论文中的细节进行分析。 新的思路/模型：论文并未提出一个全新的模型架构，而是提出了一个关于现有Transformer FFN层的功能性类比（functional analogy）——即FFN作为非归一化的键值记忆。 核心方法论： 概念重构：将FFN的数学公式 FFN(x) = ReLU(x * K^T) * V (简化后) 与神经记忆网络 MN(x) = softmax(x * K^T) * V 进行对比。 \u003e Comparing equations 1 and 2 shows that feed-forward layers are almost identical to key-value neural memories; the only difference is that neural memory uses softmax as the non-linearity f(·), while the canonical transformer does not use a normalizing function in the feed-forward layer. \u003e 对比公式1和2可以发现，前馈层与键值神经记忆几乎完全相同；唯一的区别在于，神经记忆使用softmax作为非线性函数f(·)，而标准的Transformer在前馈层中不使用归一化函数。 关键区别与优势：论文敏锐地指出了ReLU与softmax的区别。softmax会进行归一化，使得所有激活值的和为1，这意味着它倾向于只选择一个或少数几个最匹配的记忆。而ReLU是非归一化的，它允许多个记忆单元被同时、独立地激活。这恰好解释了为何FFN的输出是“组合式”的——它是多个被触发的“想法”（记忆）的叠加，而不是非此即彼的选择。这个洞察是论文的核心优势，更贴近模型在实践中的复杂行为。 分析方法：论文采用了一种“假设-验证”的实验探究方法，而非纯理论推导。 寻找触发样本（Trigger Examples）：为了探究“键”k_i的功能，研究者在庞大的训练集中搜索，找到那些能够让该键的记忆系数ReLU(x * k_i)取得最大值的输入文本x。这个方法非常巧妙，它让我们能直接“拷问”每个神经元：“什么样的信息最能让你兴奋？” 模式标注与分类：通过人类专家（NLP研究生）对这些触发样本进行归纳，识别出其中的共性模式，并将其分为“浅层”和“语义”两类。这是一种将复杂的神经活动与人类可理解的概念联系起来的有效手段。 值向量分析：将“值”向量v_i通过乘以输出嵌入矩阵E并应用softmax，将其转换为一个完整的词汇表概率分布p_i。这使得研究者可以直接评估每个“记忆”的预测倾向。 与之前笼统地将FFN视为“特征提取器”或简单地增加模型容量的方法相比，这篇论文提供了一个结构化、模块化且可操作的视角，其优势在于将抽象的数学运算赋予了具体的认知功能——记忆与联想。 3. 论文通过什么实验来验证所提出方法的有效性？实验是如何设计的？实验数据和结果如何？请引用关键数据加以说明。 论文设计了一系列环环相扣的实验来验证其核心假设。 实验一：验证“键（Keys）”捕获可解释的输入模式 设计：研究者从一个16层的Transformer语言模型中，每层随机抽取10个“键”（共160个）。对每个键，他们检索出训练集中最能激活它的前25个文本前缀。然后，由人类专家分析这25个前缀，归纳出它们共同的模式。 数据与结果： 高覆盖率：对于几乎每一个被分析的键，专家都能找到至少一种可识别的模式。平均每个键能识别出3.6个不同的模式。 模式分层：如前所述，图2清晰地展示了从低层到高层，模式由浅层（如固定短语）向语义（如主题）的转变。在第1层","date":"2024-08-07","objectID":"/transformer-feed-forward-layers-are-key-value-memories/:0:0","tags":["文献","Transformer"],"title":"Transformer Feed-Forward Layers Are Key-Value Memories","uri":"/transformer-feed-forward-layers-are-key-value-memories/"},{"categories":["","算法题"],"content":" Problem: ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:0:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"思路 注意拓扑排序最好是邻接表（哈系表实现），并用队列处理后续入度为0的点 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:1:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"解题方法 描述你的解题方法 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:2:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:3:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":["","算法题"],"content":"Code class Solution: def canFinish(self, numCourses: int, prerequisites: List[List[int]]) -\u003e bool: if not prerequisites: return True def get_zero(numCourses, prerequisites, temp): queue = [] for prerequisite in prerequisites: temp[prerequisite[0]] += 1 for i in range(len(temp)): if temp[i] == 0: queue.append(i) return queue temp = [0 for _ in range(numCourses)] queue = get_zero(numCourses, prerequisites, temp) if queue == []: return False from collections import defaultdict d = defaultdict(list) for prerequisite in prerequisites: d[prerequisite[1]].append(prerequisite[0]) while queue: idx = queue.pop(0) nexs = d[idx] if nexs: for nex in nexs: temp[nex] -= 1 if temp[nex] == 0: queue.append(nex) numCourses -= 1 return numCourses == 0 ","date":"2024-03-29","objectID":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/:4:0","tags":["算法题"],"title":"课程表（拓扑排序）","uri":"/%E8%AF%BE%E7%A8%8B%E8%A1%A8%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"categories":[""],"content":" Problem: ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:0:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"思路 讲述看到这一题的思路 ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:1:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"解题方法 描述你的解题方法 ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:2:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:3:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":[""],"content":"Code ","date":"2024-03-18","objectID":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/:4:0","tags":null,"title":"乘积最大的数组","uri":"/%E4%B9%98%E7%A7%AF%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E7%BB%84/"},{"categories":["","贪心","算法题"],"content":"跳跃游戏 Problem: ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:0:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"思路 讲述看到这一题的思路 ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:1:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"解题方法 描述你的解题方法 ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:2:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"复杂度 时间复杂度: 添加时间复杂度, 示例： \\(O(n)\\) 空间复杂度: 添加空间复杂度, 示例： \\(O(n)\\) ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:3:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","贪心","算法题"],"content":"Code ```Python3 [] # 跳跃游戏ii # 划分字母区间 [763. 划分字母区间 - 力扣（LeetCode）](https://leetcode.cn/problems/partition-labels/description/?envType=study-plan-v2\u0026envId=top-100-liked) 本题预处理完毕后思路和跳跃游戏2类似，当然也可以使用合并区间的思路来，都是贪心算法。 ## Code ```python class Solution: def partitionLabels(self, s: str) -\u003e List[int]: from collections import defaultdict d = defaultdict(list) for i, char in enumerate(s): d[char].append(i) # 也可以考虑合并区间做了，下面的解法类似跳跃游戏2 res = [] start = 0 max_jump = 0 for i, char in enumerate(s): max_jump = max(max_jump, d[char][-1]) if i == max_jump: res.append(i - start + 1) start = i + 1 max_jump = 0 return res ","date":"2024-03-18","objectID":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/:4:0","tags":["算法题","贪心"],"title":"跳跃游戏","uri":"/%E8%B7%B3%E8%B7%83%E6%B8%B8%E6%88%8F/"},{"categories":["","算法题"],"content":"41. 缺失的第一个正数 - 力扣（LeetCode） 空间复杂度o(n)很好想，但o(1)不好想，还是个408考研真题 注意O(n) == O(2n)，即相较于边遍历边判断，还是遍历两次更加方便且不会有太多损失。类似思想：73. 矩阵置零 - 力扣（LeetCode） class Solution: def firstMissingPositive(self, nums: List[int]) -\u003e int: # 将原数组当作哈希表使用。 for i, num in enumerate(nums): if num \u003c= 0: # 先把小于0的先一步处理，以便于使用标记 nums[i] = len(nums) + 1 for i, num in enumerate(nums): num = abs(num) if num \u003e= 1 and num \u003c= len(nums): if nums[num-1] \u003e 0: # 不重复添加 nums[num-1] = -nums[num-1] res = len(nums) + 1 for i, num in enumerate(nums): if num \u003e 0: res = i + 1 break return res ","date":"2024-03-16","objectID":"/%E7%BC%BA%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%95%B0/:0:0","tags":["算法题"],"title":"缺失的第一个正数","uri":"/%E7%BC%BA%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%95%B0/"},{"categories":["","算法题"],"content":"题目地址 # 思路 通过前缀和+哈希表，并有简单的数学变换。前缀和即 \\(y[i]=y[i-1]+x[i]\\) 类比于accumlate函数，注意前缀和思想也可以应用为“前缀积、后缀和、后缀积”等思想。238. 除自身以外数组的乘积 - 力扣（LeetCode） \u003e 使用前缀和的方法可以解决这个问题，因为我们需要找到和为k的连续子数组的个数。通过计算前缀和，我们可以将问题转化为求解两个前缀和之差等于k的情况。 \u003e假设数组的前缀和数组为prefixSum，其中prefixSum[i]表示从数组起始位置到第i个位置的元素之和。那么对于任意的两个下标i和j（i \u003c j），如果prefixSum[j] - prefixSum[i] = k，即从第i个位置到第j个位置的元素之和等于k，那么说明从第i+1个位置到第j个位置的连续子数组的和为k。 通过遍历数组，计算每个位置的前缀和，并使用一个哈希表来存储每个前缀和出现的次数。在遍历的过程中，我们检查是否存在prefixSum[j] - k的前缀和，如果存在，说明从某个位置到当前位置的连续子数组的和为k，我们将对应的次数累加到结果中。 这样，通过遍历一次数组，我们可以统计出和为k的连续子数组的个数，并且时间复杂度为O(n)，其中n为数组的长度。 # 代码 class Solution: def subarraySum(self, nums: List[int], k: int) -\u003e int: from collections import defaultdict d = defaultdict(int) d[0] = 1 prefix = 0 res = 0 for num in nums: prefix += num temp = prefix - k if temp in d: res += d[temp] d[prefix] += 1 return res ","date":"2024-03-14","objectID":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:0:0","tags":["算法题"],"title":"和为K的子数组","uri":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["","算法题"],"content":"题目地址 # 思路 通过前缀和+哈希表，并有简单的数学变换。前缀和即 \\(y[i]=y[i-1]+x[i]\\) 类比于accumlate函数，注意前缀和思想也可以应用为“前缀积、后缀和、后缀积”等思想。238. 除自身以外数组的乘积 - 力扣（LeetCode） \u003e 使用前缀和的方法可以解决这个问题，因为我们需要找到和为k的连续子数组的个数。通过计算前缀和，我们可以将问题转化为求解两个前缀和之差等于k的情况。 \u003e假设数组的前缀和数组为prefixSum，其中prefixSum[i]表示从数组起始位置到第i个位置的元素之和。那么对于任意的两个下标i和j（i \u003c j），如果prefixSum[j] - prefixSum[i] = k，即从第i个位置到第j个位置的元素之和等于k，那么说明从第i+1个位置到第j个位置的连续子数组的和为k。 通过遍历数组，计算每个位置的前缀和，并使用一个哈希表来存储每个前缀和出现的次数。在遍历的过程中，我们检查是否存在prefixSum[j] - k的前缀和，如果存在，说明从某个位置到当前位置的连续子数组的和为k，我们将对应的次数累加到结果中。 这样，通过遍历一次数组，我们可以统计出和为k的连续子数组的个数，并且时间复杂度为O(n)，其中n为数组的长度。 # 代码 class Solution: def subarraySum(self, nums: List[int], k: int) -\u003e int: from collections import defaultdict d = defaultdict(int) d[0] = 1 prefix = 0 res = 0 for num in nums: prefix += num temp = prefix - k if temp in d: res += d[temp] d[prefix] += 1 return res ","date":"2024-03-14","objectID":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84-1/:0:0","tags":["算法题"],"title":"和为K的子数组","uri":"/%E5%92%8C%E4%B8%BAk%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84-1/"},{"categories":["","算法题"],"content":"def find(x): if (p[x] != x): p[x] = find(p[x]) return p[x] 上面是y总的模板，实现了路径压缩。 ","date":"2024-03-13","objectID":"/%E5%B9%B6%E6%9F%A5%E9%9B%86/:0:0","tags":["算法题"],"title":"并查集","uri":"/%E5%B9%B6%E6%9F%A5%E9%9B%86/"},{"categories":["训练trick"],"content":"温度超参数t，一般为softmax结果除以该参数，或者在对比学习中，相似度除以参数t。 如图： 上图为无监督simcse中的损失函数。 t越大，结果越平滑，t越小，得到的概率分布更“尖锐”。 当t趋于0时： 此时只关注最困难的负样本（smax）。 当t趋于∞时： 此时对比损失对所有负样本的权重都相同。 因此t越大，可以避免陷入局部最优解。(局部最优是过早地确定了优化的梯度方向，失去了在其他方向上探索的机会。较大的温度使得各个方向上的梯度差异没有那么明显，从而获得了在早期更多的探索机会。) [!note] 温度系数的作用是调节对困难样本的关注程度：越小的温度系数越关注于将本样本和最相似的困难样本分开，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小 考虑两个极端情况，温度系数趋向于0时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。 可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。对比损失也是这样的。这种性质更有利于形成在超球面均匀分布的特征。 语言建模 (lena-voita.github.io)中有个小游戏可以看到随着温度的变化导致概率分布的变化 参考 CVPR2021自监督学习论文: 理解对比损失的性质以及温度系数的作用 - 知乎 (zhihu.com) ","date":"2024-01-14","objectID":"/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/:0:0","tags":["训练trick","温度超参数"],"title":"温度超参数","uri":"/%E6%B8%A9%E5%BA%A6%E8%B6%85%E5%8F%82%E6%95%B0/"},{"categories":["算法题"],"content":"leetcode地址：953. 验证外星语词典 - 力扣（LeetCode） ","date":"2024-01-08","objectID":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/:0:0","tags":["算法题","验证外星语词典"],"title":"验证外星语词典","uri":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/"},{"categories":["算法题"],"content":"简单方法 python列表之间也可以进行比较（太灵活了），比如[1, 2, 3] \u003c [2, 2, 3]成立，即按照字典序进行比较，与其是一样的比较规则。因此对于本题可以利用python的特性轻松解决。 好久没写python了，变得很生疏，一开始写的很蠢： class Solution: def isAlienSorted(self, words: List[str], order: str) -\u003e bool: d = dict(zip(order, range(len(order)))) words = list(map(lambda s: [d[i] for i in s], words)) print([1, 2, 3] \u003c [2, 2, 3]) return words == sorted(words) 后来想起来了sorted中还有个key参数，并且列表还有个index方法（我基本上没用过），于是改成了一行 class Solution: def isAlienSorted(self, words: List[str], order: str) -\u003e bool: return words == sorted(words, key=lambda w:[order.index(x) for x in w]) ","date":"2024-01-08","objectID":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/:1:0","tags":["算法题","验证外星语词典"],"title":"验证外星语词典","uri":"/%E9%AA%8C%E8%AF%81%E5%A4%96%E6%98%9F%E8%AF%AD%E8%AF%8D%E5%85%B8/"},{"categories":["Deep Learning","网络正则化"],"content":"L1正则化 L2正则化 权重衰减 L2正则化和权重衰减的区别 L2正则化是在损失函数上做文章。 权重衰减是在梯度更新时增加一项。 ","date":"2023-03-22","objectID":"/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/:0:0","tags":["Deep Learning","网络正则化","L1 L2正则化"],"title":"L1 L2正则化","uri":"/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/"},{"categories":["Deep Learning","网络正则化"],"content":"pre-norm Pre-norm:\\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\\) \\(先来看Pre-norm^{+},递归展开：\\) \\[X_{t+1}=X_t+F_t(Norm(X_t))\\] \\(=X_{0}+F_{1}(Norm(X_{1}))+\\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\\) 其中，展开\\(^{+}\\)后的每一项( \\(F_{1}( Norm( X_{1}) ) , \\ldots\\), \\(F_{t- 1}( Norm( X_{t- 1}) )\\), \\(F_{t}( Norm( X_{t}) )\\))之间都是同一量级的， 所以\\(F_1(Norm(X_1))+\\ldots F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\\)和 \\(F_1(Norm(X_1))+\\ldots F_{t-1}(Norm(X_{t-1}))\\)之间的区别就像t和t-1的区别一样，我们可以将 其记为\\(X_t+ 1= \\mathscr{O} ( t+ 1)\\) . 这种特性就导致当t足够大的时候，\\(X_{t+1}\\)和\\(X_t\\)之间区别可以忽略不计（直觉上），那么就有： \\[F_t(X_t)+F_{t+1}(X_{t+1})\\approx F_t(X_t)+F_{t+1}(X_t)=(F_t\\bigoplus F_{t+1})(X_t)\\] 这就是所谓的增加宽度，而没有增加深度。从而导致pre-norm的精度不高。 ## post-norm Post-norm:\\(X_{t+1}=Norm(X_{t}+F_{t}(x_{t}))\\) 本来layernorm是为了缓解梯度消失，但是在post-norm这里却成为了梯度消失的罪魁祸首。也导致了收敛较难、需要大量调参。 \\[X_{t+1}=Norm(X_t+F_t(X_t))=\\frac{X_t+F_t(X_t)}{\\sqrt{2}}\\] \\[=\\frac{X_0}{\\sqrt{2}^{t+1}}+\\frac{F_0(X_0)}{\\sqrt{2}^{t+1}}+\\ldots+\\frac{F_{t-1}(X_{t-1})}{\\sqrt{2}^2}+\\frac{F_t(X_t)}{\\sqrt{2}}\\:(\\] 这个结构跟pre-norm比起来充分考虑了所有分支 (残差\\(^{+})\\) 的输出，做到了真正增加深度，自然精度会相对好一些。 不过它也有它很显然的问题，当t足够大、也就是叠加的attention层足够多以后，底层那些分支(残差)的影响力被衰减掉了，残差有利于解决梯度消失，但是在Post Norm中，残差这条通道被严重削弱了，越靠近输入，削弱得越严重，残差“名存实亡”，那么势必会有梯度消失的问题，这也就是文章开头所说的postnorm难收敛、参数难调的原因。本来我们做Norm也是为了处理梯度消失，但从分析看来，transformer结构中的layernorm\\(^{+}\\)并没有完全实现它的作用。那这就意味着transformer原始结构的失败吗？并不是的，因为这种梯度消失的问题在整个结构上来看(配合上adam系优化器和学习率warmup，warmup对于post-norm极为重要) 是并不明显的。 离输入层的残差影响力弱这一特性，也有它的用武之地，比如在finetune的时候，我们就希望不要过多调整靠近输入层的参数、以免破坏预训练的效果。 ","date":"2023-03-22","objectID":"/layer-norm/:1:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"warmup的重要性 Post-LN Transformer在训练的初始阶段，输出层附近的期望梯度非常大，所以，如果没有warm-up，模型优化过程就会炸裂，非常不稳定。 模型对越靠后的层越敏感，也就是越靠后的层学习得越快，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。 很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但 Adam 的更新量是常数量级的，梯度不准，但更新量依然是常数量级，意味着可能就是一个常数量级的随机噪声了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。 从上图中就可以看出来，post-ln在开始阶段层数越高梯度越大，此时需要小学习率，而当warmup完后，梯度变得很小（绿色部分）。此时可以使用大学习率。 ","date":"2023-03-22","objectID":"/layer-norm/:2:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"很好的总结回答 ","date":"2023-03-22","objectID":"/layer-norm/:3:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"为什么 layer norm 会使方差累积和训练不稳定 让我们来追踪一下 Pre-Norm 结构中数据 x 的方差变化： 假设我们有一个输入 \\(x_l\\)，它进入第 l 个残差块。 在 Sublayer 分支中，我们首先对 \\(x_l\\) 进行 LayerNorm。\\(LayerNorm(x_l)\\) 的输出具有严格的均值为0，方差为1 的特性。 这个归一化后的结果被送入 Sublayer（例如一个全连接网络或注意力层）。经过计算后，输出 \\(Sublayer(LayerNorm(x_l))\\) 的方差通常不再是1，我们假设它的方差是 Var(S)。 最后，这个 Sublayer 的输出被加回到原始的、未归一化的输入 \\(x_l\\) 上，得到该层的最终输出 \\(x_{l+1}\\)： \\(x_{l+1} = x_l + Sublayer(LayerNorm(x_l))\\) 现在我们来计算 x_{l+1} 的方差。在统计学中，如果两个变量（这里是 x_l 和 Sublayer(…)）大致不相关，那么它们和的方差约等于它们各自方差的和： \\(Var(x_{l+1}) ≈ Var(x_l) + Var(Sublayer(LayerNorm(x_l)))\\) \\(Var(x_{l+1}) ≈ Var(x_l) + Var(S)\\) 这个公式清晰地揭示了问题所在：每一层的输出方差 \\(Var(x_{l+1})\\) 都是在前一层方差 \\(Var(x_l)\\) 的基础上，又增加了一个正数 \\(Var(S)\\)。 因此，随着网络层数的加深（l 变大），\\(x_l\\) 的方差会像滚雪球一样不断累加，导致主干分支上的数值越来越大。这可能会在训练后期导致数值不稳定。 对比 Post-Norm： 在 Post-Norm 中，\\(x_{l+1} = LayerNorm(...)\\)。无论括号里的值 \\((x_l + Sublayer(x_l))\\) 方差多大，经过最后的 LayerNorm 后，输出 \\(x_{l+1}\\) 的方差都会被强制重置为 1。因此，Post-Norm 不存在方差累积增大的问题，但它也因此带来了训练初期不稳定、需要特殊学习率热身（warm-up）等其他问题。 Prenorm公式还意味着，流经网络主干道（即 x_0, x_1, x_2, …）的信号，其“能量”（方差）在一层层地单调递增。经过几十上百层网络后，x_l 的数值大小（即激活值）就会变得非常巨大。这就是“巨量激活值”（massive activations）的直接来源。 现在我们有了“巨量激活值”，为什么这会导致训练不稳定呢？这与反向传播和参数更新的机制有关。 产生巨量梯度 (Massive Gradients)：在反向传播计算梯度时，很多层的梯度都与其前向传播时的激活值成正比。举一个简化的例子，对于一个权重 W，它的梯度 ∂L/∂W 通常会包含一个与输入激活值 x 相乘的项。如果 x 是一个“巨量激活值”，那么计算出的梯度 ∂L/∂W 也会是一个“巨量梯度”。 破坏优化器状态 (Corrupting Optimizer State)：像 Adam 这样的现代优化器，会维护梯度的历史信息（如一阶矩和二阶矩的移动平均）。一个突然出现的巨量梯度会严重“污染”这些历史平均值，使得优化器对学习率的自适应调整产生剧烈摆动。 灾难性的参数更新 (Catastrophic Weight Updates)：优化器使用这个被污染的巨量梯度来更新模型权重：W_new = W_old - lr * massive_gradient。这个更新步长会异常巨大，相当于在复杂的损失地形上进行了一次“盲目的、大跨步的跳跃”。 损失尖峰 (Loss Spikes)：这次“大跳跃”很可能会让模型参数跳到一个非常糟糕的位置，导致模型的预测性能急剧下降，训练损失（Loss）瞬间飙升，在训练曲线上就形成了一个“尖峰”。 训练发散 (Training Divergence)：如果这个“尖峰”过于极端，参数更新可能会导致数值溢出（inf）或非法操作（NaN），整个训练过程就此崩溃，即“训练发散”。 ","date":"2023-03-22","objectID":"/layer-norm/:4:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"Adam如何缓解梯度消失 其实。最关键的原因是，在当前的各种自适应优化技术“下，我们已经不大担心梯度消失问题了。这是因为，当前 NLP 中主流的优化器是 Adam 及其变种。对于 Adam 来说，由于包含了动量和二阶矩校正，所以近似来看，它的更新量大致上为 \\[\\Delta\\theta=-\\eta\\frac{\\mathbb{E}_{t}[g_{t}]}{\\sqrt{\\mathbb{E}_{t}[g_{t}^{2}]}}\\] 可以看到，分子分母是都是同量纲的，因此分式结果其实就是 (1)的量级，而更新量就是 (n)量级。也就是说，理论上只要梯度的绝对值大于随机误差，那么对应的参数都会有常数量级的更新量（意思就是参数的更新量与梯度的关系不是很大，因此受梯度消失影响较小）；这跟 SGD 不一样，SGD 的更新量是正比于梯度的，只要梯度小，更新量也会很小，如果梯度 过小，那么参数几乎会没被更新。 所以，Post Norm 的残差虽然被严重削弱，但是在 base、large 级别的模型中，它还不至于削弱到小于随机误差的地步，因此配合 Adam 等优化器，它还是可以得到有效更新的，也就有可能成功训练了。当然，只是有可能，事实上越深的 Post Norm 模型确实越难训练，比如要仔细调节学习率和 Warmup 等 ","date":"2023-03-22","objectID":"/layer-norm/:5:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"Deep-norm \\(最后再提一下DeepNet中结合Post-LN^+的良好性能以及Pre-LN的训练稳定性做出的改良\\)。 \\[X_{t+1}=Norm(\\alpha X_t+F_t(X_t))\\text{(6)}\\] \\(它在add norm之前给输入乘了一个up-scale^+的常数系数 α\u003e1\\)。 现在 (5) 的展开为： \\[X_{t+1}=\\frac{\\alpha^{t+1}X_{0}}{\\sqrt{2}^{t+1}}+\\frac{\\alpha^{t}F_{0}(X_{0})}{\\sqrt{2}^{t+1}}+\\ldots+\\frac{\\alpha F_{t-1}(X_{t-1})}{\\sqrt{2}^{2}}+\\frac{F_{t}(X_{t})}{\\sqrt{2}}\\] 因为\\(\\alpha\u003e1\\) ,所以它能够在保留post-norm真正增加了深度这优点的同时，一定程度避免了梯度消失。（本质还是post-norm） ","date":"2023-03-22","objectID":"/layer-norm/:6:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","网络正则化"],"content":"参考 Transformer梳理（一）：Post-Norm VS Pre-Norm - 知乎 (zhihu.com) 模型优化漫谈：BERT的初始标准差为什么是0.02？ - 科学空间|Scientific Spaces (kexue.fm) 为什么Pre Norm的效果不如Post Norm？ - 科学空间|Scientific Spaces (kexue.fm) 香侬读 | Transformer中warm-up和LayerNorm的重要性探究 - 知乎 (zhihu.com) Bert/Transformer 被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com) ","date":"2023-03-22","objectID":"/layer-norm/:7:0","tags":["Deep","Learning","网络正则化"],"title":"Layer Norm","uri":"/layer-norm/"},{"categories":["Deep Learning","损失函数"],"content":"在机器学习中，hinge loss是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为： 其中 \\(\\hat{y}\\) 表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。）， \\(y\\) 表示正确的类别。 - 如果 \\(\\hat{y}y\u003c1\\) ，则损失为： \\(1-\\hat{y}y\\) - 如果\\(\\hat{y}y\u003e1\\) ，则损失为：0 ","date":"2023-03-13","objectID":"/hinge-loss/:0:0","tags":["Deep Learning","损失函数","hinge loss"],"title":"hinge loss","uri":"/hinge-loss/"},{"categories":["面经"],"content":"前几天试着投了简历，没想到有两家约了面试，一个是得物一个是北京百分点，得物面试没有怎么准备，太仓促了，二面挂了，百分点拿到了offer，但决定考研了就没去，记录一下面试的问题。岗位是nlp算法岗。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:0:0","tags":["面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"一面：技术面 先自我介绍，然后介绍了一下项目。根据印象提问有以下内容： 有没有数据不平衡问题，是怎么解决的？欠采样，过采样，focal loss，着重介绍了一下focal loss的参数含义。 对分类问题最后一层要怎么做：max、mean，全连接 有没有尝试过别的模型：没有，然后面试官说其实textcnn和fasttext效果不一定比bert效果差。 对传统的神经网络了解吗？了解。介绍一下lstm。按照三个门分析就行，哪个门是什么作用，注意一下激活函数的不同。 介绍一下bert，从输入开始介绍就行，把两个任务也展开说一下。 介绍一下tfidf，这个很简单，说一下tf和idf的含义和公式就行。 数据增强怎么做的？同义词替换、回译。 介绍一下提示学习。说一下主要思想，还有离散prompts和连续prompts。 大体就记得问了这些，总体来说不是很难，而且大多数都是神经网络相关的，准备了一些传统的机器学习方法也没有用上。 然后是两道算法题： 第一题：已知一随机发生器，产生0的概率是p，产生1的概率是1-p， 现在要你构造一个发生器， 使得它构造0和1的概率均为 1/2； def res(): a = random() b = random() if a == 0 and b == 1: return 0 if a == 1 and b == 0: return 1 else: return res() 由于需要产生1/2，而用1位0，或1位1无法产生等概率， 因此，考虑将随机数扩展成2位： 00 pp 01 p(1-p) 10 (1-p)p 11 (1-p)(1-p) 有上述分析知道，01和10是等概率的，因此我们只需要产生01和10就行了。 于是可以，遇到00和11就丢弃，只记录01和10。可以令，01表示0,10表示1，则等概率1/2产生0和1了。 当时面试官问了这个问题的时候有点懵，因为之前从来没有做过，但是经过面试官的提示也是写出来了。 第二题是LCS问题，就是最长公共子序列问题，当时想到了使用动态规划，但是懒得在纸上推转移方程了，就直接暴力解决了。 def LCS(s1, s2): dp = [[0 for _ in range(len(s1))] for _ in range(len(s2))] res = 0 for i in range(len(s1)): for j in range(i, len(s2)): if s1[i:j+1] in s2: res = max(res, j+1-i) return res print(LCS(\"baced\", \"acefg\")) ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:1:0","tags":["面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"二面综合面 上来也是自我介绍加项目介绍，有些问题和一面重复了，就不赘述。 介绍几种bert的变体，albert、roberta，就介绍了这两种。 bert的mask策略。 具体的也忘了，当时也没有想直接记录下来，后面就等到了口头offer，过几天也发了正式offer，但是因为要考研等一些原因就拒了，等考完研再找吧。这次只在软件上投了投简历，没有内推等，还是挺有收获的。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:2:0","tags":["面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["面经"],"content":"总结 这几轮面试还是很有收获的，也算是模拟了以后的考研复试，虽然以后大概率是线下复试了。还有就是简历上最最重要的就是实习经历，我之前也没有实习经历，因此没有实习经历的情况下最重要的就是项目，别的基本上都没有问，不过还是要保证你写上去的都是真的会的。因此重要的就是实习和项目，获奖经历可能只是给面试的机会更大一些。这次拒绝offer少了一次实习经历还算有点可惜，不过相对而言还是考研重要一些。 ","date":"2023-03-10","objectID":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/:3:0","tags":["面经"],"title":"北京百分点面经","uri":"/%E5%8C%97%E4%BA%AC%E7%99%BE%E5%88%86%E7%82%B9%E9%9D%A2%E7%BB%8F/"},{"categories":["算法题","串"],"content":"KMP是字符串匹配问题的算法。“字符串A是否为字符串B的子串?如果是的话出现在B的哪些位置?”该问题就是字符串匹配问题，字符串A称为模式串，字符串B称为主串。 ## BF算法 BF算法就是暴力匹配，即对主串从头开始慢慢移动模式串，直到找到相匹配的位置。 代码很简单暴力： 假设n为主串长度，m为模式串长度。 每一轮字符串比较：最差的情况为模式串最后一个字与主串不同其他都相同（如模式串为AAB，主串对应部分为AAC），必须走完整个字符串才能得出结果，因此复杂度为O(m)。 所有轮字符串比较：最差的情况是移动到最后一次比较才寻找得到，总共需要n-m+1次，主串通常比模式串长很多，故Brute-Force时间复杂度为O(nm) 在匹配上没有办法进行优化，因此可以从模式串的移动上入手，由此引入了Kmp算法。 ","date":"2023-03-08","objectID":"/kmp/:0:0","tags":["算法题","串"],"title":"KMP","uri":"/kmp/"},{"categories":["算法题","串"],"content":"KMP KMP 算法的不同之处在于，它会花费空间来记录一些信息。目的就是为了减少匹配的趟数，算法的核心就是每次匹配过程中推断出后续完全不可能匹配成功的匹配过程，从而减少比较的趟数。 ","date":"2023-03-08","objectID":"/kmp/:1:0","tags":["算法题","串"],"title":"KMP","uri":"/kmp/"},{"categories":["算法题","串"],"content":"Next数组 next数组实质上就是找出模式串中前后字符重复出现的个数，为了能够跳跃不可能匹配的步骤。 next数组的定义为：next[i]表示模式串A[0]至A[i]这个字串，使得前k个字符等于后k个字符的最大值，特别的k不能取i+i,因为字串一共才i+1个字符，自己跟自己相等毫无意义。 如何确定在移动过程中需要跳过多少步呢？下图更直观的体现了跳跃的过程： 也就是跳到模式串中的后缀相同字符串开始。因为这时可以确定前面的字符串肯定无法匹配了，过的趟数=匹配上字符串中间字符长度-重复字符串长度 在实际代码编写中，移动的实际上是模式串的匹配位置。前面展示的只是理解减小匹配次数这一过程。实际上跳动就是模式串比较指针的移动。模式串向右移动也就是比较指针向左移动，移动的距离就是跳过的趟数 移动后的指针为：\\(j=j-(j-next[j-1]) =next[j-1]\\) 其中原来的j为匹配成功的字符串长度，也就是匹配失败的指针位置。next[j-1]就是匹配成功的字符串的最长相同前后缀数，也就是要跳转到的指针的下标。（下标从0开始的，如果下标从1开始则在原来基础上+1就是要跳转到的位置，这也是为什么书上要+1）。 即下图所示。 然后再用移动后的指针和主串的上一轮匹配错误的位置相比，这时已经保证了前面的字符已经匹配了，即当前指针前面的子串已经和上一轮匹配成功的子串的最长后缀相同了。 因此可以看到，Kmp算法的核心就是构造Next数组。 最简单的方法就是根据定义暴力构造时间复杂度为O(m2法)：： 第二种构建方案，是一种递推的方式进行构建，时间复杂度为O(n+m): 考虑：如果next[0], next[1], … next[x-1]均已知，那么如何求出 next[x] ？我们已经知道next[x-1],标记next[x-1]=temp,则可以讨论A[temp]和A[x]的值，分2种情况讨论： 第一种情况：A[temp]等于A[x]，也就是说在前一个next结果上又多了一个字符串相同的长度，因此next[x]为next[x-1]+1 这种情况说明了x-1时最长前缀的后一位和x位置的字符相同，则说明了前缀和后缀都增加了相同的1位，next[x] = next[x-1] + 1。 （这里给一下我自己的理解，因为next中保存的是前后缀最大相同的长度，因此通常代表着最大相同前缀的后一位） 第二种情况：当A[temp]和A[x]不相等的时候，我们需要缩小temp,把temp变成next[temp-1]，直到A[temp]=A[x]为止。A[now]=A[x]时，就可以直接向右扩展了。 如何理解这张图，当发现A[temp] != A[x]的时候，就一直缩小temp，也就是temp = next[temp-1]，也就是找原来的最长前缀中对应的next[temp-1]，因为前缀中的相同前后缀最大长度（图中为2）肯定也与后缀中的相同前后缀最大长度（图中为2）相同，也说明了前缀中的前缀（图中为0和1上的A和B）等于后缀中的后缀（图中为10和11上的A和B），此时还是原来的思路temp为前缀中的前缀的下一位，判断是否与后缀中的后缀的下一位相同（就是当前的A[x]），如果相同就是第一种情况，next[x] = temp + 1，如果一直没找到则设为0，说明没有。 学到这不得不感叹kmp三位大佬的恐怖。。 大体的思想就是这样，不过如果对于做题来说的话，有很多不一样的地方，比如王道书上的next数组是从下标1开始的，对于本文的内容，需要得到匹配失败位置-1的next数组值，因此书上将整体右移了一位，开头补上了-1，这样匹配失败的位置就是对应的next数组值。又下一次匹配要从最长前缀的下一位开始，而对于下标从0开始，next数组值就是下一位（前面解释了），而对于下标从1开始，需要再+1才是对应下标，因此也对next数组最后进行了+1操作。不管怎么样，next的含义都是最长前缀的下一个位置。主要思想都是一样的，随机应变即可。 ","date":"2023-03-08","objectID":"/kmp/:1:1","tags":["算法题","串"],"title":"KMP","uri":"/kmp/"},{"categories":["算法题","串"],"content":"代码： class KMP: def __init__(self, text, pattern) -\u003e None: self.text = text self.pattern = pattern self.next = [] def init_next(self): self.next.append(0) # first for i, s in enumerate(self.pattern[1:], 1): tmp = self.next[i-1] if s == self.pattern[tmp]: self.next.append(tmp + 1) else: while self.pattern[tmp] != s and tmp != 0: tmp = self.next[tmp-1] if tmp == 0 and self.pattern[tmp] != s: self.next.append(0) continue self.next.append(tmp + 1) def search(self): i = 0 # text index j = 0 # partten index while i \u003c len(self.text): if self.text[i] == self.pattern[j]: i += 1 j += 1 elif j: j = self.next[j-1] else: i += 1 if j == len(self.pattern): return i - j return -1 kmp = KMP(\"hello\", \"ll\") kmp.init_next() print(kmp.search()) print(kmp.next) ","date":"2023-03-08","objectID":"/kmp/:1:2","tags":["算法题","串"],"title":"KMP","uri":"/kmp/"},{"categories":["算法题","串"],"content":"nextval 还有一种改进的next，因为匹配错误的位置重新移动指针后的位置的值可能与原来的值相同，因此在构建next数组的时候可以直接跳转到不相同的位置，这样就减小了重复的比较。 比如aaabaaaab和aaaab，当在主串中第一个b位置出错时，此时模式串为a，会比较str[next[2]]与b，但此时str[next[2]]也是a，再次比较也会错误，但再移动后发现还是a还是错误，这些比较都是没有意义的，最后要移动到模式串开头，然后将主串指针+1再比较，所以构建nextval减少了这些比较。 我是根据已经构建好的next数组推理来的。代码如下： def create_nextval(self): length = len(self.pattern) self.nextval = [0] * length for i in range(1, len(self.next)): tmp = self.next[i-1] if self.pattern[tmp] != self.pattern[i]: self.nextval[i-1] = tmp else: while tmp and self.pattern[tmp] == self.pattern[i]: tmp = self.next[tmp-1] self.nextval[i-1] = tmp 因为用不到最后一位的数值，所以考研书上就直接全部右移一位，理论上都是相同的。原理就是匹配错误的位置的字符和要跳转到的位置的字符相同的话就要继续跳转直到等于0或者不相同为止。 ","date":"2023-03-08","objectID":"/kmp/:1:3","tags":["算法题","串"],"title":"KMP","uri":"/kmp/"},{"categories":["Machine Learning"],"content":"特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。 根据特征选择的形式，可分为三大类： - Filter(过滤法)：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选 - Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征 - Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的） ## 过滤式 基本想法是：分别对每个特征 \\(x_i\\) ，计算 \\(x_i\\) 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征\\(x_i\\) 。 - Pearson相关系数 - 卡方验证 - 互信息和最大信息系数 - 距离相关系数 - 方差选择法 ### ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:0:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Machine Learning"],"content":"包裹式 ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:1:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Machine Learning"],"content":"嵌入式 ","date":"2023-03-08","objectID":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/:2:0","tags":["Machine Learning","特征选择"],"title":"特征选择","uri":"/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"categories":["Deep Learning","训练trick"],"content":"介绍 早停止（Early Stopping）是 当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段。 如何判断已经收敛？主要看以下几点： - 验证集上的Loss在模型多次迭代后，没有下降 - 验证集上的Loss开始上升。 这时就可以认为模型没有必要训练了，可以停止了，因为训练下去可能就会发生过拟合，所以早停法是一种防止模型过拟合的方法。 ","date":"2023-03-06","objectID":"/early-stopping/:1:0","tags":["Deep Learning","训练trick","early-stopping"],"title":"early-stopping","uri":"/early-stopping/"},{"categories":["Deep Learning","训练trick"],"content":"代码 import numpy as np import torch import os class EarlyStopping: \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\" def __init__(self, save_path, patience=7, verbose=False, delta=0): \"\"\" Args: save_path : 模型保存文件夹 patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 \"\"\" self.save_path = save_path self.patience = patience self.verbose = verbose self.counter = 0 self.best_score = None self.early_stop = False self.val_loss_min = np.Inf self.delta = delta def __call__(self, val_loss, model): score = -val_loss if self.best_score is None: self.best_score = score self.save_checkpoint(val_loss, model) elif score \u003c self.best_score + self.delta: self.counter += 1 print(f'EarlyStopping counter: {self.counter} out of {self.patience}') if self.counter \u003e= self.patience: self.early_stop = True else: self.best_score = score self.save_checkpoint(val_loss, model) self.counter = 0 def save_checkpoint(self, val_loss, model): '''Saves model when validation loss decrease.''' if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f} --\u003e {val_loss:.6f}). Saving model ...') path = os.path.join(self.save_path, 'best_network.pth') torch.save(model.state_dict(), path) # 这里会存储迄今最优模型的参数 self.val_loss_min = val_loss ","date":"2023-03-06","objectID":"/early-stopping/:2:0","tags":["Deep Learning","训练trick","early-stopping"],"title":"early-stopping","uri":"/early-stopping/"},{"categories":["Deep Learning","损失函数"],"content":"Focal Loss Focal Loss主要是为了解决类别不平衡的问题，Focal Loss可以运用于二分类，也可以运用于多分类。下面以二分类为例： ","date":"2023-03-06","objectID":"/focal-loss/:0:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"原始Loss 原始的二分类： 其中 所以： 很容易理解，因为CE就是softmax在二分类的形式，实际运算中只关注对应标签的概率，对于二分类，如果是负样本的话，预测概率小于0.5则说明预测正确，则对应的实际的概率应该为1-p。最大化概率，就是最大化Log概率，也就是最小化-log概率。 ","date":"2023-03-06","objectID":"/focal-loss/:0:1","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"什么是易分类样本 对于正样本，如果预测的结果总是在0.5以上，就是易分类样本，如果总是在0.5以下，则说明是难分类样本。 对于负样本，如果预测的结果总是在0.5以下，就是易分类样本，如果总是在0.5以上，则说明是难分类样本。 对应\\(p_t\\)来说，就是\\(p_t\u003e0.5\\)为易分类，\\(p_t\u003c0.5\\)为难分类。 ","date":"2023-03-06","objectID":"/focal-loss/:0:2","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"gamma参数 在模型训练的时候，我们更希望关注难分类的样本，因此focal loss在原始loss上增加了一项，对整体进行了衰减： 对于公式中的参数\\(\\gamma\\)，一般会选择2，对于易分类的样本，即\\(p_t\u003e0.5\\)的样本，\\(1-p_t\\)则会小于0.5，则loss会衰减的更多，最终的损失就变的很小。而对于难分类的样本，loss会衰减的比较小，通过这种衰减的对比，则变相增加了模型对于难分类样本的权重。 ","date":"2023-03-06","objectID":"/focal-loss/:0:3","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"alpha参数 对于二分类任务，负样本的数量远远多于正样本，导致模型更多关注在负样本上，忽略正样本。因此在使用交叉熵损失的时候通常会增加一个平衡参数用来调节正负样本的比重。 所以会增加一个平衡参数来调节正负样本的比重。 其实这就是balanced cross entropy，可以将它引入focal loss 在式子中，\\(\\gamma\\)占据了主导地位，因此其实不用太在意\\(\\alpha\\)的数值。 ","date":"2023-03-06","objectID":"/focal-loss/:0:4","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"对于多分类 对于多分类任务，其实是一样的，因为如果一个类别的样本预测结果总是大于0.5，也说明它是易分类的，对于平衡因子，在实现的时候，可以提前设置好各类别的平衡因子，对于每一个类别都有一个对应的。 ","date":"2023-03-06","objectID":"/focal-loss/:1:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"为什么有效 focal loss从样本难易分类角度出发，解决样本非平衡带来的模型训练问题。 直觉上来讲样本非平衡造成的问题就是样本数少的类别分类难度较高。因此从样本难易分类角度出发，使得loss聚焦于难分样本，解决了样本少的类别分类准确率不高的问题，当然难分样本不限于样本少的类别，也就是focal loss不仅仅解决了样本非平衡的问题，同样有助于模型的整体性能提高。 ","date":"2023-03-06","objectID":"/focal-loss/:2:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"思考 难分类样本与易分类样本其实是一个动态概念，也就是说 p 会随着训练过程而变化。原先易分类样本即 p大的样本，可能随着训练过程变化为难训练样本即p小的样本。 上面讲到，由于Loss梯度中，难训练样本起主导作用，即参数的变化主要是朝着优化难训练样本的方向改变。当参数变化后，可能会使原先易训练的样本 p 发生变化，即可能变为难训练样本。当这种情况发生时，可能会造成模型收敛速度慢，正如苏剑林在他的文章中提到的那样。 为了防止难易样本的频繁变化，应当选取小的学习率。 ## 代码 ### 二分类 class Focal_Loss(): \"\"\" 二分类Focal Loss \"\"\" def __init__(self,alpha=0.25,gamma=2): super(Focal_Loss,self).__init__() self.alpha=alpha self.gamma=gamma def forward(self,preds,labels): \"\"\" preds:sigmoid的输出结果 labels：标签 \"\"\" eps=1e-7 loss_1=-1*self.alpha*torch.pow((1-preds),self.gamma)*torch.log(preds+eps)*labels loss_0=-1*(1-self.alpha)*torch.pow(preds,self.gamma)*torch.log(1-preds+eps)*(1-labels) loss=loss_0+loss_1 return torch.mean(loss) ","date":"2023-03-06","objectID":"/focal-loss/:3:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"多分类 class Focal_Loss(): def __init__(self,weight,gamma=2): super(Focal_Loss,self).__init__() self.gamma=gamma self.weight=weight def forward(self,preds,labels): \"\"\" preds:softmax输出结果 labels:真实值 \"\"\" eps=1e-7 y_pred =preds.view((preds.size()[0],preds.size()[1],-1)) #B*C*H*W-\u003eB*C*(H*W) target=labels.view(y_pred.size()) #B*C*H*W-\u003eB*C*(H*W) ce=-1*torch.log(y_pred+eps)*target floss=torch.pow((1-y_pred),self.gamma)*ce floss=torch.mul(floss,self.weight) floss=torch.sum(floss,dim=1) return torch.mean(floss) ","date":"2023-03-06","objectID":"/focal-loss/:3:1","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","损失函数"],"content":"参考 https://zhuanlan.zhihu.com/p/266023273 ","date":"2023-03-06","objectID":"/focal-loss/:4:0","tags":["Deep Learning","损失函数","focal loss"],"title":"focal loss","uri":"/focal-loss/"},{"categories":["Deep Learning","训练trick"],"content":"在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning rate，然后随着训练的批次增加，逐渐增大学习率，直到达到原本想要设置的学习率。 关于warmup的好处，有： - 有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳 - 有助于保持模型深层的稳定性。 warmup有助于网络的收敛，当达到预期的学习率时，之后的步骤就可以每固定批次进行学习率衰减，防止过拟合，以慢慢达到收敛。 ","date":"2023-03-05","objectID":"/warmup/:0:0","tags":["Deep Learning","训练trick","warmup"],"title":"warmup","uri":"/warmup/"},{"categories":["Deep Learning","训练trick"],"content":"神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为onehot本身就是一个稀疏的向量，如果所有无关类别都为0的话，就可能会疏忽某些类别之间的联系。 具体的缺点有： - 真是标签与其它标签之间的关系被忽略了，很多有用的知识学不到了。 - 倾向于让模型更加武断，导致泛化性能差 - 面对有噪声的数据更容易收到影响。 label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 增加label smoothing后真实的概率分布有如下改变： ","date":"2023-03-05","objectID":"/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/:0:0","tags":["Deep Learning","训练trick","标签平滑"],"title":"标签平滑","uri":"/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/"},{"categories":["Deep Learning","训练trick"],"content":" 基本原则：快速试错。 小步试错，快速迭代 可以试试无脑的配置 实时打印一些结果 自动调参：网格搜索、random search、贝叶斯优化、 参数初始化 学习率warmup，慢慢增加，然后学习率衰减。 batch_size和lr 大的batchsize收敛到sharp minimum，而小的batchsize收敛到flat minimum，后者具有更好的泛化能力。两者的区别就在于变化的趋势，一个快一个慢，如下图，造成这个现象的主要原因是小的batchsize带来的噪声有助于逃离sharp minimum。 大的batchsize性能下降是因为训练时间不够长，本质上并不少batchsize的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。 如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。 尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。 ","date":"2023-03-02","objectID":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/:0:0","tags":["Deep Learning","训练trick","调参技巧"],"title":"调参技巧","uri":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"},{"categories":["Deep Learning","训练trick"],"content":"总结 学习率直接影响模型的收敛状态，batchsize则影响模型的泛化性能 ","date":"2023-03-02","objectID":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/:1:0","tags":["Deep Learning","训练trick","调参技巧"],"title":"调参技巧","uri":"/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"},{"categories":["Deep Learning","训练trick"],"content":"Min-Max公式 $$ {} {(x,y) }U[{r{adv}}L(,x+r_{adv},y)] $$ 内部max是为了找到worst-case的扰动，也就是攻击，其中， \\(L\\)为损失函数， \\(\\mathbb{S}\\) 为扰动的范围空间。 外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 \\(\\mathbb{D}\\) 是输入样本的分布。 简单理解就是在输入上进行梯度上升(增大loss)，在参数上进行梯度下降(减小loss) 加入扰动后的损失函数 $$ {} -P(y |x+r{adv};) \\[ 那扰动要如何计算呢？Goodfellow认为，**神经网络由于其线性的特点，很容易受到线性扰动的攻击。** # Fast Gradient Sign Method (FGSM) \\] r_{adv} = sgn(_{x}L(,x,y)) $$ # FGM ## 代码 代码来自[1]，注意的是一般扰动加在了embedding矩阵上，相当于x+r。 import torch class FGM(): def __init__(self, model): self.model = model self.backup = {} def attack(self, epsilon=1., emb_name='emb.'): # emb_name为embedding矩阵参数对应名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = epsilon * param.grad / norm param.data.add_(r_at) def restore(self, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.backup param.data = self.backup[name] self.backup = {} 使用时： # 初始化 fgm = FGM(model) for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad # 对抗训练 fgm.attack() # 在embedding上添加对抗扰动 loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 fgm.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() 参考文献 【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现 - 知乎 (zhihu.com) ","date":"2023-03-02","objectID":"/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/:0:0","tags":["Deep","Learning","训练trick","对抗训练"],"title":"对抗训练","uri":"/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/"},{"categories":["Deep Learning","训练trick"],"content":"数据不均衡 所谓的不平衡指的是不同类别的样本量差异非常大，或者少数样本代表了业务的关键数据（少量样本更重要），需要对少量样本的模式有很好的学习。样本类别分布不平衡主要出现在分类相关的建模问题上。样本类别分布不平衡从数据规模上可以分为大数据分布不平衡和小数据分布不平衡两种。 大数据分布不均衡。这种情况下整体数据规模大，只是其中的少样本类的占比较少。但是从每个特征的分布来看，小样本也覆盖了大部分或全部的特征。例如拥有1000万条记录的数据集中，其中占比50万条的少数分类样本便于属于这种情况。 小数据分布不均衡。这种情况下整体数据规模小，并且占据少量样本比例的分类数量也少，这会导致特征分布的严重不平衡。例如拥有1000条数据样本的数据集中，其中占有10条样本的分类，其特征无论如何拟合也无法实现完整特征值的覆盖，此时属于严重的数据样本分布不均衡。 如果不同分类间的样本量差异达到超过10倍就需要引起警觉并考虑处理该问题，超过20倍就要一定要解决该问题。 主要有三种解决方法： - 欠采样：在少量样本数量不影响模型训练的情况下，可通过对多数类样本欠采样，实现少数样本和多数样本均衡。 - 过采样：在少量样本数量不支持模型训练的情况下，可以通过对少数类样本过采样，实现少数样本和多数样本的均衡。 - 模型算法：通过引入有权重的模型算法，针对少量样本着重拟合，以提升对少量样本特征的学习。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:1:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"欠采样 通过减少分类中多数类样本的样本数量来实现样本均衡。通过欠采样，在保留少数类样本的同时，会丢失多数样本中的一些信息。经过欠采样，样本数量在变少。因此我个人并不倾向于这种方法。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"随机法 随机的删除一些多数类样本，使少数类样本和多数类样本数量达到均衡。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"原型生成 PG 算法主要是在原有样本的基础上生成新的样本来实现样本均衡，对多数类样本生成新的样本去替代原样本，使得样本数目减少, 具体做法如下： ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"原型选择 原理：从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。 NearMiss 采用一些启发式的规则来选择样本，根据规则的不同可分为 3 类,通过设定 version 参数来确定： - NearMiss-1：选择到最近的 K 个少数类样本平均距离最近的多数类样本 - NearMiss-2：选择到最远的 K 个少数类样本平均距离最近的多数类样本 - 3: 对于每个少数类样本选择 K 个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:2:3","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"过采样 过采样（over-sampling）方法通过增加分类中少数的数量来实现样本均衡，最直接的方法是简单的复制少数类样本形成多条记录，这种方式可能导致样本特征少而可能出现过拟合的问题。经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或者通过一定规则产生新的合成样本。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"随机复制 就是随机选择少量样本进行复制。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"SMOTE 在随机过采样的基础上，通过样本构造一方面降低了直接复制样本代理的过拟合的风险，另一方法实现了样本的均衡。比如样本构造方法 SMOTE（Synthetic minority over-sampling technique）及其衍生算法。 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:3:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"模型算法 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:0","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"cost sensitive算法 ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:1","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","训练trick"],"content":"focal loss 可以查看本博客focal loss内容Focal Loss ","date":"2023-03-02","objectID":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/:4:2","tags":["Deep Learning","训练trick","数据不平衡"],"title":"数据不平衡","uri":"/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"categories":["Deep Learning","GAN系列"],"content":"简介 生成对抗网络（Generative Adversarial Network，简称GAN）是无监督学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。 大白话： 说就是生成对抗网络有一个生成网络和一个判别网络，假设有一个真实数据和一个生成网络生成的假数据，那么判别网络就是识别出这两种数据，判别网络努力分类成功，生成网络努力生成和真实数据相似的数据使判别网络分类不出来。这就是所说的相互博弈的方式。 专业的话： 生成式对抗网络由生成器和判别器构成。生成对抗网络的核心目的是训练生成器。生成器的目的是生成与真实样本尽可能相似的“假样本”，判别器的目的是尽可能区分出给定样本是真实样本还是生成的“假样本”。二者目的相悖，在不断博弈的过程中相互提高，最终在判别器判别能力足够可靠的前提下仍无法区分给定样本是真实样本还是生成样本，从而我们说生成器能够生成“以假乱真”的样本。 ","date":"2022-12-22","objectID":"/gan/:1:0","tags":["Deep Learning","GAN系列","GAN"],"title":"GAN","uri":"/gan/"},{"categories":["Deep Learning","GAN系列"],"content":"优化 优化目标 - 价值函数 (Value Function) \\(\\min_G \\max_D V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\\) - 优化方式 - 生成器优化方向: 最小化价值函数 - 判别器优化方向: 最大化价值函数 简单理解，判别器就是要使正确的样本分为1，生成的样本分为0，那么就是最大化价值函数。 生成器就是要以假乱真，使判别器判生成的样本为1，即最大化\\(D(G(z))\\)，就是最小化价值函数。 这里的价值函数与交叉熵类似。真实样本为正例，生成样本为负例，一般都是最小化损失函数，因此需要做一下变形： 这个很好理解，与交叉熵一样。 上面是判别器的优化，下面是生成器的优化： 总的来说，生成器得损失函数就是 \\[ J^{(G)} = -\\frac{1}{2}E_z\\log D(G(z)) \\] 即“以假乱真”。 ","date":"2022-12-22","objectID":"/gan/:2:0","tags":["Deep Learning","GAN系列","GAN"],"title":"GAN","uri":"/gan/"},{"categories":["Machine Learning","降维算法"],"content":"线性判别分析LDA(Linear Discriminant Analysis) 线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别，一起学习一下吧。 ","date":"2022-12-21","objectID":"/lda/:0:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"主要思想 类内方差小 类间方差大 ","date":"2022-12-21","objectID":"/lda/:1:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"推导 这里以二类为例，即只有两个类别。 首先是投影，我们假定原来的数据是向量 \\(x\\)，那么顺着 $ w$ 方向的投影就是标量： \\[ z=w^T\\cdot x(=|w|\\cdot|x|\\cos\\theta) \\] 对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 \\(N_1\\)和 \\(N_2\\)，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 \\(S\\) 表示原数据的协方差： \\[ \\begin{aligned} C_1:Var_z[C_1]\u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_{c1}})(z_i-\\bar{z_{c1}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(x_i-\\bar{x_{c1}})(x_i-\\bar{x_{c1}})^Tw\\nonumber\\\\\\\\ =w^TS_1w\\\\\\\\\\\\\\\\ C_2:Var_z[C_2]\u0026=\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_{c2}})(z_i-\\bar{z_{c2}})^T\\nonumber\\\\\\\\ =w^TS_2w \\end{aligned} \\] 所以类内距离为： \\[ \\begin{align} Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w \\end{align} \\] 对于第二点，我们可以用两类的均值表示这个距离： \\[ \\begin{align} (\\bar{z_{c1}}-\\bar{z_{c2}})^2\u0026=(\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i-\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i)^2\\nonumber\\\\\\\\ \u0026=(w^T(\\bar{x_{c1}}-\\bar{x_{c2}}))^2\\nonumber\\\\\\\\ \u0026=w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw \\end{align} \\] 合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值： \\[ \\begin{align} \\hat{w}=\\mathop{argmax}\\limits_wJ(w)\u0026=\\mathop{argmax}\\limits_w\\frac{(\\bar{z_{c1}}-\\bar{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\\nonumber\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw}{w^T(S_1+S_2)w}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^TS_bw}{w^TS_ww} \\end{align} \\] 这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对w的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了： \\[ \\begin{aligned} \u0026\\frac{\\partial}{\\partial w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow w\\propto S_w^{-1}S_bw=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw\\propto S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}}) \\end{aligned} \\] 也就是说最后我们的结果就是\\(w=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})\\) 可以归一化求得单位的w值。 ","date":"2022-12-21","objectID":"/lda/:2:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"多类情况 前面的很容易类比二类的情况，现在的目标函数变成了： \\[ \\frac{W^TS_bW}{W^TS_wW} \\] 现在的问题就是这些都是矩阵，不能像上面那样直接优化，需要替换优化目标。 \\[ \\underbrace{arg\\;max}_W\\;\\;J(W) = \\frac{\\prod\\limits_{diag}W^TS_bW}{\\prod\\limits_{diag}W^TS_wW} \\] 其中 \\(\\prod_{diag}A\\)为A的主对角线元素的乘积,W为\\(n \\times d\\)的矩阵，n为原来的维度，d为映射到超平面的维度，则最终的目标就变成了： \\[ J(W) = \\frac{\\prod\\limits_{i=1}^dw_i^TS_bw_i}{\\prod\\limits_{i=1}^dw_i^TS_ww_i} = \\prod\\limits_{i=1}^d\\frac{w_i^TS_bw_i}{w_i^TS_ww_i} \\] 根据广式瑞利商，最大值是矩阵\\(S_w^{-1}S_b\\)的最大特征值,最大的d个值的乘积就是矩阵的\\(S_w^{-1}S_b\\)最大的d个特征值的乘积,此时对应的矩阵\\(W\\)为这最大的d个特征值对应的特征向量张成的矩阵。 ","date":"2022-12-21","objectID":"/lda/:3:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"总结 LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。 实际上LDA除了可以用于降维以外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。 我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 ","date":"2022-12-21","objectID":"/lda/:4:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Machine Learning","降维算法"],"content":"代码 mean_list = [] for i in range(2): mean_list.append(np.mean(X_train[y_train==i], axis=0)) mean_list = np.array(mean_list) S_W = np.zeros((X_train.shape[1], X_train.shape[1])) # 类内散度矩阵 for c, mv in zip(range(2), mean_list): class_scatter = np.zeros((X_train.shape[1], X_train.shape[1])) for row in X_train[y_train==c]: row, mv = row.reshape(X_train.shape[1], -1), mv.reshape(X_train.shape[1], -1) class_scatter += (row-mv).dot((row-mv).T) S_W += class_scatter over_all_mean = np.mean(X_train, axis=0) S_B = np.zeros((X_train.shape[1], X_train.shape[1])) # 类间散度矩阵 for i, mean_vec in enumerate(mean_list): n = X_train[y_train==i, :].shape[0] mean_list_temp = mean_list[i, :].reshape(1, -1) over_all_mean = over_all_mean.reshape(X_train.shape[1], 1) S_B += n*(mean_vec-over_all_mean).dot((mean_vec-over_all_mean).T) eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))] eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True) # eigv_sum = sum(eig_vals) # for i, j in enumerate(eig_pairs): # print('eigenvalue {0:}: {1:.2%}'.format(i + 1, (j[0] / eigv_sum).real)) # 根据百分比显示特征值，从而选取最大的n个特征值 W = np.hstack((eig_pairs[0][1].reshape(X_train.shape[1], 1), eig_pairs[1][1].reshape(X_train.shape[1], 1))) ","date":"2022-12-21","objectID":"/lda/:5:0","tags":["Machine Learning","降维算法","LDA"],"title":"LDA","uri":"/lda/"},{"categories":["Deep Learning","循环神经网络系列"],"content":" image.png 其中\\(r_{t}\\)为reset门，用于重置上一step的状态。\\(z_{t}\\)为update门，用于得到当前step的状态。 ","date":"2022-12-04","objectID":"/gru/:0:0","tags":["Deep Learning","循环神经网络系列","GRU"],"title":"GRU","uri":"/gru/"},{"categories":["pandas","api"],"content":" 很简单，就是统计x中的数出现次数，返回结果的最大长度就是x中的最大值+1，idx为对应的数，值为出现的次数，没有出现的为0。 x = np.array([7, 6, 2, 1, 4]) # 索引0出现了0次，索引1出现了1次......索引5出现了0次...... np.bincount(x) #输出结果为：array([0, 1, 1, 0, 1, 0, 1, 1]) weight这个参数也很好理解，x会被它加权，也就是说，如果值n发现在位置i，那么out[n] += weight[i]而不是out[n] += 1。所以weight必须和x等长。 w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # 我们可以看到x中最大的数为4，因此bin的数量为5，那么它的索引值为0-\u003e4 x = np.array([2, 1, 3, 4, 4, 3]) # 索引0 -\u003e 0 # 索引1 -\u003e w[1] = 0.5 # 索引2 -\u003e w[0] = 0.3 # 索引3 -\u003e w[2] + w[5] = 0.2 - 0.6 = -0.4 # 索引4 -\u003e w[3] + w[4] = 0.7 + 1 = 1.7 np.bincount(x, weights=w) # 因此，输出结果为：array([ 0. , 0.5, 0.3, -0.4, 1.7]) 没出现的还是0，出现的要按照出现的地方的weight计算。 ","date":"2022-11-29","objectID":"/bincount/:0:0","tags":["pandas","api","bincount"],"title":"bincount","uri":"/bincount/"},{"categories":["算法题"],"content":"把数字翻译成字符串 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ba-shu-zi-fan-yi-cheng-zi-fu-chuan-lcof/ ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:1:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： dp思想，不用管是什么字符，定义dp[i]为长度为i时 有多少个方法 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:2:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["算法题"],"content":"代码: class Solution: def translateNum(self, num: int) -\u003e int: s = str(num) if len(s) \u003c 2: return 1 dp = [0] * len(s) dp[0] = 1 dp[1] = 2 if int(s[0] + s[1]) \u003c 26 else 1 for i in range(2,len(s)): dp[i] = dp[i-1] + dp[i-2] if int(s[i-1] + s[i]) \u003c 26 and s[i-1] != '0' else dp[i-1] return dp[-1] 注意如果长度小于等于1 则直接返回1 如果不是26个英文字母里面的 则dp[i] = dp[i-1] 说明方法次数并不改变 注意有首位为0的情况 所以要int一下 ","date":"2022-11-17","objectID":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/:3:0","tags":["算法题","把数字翻译成字符串"],"title":"把数字翻译成字符串","uri":"/%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"categories":["NLP"],"content":"Seq2Seq （本文只介绍最原始的seq2seq，带有注意力在attention文章中） ","date":"2022-11-09","objectID":"/seq2seq/:0:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"RNN 有关RNN Seq2Seq是典型的Encoder-decoder框架的模型，其中编码器和解码器都采用的RNN模型或者RNN模型的变体：GRU、LSTM等。 一般的RNN模型有几种形式，分别为一对一、一对多、多对一、多对多。 一对一： 就是一般的MLP，并不能称之为RNN模型 一对多: 典型的例子就是语音生成，比如输入某个值可以由程序生成一段音乐。 多对一： 最常见的文本分类或者情感分析就是这个模型架构 多对多： 序列标注、NER、分词，大多数标注任务就是用的这个模型架构。而Seq2Seq也属于多对多的任务，不过由于输入和输出的长度可能会不一样，因此采用encoder-decoder的框架，主要思想就是通过encoder将输入的信息编码，然后传入decoder再进行解码得到想要的结果。这常用于生成式的任务，比如说机器翻译、对话系统、文本摘要等等，都可以使用这个框架进行实现。著名的Transformer 就是使用的这个框架。 ","date":"2022-11-09","objectID":"/seq2seq/:1:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"encoder-decoder 框架 encoder-decoder框架可以看作一种深度学习领域的研究模式，应用场景十分广泛， 文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对\u003cSource,Target\u003e，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成。 \\[ Source = \u003cx_1, x_2, \\dots, x_m\u003e \\\\\\\\ Target = \u003cy_1, y_2, \\dots, y_n\u003e \\] Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： \\[ C = F(x_1, x_2, \\dots, x_m) \\] 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息\\(y_1,y_2,\\dots, y_{i-1}\\)来生成i时刻要生成的单词\\(y_i\\)： \\[ y_i = G(C, y_1,y_2,\\dots,y_{i-1}) \\] 每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。 Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。 ","date":"2022-11-09","objectID":"/seq2seq/:2:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"Seq2Seq模型 Seq2Seq模型是输出的长度不确定时采用的模型，这种情况一般是在机器翻译的任务中出现，将一句中文翻译成英文，那么这句英文的长度有可能会比中文短，也有可能会比中文长，所以输出的长度就不确定了。 ","date":"2022-11-09","objectID":"/seq2seq/:3:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"结构 seq2seq属于encoder-decoder结构的一种，这里看看常见的encoder-decoder结构，基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder。encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量C。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 而decoder则负责根据语义向量生成指定的序列，这个过程也称为解码，最简单的方式是将encoder得到的语义变量作为初始状态输入到decoder的RNN中，得到输出序列。可以看到上一时刻的输出会作为当前时刻的输入，而且其中语义向量C只作为初始状态参与运算，后面的运算都与语义向量C无关。 decoder处理方式还有另外一种，就是语义向量C参与了序列所有时刻的运算，如下图，上一时刻的输出仍然作为当前时刻的输入，但语义向量C会参与所有时刻的运算。 上面的这两种结构是我刚学的时候疑惑的一个地方，因为我在有的地方看到的代码是第一种结构的，而沐神的教程中的结构用的是第二种结构，其实这两种都是可以的。 ### 训练 最主要的思路就是语言模型，因为在RNN中，每一个step的输出层的大小就是单词表的大小，要预测最大概率出现的那个词汇，即最大化最有可能是当前输出的单词所在神经元的概率，实际上就是一个多分类问题，使用softmax归一化表示概率。 encoder就是简单的RNN系列模型，前文说的可以有多种方式计算语义向量c，并且c可以有两种方式参与到decoder的计算中。 decoder的主要训练方式就是将前一时刻的结果作为后一时刻的输入，也就是自回归。在训练中的体现是将语料进行错位训练，比如[“你好“ 世界”]，在训练中decoder中就是输入为[“S” ”你好“ “世界”]，而标签也就是实际值为[“你好” “世界” ”E“] ，按照这样的方式进行训练，训练过后，再进行测试，测试的过程就是严格按照前一时刻的输出作为后一时刻的输入，因为这时你也没有要输入的数据。也就是说训练时和测试时的decoder是不一样的。训练的时候我们有真实的数据，而预测的时候没有，只能自产自销，其实就是一个语言模型，叫做条件语言模型。这里引用两张图 既然是语言模型，可以用极大似然估计最大化输出序列的概率： \\[ \\begin{aligned} P(y_1,\\dots,y_{T} | x_1, \\dots x_T) = \\prod_{t=1}^TP(y_t|y_1 , \\dots y_{t-1}; c) \\end{aligned} \\] 在计算损失的时候，我们使用交叉熵作为损失函数，所以我们要找出这个V维向量中，正确预测对应的词的那一维的概率大小\\(\\hat{p}\\)，则这一步的损失就是它的负导数\\(-log(\\hat{p})\\)，将每一步的损失求和，即得到总体的损失函数： \\[ \\begin{aligned} J = -\\frac{1}{T}\\sum_{i}^Tlog(p(\\hat{y_i})) \\end{aligned} \\] 其中的\\(p(\\hat{y_i})\\)为时间t=i上的正确输出节点的概率值，即softmax值。 其中有三个特殊的标记，一个是S代表句子的开头，E代表句子的结尾，P代表Padding。 ## 束搜索(beam search) 一般来说，用前一步的结果作为下一步的输出，这种方式就是贪心策略，但也存在问题，也就是说每一步最优并不是全局最优，改进的办法就是束搜索。思想很简单，每一步就是多选几个作为候选，最后综合考虑，选出最优的组合。是不是和HMM中的维特比算法很像呢？ 以下为束搜索的步骤： - 首先需要设定一个候选集的大小beam size=k。 - 每一步的开始，我们从每个当前输入对应的所有可能输出，计算每一条路的序列得分 - 保留序列得分最大的k个作为下一步的输入 - 不断重复以上步骤，直至结束，选择序列得分最大的那个序列作为最终结果。 其中序列得分为： \\[ score(y_1,y_2, \\dots y_t) = \\sum_{i=1}^t \\log P(y_i|y_1,y_2,\\dots y_{i-1};x) \\] 过程如图所示： ","date":"2022-11-09","objectID":"/seq2seq/:3:1","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"评价标准 ","date":"2022-11-09","objectID":"/seq2seq/:4:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"BLUE指标 BLEU，全称是Bilingual Evaluation Understudy，它的主要思想是基于N-gram等特征来比较人工翻译和机器翻译结果的相似程度。 我们将BLEU定义为： \\[ \\exp \\left(\\min\\left(0,1-\\frac{len_{\\text {label }}}{len_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}} \\] 长的 \\(n\\) 元语法。另外, 用 \\(p_{n}\\) 表示 \\(n\\) 元语法的精确度, 它是两个数量的比值：第一个是预测序 列与标签序列中匹配的 \\(n\\) 元语法的数量, 第二个是预测序列中 \\(n\\) 元语法的数量的比率。具体 地说, 给定标签序列 \\(A 、 B 、 C 、 D 、 E 、 F\\) 和预测序列 \\(A 、 B 、 B 、 C 、 D\\), 我们有 \\(p_{1}=4 / 5 、 p_{2}=3 / 4 、 p_{3}=1 / 3\\) 和 \\(p_{4}=0\\) 。 根据 (9.7.4)中BLEU的定义，当预测序列与标签序列完全相同时, BLEU为 1 。 此外, 由于 \\(n\\) 元语法越长则匹配难度越大, 所以BLEU为更长的 \\(n\\) 元语法的精确度分配更大的权重。具体 来说, 当 \\(p_{n}\\) 固定时, \\(p_{n}^{1 / 2^{n}}\\) 会随着 \\(n\\) 的增长而增加（原始论文使用 \\(p_{n}^{1 / n}\\) )。而且, 由于预测 的序列越短获得的 \\(p_{n}\\) 值越高, 所以 (9.7.4)中乘法项之前的系数用于惩罚较短的预测序列。 例如, 当 \\(k=2\\) 时，给定标签序列 \\(A 、 B 、 C 、 D 、 E 、 F\\) 和预测序列 \\(A 、 B\\) ，尽管 \\(p_{1}=p_{2}=1\\) ， 惩罚因子 \\(\\exp (1-6 / 2) \\approx 0.14\\) 会降低BLEU。 ","date":"2022-11-09","objectID":"/seq2seq/:4:1","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["NLP"],"content":"代码 # code by Tae Hwan Jung @graykode import numpy as np import torch import torch.nn as nn # S: Symbol that shows starting of decoding input # E: Symbol that shows starting of decoding output # P: Symbol that will fill in blank sequence if current batch data size is short than time steps def make_batch(): input_batch, output_batch, target_batch = [], [], [] for seq in seq_data: for i in range(2): seq[i] = seq[i] + 'P' * (n_step - len(seq[i])) input = [num_dic[n] for n in seq[0]] output = [num_dic[n] for n in ('S' + seq[1])] target = [num_dic[n] for n in (seq[1] + 'E')] input_batch.append(np.eye(n_class)[input]) output_batch.append(np.eye(n_class)[output]) target_batch.append(target) # not one-hot # make tensor return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch) # make test batch def make_testbatch(input_word): input_batch, output_batch = [], [] input_w = input_word + 'P' * (n_step - len(input_word)) input = [num_dic[n] for n in input_w] output = [num_dic[n] for n in 'S' + 'P' * n_step] input_batch = np.eye(n_class)[input] output_batch = np.eye(n_class)[output] return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0) # Model class Seq2Seq(nn.Module): def __init__(self): super(Seq2Seq, self).__init__() self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) self.fc = nn.Linear(n_hidden, n_class) def forward(self, enc_input, enc_hidden, dec_input): enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class] dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class] # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden] _, enc_states = self.enc_cell(enc_input, enc_hidden) # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)] outputs, _ = self.dec_cell(dec_input, enc_states) model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class] return model if __name__ == '__main__': n_step = 5 n_hidden = 128 char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz'] num_dic = {n: i for i, n in enumerate(char_arr)} seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']] n_class = len(num_dic) batch_size = len(seq_data) model = Seq2Seq() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) input_batch, output_batch, target_batch = make_batch() for epoch in range(5000): # make hidden shape [num_layers * num_directions, batch_size, n_hidden] hidden = torch.zeros(1, batch_size, n_hidden) optimizer.zero_grad() # input_batch : [batch_size, max_len(=n_step, time step), n_class] # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class] # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot output = model(input_batch, hidden, output_batch) # output : [max_len+1, batch_size, n_class] output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class] loss = 0 for i in range(0, len(target_batch)): # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1] loss += criterion(output[i], target_batch[i]) if (epoch + 1) % 1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Test def translate(word): input_batch, output_batch = make_testbatch(word) # make hidden shape [num_layers * num_directions, batch_size, n_hidden] hidden = torch.zeros(1, 1, n_hidden) output = model(input_batch, hidden, output_batch) # output : [max_len+1(=6), batch_size(=1), n_class] predict = output.data.max(2, keepdim=True)[1] # select n_class dimension decoded = [char_arr[i] for i in predict] end = decoded.index('E') translated = ''.join(decoded[:end]) return translated.replace('P', '') print('test') print('man -\u003e', translate('man')) prin","date":"2022-11-09","objectID":"/seq2seq/:5:0","tags":["NLP","seq2seq"],"title":"seq2seq","uri":"/seq2seq/"},{"categories":["Machine Learning","聚类算法"],"content":"DBSCAN属于密度聚类的一种。通常情形下，密度聚类算法从样 本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇 以获得最终的聚类结果。 DBSCAN基于一组“邻域”参数\\((\\epsilon, Minpts)\\)来刻画样本分布的紧密程度，给定数据集\\(D=\\\\{x_1,x_2, \\dots,x_m \\\\}\\)，定义几个概念： \\(\\epsilon\\)-邻域：对\\(x_j\\in D\\)，其\\(\\epsilon\\)-邻域包含样本集D中与\\(x_j\\)的距离不大于\\(\\epsilon\\)的样本，即\\(N_{\\epsilon}(x_j) = \\\\{dist(x_i, x_j) \\leq \\epsilon\\\\}\\)。 核心对象 (core object): 若 \\(x_j\\) 的 \\(\\epsilon\\)-邻域至少包含 MinPts 个样本, 即 \\(\\left|N_\\epsilon\\left(\\boldsymbol{x}_j\\right)\\right| \\geqslant \\operatorname{MinPts}\\), 则 \\(\\boldsymbol{x}_j\\) 是一个核心对象; 密度直达(directly density-reachable): 若 \\(\\boldsymbol{x}_j\\) 位于 \\(\\boldsymbol{x}_i\\) 的 \\(\\epsilon\\)-邻域中, 且 \\(\\boldsymbol{x}_i\\) 是 核心对象, 则称 \\(\\boldsymbol{x}_j\\) 由 \\(\\boldsymbol{x}_i\\) 密度直达; 密度可达(density-reachable): 对 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\), 若存在样本序列 \\(\\boldsymbol{p}_1, \\boldsymbol{p}_2, \\ldots, \\boldsymbol{p}_n\\), 其中 \\(\\boldsymbol{p}_1=\\boldsymbol{x}_i, \\boldsymbol{p}_n=\\boldsymbol{x}_j\\) 且 \\(\\boldsymbol{p}_{i+1}\\) 由 \\(\\boldsymbol{p}_i\\) 密度直达, 则称 \\(\\boldsymbol{x}_j\\) 由 \\(\\boldsymbol{x}_i\\) 密度可达; 密度相连 (density-connected): 对 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\), 若存在 \\(\\boldsymbol{x}_k\\) 使得 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\) 均由 \\(\\boldsymbol{x}_k\\) 密度可达, 则称 \\(\\boldsymbol{x}_i\\) 与 \\(\\boldsymbol{x}_j\\) 密度相连. 既然是聚类，那就要定义簇的概念 ","date":"2022-11-03","objectID":"/dbscan/:0:0","tags":["Machine Learning","聚类算法","DBSCAN"],"title":"DBSCAN","uri":"/dbscan/"},{"categories":["Machine Learning","关联规则算法"],"content":"参考：https://www.cnblogs.com/bill-h/p/14863262.html 大家可能听说过用于宣传数据挖掘的一个案例:啤酒和尿布；据说是沃尔玛超市在分析顾客的购买记录时，发现许多客户购买啤酒的同时也会购买婴儿尿布，于是超市调整了啤酒和尿布的货架摆放，让这两个品类摆放在一起；结果这两个品类的销量都有明显的增长；分析原因是很多刚生小孩的男士在购买的啤酒时，会顺手带一些婴幼儿用品。 不论这个案例是否是真实的，案例中分析顾客购买记录的方式就是关联规则分析法Association Rules。 关联规则分析也被称为购物篮分析，用于分析数据集各项之间的关联关系。 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:0:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"项集 item的集合，如集合{牛奶、麦片、糖}是一个3项集，可以认为是购买记录里物品的集合。 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:1:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"频繁项集 顾名思义就是频繁出现的item项的集合。如何定义频繁呢？用比例来判定，关联规则中采用支持度和置信度两个概念来计算比例值 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:2:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"支持度（support) 共同出现的项在整体项中的比例。以购买记录为例子，购买记录100条，如果商品A和B同时出现50条购买记录（即同时购买A和B的记录有50），那边A和B这个2项集的支持度为50% \\[ Support(A\\cap B) = \\frac{Freq(A\\cap B)}{N} \\] ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:3:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"置信度（Confidence） 购买A后再购买B的条件概率，根据贝叶斯公式，可如下表示： \\[ Confidence = \\frac{Freq(A\\cap B)}{Freq(A)} \\] ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:4:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning","关联规则算法"],"content":"提升度 为了判断产生规则的实际价值，即使用规则后商品出现的次数是否高于商品单独出现的评率，提升度和衡量购买X对购买Y的概率的提升作用。如下公式可见，如果X和Y相互独立那么提升度为1，提升度越大，说明X-\u003eY的关联性越强 ","date":"2022-11-02","objectID":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/:5:0","tags":["Machine Learning","关联规则算法","关联规则概念"],"title":"关联规则概念","uri":"/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%A6%82%E5%BF%B5/"},{"categories":["算法题"],"content":"字符串转换整数 (atoi) https://leetcode-cn.com/problems/string-to-integer-atoi/ #重点是正则表达式 class Solution: def myAtoi(s: str): import re ss = re.findall(\"^[\\+\\-]?\\d+\",s.strip()) res = int(*ss) if res \u003e (231-1): res = (231-1) if res \u003c -231: res = -231 return res WA了四次才整出来，太菜了，以为很简单，没有认真读题，要吸取教训。 ","date":"2022-10-26","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi/:0:0","tags":["算法题","字符串转换整数 (atoi)"],"title":"字符串转换整数 (atoi)","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi/"},{"categories":["Machine Learning","聚类算法"],"content":"基础就是高斯混合模型，假设我们熟知的高斯分布的概率密度函数为\\(p(x\\mid \\mu, \\Sigma)\\)。则高斯混合分布为： \\[ p_{\\mathcal{M}}(\\boldsymbol{x})=\\sum_{i=1}^k \\alpha_i \\cdot p\\left(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i\\right) \\] 分布共由 \\(k\\) 个混合成分组成, 每个混合成分对应一个高斯分布. 其中 \\(\\mu_i\\) 与 \\(\\Sigma_i\\) 是第 \\(i\\) 个高斯混合成分的参数, 而 \\(\\alpha_i\u003e0\\) 为相应的 “混合系数” (mixture coefficient), \\(\\sum_{i=1}^k \\alpha_i=1\\)。 假设样本的生成过程由高斯混合分布给出: 首先, 根据 \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k\\) 定义 的先验分布选择高斯混合成分, 其中 \\(\\alpha_i\\) 为选择第 \\(i\\) 个混合成分的概率; 然后, 根 据被选择的混合成分的概率密度函数进行采样, 从而生成相应的样本。 ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:0:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"聚类原理 如何利用高斯混合分布进行聚类？观察这个混合系数，思路就是有多少个混合的模型，就代表要聚多少类，对于给定数据集，可以定义 \\[ \\gamma_{j k}= \\begin{cases}1, \u0026 \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] 则样本j的簇标记\\(\\lambda_j= \\underbrace{\\arg \\max}_{i\\in {1,2, \\dots ,k}} \\gamma_{jk}\\) ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:1:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"EM算法 如何计算\\(\\gamma_{jk}\\)呢，如下式所示，其中\\(\\alpha_k\\)为混合系数，\\(\\theta_k\\)为第k个高斯分布的参数 \\[ \\begin{aligned} \\hat{\\gamma}_{j k} \u0026=E\\left(\\gamma_{j k} \\mid y, \\theta\\right)=P\\left(\\gamma_{j k}=1 \\mid y, \\theta\\right) \\\\\\\\ \u0026=\\frac{P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 式子里的y其实就是观测样本。 那么模型的参数要怎么估计呢，很显然可以使用EM算法，\\(\\gamma\\)为隐变量，其实我这里的叙述顺序是有问题的，其实是EM算法中求Q函数的过程中需要计算的一个值，详细的过程在本博客的EM算法里面。总之得到了\\(\\gamma_{jk}\\)后，就得到了Q函数: \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{k=1}^K\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\} \\] 极大似然估计Q函数就可以得到参数的下一轮估计值： \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 用 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 及 \\(\\hat{\\alpha}_k, k=1,2, \\cdots, K\\), 表示 \\(\\theta^{(i+1)}\\) 的各参数。求 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 只需分别对 \\(\\mu_k, \\sigma_k^2\\) 求偏导数并令其为 0 , 即可得到; 求 \\(\\hat{\\alpha}_k\\) 是在 \\(\\sum_{k=1}^K \\alpha_k=1\\) 条件 下求偏导数并令其为 0 得到的。结果如下: \\[ \\begin{gathered} \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left(y_j-\\mu_k\\right)^2}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\alpha}_k=\\frac{n_k}{N}=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}{N}, \\quad k=1,2, \\cdots, K \\end{gathered} \\] 得到参数后，再进行新的一轮迭代，计算\\(\\gamma\\)值，如此反复。 算法收敛后，就可以对样本进行聚类，根据\\(\\lambda_j= \\underbrace{\\arg \\max}_{i\\in {1,2, \\dots ,k}} \\gamma_{jk}\\)可以得到每个样本的簇标记。具体的流程如下： ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:2:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"总结 高斯混合分布的形式就注定了它可以用来进行聚类，并且还有EM算法如此强大的数学工具进行模型参数的学习，高斯混合聚类与Kmeans都属于原型聚类。 ","date":"2022-10-25","objectID":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/:3:0","tags":["Machine Learning","聚类算法","高斯混合聚类"],"title":"高斯混合聚类","uri":"/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB/"},{"categories":["NLP"],"content":"Tokenization技术 本文章主要说说NLP领域中的Tokenization技术，这是很基础的但也是很容易被忽视的一个步骤。在我接的单子中经常会有此类问题，并且都是外国学校的，说明外国学校还是比较注重这一块的基础的。 首先明确一个概念：token可以理解为一个符号，就代表一个语言单位，tokenize的意思就是把一个句子或语料分成token. ","date":"2022-10-17","objectID":"/tokenization/:0:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"word ","date":"2022-10-17","objectID":"/tokenization/:1:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"char ","date":"2022-10-17","objectID":"/tokenization/:2:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"子词(subword) ","date":"2022-10-17","objectID":"/tokenization/:3:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"BPE BPE 是一种简单的数据压缩算法，它在 1994 年发表的文章“A New Algorithm for Data Compression”中被首次提出。下面的示例将解释 BPE。老规矩，我们先用一句话概括它的核心思想： BPE每一步都将最常见的一对相邻数据单位替换为该数据中没有出现过的一个新单位，反复迭代直到满足停止条件。 BPE 确保最常见的词在token列表中表示为单个token，而罕见的词被分解为两个或多个subword tokens，因此BPE也是典型的基于subword的tokenization算法。 合并字符可以让你用最少的token来表示语料库，这也是 BPE 算法的主要目标，即数据的压缩。为了合并，BPE 寻找最常出现的字节对。在这里，我们将字符视为与字节等价。当然，这只是英语的用法，其他语言可能有所不同。现在我们将最常见的字节对合并成一个token，并将它们添加到token列表中，并重新计算每个token出现的频率。这意味着我们的频率计数将在每个合并步骤后发生变化。我们将继续执行此合并步骤，直到达到我们预先设置的token数限制或迭代限制。 ","date":"2022-10-17","objectID":"/tokenization/:4:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"算法过程 准备语料库，确定期望的 subword 词表大小等参数 通常在每个单词末尾添加后缀 ，统计每个单词出现的频率，例如，low 的频率为 5，那么我们将其改写为 “l o w ”：5 将语料库中所有单词拆分为单个字符，用所有单个字符建立最初的词典，并统计每个字符的频率，本阶段的 subword 的粒度是字符 挑出频次最高的符号对 ，比如说 t 和 h 组成的 th，将新字符加入词表，然后将语料中所有该字符对融合（merge），即所有 t 和 h 都变为 th。 重复上述操作，直到词表中单词数达到设定量 或下一个最高频数为 1 ，如果已经打到设定量，其余的词汇直接丢弃 ","date":"2022-10-17","objectID":"/tokenization/:4:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"例子 获取语料库，这样一段话为例：“ FloydHub is the fastest way to build, train and deploy deep learning models. Build deep learning models in the cloud. Train deep learning models. ” 拆分，加后缀\u003c/w\u003e ，统计词频 建立词表，统计字符频率（顺便排个序）： 以第一次迭代为例，将字符频率最高的 d 和 e 替换为 de，后面依次迭代： 更新词表 继续迭代直到达到预设的 subwords 词表大小或下一个最高频的字节对出现频率为 1。 ### 优点 BPE 的优点就在于，可以很有效地平衡词典大小和编码步骤数（将语料编码所需要的 token 数量）。 随着合并的次数增加，词表大小通常先增加后减小。迭代次数太小，大部分还是字母，没什么意义；迭代次数多，又重新变回了原来那几个词。所以词表大小要取一个中间值。 ","date":"2022-10-17","objectID":"/tokenization/:4:2","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"适用范围 BPE 一般适用在欧美语言拉丁语系中，因为欧美语言大多是字符形式，涉及前缀、后缀的单词比较多。而中文的汉字一般不用 BPE 进行编码，因为中文是字无法进行拆分。对中文的处理通常只有分词和分字两种。理论上分词效果更好，更好的区别语义。分字效率高、简洁，因为常用的字不过 3000 字，词表更加简短。 ","date":"2022-10-17","objectID":"/tokenization/:4:3","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"编码过程 BPE的总体思想就是利用替换字节对来逐步构造词汇表。在使用的过程有编码和解码两种。 在之前的算法中，我们已经得到了subword的词表，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。 我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如 编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。 ","date":"2022-10-17","objectID":"/tokenization/:4:4","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"代码 import re, collections def get_vocab(filename): vocab = collections.defaultdict(int) with open(filename, 'r', encoding='utf-8') as fhand: for line in fhand: words = line.strip().split() for word in words: vocab[' '.join(list(word)) + ' \u003c/w\u003e'] += 1 return vocab def get_stats(vocab): # 构建字符对频数字典 pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs def merge_vocab(pair, v_in): # 将频率最大的字符对替换 v_out = {} bigram = re.escape(' '.join(pair)) # 不转义 p = re.compile(r'(?\u003c!\\S)' + bigram + r'(?!\\S)') # 意思是bigram前面没有非空格字符，后面也没有非空格字符 for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_out def get_tokens(vocab): tokens = collections.defaultdict(int) for word, freq in vocab.items(): word_tokens = word.split() for token in word_tokens: tokens[token] += freq return tokens vocab = {'l o w \u003c/w\u003e': 5, 'l o w e r \u003c/w\u003e': 2, 'n e w e s t \u003c/w\u003e': 6, 'w i d e s t \u003c/w\u003e': 3} print('==========') print('Tokens Before BPE') tokens = get_tokens(vocab) print('Tokens: {}'.format(tokens)) print('Number of tokens: {}'.format(len(tokens))) print('==========') num_merges = 5 for i in range(num_merges): pairs = get_stats(vocab) if not pairs: break best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print('Iter: {}'.format(i)) print('Best pair: {}'.format(best)) print(vocab) # tokens = get_tokens(vocab) # print('Tokens: {}'.format(tokens)) ","date":"2022-10-17","objectID":"/tokenization/:4:5","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"wordpiece ","date":"2022-10-17","objectID":"/tokenization/:5:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"构造 wordpiece 词表的构造与BPE很相似，都是选择两个子词合并成新的子词。 最大的区别在于，BPE是选择频数最高的相邻子词合并，而wordpiece选择能够提升语言模型概率最大的相邻子词加入词表。 如何理解？ 假设各个子词之间是独立存在的，则句子S的语言模型似然值等价于所有子词概率的乘积 假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为： 从上面的公式，很容易发现，似然值的变化就是两个子词之间的互信息。简而言之，WordPiece每次选择合并的两个子词，他们具有最大的互信息值，也就是两子词在语言模型上具有较强的关联性，它们经常在语料中以相邻方式同时出现。 与BPE相似，通过以上方式构建词表。 ### 编码 1. 从第一个位置开始，由于是最长匹配，结束位置需要从最右端依次递减，所以遍历的第一个子词 是其本身 unaffable，该子词不在词汇表中 2. 结束位置左移一位得到子词 unaffabl，同样不在词汇表中 3. 重复这个操作，直到 un，该子词在词汇表中，将其加入 output_tokens，以第一个位置开始 的遍历结束 4. 跳过 un，从其后的 a 开始新一轮遍历，结束位置依然是从最右端依次递减，但此时需要在前 面加上 ## 标记，得到 ##affable 不在词汇表中 5. 结束位置左移一位得到子词 ##affabl，同样不在词汇表中 6. 重复这个操作，直到 ##aff，该字词在词汇表中， 将其加入 output_tokens，此轮遍历结束 7. 跳过 aff，从其后的 a 开始新一轮遍历，结束位置依然是从最右端依次递减。##able 在词汇 表中，将其加入 output_tokens 8. able 后没有字符了，整个遍历结束 ","date":"2022-10-17","objectID":"/tokenization/:5:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"代码 (来自https://github.com/google-research/bert/blob/master/tokenization.py) class WordpieceTokenizer(object): \"\"\"Runs WordPiece tokenziation.\"\"\" def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200): self.vocab = vocab self.unk_token = unk_token self.max_input_chars_per_word = max_input_chars_per_word def tokenize(self, text): \"\"\"Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary. For example: input = \"unaffable\" output = [\"un\", \"##aff\", \"##able\"] Args: text: A single token or whitespace separated tokens. This should have already been passed through `BasicTokenizer. Returns: A list of wordpiece tokens. \"\"\" text = convert_to_unicode(text) output_tokens = [] for token in whitespace_tokenize(text): chars = list(token) if len(chars) \u003e self.max_input_chars_per_word: output_tokens.append(self.unk_token) continue is_bad = False start = 0 sub_tokens = [] while start \u003c len(chars): end = len(chars) cur_substr = None while start \u003c end: substr = \"\".join(chars[start:end]) if start \u003e 0: substr = \"##\" + substr if substr in self.vocab: cur_substr = substr break end -= 1 if cur_substr is None: is_bad = True break sub_tokens.append(cur_substr) start = end if is_bad: output_tokens.append(self.unk_token) else: output_tokens.extend(sub_tokens) return output_tokens ","date":"2022-10-17","objectID":"/tokenization/:5:2","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"Unigram Language Model 与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。 初始时，建立一个足够大的词表(使用一些算法)。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。 针对当前词表，用EM算法求解每个子词在语料上的概率。 对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。 将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。 重复步骤2到4，直到词表大小减少到设定范围。 可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。 ### 代码 encode代码 def encode_word(word, model): # word初步分词，model中为-log值 best_segmentations = [{\"start\": 0, \"score\": 1}] + [ {\"start\": None, \"score\": None} for _ in range(len(word)) ] for start_idx in range(len(word)): # This should be properly filled by the previous steps of the loop best_score_at_start = best_segmentations[start_idx][\"score\"] for end_idx in range(start_idx + 1, len(word) + 1): token = word[start_idx:end_idx] if token in model and best_score_at_start is not None: score = model[token] + best_score_at_start # 加上start，即前缀对应的损失，log中相加等于原来概率相乘 # If we have found a better segmentation ending at end_idx, we update if ( best_segmentations[end_idx][\"score\"] is None or best_segmentations[end_idx][\"score\"] \u003e score # score即loss。越小越好 ): best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score} print(token) print(best_segmentations[end_idx][\"score\"], score) print(best_segmentations) segmentation = best_segmentations[-1] if segmentation[\"score\"] is None: # We did not find a tokenization of the word -\u003e unknown return [\"\u003cunk\u003e\"], None # 从后向前的最佳路径，即维特比算法 score = segmentation[\"score\"] start = segmentation[\"start\"] end = len(word) tokens = [] while start != 0: tokens.insert(0, word[start:end]) next_start = best_segmentations[start][\"start\"] end = start start = next_start tokens.insert(0, word[start:end]) return tokens, score 计算Loss def compute_loss(model): loss = 0 for word, freq in word_freqs.items(): _, word_loss = encode_word(word, model) loss += freq * word_loss return loss 计算score(用于删除token) import copy def compute_scores(model): scores = {} model_loss = compute_loss(model) for token, score in model.items(): # We always keep tokens of length 1 if len(token) == 1: continue model_without_token = copy.deepcopy(model) _ = model_without_token.pop(token) scores[token] = compute_loss(model_without_token) - model_loss return scores ","date":"2022-10-17","objectID":"/tokenization/:6:0","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"参考 Unigram tokenization - Hugging Face NLP 课程 ","date":"2022-10-17","objectID":"/tokenization/:6:1","tags":["NLP","tokenization"],"title":"tokenization","uri":"/tokenization/"},{"categories":["NLP"],"content":"CoVe Cove代表上下文向量，它是一种有监督的预训练模型，其主要思想就是训练了一个NMT系统，并使用它的编码器， ","date":"2022-10-09","objectID":"/cove/:0:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"模型训练 主要假设是，为了翻译一个句子，NMT编码器学会理解句子。 因此来自编码器的向量包含有关单词上下文的信息。 形式上，作者训练了一个带注意力的LSTM模型，比如Bahdanau Model，由于最终我们想使用经过训练的编码器来处理英文句子（不是因为我们只关心英文，而是因为下游任务的大多数数据集都是英文的），所以 NMT 系统必须从英文翻译成其他的语言（例如，德语）。 ","date":"2022-10-09","objectID":"/cove/:1:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"双向编码器 请注意，在这个 NMT 模型中，编码器是双向的：它连接前向和后向 LSTM 的输出。因此，编码器输出包含有关令牌左右上下文的信息。 ","date":"2022-10-09","objectID":"/cove/:2:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"获取表示(连接 GloVe 和 Cove 向量) 训练 NTM 模型后，我们只需要它的编码器。对于给定的文本，CoVe 向量是编码器的输出。对于下游任务，作者建议使用 Glove（代表单个令牌）和 CoVe（在上下文中编码的令牌）向量的串联。这个想法是这些向量编码不同类型的信息，它们的组合可能很有用。 ","date":"2022-10-09","objectID":"/cove/:3:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["NLP"],"content":"总结 其实思想很简单，就是将已经训练好的机器翻译模型的编码器作为编码，最后再与GloVe进行拼接，可以达到很好的效果，因为使用的是机器翻译模型，因此可以视为有监督学习，有监督的预训练模型是很少的，CoVe就是其中之一，其中更具体的分类可以看本博客的预训练模型。 ","date":"2022-10-09","objectID":"/cove/:4:0","tags":["NLP","CoVe"],"title":"CoVe","uri":"/cove/"},{"categories":["Machine Learning"],"content":"EM算法 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:0:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"引入 我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。（最大似然估计：利用已知的样本结果，反推最有可能导致这样结果的一组参数）但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。用EM算法可以解决。 EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。 EM算法的每次迭代由两步组成：E步，求期望；M步，求极大。所以被称为期望极大算法。 EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:1:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"三硬币模型 首先介绍一个使用 EM算法的例子。 (三硬币模型) 假设有 3 枚硬币, 分别记作 A, B, C。这些硬币正面出现 的概率分别是 \\(\\pi, p\\) 和 \\(q\\) 。进行如下郑硬币试验: 先掷硬币 \\(\\mathrm{A}\\), 根据其结果选出硬币 \\(\\mathrm{B}\\) 或硬币 \\(\\mathrm{C}\\), 正面选硬币 \\(\\mathrm{B}\\), 反面选硬币 \\(\\mathrm{C}\\); 然后郑选出的硬币, 掷硬币的结果, 出现正 面记作 1 , 出现反面记作 0 ; 独立地重复 \\(n\\) 次试验 (这里, \\(n=10\\) ), 观测结果如下: \\[ 1,1,0,1,0,0,1,0,1,1 \\] 假设只能观测到郑硬币的结果, 不能观测郑硬币的过程。问如何估计三硬币正面出现 的概率, 即三硬币模型的参数。 解 三硬币模型可以写作 \\[ \\begin{aligned} P(y \\mid \\theta) \u0026=\\sum_z P(y, z \\mid \\theta)=\\sum_z P(z \\mid \\theta) P(y \\mid z, \\theta) \\\\\\\\ \u0026=\\pi p^y(1-p)^{1-y}+(1-\\pi) q^y(1-q)^{1-y} \\end{aligned} \\] 这里, 随机变量 \\(y\\) 是观测变量, 表示一次试验观测的结果是 1 或 0 ; 随机变量 \\(z\\) 是隐 变量, 表示末观测到的掷硬币 \\(\\mathrm{A}\\) 的结果; \\(\\theta=(\\pi, p, q)\\) 是模型参数。这一模型是以上数 据的生成模型。注意, 随机变量 \\(y\\) 的数据可以观测, 随机变量 \\(z\\) 的数据不可观测。 将观测数据表示为 \\(Y=\\left(Y_1, Y_2, \\cdots, Y_n\\right)^{\\mathrm{T}}\\), 末观测数据表示为 \\(Z=\\left(Z_1, Z_2, \\cdots, Z_n\\right)^{\\mathrm{T}}\\) 则观测数据的似然函数为 \\[ P(Y \\mid \\theta)=\\sum_Z P(Z \\mid \\theta) P(Y \\mid Z, \\theta) \\] 即 \\[ P(Y \\mid \\theta)=\\prod_{j=1}^n\\left[\\pi p^{y_j}(1-p)^{1-y_j}+(1-\\pi) q^{y_j}(1-q)^{1-y_j}\\right] \\] 考虑求模型参数 \\(\\theta=(\\pi, p, q)\\) 的极大似然估计, 即 \\[ \\hat{\\theta}=\\arg \\max_\\theta \\log P(Y \\mid \\theta) \\] 这个问题没有解析解, 只有通过迭代的方法求解。EM算法就是可以用于求解这 个问题的一种迭代算法。下面给出针对以上问题的 EM算法, 其推导过程省略。 EM算法首先选取参数的初值, 记作 \\(\\theta^{(0)}=\\left(\\pi^{(0)}, p^{(0)}, q^{(0)}\\right)\\), 然后通过下面的 步骤迭代计算参数的估计值, 直至收敛为止。第 \\(i\\) 次迭代参数的估计值为 \\(\\theta^{(i)}=\\) \\(\\left(\\pi^{(i)}, p^{(i)}, q^{(i)}\\right)\\) 。EM算法的第 \\(i+1\\) 次迭代如下。 \\(\\mathrm{E}\\) 步：计算在模型参数 \\(\\pi^{(i)}, p^{(i)}, q^{(i)}\\) 下观测数据 \\(y_j\\) 来自郑硬币 \\(\\mathrm{B}\\) 的概率。这里就是使用的贝叶斯定理。 \\[ \\mu_j^{(i+1)}=\\frac{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_j}\\left(1-p^{(i)}\\right)^{1-y_j}}{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_j}\\left(1-p^{(i)}\\right)^{1-y_j}+\\left(1-\\pi^{(i)}\\right)\\left(q^{(i)}\\right)^{y_j}\\left(1-q^{(i)}\\right)^{1-y_j}} \\] \\(\\mathrm{M}\\) 步：计算模型参数的新估计值 \\[ \\pi^{(i+1)}=\\frac{1}{n} \\sum_{j=1}^n \\mu_j^{(i+1)} \\] \\[ \\begin{gathered} p^{(i+1)}=\\frac{\\sum_{j=1}^n \\mu_j^{(i+1)} y_j}{\\sum_{j=1}^n \\mu_j^{(i+1)}} \\\\\\\\ q^{(i+1)}=\\frac{\\sum_{j=1}^n\\left(1-\\mu_j^{(i+1)}\\right) y_j}{\\sum_{j=1}^n\\left(1-\\mu_j^{(i+1)}\\right)} \\end{gathered} \\] 进行数值计算。假设模型参数的初值取为 \\[ \\pi^{(0)}=0.5, \\quad p^{(0)}=0.5, \\quad q^{(0)}=0.5 \\] 对 \\(y_j=1\\) 与 \\(y_j=0\\) 均有 \\(\\mu_j^{(1)}=0.5\\) 。 利用迭代公式, 得到 \\[ \\pi^{(1)}=0.5, \\quad p^{(1)}=0.6, \\quad q^{(1)}=0.6 \\] \\[ \\mu_j^{(2)}=0.5, \\quad j=1,2, \\cdots, 10 \\] 继续迭代, 得 \\[ \\pi^{(2)}=0.5, \\quad p^{(2)}=0.6, \\quad q^{(2)}=0.6 \\] 于是得到模型参数 \\(\\theta\\) 的极大似然估计: \\[ \\hat{\\pi}=0.5, \\quad \\hat{p}=0.6, \\quad \\hat{q}=0.6 \\] \\(\\pi=0.5\\) 表示硬币 A 是均匀的, 这一结果容易理解。 如果取初值 \\(\\pi^{(0)}=0.4, p^{(0)}=0.6, q^{(0)}=0.7\\), 那么得到的模型参数的极大似然 估计是 \\(\\hat{\\pi}=0.4064, \\hat{p}=0.5368, \\hat{q}=0.6432\\) 。这就是说, EM算法与初值的选择有关, 选择不同的初值可能得到不同的参数估计值。 一般地, 用 \\(Y\\) 表示观测随机变量的数据, \\(Z\\) 表示隐随机变量的数据。 \\(Y\\) 和 \\(Z\\) 连 在一起称为完全数据 (complete-data), 观测数据 \\(Y\\) 又称为不完全数据 (incompletedata）。假设给定观测数据 \\(Y\\), 其概率分布是 \\(P(Y \\mid \\theta)\\), 其中 \\(\\theta\\) 是需要估计的模型参数, 那么不完全数据 \\(Y\\) 的似然函数是 \\(P(Y \\mid \\theta)\\), 对数似然函数 \\(L(\\theta)=\\log P(Y \\mid \\theta)\\); 假设 \\(Y\\) 和 \\(Z\\) 的联合概率分布是 \\(P(Y, Z \\mid \\theta)\\), 那么完全数据的对数似然函数是 \\(\\log P(Y, Z \\mid \\theta)\\) 。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:2:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"算法步骤 输入: 观测变量数据 \\(Y\\), 隐变量数据 \\(Z\\), 联合分布 \\(P(Y, Z \\mid \\theta)\\), 条件分布 \\(P(Z \\mid Y, \\theta)\\); 输出：模型参数 \\(\\theta\\) 。 （1）选择参数的初值 \\(\\theta^{(0)}\\), 开始迭代; (2) \\(\\mathrm{E}\\) 步: 记 \\(\\theta^{(i)}\\) 为第 \\(i\\) 次迭代参数 \\(\\theta\\) 的估计值, 在第 \\(i+1\\) 次迭代的 \\(\\mathrm{E}\\) 步, 计算 \\[ \\begin{aligned} Q\\left(\\theta, \\theta^{(i)}\\right) \u0026=E_Z\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\\\\ \u0026=\\sum_Z \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\end{aligned} \\] 这里, \\(P\\left(Z \\mid Y, \\theta^{(i)}\\right)\\) 是在给定观测数据 \\(Y\\) 和当前的参数估计 \\(\\theta^{(i)}\\) 下隐变量数据 \\(Z\\) 的条 件概率分布; (3) \\(\\mathrm{M}\\) 步：求使 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 极大化的 \\(\\theta\\), 确定第 \\(i+1\\) 次迭代的参数的估计值 \\(\\theta^{(i+1)}\\) \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 重复第 (2) 步和第 (3) 步, 直到收敛。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:3:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"Q函数 函数 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 是 EM算法的核心, 称为 \\(Q\\) 函数 ( \\(Q\\) function)。 \\({Q}\\) 函数 : 完全数据 的对数似然函数 \\(\\log P(Y, Z \\mid \\theta)\\) 关于在给定观测数 据 \\(Y\\) 和当前参数 \\(\\theta^{(i)}\\) 下对未观测数据 \\(Z\\) 的条件概率分布 \\(P\\left(Z \\mid Y, \\theta^{(i)}\\right)\\) 的期望称为 \\(Q\\) 函数, 即 \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=E_Z\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] = \\sum_Z \\log P(Y,Z\\mid \\theta) P\\left (Z\\mid Y, \\theta^{(i)}\\right) \\] ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:4:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"Jensen不等式 如果f是凸函数，X是随机变量，那么有 \\[ E[f(X)] \\geq f[E(X)] \\] 如果f是凹函数则相反 这个图可以比较清晰的看出这个结论。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:5:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"EM算法的导出 为什么 EM算法能近似实现对观测数据的极大似然估计 呢? 下面通过近似求解观测数据的对数似然函数的极大化问题来导出 EM算法, 由此 可以清楚地看出 EM算法的作用。 我们面对一个含有隐变量的概率模型, 目标是极大化观测数据 (不完全数据) \\(Y\\) 关于参数 \\(\\theta\\) 的对数似然函数, 即极大化 \\[ \\begin{aligned} L(\\theta) \u0026=\\log P(Y \\mid \\theta)=\\log \\sum_Z P(Y, Z \\mid \\theta) \\\\\\\\ \u0026=\\log \\left(\\sum_Z P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right) \\end{aligned} \\] 注意到这一极大化的主要困难是式中有末观测数据并有包含和 (或积分) 的 对数。 事实上, EM算法是通过迭代逐步近似极大化 \\(L(\\theta)\\) 的。假设在第 \\(i\\) 次迭代后 \\(\\theta\\) 的 估计值是 \\(\\theta^{(i)}\\) 。我们希望新估计值 \\(\\theta\\) 能使 \\(L(\\theta)\\) 增加, 即 \\(L(\\theta)\u003eL\\left(\\theta^{(i)}\\right)\\), 并逐步达到极 大值。为此, 考虑两者的差: \\[ L(\\theta)-L\\left(\\theta^{(i)}\\right)=\\log \\left(\\sum_Z P(Y \\mid Z, \\theta) P(Z \\mid \\theta)\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\] 利用 Jensen 不等式 (Jensen inequality)得到其下界，这里的f即为log函数，是凹函数，则结论与凸函数时的结论是相反的。: \\[ \\begin{aligned} L(\\theta)-L\\left(\\theta^{(i)}\\right) \u0026=\\log \\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}\\right)-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\\\\ \u0026 \\geqslant \\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right)}-\\log P\\left(Y \\mid \\theta^{(i)}\\right) \\\\\\\\ \u0026=\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)} \\end{aligned} \\] 令 \\[ B\\left(\\theta, \\theta^{(i)}\\right) \\hat{=} L\\left(\\theta^{(i)}\\right)+\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)} \\] 则要求： \\[ L(\\theta) \\geqslant B\\left(\\theta, \\theta^{(i)}\\right) \\] 即函数 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 是 \\(L(\\theta)\\) 的一个下界, 可知, \\[ L\\left(\\theta^{(i)}\\right)=B\\left(\\theta^{(i)}, \\theta^{(i)}\\right) \\] 因此, 任何可以使 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 增大的 \\(\\theta\\), 也可以使 \\(L(\\theta)\\) 增大。这里回顾一下我们最原始的目标，就是为了最大化\\(L(\\theta)\\)，为了使 \\(L(\\theta)\\) 有尽可能大 的增长, 选择 \\(\\theta^{(i+1)}\\) 使 \\(B\\left(\\theta, \\theta^{(i)}\\right)\\) 达到极大, 即 \\[ \\theta^{(i+1)}=\\arg \\max_\\theta B\\left(\\theta, \\theta^{(i)}\\right) \\] 现在求 \\(\\theta^{(i+1)}\\) 的表达式。省去对 \\(\\theta\\) 的极大化而言是常数的项 \\[ \\begin{aligned} \\theta^{(i+1)} \u0026=\\arg \\max_\\theta\\left(L\\left(\\theta^{(i)}\\right)+\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log \\frac{P(Y \\mid Z, \\theta) P(Z \\mid \\theta)}{P\\left(Z \\mid Y, \\theta^{(i)}\\right) P\\left(Y \\mid \\theta^{(i)}\\right)}\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta\\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log (P(Y \\mid Z, \\theta) P(Z \\mid \\theta))\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta\\left(\\sum_Z P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\log P(Y, Z \\mid \\theta)\\right) \\\\\\\\ \u0026=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\end{aligned} \\] 这等价于 EM算法的一次迭代, 即求 \\(Q\\) 函数及其极大化。EM算法是通过 不断求解下界的极大化逼近求解对数似然函数极大化的算法。 下图给出 EM算法的直观解释。注意两个曲线的交点就是在\\(\\theta^{(i)}\\) 这里其实就是相当于推导为什么最大化Q函数对应的参数就是当前迭代的最佳参数。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:6:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"高斯混合模型 高斯混合模型是指有如下形式的概率分布模型： \\[ P(y\\mid \\theta ) = \\sum_{k=1}^K\\alpha_k \\Phi(y\\mid \\theta_k) \\] 其中\\(\\alpha_k\\)为系数，\\(\\alpha_k \\geq 0, \\sum_{k=1}^K\\alpha_k=1\\); \\(\\Phi(y\\mid \\theta_k)\\)为高斯密度函数，\\(\\theta_k=(\\mu_k,\\sigma_k^2)\\) \\[ \\Phi(y\\mid \\theta_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp \\left(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2} \\right) \\] 为第k个模型。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"EM算法的应用 明确隐变量, 写出完全数据的对数似然函数 可以设想观测数据 \\(y_j, j=1,2, \\cdots, N\\), 是这样产生的: 首先依概率 \\(\\alpha_k\\) 选择第 \\(k\\) 个高斯分布分模型 \\(\\phi\\left(y \\mid \\theta_k\\right)\\), 然后依第 \\(k\\) 个分模型的概率分布 \\(\\phi\\left(y \\mid \\theta_k\\right)\\) 生成观测数据 \\(y_j\\) 。这时观测数据 \\(y_j, j=1,2, \\cdots, N\\), 是已知的; 反映观测数据 \\(y_j\\) 来自第 \\(k\\) 个分模 型的数据是末知的, \\(k=1,2, \\cdots, K\\), 以隐变量 \\(\\gamma_{j k}\\) 表示, 其定义如下: \\[ \\gamma_{j k}= \\begin{cases}1, \u0026 \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] \\[ j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\] \\(\\gamma_{j k}\\) 是 0-1 随机变量。 有了观测数据 \\(y_j\\) 及末观测数据 \\(\\gamma_{j k}\\), 那么完全数据是 \\[ \\left(y_j, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K}\\right), \\quad j=1,2, \\cdots, N \\] 于是, 可以写出完全数据的似然函数: \\[ \\begin{aligned} P(y, \\gamma \\mid \\theta) \u0026=\\prod_{j=1}^N P\\left(y_j, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K} \\mid \\theta\\right) \\\\\\\\ \u0026=\\prod_{k=1}^K \\prod_{j=1}^N\\left[\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)\\right]^{\\gamma_{j k}} \\\\\\\\ \u0026=\\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N\\left[\\phi\\left(y_j \\mid \\theta_k\\right)\\right]^{\\gamma_{j k}} \\\\\\\\ \u0026=\\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N\\left[\\frac{1}{\\sqrt{2 \\pi} \\sigma_k} \\exp \\left(-\\frac{\\left(y_j-\\mu_k\\right)^2}{2 \\sigma_k^2}\\right)\\right]^{\\gamma_{j k}} \\end{aligned} \\] 式中, \\(n_k=\\sum_{j=1}^N \\gamma_{j k}, \\sum_{k=1}^K n_k=N\\) 。 那么, 完全数据的对数似然函数为 \\[ \\log P(y, \\gamma \\mid \\theta)=\\sum_{k=1}^K\\left\\\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\gamma_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right\\} \\] EM 算法的 \\(\\mathrm{E}\\) 步: 确定 \\(Q\\) 函数 \\[ \\begin{aligned} Q\\left(\\theta, \\theta^{(i)}\\right) \u0026=E\\left[\\log P(y, \\gamma \\mid \\theta) \\mid y, \\theta^{(i)}\\right] \\\\\\\\ \u0026=E\\left{\\sum_{k=1}^K\\left{n_k \\log \\alpha_k+\\sum_{j=1}^N \\gamma_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right}\\right} \\\\\\\\ \u0026=\\sum_{k=1}^K\\left\\\\{\\sum_{j=1}^N\\left(E \\gamma_{j k}\\right) \\log \\alpha_k+\\sum_{j=1}^N\\left(E \\gamma_{j k}\\right)\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right} \\end{aligned} \\] 这里需要计算 \\(E\\left(\\gamma_{j k} \\mid y, \\theta\\right)\\), 记为 \\(\\hat{\\gamma}_{j k}\\) 。 \\[ \\begin{aligned} \\hat{\\gamma}_{j k} \u0026=E\\left(\\gamma_{j k} \\mid y, \\theta\\right)=P\\left(\\gamma_{j k}=1 \\mid y, \\theta\\right) \\\\\\\\ \u0026=\\frac{P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(\\gamma_{j k}=1, y_j \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)}{\\sum_{k=1}^K P\\left(y_j \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)} \\\\\\\\ \u0026=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] \\(\\hat{\\gamma}_{j k}\\) 是在当前模型参数下第 \\(j\\) 个观测数据来自第 \\(k\\) 个分模型的概率, 称为分模型 \\(k\\) 对 观测数据 \\(y_j\\) 的响应度。 将 \\(\\hat{\\gamma}_{j k}=E \\gamma_{j k}\\) 及 \\(n_k=\\sum_{j=1}^N E \\gamma_{j k}\\) 代入, 即得 \\[ Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{k=1}^K\\left\\\\{n_k \\log \\alpha_k+\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left[\\log \\left(\\frac{1}{\\sqrt{2 \\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2 \\sigma_k^2}\\left(y_j-\\mu_k\\right)^2\\right]\\right\\\\\\} \\] 确定 EM 算法的 \\(M\\) 步 迭代的 \\(\\mathrm{M}\\) 步是求函数 \\(Q\\left(\\theta, \\theta^{(i)}\\right)\\) 对 \\(\\theta\\) 的极大值, 即求新一轮迭代的模型参数: \\[ \\theta^{(i+1)}=\\arg \\max_\\theta Q\\left(\\theta, \\theta^{(i)}\\right) \\] 用 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 及 \\(\\hat{\\alpha}_k, k=1,2, \\cdots, K\\), 表示 \\(\\theta^{(i+1)}\\) 的各参数。求 \\(\\hat{\\mu}_k, \\hat{\\sigma}_k^2\\) 只需分别对 \\(\\mu_k, \\sigma_k^2\\) 求偏导数并令其为 0 , 即可得到; 求 \\(\\hat{\\alpha}_k\\) 是在 \\(\\sum_{k=1}^K \\alpha_k=1\\) 条件 下求偏导数并令其为 0 得到的。结果如下: \\[ \\begin{gathered} \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\\\\ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k}\\left(y_j-\\mu_k\\right)^2}{\\sum_{j=1}^N \\ha","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:1","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"算法应用总结 (高斯混合模型参数估计的EM算法) 输入: 观测数据 \\(y_1, y_2, \\cdots, y_N\\), 高斯混合模型; 输出：高斯混合模型参数。 （1）取参数的初始值开始迭代; (2) \\(\\mathrm{E}\\) 步: 依据当前模型参数, 计算分模型 \\(k\\) 对观测数据 \\(y_j\\) 的响应度 \\[ \\hat{\\gamma}_{j k}=\\frac{\\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}{\\sum_{k=1}^K \\alpha_k \\phi\\left(y_j \\mid \\theta_k\\right)}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\] \\(\\mathrm{M}\\) 步：计算新一轮迭代的模型参数 \\[ \\hat{\\mu}_k=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{j k} y_j}{\\sum_{j=1}^N \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\] \\[ \\hat{\\sigma}_k^2=\\frac{\\sum_{j=1}^N \\hat{\\gamma}_{jk}(y_j-\\mu_k)^2}{\\sum_{j=1}^N \\hat{\\gamma}_{jk}}, \\quad k= 1,2,\\dots, K \\] \\[ \\hat{\\alpha}_k = \\frac{\\sum_{j=1}^N \\hat{\\gamma}_{jk}}{N} ,\\quad k=1,2,\\dots, K \\] 重复直到收敛。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:7:2","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning"],"content":"总结 总之来说em算法作为数据挖掘十大算法之一，应用范围十分广泛，它不能看作是一个具体的模型，常常用于模型的求解，比如HMM的学习参数问题等等，是必须要学会的算法之一。 ","date":"2022-10-03","objectID":"/em%E7%AE%97%E6%B3%95/:8:0","tags":["Machine Learning","EM算法"],"title":"EM算法","uri":"/em%E7%AE%97%E6%B3%95/"},{"categories":["算法题"],"content":"分发饼干 https://leetcode-cn.com/problems/assign-cookies/ class Solution: def findContentChildren(g, s) -\u003e int: g = sorted(g) s = sorted(s) n = 0 for i in range(len(s)): if g[n] \u003c= s[i]: n += 1 if n == len(g): return n return n 贪心算法的题目，考虑局部最优 ","date":"2022-10-03","objectID":"/%E5%88%86%E5%8F%91%E9%A5%BC%E5%B9%B2/:0:0","tags":["算法题","分发饼干"],"title":"分发饼干","uri":"/%E5%88%86%E5%8F%91%E9%A5%BC%E5%B9%B2/"},{"categories":["工具"],"content":"正则表达式 [abcd]匹配中括号里的所有字符 [^abcd]匹配除了括号里的所有字符 [A-Za-z]匹配所有字母 [\\s\\S]是匹配所有空白符，包括换行，非空白符，包括换行 [\\w] 匹配字母、数字、下划线。等价于 [A-Za-z0-9_] 匹配有特殊含义的，比如* ^ 等 记得要加上反斜杠进行转义 ？匹配前面的表达式0次或1次 **.匹配除* +匹配前面的表达式1次或多次 *匹配前面的表达式0次或多次 {n}匹配n次，{n,m}最少匹配n次，最多m次，{n,}至少匹配n次 定位符：^ $ 即限定在哪里匹配 （）灵活应用 [.]等价于\\. ","date":"2022-09-28","objectID":"/regex/:0:0","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["工具"],"content":"运算优先级 image.png ","date":"2022-09-28","objectID":"/regex/:1:0","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["工具"],"content":"零宽度断言（前后预查） ","date":"2022-09-28","objectID":"/regex/:2:0","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["工具"],"content":"?=…正先行断言 ?=...表示正先行断言，表示第一部分表达式之后必须跟着?=...定义的表达式。 返回结果只包含满足匹配条件的第一部分表达式。 定义一个正先行断言要使用 ()。在括号内部使用一个问号和等号： (?=...)。 正先行断言的内容写在括号中的等号后面。 例如，表达式 (T|t)he(?=\\sfat) 匹配 The 和 the，在括号中我们又定义了正先行断言 (?=\\sfat) ，即 The 和 the 后面紧跟着 (空格)fat。 ","date":"2022-09-28","objectID":"/regex/:2:1","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["工具"],"content":"?!… 负先行断言 负先行断言 ?! 用于筛选所有匹配结果，筛选条件为 其后不跟随着断言中定义的格式。 正先行断言 定义和 负先行断言 正好相反。 表达式 (T|t)he(?!\\sfat) 匹配 The 和 the，且其后不跟着 (空格)fat。 ### ?\u003c=… 正后发断言 正后发断言 记作(?\u003c=...) 用于筛选所有匹配结果，筛选条件为 其前跟随着断言中定义的格式。 例如，表达式 (?\u003c=(T|t)he\\s)(fat|mat) 匹配 fat 和 mat，且其前跟着 The 或 the。 ### ?\u003c!… 负后发断言 负后发断言 记作 (?\u003c!...) 用于筛选所有匹配结果，筛选条件为 其前不跟随着断言中定义的格式。 例如，表达式 (?\u003c!(T|t)he\\s)(cat) 匹配 cat，且其前不跟着 The 或 the。 此外还有一种是?:，用于括号匹配中，即non-capturing group， # 实战 ","date":"2022-09-28","objectID":"/regex/:2:2","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["工具"],"content":"Matching a decimal numbers 来源:https://regexone.com/problem/matching_decimal_numbers 意思就是要匹配Match的还要避开Skip的 具体实现： ^-?\\d+(,\\d+)*(\\.\\d+(e\\d+)?)?$ 解释： ​ 开头匹配负号，其实这里可以变成[\\\\-\\\\+]? 以防出现正号 ​ 然后匹配带逗号的数字，匹配0次或多次 ​ 然后匹配带小数点的数字，匹配0次或1次，因为小数点最多出现一次 ​ 显然后面跟着匹配指数，显然指数也最多出现一次。 ","date":"2022-09-28","objectID":"/regex/:3:0","tags":["工具","regex"],"title":"regex","uri":"/regex/"},{"categories":["pandas"],"content":"pandas实践1 在读取数据之前，我修改了表格里面的表头，以便程序的编写。 先从 excel 读取数据,然后看看 shape 了解行数列数,然后调用 info 方法， 看看有没有缺失值，发现并没有缺失值，但题目里说了可能有重复或者格式 不对的数据，因为最主要的是学号,一般学号的长度都是 12 个数字，所以筛 选出不是 12 位数的 data[data['studentid'].apply(lambda x:len(x)!=12)] 考虑到可能出现中文的情况，先尝试转化为整数试试 data[‘studentid’] = data[‘studentid’].astype(“int64”) 发现报错了，然后就看见了那个学号是’忘记了’的 最后修改成了 data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')] 将这些数据删除 data = data.drop(data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')].index) 考虑到有重复，重复的两个因素就是姓名和学号，因此进行去重处理 data.drop_duplicates(subset=['name','studentid'],keep='first',inplace=Tru e) 此外，对专业的处理，将无用的 xx-x 去掉即可，这里考虑到了正则表达式 data['class'] = data['class'].apply(lambda s:re.sub(r\"[\\s*\\d*\\-*\\—*\\ － *\\–*\\/*]?\",'',s)) 因为各种各样的-负号千奇百怪，我只能一次次修改后然后统计一下即调用 data[‘class’].value_counts() 有没有没有处理到的，然后把那个-符号加进去 还发现了有/号。 最后就成了那样，写到这里我有了更好的想法，和下面的 某两个个例有关系。 然后就是那个 maps 表，都简化为简称，对称呼进行统一，用了 apply 方 法 再统计一下，发现了两个专业后面带名字的学长学姐，因为就两个，就把他 们加到 maps 里面了，其实也可以判断名字是否在专业里面，如果在就替换 为空吧。 之后就差不多可以了，数据预处理完毕，按照要求保存即可。 #数据预处理文件 import pandas as pd import re data = pd.read_excel(\"附件1.xlsx\") #去除错误数据 data = data.drop(data[data['studentid'].apply(lambda x:len(x)!=12 or x=='忘记了')].index) #去重 data.drop_duplicates(subset=['name','studentid'], keep='first', inplace=True) data['class'] = data['class'].apply(lambda s:re.sub(r\"[\\s*\\d*\\-*\\—*\\－*\\–*\\/*]?\", '', s)) maps = { '智能科学':'智科', '云计算':'云计', '应用统计学':'统计', '信息与计算科学':'信计', '智能科学与技术':'智科', '应用统计':'统计', '软件工程':'软工', '信息与计算科学（云计算）':'信计', '光电信息与科学':'光电', '信计（云计算）':'信计', '光电信息科学与工程':'光电', '数据科学':'大数据', '智科科学':'智科', '信计学长':'信计', '信计学姐':'信计', '统计学':'统计', '信息计算与科学':'信计', '信计与计算科学':'信计' } def replaces(clas): if clas in maps.keys(): return maps[clas] else: return clas data['class'] = data['class'].apply(replaces) res = pd.DataFrame() res['账号'] = '21aidc' + data['studentid'] res['姓名'] = data['name'] res['密码'] = res['账号'] res['专业'] = data['class'] res.to_excel(\"result.xlsx\", index=False,encoding='utf-8') ","date":"2022-09-20","objectID":"/aidc%E6%B5%8B%E8%AF%95/:0:0","tags":["pandas","task1"],"title":"aidc测试","uri":"/aidc%E6%B5%8B%E8%AF%95/"},{"categories":["Deep Learning","优化算法","优化器"],"content":"moment(矩) 矩在数学中的定义，一阶矩(first moment)就是样本的均值(mean), 二阶矩就是方差（variance）。 ## 滑动平均 滑动平均(exponential moving average)，或者叫做指数加权平均(exponentially weighted moving average)，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。在时间序列预测中也常用。 变量 \\(v\\) 在 \\(t\\) 时刻记为 \\(v_{t} ，\\text{可以理解为0到t时刻的平均值} 。\\quad \\theta_{t}\\) 为变量 \\(v\\) 在 \\(t\\) 时刻的取值，即在不使用滑动平均模型时 \\(v_{t}=\\theta_{t}\\) ，在使用滑动平均模型后， \\(v_{t}\\) 的更新公式如下: \\[ v_{t}=\\beta \\cdot v_{t-1}+(1-\\beta) \\cdot \\theta_{t} \\] 上式中， \\(\\beta \\in[0,1) ， \\beta=0\\) 相当于没有使用滑动平均。 这也是RMSProp和Adam等算法里使用的最重要的思想。通过滑动平均来降低梯度的波动值。 ## Adam 下面看最经典的伪代码： adam算法比起adagrad和RMSProp，不仅加入了一阶和二阶moment的计算。而且加入了bias-correction term。以下将展开分析： ","date":"2022-09-11","objectID":"/adam/:1:0","tags":["Deep","Learning","优化算法","Adam算法","优化器"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法","优化器"],"content":"adam的更新率（stepsize) adam算法中最重要的就是每次迭代的迭代率（step size），他决定了adam算法的效率。根据上 文的算法， step size等于: \\(\\Delta_{t}=\\alpha \\cdot \\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\) 1) 当 \\(\\left(1-\\beta_{1}\\right)\u003e\\sqrt{1-\\beta_{2}}\\) 的时候，它的上界满足不等式: \\(\\left|\\Delta_{t}\\right| \\leq \\alpha \\cdot\\left(1-\\beta_{1}\\right) / \\sqrt{1-\\beta_{2}}\\) 2) 否则 \\(\\left|\\Delta_{t}\\right| \\leq \\alpha\\) 1）通常发生在数据很稀疏的时候。当数据密集的时候， stepsize会更小。 3) 当 \\(\\left(1-\\beta_{1}\\right)=\\sqrt{1-\\beta_{2}}\\) 的时候，因为 \\(\\left|\\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\right|\u003c1\\) 所以，也满足条件 2 的 \\(\\left|\\Delta_{t}\\right| \\leq \\alpha\\) 总结以上3个条件，可以近似得出stepsize 满足 \\(\\left|\\Delta_{t}\\right| \\cong \\alpha\\) 这里的 \\(\\widehat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\) 通常也成为信噪比（Signal-to-noise ratio SNR)，并且满足SND越小， stepsize也越小。 ","date":"2022-09-11","objectID":"/adam/:1:1","tags":["Deep","Learning","优化算法","Adam算法","优化器"],"title":"Adam算法","uri":"/adam/"},{"categories":["Deep Learning","优化算法","优化器"],"content":"初始化偏差矫正项 原算法中的这两行 \\[ \\begin{aligned} \u0026\\widehat{m}_{t} \\leftarrow m_{t} /\\left(1-\\beta_{1}^{t}\\right) \\\\\\\\ \u0026\\hat{v}_{t} \\leftarrow v_{t} /\\left(1-\\beta_{2}^{t}\\right) \\end{aligned} \\] 称为偏差校正项(bias-correction term),他使用了滑动平均值(EMA: exponential moving average)的思想，例如计算二次moment的 \\(v_{t}=\\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}\\) 可以写成如下的形 式： \\[ v_{t}=\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2} \\] 我们的目的是求得 \\(\\mathbb{E}\\left[v_{t}\\right]\\) (EMA) 和二阶moment \\(\\mathbb{E}\\left[g_{t}^{2}\\right]\\) 之间的关系，推导如下: \\[ \\begin{aligned} \\mathbb{E}\\left[v_{t}\\right] \u0026=\\mathbb{E}\\left[\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2}\\right] \\\\\\\\ \u0026=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i}+\\zeta \\\\\\\\ \u0026=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}^{t}\\right)+\\zeta \\end{aligned} \\] 最后得出 \\(\\mathbb{E}\\left[g_{t}^{2}\\right]=\\frac{\\mathbb{E}\\left[v_{t}\\right]-\\zeta}{\\left(1-\\beta_{2}^{t}\\right)}\\) 通常可以忽略常数 \\(\\zeta\\) 。得出 \\[ \\bar{v_t} = \\frac{v_t}{1-\\beta_2^t} \\] 综上所述，Adam 优化器可以根据历史梯度的震荡情况和过滤震荡后的真实历史梯度对变量进行更新 ","date":"2022-09-11","objectID":"/adam/:1:2","tags":["Deep","Learning","优化算法","Adam算法","优化器"],"title":"Adam算法","uri":"/adam/"},{"categories":["Machine Learning","性能指标"],"content":"精确率和召回率 ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"混淆矩阵 True Positive(真正, TP)：将正类预测为正类数. True Negative(真负 , TN)：将负类预测为负类数. False Positive(假正, FP)：将负类预测为正类数 False Negative(假负 , FN)：将正类预测为负类数 ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:1","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"精确率 \\[ P = \\frac{TP}{TP+FP} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:1:2","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"准确率 \\[ ACC = \\frac{TP+TN}{TP+TN+FP+FN} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:2:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"召回率 \\[ R = \\frac{TP}{TP+FN} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:3:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"F1 \\[ \\frac{2}{F_1} = \\frac{1}{P} + \\frac{1}{R} \\] ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:4:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","性能指标"],"content":"区别 精确率（查准率）：在所有预测为正的样本中（分母），真正为正的有多少（分子）。 召回率（查全率）：在所有实际为正的样本中（分母），成功预测出来的有多少（分子） img img ","date":"2022-08-21","objectID":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/:5:0","tags":["Machine Learning","性能指标","精确率和召回率"],"title":"精确率和召回率","uri":"/%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"},{"categories":["Machine Learning","分类算法"],"content":"参考：https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html 《机器学习》周志华 决策树 决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。这样，结点说法就与模块说法对应上了，理解就好。 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"步骤 1.特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。 特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。 (1).香农熵 在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)，这个名字来源于信息论之父克劳德·香农。 如果看不明白什么是信息增益和熵，请不要着急，因为他们自诞生的那一天起，就注定会令世人十分费解。克劳德·香农写完信息论之后，约翰·冯·诺依曼建议使用”熵”这个术语，因为大家都不知道它是什么意思。 熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为 ： \\[ l(x_i) = -log_{2}p(x_i) \\] 其中p(xi)是选择该分类的概率。有人可能会问，信息为啥这样定义啊？答曰：前辈得出的结论。这就跟1+1等于2一样，记住并且会用即可。上述式中的对数以2为底，也可以e为底(自然对数)。 通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到： \\[ H = -\\sum_{i=1}^np(x_i)log_2p(x_i) \\] 期中n是分类的数目。熵越大，随机变量的不确定性就越大。 当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, = 1,2,3,.. .,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为 ： \\[ H(D) = -\\sum_{k=1}^K \\frac{|c_k|}{|D|}log_2\\frac{|c_k|}{|D|} \\] 根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为： \\[ H(D) = -\\frac{9}{15}log_2\\frac{9}{15} - \\frac{6}{15}log_2\\frac{6}{15} = 0.971 \\] (2)信息增益 在上面，我们已经说过，如何选择特征，需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。 在讲解信息增益定义之前，我们还需要明确一个概念，条件熵。 熵我们知道是什么，条件熵又是个什么鬼？条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望： \\[ H(Y|X) = \\sum_{i=1}^np_iH(Y|X = x_i), \\\\\\\\ p_i = P(X = x_i) \\] 明确了条件熵和经验条件熵的概念。接下来，让我们说说信息增益。前面也提到了，信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即： \\[ g(D,A) = H(D) - H(D|A) \\] 一般地，熵H(D)与条件熵H(D|A)之差称为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 设特征A有n个不同的取值{a1,a2,···,an}，根据特征A的取值将D划分为n个子集{D1,D2，···,Dn}，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik = Di ∩ Ck，|Dik|为Dik的样本个数。于是经验条件熵的公式可以些为 \\[ H(D|A) = \\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|} \\] 说了这么多概念性的东西，没有听懂也没有关系，举几个例子，再回来看一下概念，就懂了。 以贷款申请样本数据表为例进行说明。看下年龄这一列的数据，也就是特征A1，一共有三个类别，分别是：青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集出现的概率是十五分之五，也就是三分之一。同理，年龄是中年和老年的数据在训练数据集出现的概率也都是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在五个数据中，只有两个数据显示拿到了最终的贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别为五分之三、五分之四。所以计算年龄的信息增益，过程如下： 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 同理，计算其余特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为： 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 机器学习实战教程（二）：决策树基础篇之让我们从相亲说起 最后，比较特征的信息增益，由于特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征。 由于特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。它将训练集D划分为两个子集D1(A3取值为”是”)和D2(A3取值为”否”)。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。 对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益： 机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜 根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。由于A2有两个可能取值，从这一结点引出两个子结点：一个对应”是”(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为”是”；另一个是对应”否”(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为”否”。 这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示。 机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:1:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"总结 我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。 构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。 决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 决策树的一些优点： 易于理解和解释。决策树可以可视化。 几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。 使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。 可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。 可以处理多值输出变量问题。 使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。 即使对真实模型来说，假设无效的情况下，也可以较好的适用。 决策树的一些缺点： 决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。 概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems。 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。 ","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:2:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"代码 import numpy as np import math def equalNums(label_list, label): \"\"\" 函数说明： 计算标记集中某个标记的数量 Parameters： label_list - 标记集 label - 某个标记 Returns： num - 某个标记的数量 \"\"\" return np.sum(label_list == label) def calcShannonEnt(label_list): \"\"\" 函数说明： 计算信息熵 对应公式 Ent(D) = -∑ Pk*log2(Pk) k=1..len(label_set) Parameters： label_list - 标记集 Returns： shannonEnt - 当前标记集的信息熵 \"\"\" label_set = set(label_list) len_label_list = label_list.size shannonEnt = 0.0 for label in label_set: prob = equalNums(label_list, label)/len_label_list shannonEnt -= prob * math.log2(prob) return shannonEnt def conditionnalEntropy(feature_list, label_list): \"\"\" 函数说明： 计算条件信息熵，对应信息增益公式中的被减项 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： entropy - 条件信息熵 \"\"\" feature_list = np.asarray(feature_list) label_list = np.asarray(label_list) feature_set = set(feature_list) entropy = 0.0 for feat in feature_set: pro = equalNums(feature_list, feat)/feature_list.size entropy += pro * calcShannonEnt(label_list[feature_list == feat]) return entropy def calcInfoGain(feature_list, label_list): \"\"\" 函数说明： 计算信息增益 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： 当前属性的信息增益 \"\"\" return calcShannonEnt(label_list) - conditionnalEntropy(feature_list, label_list) def splitDataSet(sample_list, label_list, axis, value): \"\"\" 函数说明： 决策树在选好当前最优划分属性之后划分样本集 依据value选择对应样例，并去除第axis维属性 Parameters： feature_list - sample_list中某一列，表示当前属性的所有值 label_list - 标记集 Returns： return_sample_list, return_label_list \"\"\" # sample_list[sample_list[...,axis] == value] 利用了numpy数组的布尔索引 filtered_sample_list = sample_list[sample_list[...,axis] == value] return_label_list = label_list[sample_list[...,axis] == value] # np.hstack 将数组横向拼接，也就是去除第axis维属性 return_sample_list = np.hstack((filtered_sample_list[...,:axis], filtered_sample_list[...,axis+1:])) return return_sample_list, return_label_list def chooseBestFeatureToSplit(sample_list, label_list): \"\"\" 函数说明： 选取最优划分属性 Parameters： sample_list - 样本集 label_list - 标记集 Returns： bestFeat_index - 最优划分属性的索引值 \"\"\" numFeatures = sample_list.shape[1] bestInfoGain = 0 bestFeat_index = -1 for i in range(numFeatures): infoGain = calcInfoGain(sample_list[..., i], label_list) if infoGain \u003e bestInfoGain: bestInfoGain = infoGain bestFeat_index = i return bestFeat_index def createTree(sample_list, label_list, attr_list_copy): \"\"\" 函数说明： 生成决策树 Parameters： sample_list - 样本集 label_list - 标记集 attr_list_copy - 属性集（之所以加copy是为了删属性的时候是在副本上，防止递归出错） Returns： myTree - 最终的决策树 \"\"\" # attr_list 有del操作，不用副本的话递归会出错 attr_list = attr_list_copy.copy() if len(set(label_list)) == 1: # 如果只有一种标记，直接返回标记 return label_list[0] elif sample_list.size == 0: # 如果所有属性都被遍历，返回最多的标记 return voteLabel(label_list) # bestFeat_index 最优划分属性的索引值 # bestAttr 最优划分属性对应的名字 bestFeat_index = chooseBestFeatureToSplit(sample_list, label_list) bestAttr = attr_list[bestFeat_index] myTree = {bestAttr: {}} del(attr_list[bestFeat_index]) feat_set = set(sample_list[..., bestFeat_index]) # 依据最优划分属性进行划分，并向下递归 for feat in feat_set: return_sample_list, return_label_list = splitDataSet(sample_list, label_list, bestFeat_index, feat) myTree[bestAttr][feat] = createTree(return_sample_list, return_label_list, attr_list) return myTree def voteLabel(label_list): \"\"\" 函数说明： 这个函数是用在遍历完所有特征时，返回最多的类别 Parameters： label_list: 标记列表 Returns： 数量最多的标记 \"\"\" # unique_label_list 是label_list中标记种类列表 # label_num 是unique_label_list对应的数量列表 unique_label_list = list(set(label_list)) label_num_list = [] for label in unique_label_list: label_num_list.append(equalNums(label_list, label)) # label_num.index(max(label_num))是label_num数组中最大值的下标 return unique_label_list[label_num_list.index(max(label_num_list))] def classify(decisionTree, testVec, attr_list): \"\"\" 函数说明： 对tesVec进行分类 Parameters： decisionTree - 决策树 attr_list - 属性名列表 testVec - 测试向量 Returns： label - 预测的标记 \"\"\" feature = list(decisionTree.keys())[0] # feature为决策树的根节点 feature_dict = decisionTree[feature] # feature_dict为根节点下的子树 feature_index = attr_list.index(feature) # feature_index为featur","date":"2022-08-21","objectID":"/%E5%86%B3%E7%AD%96%E6%A0%91/:3:0","tags":["Machine Learning","分类算法","决策树"],"title":"决策树","uri":"/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"分类算法 主要区分一下生成模型和判别模型，首先要知道生成模型和判别模型都属于监督学习，即样本有其对应的标签的。还有一个概念就是硬分类和软分类，简单理解就是硬分类是直接分出类别，比如线性判别分析、感知机。而软分类是计算出概率，根据概率来得到类别，生成模型和判别模型都是软分类。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:0:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"生成模型 学习得到联合概率分布\\(P(x,y)\\)，即特征x与标签y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。通俗的说就是如果有k类，就学习k个概率密度分布，对于样本，算出在每个概率密度分布下的概率，哪个概率大就属于哪一类。 生成模型要求的数据量比较大，能够更好地估计概率密度。 ## 判别模型 学习得到条件概率分布\\(P(y|x)\\)，即在特征x出现的情况下标记y出现的概率。 判别模型对样本的要求没有那么多。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:1:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"理解 无论是生成还是判别模型都是来求有监督模型的，目的是通过分类函数或者条件概率函数进行数据分类。 算出属于正负样本的概率再互相对比的就是生成模型，直接得到结果概率的就是判别模型，生成模型得到分布，判别模型得到最优划分。 生成模型可以得到判别模型，反之不成立。 生成模型是求联合概率分布，判别模型是求条件概率分布。 生成方法的学习收敛速度更快，当样本容量增加的时候，学到的模型可以更快的收敛于真实模型。 判别学习不能反映训练数据本身的特性，但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异，直接面对预测，往往学习的准确率高于生成模型。 简单的说，生成模型是从大量的数据中找规律，属于统计学习；而判别模型只关心不同类型的数据的差别，利用差别来分类。 ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:0","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"生成式模型： 朴素贝叶斯 混合高斯模型 隐马尔科夫模型(HMM) 贝叶斯网络 Sigmoid Belief Networks 马尔科夫随机场(Markov Random Fields) 深度信念网络(DBN) ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:1","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["Machine Learning","分类算法"],"content":"判别式模型 K近邻(KNN) 线性回归(Linear Regression) 逻辑回归(Logistic Regression) 神经网络(NN) 支持向量机(SVM) 高斯过程(Gaussian Process) 条件随机场(CRF) CART(Classification and Regression Tree) ","date":"2022-08-12","objectID":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/:2:2","tags":["Machine Learning","分类算法","分类算法概述"],"title":"分类算法概述","uri":"/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"categories":["NLP"],"content":"BM25算法 BM25算法，通常用来作搜索相关性平分。一句话概况其主要思想：对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，计算每个语素qi与D的相关性得分，最后，将qi相对于D的相关性得分进行加权求和，从而得到Query与D的相关性得分。 ","date":"2022-08-07","objectID":"/bm25/:0:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"原理 BM25一般公式如下： \\[ score(Q,d) = \\sum_{i}^nW_iR(q_i, d) \\] 其中Q表示为Query，\\(q_i\\)表示Q解析后的一个语素(对中文而言，我们可以把对Query的分词作为语素分析，每个词看成语素)。d表示一个搜索结果文档，\\(W_i\\)表示语素\\(q_i\\)的权重，\\(R(q_i, d)\\)表示语素\\(q_i\\)与文档d的相关性得分。 ","date":"2022-08-07","objectID":"/bm25/:1:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"权重 下面我们来看如何定义Wi。判断一个词与一个文档的相关性的权重，方法有多种，较常用的是IDF。这里以IDF为例，公式如下： \\[ IDF(q_i) = \\log \\frac{N-n(q_i)+0.5}{n(q_i)+0.5} \\] 其中，N为索引中的全部文档数，n(qi)为包含了qi的文档数。 根据IDF的定义可以看出，对于给定的文档集合，包含了qi的文档数越多，qi的权重则越低。也就是说，当很多文档都包含了qi时，qi的区分度就不高，因此使用qi来判断相关性时的重要度就较低。 ","date":"2022-08-07","objectID":"/bm25/:2:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"相关性得分 下面定义相关性得分\\(R(q_i,d)\\) 首先来看BM25中相关性得分的一般形式： 其中\\(k_1, k_2, b\\)为调节因子，根据经验设置，一般\\(k_1=2,b=0.75\\) ，\\(f_i\\)为\\(q_i\\)在文档d中出现的频率，\\(qf_i\\)为\\(q_i\\)在Query中出现的频率，dl为文档d的长度，avgdl为所有文档的平均长度，绝大部分情况下\\(q_i\\)在Query中只会出现一次，因此 \\(qf_i=1\\)，所以可以简化为： 从K的定义中可以看到，参数b的作用是调整文档长度对相关性影响的大小。b越大，文档长度的对相关性得分的影响越大，反之越小。而文档的相对长度越长，K值将越大，则相关性得分会越小。这可以理解为，当文档较长时，包含qi的机会越大，因此，同等fi的情况下，长文档与qi的相关性应该比短文档与qi的相关性弱。 综上，BM25算法的相关性得分公式可总结为： 从BM25的公式可以看到，通过使用不同的语素分析方法、语素权重判定方法，以及语素与文档的相关性判定方法，我们可以衍生出不同的搜索相关性得分计算方法，这就为我们设计算法提供了较大的灵活性。 ","date":"2022-08-07","objectID":"/bm25/:3:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"代码 import math import jieba import re text = ''' 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。 因此，这一领域的研究将涉及自然语言，即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。 自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的计算机系统， 特别是其中的软件系统。因而它是计算机科学的一部分。 ''' class BM25(object): def __init__(self, docs): self.D = len(docs) # doc个数 self.avgdl = sum([len(doc)+0.0 for doc in docs]) / self.D # 每篇平均长度 self.docs = docs self.f = [] # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数 self.df = {} # 存储每个词及出现了该词的文档数量(count) self.idf = {} # 存储每个词的idf值，当作权重 self.k1 = 1.5 self.b = 0.75 self.init() def init(self): for doc in self.docs: tmp = {} for word in doc: tmp[word] = tmp.get(word, 0) + 1 # 存储每个文档中每个词的出现次数（也可以用defaultdict) self.f.append(tmp) # idx为索引，f[idx]为一个dict，dict存储着第idx+1个文档中每个词的出现次数,idx代表第几个文档。 for k in tmp.keys(): # 如果词k出现在了当前文档中，则df[k]即文档数量加1 self.df[k] = self.df.get(k, 0) + 1 for k, v in self.df.items(): self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5) # 计算idf def sim(self, query, index): # 与单个文档的相似度 score = 0 for word in query: if word not in self.f[index]: continue d = len(self.docs[index]) # 当前文档的长度 score += (self.idf[word]*(self.f[index][word]/d)*(self.k1+1) / ((self.f[index][word]/d)+self.k1*(1-self.b+self.b*d / self.avgdl))) return score def simall(self, query): scores = [] for index in range(self.D): score = self.sim(query, index) scores.append(score) return scores def get_sentences(doc): line_break = re.compile('[\\r\\n]') # 以换行符分割 delimiter = re.compile('[，。？！；]') # 以中文标点符号分割 sentences = [] for line in line_break.split(doc): line = line.strip() if not line: continue for sent in delimiter.split(line): sent = sent.strip() if not sent: continue sentences.append(sent) return sentences if __name__ == '__main__': sents = get_sentences(text) print(sents) doc = [] for sent in sents: words = list(jieba.cut(sent)) doc.append(words) # print(doc) s = BM25(doc) # print(s.f) # print(s.df) # print(s.idf) print(s.simall(['自然语言', '计算机科学', '领域', '人工智能', '领域'])) ","date":"2022-08-07","objectID":"/bm25/:4:0","tags":["NLP","BM25"],"title":"BM25","uri":"/bm25/"},{"categories":["NLP"],"content":"GPT ","date":"2022-07-27","objectID":"/gpt/:0:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["NLP"],"content":"预训练(从左到右的 Transformer 语言模型) GPT 是一种基于 Transformer 的从左到右的语言模型。该架构是一个 12 层的 Transformer 解码器（没有解码器-编码器）。 ## 模型架构 就是12层的transformer-decoder。其中只使用了transformer模型中的decoder部分，并且把decoder里面的encoder-decoder attention部分去掉了，只保留了masked self-attention，再加上feed-forward部分。再提一句，masked self-attention保证了GPT模型是一个单向的语言模型。 另外，作者在position encoding上做了调整，使用了可学习的位置编码，不同于transformer的三角函数位置编码。 ","date":"2022-07-27","objectID":"/gpt/:1:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["NLP"],"content":"微调：将 GPT 用于下游任务 微调损失包括特定于任务的损失以及语言建模损失： \\[ L = L_{xent} + \\lambda \\cdot L_{task}. \\] ","date":"2022-07-27","objectID":"/gpt/:2:0","tags":["NLP","GPT"],"title":"GPT","uri":"/gpt/"},{"categories":["Deep Learning","网络正则化"],"content":"Dropout 在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值h以保留概率概率p由随机变量替换(即drop经过神经元后的值代替drop神经元) \\[ h^{'}= \\begin{cases} 0, \\quad 概率为1-p \\\\\\\\ \\frac{h}{p}, \\quad 概率为p \\end{cases} \\] 注意期望不要变，即 \\[ E[h^{'}] = (1-p)*0 + p *\\frac{h}{p} = h \\] 也可以训练时非丢弃单元不除以概率p，而是测试时模型参数乘以p，这样可以保证训练集和测试集的期望相同。和上面的效果相同。 注意：正则项（Dropout）只在训练过程中使用，因为其会影响模型参数的更新 所以在推理过程中，丢弃法直接返回输入。 ","date":"2022-07-21","objectID":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/:0:0","tags":["Deep Learning","网络正则化","Dropout正则化"],"title":"Dropout正则化","uri":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning","网络正则化"],"content":"代码 import torch from torch import nn from d2l import torch as d2l import torchvision from torchvision import transforms from torch.utils import data def dropout_layer(X,dropout): assert 0\u003c=dropout \u003c= 1 # 如果keep_prob设置为1，全部元素被保留 if dropout == 1: return X # 如果keep_prob设置为0，全部元素被丢弃 if dropout == 0: return torch.zeros_like(X) mask = (torch.rand(X.shape) \u003c dropout).float() #使用mask而不是直接置零是为了提高计算效率 return mask * X/(dropout) ","date":"2022-07-21","objectID":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/:1:0","tags":["Deep Learning","网络正则化","Dropout正则化"],"title":"Dropout正则化","uri":"/dropout%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["算法题"],"content":"最长回文子串 ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:0:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/longest-palindromic-substring/ ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:1:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： ​ 一开始暴力解法，比较好想，结果超时了哎，后来看见了标签是动态规划，才知道不能暴力 class Solution: def longestPalindrome(self, s: str) -\u003e str: if len(s) \u003c= 1: return s maxs = -float(\"inf\") res = collections.defaultdict(list) left,right = 0,len(s)-1 while left \u003c right: for i in range(left,right+2): if s[left:i] == s[left:i][::-1]: maxs = max(maxs,len(s[left:i])) res[maxs].append(s[left:i]) left += 1 return max(res[max(res.keys())],key=len) 也用到了双指针，超时在情理之中。 后来用到了动态规划 class Solution: def longestPalindrome(self, s: str) -\u003e str: if len(s) \u003c= 1: return s length = len(s) dp = [[False for _ in range(length)] for _ in range(length)] for i in range(length): dp[i][i] = True start = 0 max_len = 1 for j in range(1, length): for i in range(0, j): if s[i] == s[j]: if j - i \u003c 3: dp[i][j] = True else: dp[i][j] = dp[i + 1][j - 1] else: dp[i][j] = False if dp[i][j]: cur_len = j - i + 1 if cur_len \u003e max_len: max_len = cur_len start = i return s[start:start + max_len] ","date":"2022-07-21","objectID":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/:2:0","tags":["算法题","最长回文子串"],"title":"最长回文子串","uri":"/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/"},{"categories":["算法题"],"content":"有效的数独 https://leetcode-cn.com/problems/valid-sudoku/ #有效的数独 难点在将3*3里的数取出来 class Solution: def isValidSudoku(board) -\u003e bool: for line1,line2 in zip(board,zip(*board)): #行列 for n1,n2 in zip(line1,line2): if (n1 != '.' and line1.count(n1) \u003e 1) or (n2!='.' and line2.count(n2) \u003e1): return False pal = [[board[i+m][j+n] for m in range(3) for n in range(3) if board[i+m][j+n] != '.'] for i in (0, 3, 6) for j in (0, 3, 6)] for line in pal: if len(set(line)) != len(line): return False return True ","date":"2022-07-20","objectID":"/%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E7%8B%AC/:0:0","tags":["算法题","有效的数独"],"title":"有效的数独","uri":"/%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E7%8B%AC/"},{"categories":["算法题"],"content":"使括号有效的最少添加 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:0:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-add-to-make-parentheses-valid/ ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:1:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"思路： 通过一个值来判断是否匹配 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:2:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["算法题"],"content":"代码： class Solution: def minAddToMakeValid(self, S: str) -\u003e int: res,temp = 0,0 for i in S: if i == '(': temp += 1 if i == ')': temp -= 1 if temp == -1: temp = 0 res += 1 return res + temp 如果右括号过多的话，就在左边补一个左括号。这时结果+1 如果一直是左括号的话，res 为0 temp就是应该补的个数 如果都相匹配的话，temp = 0 相应 res也为0 ","date":"2022-07-17","objectID":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/:3:0","tags":["算法题","使括号有效的最少添加"],"title":"使括号有效的最少添加","uri":"/%E4%BD%BF%E6%8B%AC%E5%8F%B7%E6%9C%89%E6%95%88%E7%9A%84%E6%9C%80%E5%B0%91%E6%B7%BB%E5%8A%A0/"},{"categories":["Machine Learning","集成学习"],"content":"集成学习 在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。 集成学习在各个规模的数据集上都有很好的策略。 数据集大：划分成多个小数据集，学习多个模型进行组合 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合 集成学习主要有两类： 1. Bagging Boosting Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。 Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。 ","date":"2022-07-09","objectID":"/ensemble-learning/:0:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"Bagging bagging的名称来源于 （ Bootstrap Aggregating ），意思是自助抽样集成，这种方法将训练集分成m个新的训练集，然后在每个新训练集上构建一个模型，各自不相干，最后预测时我们将这个m个模型的结果进行整合，得到最终结果。整合方式就是：分类问题用majority voting，回归用均值。 因此Bagging使用的抽样方法是Bootstrap方法，即自助法，本质上就是一个有放回的随机抽样问题。 每一个样本在每一次抽的时候有同样的概率\\(\\frac{1}{N}\\)被抽中。没被抽中的概率为\\(1-\\frac{1}{N}\\)，一共抽了N次，即\\(1-(\\frac{1}{N})^N\\)当N趋于无穷时，由高等数学学的极限的求解可以算出来是\\(\\frac{1}{e}\\)，大概为36.8%，这些留下来的1/3的样本可以作为验证集，这样的方式叫做包外估计(out of bag estimate) ","date":"2022-07-09","objectID":"/ensemble-learning/:1:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"Boosting Boosting与Bagging的区别就是取样方式不同，Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。很好理解吧。 ","date":"2022-07-09","objectID":"/ensemble-learning/:2:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["Machine Learning","集成学习"],"content":"一个故事 一个故事用于理解，来源：https://www.joinquant.com/view/community/detail/adfb5ce37f0b39e348aae32e8412c68c 有一个医学专（砖）家，他看过很多很多病人，还记了小本本来归类这些病人的特征和病情来方便以后诊断。有一天来了个病人，这个专家就问病人了，“大爷您贵姓？多少岁，哪不舒服，病情怎么样？”大爷说 “我姓李，48，最近老吐痰，老咳嗽，还发烧…”。这个专家拿出他的小本本一查“”属性：“姓李”，“年龄48”，“吐痰”，“咳嗽”，“发烧”…,根据我的小本本以前这样的病例有80个，有76个是感冒，成，就诊断他是感冒了！”这个专家就是棵决策树 镜头一转，来到医院A，同样的病人来医院A治病。医院有很大的病例数据库，有100个医学专家通过学习数据库的一部分知识形成了自己的诊断方案。医院想：“我财大气粗，为了提供更好的医疗服务，我让100个专家做诊断，然后他们投票决出最后的判断”。 随机森林 完成 医院B就不爽了，你这治疗方案太受欢迎，把客户都抢走了，我要用科学的治疗方案来击败你。医院B想了想，大家投票不一定针对到用户情况，我先从我的100个专家里找一个最好的先给病人做一个诊疗方案，再根据第一个专家的不足找第二个，根据第二个再找第三个…… 最后不同专家再根据他们诊断的表现以不同权重投票。这样岂不是更针对病人痛点？ Boosting 方法就被这群人建立起来了 到了医院C想搞差异化，你医院B根据上一个专家的全部不足找新专家，那我就根据上一个专家判断最偏颇的方向找专家，虽然听起来差不多，但我的差异化说不定就能更好。 Gradient Boosting 产生了 更加财大气粗的医院D来了，他觉得虽然整个市场的大格调基本确定，但我可以通过提升整个诊疗的流程大大小小的细节来取胜啊！于是医院D在C的基础上改进了很多，于是找偏颇的方向更快更准，纠结专家，诊疗的速度也大大加快，整个医院的硬件设施也前所未有的提高。这差不多就是 XGBoost 了 突然有股叫大数据的潮流吹来，本来医院D已经在医院C一分钟治疗10000人的基础上提升了10多倍速度，但新的要求是：一分钟不行，最多给你3秒，10万人也不行，我现在有全世界的数据，你得分秒内召集几百几千个专家，这些专家每一个的知识得相当于以前一个医院那没多，还得分秒内服务数10倍的病人，最后治疗的精度不能下降。 LightGBM 出场了，虽然精度提高不多，但速度大大加快了。 ","date":"2022-07-09","objectID":"/ensemble-learning/:3:0","tags":["Machine Learning","集成学习","Ensemble Learning"],"title":"Ensemble Learning","uri":"/ensemble-learning/"},{"categories":["算法题"],"content":"旋转图像 https://leetcode-cn.com/problems/rotate-image/ 没难度的中等题，这方法很python class Solution: def rotate(self, matrix: List[List[int]]) -\u003e None: \"\"\" Do not return anything, modify matrix in-place instead. \"\"\" n = len(matrix) for i in list(map(list,map(reversed,zip(*matrix)))): matrix.append(i) del matrix[:n] ","date":"2022-07-07","objectID":"/%E6%97%8B%E8%BD%AC%E5%9B%BE%E5%83%8F/:0:0","tags":["算法题","旋转图像"],"title":"旋转图像","uri":"/%E6%97%8B%E8%BD%AC%E5%9B%BE%E5%83%8F/"},{"categories":["Machine Learning","分类算法"],"content":"KNN 参考：https://cuijiahua.com/blog/2017/11/ml_1_knn.html 《统计学习方法》李航（kd树） ","date":"2022-06-25","objectID":"/knn/:0:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"简介 k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 ","date":"2022-06-25","objectID":"/knn/:1:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"步骤 k-近邻算法步骤如下： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。 ","date":"2022-06-25","objectID":"/knn/:2:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["Machine Learning","分类算法"],"content":"总结 优点 简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归； 可用于数值型数据和离散型数据； 训练时间复杂度为O(n)；无数据输入假定； 对异常值不敏感 缺点 计算复杂性高；空间复杂性高； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）； 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。 最大的缺点是无法给出数据的内在含义。 关于algorithm参数kd_tree的原理，可以查看《统计学方法 李航》书中的讲解； 关于距离度量的方法还有切比雪夫距离、马氏距离、巴氏距离等； ","date":"2022-06-25","objectID":"/knn/:3:0","tags":["Machine Learning","分类算法","KNN"],"title":"KNN","uri":"/knn/"},{"categories":["NLP"],"content":"主题模型 主题模型也可以看成一种词向量表达，主要有LSA、PLSA、LDA。按照这个顺序来逐渐发展的 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"词袋模型 将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的 例子： 句子1：我 爱 北 京 天 安 门 转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0] 句子2：我 喜 欢 上 海 转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] from sklearn.feature_extraction.text import CountVectorizer corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ] vectorizer = CountVectorizer() vectorizer.fit_transform(corpus).toarray() 结果： [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LSA LSA就是潜在语义分析。特点是通过矩阵分解发现文本与单词之间基于主题（话题）的语义关系。 首先要清楚几个概念： ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"单词-文本矩阵 \\[ X=\\left[\\begin{array}{cccc} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1 n} \\\\\\\\ x_{21} \u0026 x_{22} \u0026 \\cdots \u0026 x_{2 n} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ x_{m 1} \u0026 x_{m 2} \u0026 \\cdots \u0026 x_{m n} \\end{array}\\right] \\] 这是一个 \\(m \\times n\\) 矩阵, 元素 \\(x_{i j}\\) 表示单词 \\(w_i\\) 在文本 \\(d_j\\) 中出现的频数或权值。由于单 词的种类很多, 而每个文本中出现单词的种类通常较少, 所以单词-文本矩阵是一个稀 疏矩阵。 权值通常用单词频率-逆文本频率 (term frequency-inverse document frequency, TF-IDF）表示，其定义是 \\[ \\operatorname{TFIDF}_{i j}=\\frac{\\mathrm{tf}_{i j}}{\\mathrm{tf}_{\\bullet j}} \\log \\frac{\\mathrm{df}}{\\mathrm{df}_i}, \\quad i=1,2, \\cdots, m ; \\quad j=1,2, \\cdots, n \\] 直观上讲，可以直接用每一列作为文本语义表达， 因此可以通过余弦相似度等计算文本之间的相似性，并且矩阵稀疏，计算量较少。但其并不关心文本中词语出现的顺序等信息，因此需要改进。 ### 单词-主题矩阵 假设所有文本共含有 \\(k\\) 个话题。假设每个话题由一个定义在单词集合 \\(W\\) 上的 \\(m\\) 维向量表示, 称为话题向量, 即 \\[ t_l=\\left[\\begin{array}{c} t_{1 l} \\\\\\\\ t_{2 l} \\\\\\\\ \\vdots \\\\\\\\ t_{m l} \\end{array}\\right], \\quad l=1,2, \\cdots, k \\] 其中 \\(t_{i l}\\) 是单词 \\(w_i\\) 在话题 \\(t_l\\) 的权值, \\(i=1,2, \\cdots, m\\), 权值越大, 该单词在该话题中 的重要度就越高。这 \\(k\\) 个话题向量 \\(t_1, t_2, \\cdots, t_k\\) 张成一个话题向量空间 (topic vector 话题向量空间 \\(T\\) 也可以表示为一个矩阵, 称为单词-主题矩阵 (word-topic matrix）, 记作 \\[ T=\\left[\\begin{array}{cccc} t_{11} \u0026 t_{12} \u0026 \\cdots \u0026 t_{1 k} \\\\\\\\ t_{21} \u0026 t_{22} \u0026 \\cdots \u0026 t_{2 k} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ t_{m 1} \u0026 t_{m 2} \u0026 \\cdots \u0026 t_{m k} \\end{array}\\right] \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"主题-文本矩阵 将单词-文本矩阵中的文本\\(x_j\\)投影到主题向量空间\\(J\\)中，得到在主题空间中的一个向量\\(y_j\\)。 \\[ Y=\\left[\\begin{array}{cccc} y_{11} \u0026 y_{12} \u0026 \\cdots \u0026 y_{1 n} \\\\\\\\ y_{21} \u0026 y_{22} \u0026 \\cdots \u0026 y_{2 n} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ y_{k 1} \u0026 y_{k 2} \u0026 \\cdots \u0026 y_{k n} \\end{array}\\right] \\] 从单词向量空间到主题向量空间的线性变换 单词-文本矩阵\\(X\\)可以近似表示为单词-主题矩阵\\(T\\)与主题-文本矩阵\\(Y\\)的乘积，这就是潜在语义分析： \\[ X\\approx TY \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"潜在语义分析 给定单词-文本矩阵\\(X\\)，每一行代表一个单词，每一列代表一个文本。其中的元素代表单词在文本中的权重或者频数（词袋模型）。 截断奇异值分析 \\[ X \\approx U_k \\Sigma_k V_k^{\\mathrm{T}}=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 \u0026 0 \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 \\sigma_2 \u0026 0 \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 \\ddots \u0026 0 \\\\\\\\ 0 \u0026 0 \u0026 0 \u0026 \\sigma_k \\end{array}\\right]\\left[\\begin{array}{c} v_1^{\\mathrm{T}} \\\\\\\\ v_2^{\\mathrm{T}} \\\\\\\\ \\vdots \\\\\\\\ v_k^{\\mathrm{T}} \\end{array}\\right] \\] 接下来考虑文本在主题空间中的表示。 \\[ \\begin{aligned} X \u0026=\\left[\\begin{array}{llll} x_1 \u0026 x_2 \u0026 \\cdots \u0026 x_n \\end{array}\\right] \\approx U_k \\Sigma_k V_k^{\\mathrm{T}} \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 \u0026 \u0026 \u0026 \\\\\\\\ \u0026 \\sigma_2 \u0026 0 \u0026 \\\\\\\\ 0 \u0026 \\ddots \u0026 \\\\\\\\ \u0026 \u0026 \\sigma_k \\end{array}\\right]\\left[\\begin{array}{cccc} v_{11} \u0026 v_{21} \u0026 \\cdots \u0026 v_{n 1} \\\\\\\\ v_{12} \u0026 v_{22} \u0026 \\cdots \u0026 v_{n 2} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ v_{1 k} \u0026 v_{2 k} \u0026 \\cdots \u0026 v_{n k} \\end{array}\\right] \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{cccc} \\sigma_1 v_{11} \u0026 \\sigma_1 v_{21} \u0026 \\cdots \u0026 \\sigma_1 v_{n 1} \\\\\\\\ \\sigma_2 v_{12} \u0026 \\sigma_2 v_{22} \u0026 \\cdots \u0026 \\sigma_2 v_{n 2} \\\\\\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\\\\\ \\sigma_k v_{1 k} \u0026 \\sigma_k v_{2 k} \u0026 \\cdots \u0026 \\sigma_k v_{n k} \\end{array}\\right] \\end{aligned} \\] 其中: \\[ u_l = \\begin{bmatrix}u_{1l} \\\\\\\\u_{2l} \\\\\\\\ \\vdots \\\\\\\\u_{ml} \\end{bmatrix}, \\quad l= 1, 2, \\dots, k \\] 代表单词对主题的权重。 由式知, 矩阵 \\(X\\) 的第 \\(j\\) 列向量 \\(x_j\\) 满足 \\[ \\begin{aligned} x_j \u0026 \\approx U_k\\left(\\Sigma_k V_k^{\\mathrm{T}}\\right)\\_j \\\\\\\\ \u0026=\\left[\\begin{array}{llll} u_1 \u0026 u_2 \u0026 \\cdots \u0026 u_k \\end{array}\\right]\\left[\\begin{array}{c} \\sigma_1 v_{j 1} \\\\\\\\ \\sigma_2 v_{j 2} \\\\\\\\ \\vdots \\\\\\\\ \\sigma_k v_{j k} \\end{array}\\right] \\\\\\\\ \u0026=\\sum_{l=1}^k \\sigma_l v_{j l} u_l, \\quad j=1,2, \\cdots, n \\end{aligned} \\] 则\\(\\Sigma_kV_k^T\\)每一个列向量是一个文本在主题向量空间中的表示。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:2:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"PLSA ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"生成模型 假设有单词集合 \\(W={w_1, w_2, \\cdots, w_M}\\), 其中 \\(M\\) 是单词个数; 文本 (指标) 集 合 \\(D={d_1, d_2, \\cdots, d_N}\\), 其中 \\(N\\) 是文本个数; 话题集合 \\(Z={z_1, z_2, \\cdots, z_K}\\), 其中 \\(K\\) 是预先设定的话题个数。随机变量 \\(w\\) 取值于单词集合; 随机变量 \\(d\\) 取值于文本集 合, 随机变量 \\(z\\) 取值于话题集合。概率分布 \\(P(d)\\) 、条件概率分布 \\(P(z \\mid d)\\) 、条件概率分 布 \\(P(w \\mid z)\\) 皆属于多项分布, 其中 \\(P(d)\\) 表示生成文本 \\(d\\) 的概率, \\(P(z \\mid d)\\) 表示文本 \\(d\\) 生 成话题 \\(z\\) 的概率, \\(P(w \\mid z)\\) 表示话题 \\(z\\) 生成单词 \\(w\\) 的概率。 每个文本 \\(d\\) 拥有自己的话题概率分布 \\(P(z \\mid d)\\), 每个话题 \\(z\\) 拥有自己的单词概率分 布 \\(P(w \\mid z)\\); 也就是说一个文本的内容由其相关话题决定, 一个话题的内容由其相关单词决定。 生成模型通过以下步骤生成文本-单词共现数据: (1) 依据概率分布 \\(P(d)\\), 从文本 (指标) 集合中随机选取一个文本 \\(d\\), 共生成 \\(N\\) 个文本; 针对每个文本, 执行以下操作; (2) 在文本 \\(d\\) 给定条件下, 依据条件概率分布 \\(P(z \\mid d)\\), 从话题集合随机选取一个 话题 \\(z\\), 共生成 \\(L\\) 个话题, 这里 \\(L\\) 是文本长度; (3) 在话题 \\(z\\) 给定条件下, 依据条件概率分布 \\(P(w \\mid z)\\), 从单词集合中随机选取一 个单词 \\(w\\) 。 生成模型中, 单词变量 \\(w\\) 与文本变量 \\(d\\) 是观测变量, 话题变量 \\(z\\) 是隐变量。也就 是说模型生成的是单词-话题-文本三元组 \\((w, z, d)\\) 的集合, 但观测到的是单词-文本二 元组 \\((w, d)\\) 的集合, 观测数据表示为单词-文本矩阵 \\(T\\) 的形式, 矩阵 \\(T\\) 的行表示单词, 列表示文本, 元素表示单词-文本对 \\((w, d)\\) 的出现次数。 从数据的生成过程可以推出, 文本-单词共现数据 \\(T\\) 的生成概率为所有单词-文本 对 \\((w, d)\\) 的生成概率的乘积, \\[ P(T)=\\prod_{(w, d)} P(w, d)^{n(w, d)} \\] 这里 \\(n(w, d)\\) 表示 \\((w, d)\\) 的出现次数, 单词-文本对出现的总次数是 \\(N \\times L\\) 。每个单 词-文本对 \\((w, d)\\) 的生成概率由以下公式决定: \\[ \\begin{aligned} P(w, d) \u0026=P(d) P(w \\mid d) \\\\\\\\ \u0026=P(d) \\sum_z P(w, z \\mid d) \\\\\\\\ \u0026=P(d) \\sum_z P(z \\mid d) P(w \\mid z) \\end{aligned} \\] 即生成模型的定义。 生成模型假设在话题 \\(z\\) 给定条件下, 单词 \\(w\\) 与文本 \\(d\\) 条件独立, 即 \\[ P(w, z \\mid d)=P(z \\mid d) P(w \\mid z) \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"共现模型 \\[ P(T)=\\prod_{(w, d)} P(w, d)^{n(w, d)} \\] 每个单词-文本对 \\((w, d)\\) 的概率由以下公式决定: \\[ P(w, d)=\\sum_{z \\in Z} P(z) P(w \\mid z) P(d \\mid z) \\] 式 (18.5) 即共现模型的定义。容易验证, 生成模型 (18.2) 和共现模型 (18.5) 是等价的。 共现模型假设在话题 \\(z\\) 给定条件下, 单词 \\(w\\) 与文本 \\(d\\) 是条件独立的, 即 \\[ P(w, d \\mid z)=P(w \\mid z) P(d \\mid z) \\] 直观解释： ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"与潜在语义分析的关系 共现模型也可以表示为三个矩阵乘积的形式。这样, 概率潜在语义分析与 潜在语义分析的对应关系可以从中看得很清楚。下面是共现模型的矩阵乘积形式: \\[ \\begin{aligned} X^{\\prime} \u0026=U^{\\prime} \\Sigma^{\\prime} V^{\\prime \\mathrm{T}} \\\\\\\\ X^{\\prime} \u0026=[P(w, d)]\\_{M \\times N} \\\\\\\\ U^{\\prime} \u0026=[P(w \\mid z)]\\_{M \\times K} \\\\\\\\ \\Sigma^{\\prime} \u0026=[P(z)]\\_{K \\times K} \\\\\\\\ V^{\\prime} \u0026=[P(d \\mid z)]\\_{N \\times K} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概率潜在语义分析的算法 Plsa是含有隐变量的模型，其学习通常使用EM算法。 E步是计算Q函数，M步是极大化Q函数。 设单词集合为 \\(W={w_1, w_2, \\cdots, w_M}\\), 文本集合为 \\(D={d_1, d_2, \\cdots, d_N}\\), 话 题集合为 \\(Z={z_1, z_2, \\cdots, z_K}\\) 。给定单词-文本共现数据 \\(T={n\\left(w_i, d_j\\right)}, i=\\) \\(1,2, \\cdots, M, j=1,2, \\cdots, N\\), 目标是估计概率潜在语义分析模型（生成模型）的 参数。如果使用极大似然估计, 对数似然函数是 \\[ \\begin{aligned} L \u0026=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\log P\\left(w_i, d_j\\right) \\\\\\\\ \u0026=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\log \\left[\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right] \\end{aligned} \\] 但是模型含有隐变量, 对数似然函数的优化无法用解析方法求解, 这时使用 EM算法。 应用 EM算法的核心是定义 \\(Q\\) 函数。 \\(\\mathrm{E}\\) 步：计算 \\(Q\\) 函数 \\(Q\\) 函数为完全数据的对数似然函数对不完全数据的条件分布的期望。针对概率潜 在语义分析的生成模型, \\(Q\\) 函数是 \\[ Q=\\sum_{k=1}^K{\\sum_{j=1}^N n\\left(d_j\\right)\\left[\\log P\\left(d_j\\right)+\\sum_{i=1}^M \\frac{n\\left(w_i, d_j\\right)}{n\\left(d_j\\right)} \\log P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right]} P\\left(z_k \\mid w_i, d_j\\right) \\] 式中 \\(n\\left(d_j\\right)=\\sum_{i=1}^M n\\left(w_i, d_j\\right)\\) 表示文本 \\(d_j\\) 中的单词个数, \\(n\\left(w_i, d_j\\right)\\) 表示单词 \\(w_i\\) 在文本 \\(d_j\\) 中出现的次数。条件概率分布 \\(P\\left(z_k \\mid w_i, d_j\\right)\\) 代表不完全数据, 是已知变量。条件概 率分布 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 的乘积代表完全数据, 是末知变量。 由于可以从数据中直接统计得出 \\(P\\left(d_j\\right)\\) 的估计, 这里只考虑 \\(P\\left(w_i \\mid z_k\\right), P\\left(z_k \\mid d_j\\right)\\) 的估计, 可将 \\(Q\\) 函数简化为函数 \\(Q^{\\prime}\\) \\[ Q^{\\prime}=\\sum_{i=1}^M \\sum_{j=1}^N n\\left(w_i, d_j\\right) \\sum_{k=1}^K P\\left(z_k \\mid w_i, d_j\\right) \\log \\left[P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)\\right] \\] \\(Q^{\\prime}\\) 函数中的 \\(P\\left(z_k \\mid w_i, d_j\\right)\\) 可以根据贝叶斯公式计算 \\[ P\\left(z_k \\mid w_i, d_j\\right)=\\frac{P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)}{\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)} \\] 其中 \\(P\\left(z_k \\mid d_j\\right)\\) 和 \\(P\\left(w_i \\mid z_k\\right)\\) 由上一步迭代得到。 \\(\\mathrm{M}\\) 步: 极大化 \\(Q\\) 函数。 通过约束最优化求解 \\(Q\\) 函数的极大值, 这时 \\(P\\left(z_k \\mid d_j\\right)\\) 和 \\(P\\left(w_i \\mid z_k\\right)\\) 是变量。因为 变量 \\(P\\left(w_i \\mid z_k\\right), P\\left(z_k \\mid d_j\\right)\\) 形成概率分布, 满足约束条件 \\[ \\begin{aligned} \u0026\\sum_{i=1}^M P\\left(w_i \\mid z_k\\right)=1, \\quad k=1,2, \\cdots, K \\\\\\\\ \u0026\\sum_{k=1}^K P\\left(z_k \\mid d_j\\right)=1, \\quad j=1,2, \\cdots, N \\end{aligned} \\] 应用拉格朗日法, 引入拉格朗日乘子 \\(\\tau_k\\) 和 \\(\\rho_j\\), 定义拉格朗日函数 \\(A\\) \\[ \\Lambda=Q^{\\prime}+\\sum_{k=1}^K \\tau_k\\left(1-\\sum_{i=1}^M P\\left(w_i \\mid z_k\\right)\\right)+\\sum_{j=1}^N \\rho_j\\left(1-\\sum_{k=1}^K P\\left(z_k \\mid d_j\\right)\\right) \\] 将拉格朗日函数 \\(\\Lambda\\) 分别对 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 求偏导数, 并令其等于 0 , 得到下面 的方程组 \\[ \\begin{aligned} \u0026\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)-\\tau_k P\\left(w_i \\mid z_k\\right)=0, \\quad i=1,2, \\cdots, M ; \\quad k=1,2, \\cdots, K \\\\\\\\ \u0026\\sum_{i=1}^M n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)-\\rho_j P\\left(z_k \\mid d_j\\right)=0, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 解方程组得到 \\(M\\) 步的参数估计公式: \\[ P\\left(w_i \\mid z_k\\right)=\\frac{\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{\\sum_{m=1}^M \\sum_{j=1}^N n\\left(w_m, d_j\\right) P\\left(z_k \\mid w_m, d_j\\right)} \\] \\[ P(z_k\\mid d_j) = \\frac{\\sum_{i=1}^Mn(w_i, d_j)P(z_k\\mid w_i,d_j)}{n(d_j)} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:4","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"总结算法 输入: 设单词集合为 \\(W={w_1, w_2, \\cdots, w_M}\\), 文本集合为 \\(D={d_1, d_2, \\cdots, d_N}\\), 话题集合为 \\(Z={z_1, z_2, \\cdots, z_K}\\), 共现数据 \\({n\\left(w_i, d_j\\right)}, i=1,2, \\cdots, M, j=1\\), \\(2, \\cdots, N\\); 输出: \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 。 (1) 设置参数 \\(P\\left(w_i \\mid z_k\\right)\\) 和 \\(P\\left(z_k \\mid d_j\\right)\\) 的初始值。 (2) 迭代执行以下 \\(\\mathrm{E}\\) 步, \\(\\mathrm{M}\\) 步, 直到收敛为止。 \\(\\mathrm{E}\\) 步: \\[ P\\left(z_k \\mid w_i, d_j\\right)=\\frac{P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)}{\\sum_{k=1}^K P\\left(w_i \\mid z_k\\right) P\\left(z_k \\mid d_j\\right)} \\] M 步: \\[ \\begin{aligned} P\\left(w_i \\mid z_k\\right) \u0026=\\frac{\\sum_{j=1}^N n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{\\sum_{m=1}^M \\sum_{j=1}^N n\\left(w_m, d_j\\right) P\\left(z_k \\mid w_m, d_j\\right)} \\\\\\\\ P\\left(z_k \\mid d_j\\right) \u0026=\\frac{\\sum_{i=1}^M n\\left(w_i, d_j\\right) P\\left(z_k \\mid w_i, d_j\\right)}{n\\left(d_j\\right)} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:5","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"用法 与LSA类似，可以把文档对各个主题的概率看作是文档的表示，最后用到的就是\\(P(z_k\\mid d_j)\\)。 k就是我们自己设定的主题数，一般来说K远远小于文档个数和词汇表大小，这样也达到了降维的目的。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:6","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"优点与不足 优点 pLSA是在一套比较完整的思想的基础上提出来的，模型中各项参数有明确的物理含义，可解释性比较强。相比LSA，pLSA对人类生成文本机制的刻画更加细致、更加符合我们的常识，比如，pLSA基于条件概率，引入了一个“隐含变量”（相对于可以看到的文档和词语，是不可观测变的），即主题，来描述文本生成的过程。 #### 不足 pLSA的理论与我们的实践不是那么的统一: (1) 我们说话的时候，根本不会考虑” 我说这段话的概率大小”，即 \\(p\\left(d_t\\right)\\) (2) pLSA认为，我们说话时面向的主题分布，取决于 “文档” （实际上是文档ID)。这个假设显然是不合理的，小说家不会因为自己写到第666回而调整 主题。 (3) 类似 (2)，随着上下文的变化，我们围绕一个主题说话的内容和方式也 会发生改变。在主题模型中，这种改变的体现，就是一个主题下的词语概率分 布会发生改变。而pLSA忽略了这样的事实。 从计算复杂度的角度看pLSA有两个比较大的缺陷: (1) pLSA中，对文档 出现的概率估计，来自对训练语料的学习。而对于一个 末知文档，我们是无法估计它出现的概率的一一因此pLSA无法对训练语料之 外的文档进行处理。pLSA的这个特点决定了，在在线(online) 场景中(数据是 持续增加的)，那么文档处理系统就需要定时使用pLSA对整个语料库进行计 算。因此，pLSA比较适合允许一定时滞的离线计算。 (2) pLSA认为一个文档对各个主题的隶属度是一定的——而一个主题对各个词语的隶属度也是一定的，因此pLSA在生成一个文档的各个词语时、使用了相同的词语概率分布。这样，pLSA需要为每一个文档记录一个专门的随着语料数据集规模的增加，pLSA的参数规模也会增加，导致模型训练越来越困难。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:3:7","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LDA LDA模型是文本集合的生成概率模型。 LDA 的文本集合的生成过程如下: 首先随机生成一个文本的话题分布, 之后在该 文本的每个位置, 依据该文本的话题分布随机生成一个话题, 然后在该位置依据该话 题的单词分布随机生成一个单词, 直至文本的最后一个位置, 生成整个文本。重复以 上过程生成所有文本。 LDA 模型是含有隐变量的概率图模型。模型中, 每个话题的单词分布, 每个文 本的话题分布, 文本的每个位置的话题是隐变量; 文本的每个位置的单词是观测变 量。LDA 模型的学习与推理无法直接求解, 通常使用吉布斯抽样 (Gibbs sampling) 和 变分 EM算法 (variational EM algorithm), 前者是蒙特卡罗法, 而后者是近似算法。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"多项分布 (多项分布) 若多元离散随机变量 \\(X=\\left(X_1, X_2, \\cdots, X_k\\right)\\) 的概率质 量函数为 \\[ \\begin{aligned} P\\left(X_1=n_1, X_2=n_2, \\cdots, X_k=n_k\\right) \u0026=\\frac{n !}{n_{1} ! n_{2} ! \\cdots n_{k} !} p_1^{n_1} p_2^{n_2} \\cdots p_k^{n_k} \\\\\\\\ \u0026=\\frac{n !}{\\prod_{i=1}^k n_{i} !} \\prod_{i=1}^k p_i^{n_i} \\end{aligned} \\] 其中 \\(p=\\left(p_1, p_2, \\cdots, p_k\\right), p_i \\geqslant 0, i=1,2, \\cdots, k, \\sum_{i=1}^k p_i=1, \\sum_{i=1}^k n_i=n\\), 则称随机变 量 \\(X\\) 服从参数为 \\((n, p)\\) 的多项分布, 记作 \\(X \\sim \\operatorname{Mult}(n, p)\\) 。 当试验的次数 \\(n\\) 为 1 时, 多项分布变成类别分布 (categorical distribution)。类 别分布表示试验可能出现的 \\(k\\) 种结果的概率。显然多项分布包含类别分布。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:1","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"狄利克雷分布 狄利克雷分布 (Dirichlet distribution) 是一种多元连续随机变量的概率分布, 是 贝塔分布 (beta distribution) 的扩展。在贝叶斯学习中, 狄利克雷分布常作为多项分 布的先验分布使用。 (狄利克雷分布) 若多元连续随机变量 \\(\\theta=\\left(\\theta_1, \\theta_2, \\cdots, \\theta_k\\right)\\) 的概率密 度函数为 \\[ p(\\theta \\mid \\alpha)=\\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\] 其中 \\(\\sum_{i=1}^k \\theta_i=1, \\theta_i \\geqslant 0, \\alpha=\\left(\\alpha_1, \\alpha_2, \\cdots, \\alpha_k\\right), \\alpha_i\u003e0, i=1,2, \\cdots, k\\), 则称随机变量 \\(\\theta\\) 服从参数为 \\(\\alpha\\) 的狄利克雷分布, 记作 \\(\\theta \\sim \\operatorname{Dir}(\\alpha)\\) 。 式中 \\(\\Gamma(s)\\) 是伽马函数, 定义为 \\[ \\Gamma(s)=\\int_0^{\\infty} x^{s-1} \\mathrm{e}^{-x} \\mathrm{~d} x, \\quad s\u003e0 \\] 具有性质： \\[ \\Gamma(s+1) = s\\Gamma(s) \\] 当s为自然数时，有： \\[ \\Gamma(s+1) = s! \\] 令 \\[ \\mathrm{B}(\\alpha)=\\frac{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)}{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)} \\] 则狄利克雷分布的密度函数可以写成 \\[ p(\\theta \\mid \\alpha)=\\frac{1}{\\mathrm{~B}(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\] \\(\\mathrm{B}(\\alpha)\\) 是规范化因子, 称为多元贝塔函数 (或扩展的贝塔函数)。由密度函数的性质 \\[ \\int \\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\prod_{i=1}^{\\alpha_i-1} \\mathrm{~d} \\theta=\\frac{\\Gamma\\left(\\sum_{i=1}^k \\alpha_i\\right)}{\\prod_{i=1}^k \\Gamma\\left(\\alpha_i\\right)} \\int \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\mathrm{~d} \\theta=1 \\] 得 \\[ \\mathrm{B}(\\alpha)=\\int \\prod_{i=1}^k \\theta_i^{\\alpha_i-1} \\mathrm{~d} \\theta \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:2","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"二项分布与贝塔分布 二项分布是多项分布的特殊情况, 贝塔分布是狄利克雷分布的特殊情况。 二项分布是指如下概率分布。 \\(X\\) 为离散随机变量, 取值为 \\(m\\), 其概率质量函数为 \\[ P(X=m)=\\left(\\begin{array}{c} n \\\\\\\\ m \\end{array}\\right) p^m(1-p)^{n-m}, \\quad m=0,1,2, \\cdots, n \\] 其中 \\(n\\) 和 \\(p(0 \\leqslant p \\leqslant 1)\\) 是参数。 贝塔分布是指如下概率分布, \\(X\\) 为连续随机变量, 取值范围为 \\([0,1]\\), 其概率密度 函数为 \\[ p(x)= \\begin{cases}\\frac{1}{\\mathrm{~B}(s, t)} x^{s-1}(1-x)^{t-1}, \u0026 0 \\leqslant x \\leqslant 1 \\\\\\\\ 0, \u0026 \\text { 其他 }\\end{cases} \\] 其中 \\(s\u003e0\\) 和 \\(t\u003e0\\) 是参数, \\(\\mathrm{B}(s, t)=\\frac{\\Gamma(s) \\Gamma(t)}{\\Gamma(s+t)}\\) 是贝塔函数, 定义为 \\[ \\mathrm{B}(s, t)=\\int_0^1 x^{s-1}(1-x)^{t-1} \\mathrm{~d} x = \\frac{\\Gamma(s)\\Gamma(t)}{\\Gamma(s+t)} \\] 当 \\(s, t\\) 是自然数时(\\(\\Gamma(s+1) = s!\\)), \\[ \\mathrm{B}(s, t)=\\frac{(s-1) !(t-1) !}{(s+t-1) !} \\] 当 \\(n\\) 为 1 时, 二项分布变成伯努利分布（Bernoulli distribution）或 0-1 分布。 伯努利分布表示试验可能出现的 2 种结果的概率。显然二项分布包含伯努利分布。给出几种概率分布的关系。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:3","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"基本想法 在LDA主题模型下，一篇文章由词语的序列组成。首先以一定概率选择一个主题，其次以一定概率在这个主题中选择一个词。如果一篇文章由1000个词组成，那么就把上述方式重复1000遍，就能组成这篇文章。那么值得注意的是，以一定概率选择一个主题是服从多项式分布的，而多项式分布的参数是服从Dirichlet分布的。以一定概率在特定主题中选择一个词也是服从多项式分布的，多项式分布的参数是服从Dirichlet分布的。为什么呢？因为Dirichlet分布是多项式分布的共轭分布，也就是说由贝叶斯估计得到的后验分布仍然是Dirichlet分布。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:4","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"LDA与PLSA的关系 二者都是概率模型，都是利用概率生成模型对文本集合进行主题分析的无监督学习方法。 PLSA是用了频率派的方法，利用极大似然进行学习，而LDA使用了贝叶斯派的方法，进行贝叶斯推断。 二者都假设存在两个分布：话题是单词的多项分布，文本是话题的多项分布，不同的在于LDA认为多项分布的参数也服从一个分布，而不是固定不变的，使用狄利克雷分布作为多项分布的先验分布，也就是多项分布的参数服从狄利克雷分布。 引入先验概率的作用可以防止过拟合。为啥选择狄利克雷分布呢？因为它是多项分布的共轭先验分布，先验分布与后验分布形式相同，便于由先验分布得到后验分布。 LDA是在Plsa的基础上，为单词分布和主题分布增加了两个狄利克雷先验。 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:5","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"模型定义 模型要素 潜在狄利克雷分配 (LDA) 使用三个集合: 一是单词集合 \\(W={w_1, \\cdots, w_v, \\cdots}\\), , 其中 \\(w_v\\) 是第 \\(v\\) 个单词, \\(v=1,2, \\cdots, V, V\\) 是单词的个数。二是文本集合 \\(D={\\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\cdots, \\mathbf{w}_M}\\), 其中 \\(\\mathbf{w}_m\\) 是第 \\(m\\) 个文本, \\(m=1,2, \\cdots, M, M\\) 是文本 的个数。文本 \\(\\mathbf{w}_m\\) 是一个单词序列 \\(\\mathbf{w}_m=\\left(w_{m 1}, \\cdots, w_{m n}, \\cdots, w_{m N_m}\\right)\\), 其中 \\(w_{m n}\\) 是 文本 \\(\\mathbf{w}_m\\) 的第 \\(n\\) 个单词, \\(n=1,2, \\cdots, N_m, N_m\\) 是文本 \\(\\mathbf{w}_m\\) 中单词的个数。三是主题集合集合 \\(Z={z_1, \\cdots, z_k, \\cdots, z_K}\\), 其中 \\(z_k\\) 是第 \\(k\\) 个话题, \\(k=1,2, \\cdots, K, K\\) 是话题的个数。 每一个话题 \\(z_k\\) 由一个单词的条件概率分布 \\(p\\left(w \\mid z_k\\right)\\) 决定, \\(w \\in W\\) 。分布 \\(p\\left(w \\mid z_k\\right)\\) 服从多项分布 (严格意义上类别分布), 其参数为 \\(\\varphi_k\\) 。参数 \\(\\varphi_k\\) 服从狄利克雷分布 (先验分布), 其超参数为 \\(\\beta\\) 。参数 \\(\\varphi_k\\) 是一个 \\(V\\) 维向量 \\(\\varphi_k=\\left(\\varphi_{k 1}, \\varphi_{k 2}, \\cdots, \\varphi_{k V}\\right)\\), 其中 \\(\\varphi_{k v}\\) 表示话题 \\(z_k\\) 生成单词 \\(w_v\\) 的概率。所有话题的参数向量构成一个 \\(K \\times V\\) 矩阵 \\(\\varphi=\\{\\varphi_k\\}_{k=1}^K\\) 。超参数 \\(\\beta\\) 也是一个 \\(V\\) 维向量 \\(\\beta=\\left(\\beta_1, \\beta_2, \\cdots, \\beta_V\\right)\\_{\\text {。 }}\\)(对于话题\\(z_k\\)其生成单词\\(w_v\\)先验服从狄利克雷分布，因此是一个V维向量) 每一个文本 \\(\\mathbf{w}_m\\) 由一个话题的条件概率分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 决定, \\(z \\in Z_{\\text {。 }}\\) 分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 服从多项分布 (严格意义上类别分布), 其参数为 \\(\\theta_m\\) 。参数 \\(\\theta_m\\) 服从狄利克雷分布 (先验分布), 其超参数为 \\(\\alpha\\) , 参数 \\(\\theta_m\\) 是一个 \\(K\\) 维向量 \\(\\theta_m=\\left(\\theta_{m 1}, \\theta_{m 2}, \\cdots, \\theta_{m K}\\right)\\), 其中 \\(\\theta_{m k}\\) 表示文本 \\(\\mathrm{w}_m\\) 生成话题 \\(z_k\\) 的概率。所有文本的参数向量构成一个 \\(M \\times K\\) 矩阵 \\(\\theta=\\{\\theta_m\\}_{m=1}^M\\) 。超参数 \\(\\alpha\\) 也是一个 \\(K\\) 维向量 \\(\\alpha=\\left(\\alpha_1, \\alpha_2, \\cdots, \\alpha_K\\right)\\) 。 每一个文本 \\(\\mathbf{w}_m\\) 中的每一个单词 \\(w_{m n}\\) 由该文本的话题分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right)\\) 以及所有话 题的单词分布 \\(p\\left(w \\mid z_k\\right)\\) 决定。 生成过程 LDA 文本集合的生成过程如下: 给定单词集合 \\(W\\), 文本集合 \\(D\\), 话题集合 \\(Z\\), 狄利克雷分布的超参数 \\(\\alpha\\) 和 \\(\\beta\\) 。 1.生成单词分布 随机生成 \\(K\\) 个话题的单词分布。具体过程如下, 按照狄利克雷分布 \\(\\operatorname{Dir}(\\beta)\\) 随机 生成一个参数向量 \\(\\varphi_k, \\varphi_k \\sim \\operatorname{Dir}(\\beta)\\), 作为话题 \\(z_k\\) 的单词分布 \\(p\\left(w \\mid z_k\\right), w \\in W, k=\\) \\(1,2, \\cdots, K\\) 。 2.生成主题分布 随机生成 \\(M\\) 个文本的主题分布。具体过程如下: 按照狄利克雷分布 \\(\\operatorname{Dir}(\\alpha)\\) 随 机生成一个参数向量 \\(\\theta_m, \\theta_m \\sim \\operatorname{Dir}(\\alpha)\\), 作为文本 \\(\\mathbf{w}_m\\) 的主题分布 \\(p\\left(z \\mid \\mathbf{w}_m\\right), m=\\) \\(1,2, \\cdots, M_{}\\) 。 3.生成文本的单词序列 随机生成 \\(M\\) 个文本的 \\(N_m\\) 个单词。文本 \\(\\mathbf{w}_m(m=1,2, \\cdots, M)\\) 的单词 \\(w_{m n}(n=\\) \\(\\left.1,2, \\cdots, N_m\\right)\\) 的生成过程如下: 3.1 首先按照多项分布 \\(\\operatorname{Mult}\\left(\\theta_m\\right)\\) 随机生成一个话题 \\(z_{m n}, z_{m n} \\sim \\operatorname{Mult}\\left(\\theta_m\\right)\\) 3.2 然后按照多项分布 \\(\\operatorname{Mult}\\left(\\varphi_{z_{m n}}\\right)\\) 随机生成一个单词 \\(w_{m n}, w_{m n} \\sim \\operatorname{Mult}\\left(\\varphi_{z_{m n}}\\right)\\_{\\text {。 }}\\) 文本 \\(\\mathbf{w}_m\\) 本身是单词序列 \\(\\mathbf{w}_m=\\left(w_{m 1}, w_{m 2}, \\cdots, w_{m N_m}\\right)\\), 对应着隐式的话题序列 \\(\\mathbf{z}_m=\\left(z_{m 1}, z_{m 2}, \\cdots, z_{m N_m}\\right) 。\\) 引用一下LDA数学八卦的图： \\(\\vec{\\alpha} \\rightarrow \\vec{\\theta}_m \\rightarrow z_{m, n}\\), 这个过程表示在生成第 \\(m\\) 篇文档的时候，先从第一个坛子中抽了一个doc-topic 骰子 \\(\\vec{\\theta}_m\\),然后投这个骰子生成了文档\\(m\\)中第 \\(n\\) 个词的topic编号 \\(z_{m, n}\\) ； \\(\\vec{\\beta} \\rightarrow \\vec{\\varphi}_k \\rightarrow w_{m, n} \\mid k=z_{m, n}\\), 这个过程表示用如下动作生成语料中第 \\(m\\) 篇文档的第 \\(n\\) 个词: 在上帝手头的 \\(K\\) 个topic-word 骰子 \\(\\vec{\\varphi}_k\\) 中，挑选编号为 \\(k=z_{m, n}\\) 的那个骰子进行投掷，然后生成 word \\(w_{m, n}\\) ; 理解 LDA最重要的就是理解这两个物理过程。LDA 模型在基于 \\(K\\) 个 topic 生成语料中的 \\(M\\) 篇文档的过程中， 由于是 bag-of-words 模型，有一些物理过程是相互独立可交换的。由此，LDA生成模型中， \\(M\\) 篇文档会对应 于 \\(M\\) 个独立的 Dirichlet-Multinomial 共轭结构；K个 个 topic 会对应于 \\(K\\) 个独立的 Dirichlet-Multinomial 共轭结 构。所以理解 LDA 所需要的所有数学就是理解 Dirichlet-Multiomail 共轭，其它都就是理解物理过程。 总结 对于话题 \\(z_k(k=1,2, \\cdots, K)\\) : 生成多项分布参数 \\(\\varphi_k \\sim \\operatorname{Dir}(\\beta)\\), 作为话题的单词分布 \\(p\\left(w \\mid z_k\\right)\\); 对于文本 \\(\\mathbf{w}_m(m=1,2, \\cdots, M)\\); 生成多项分布参数 \\(\\theta_m \\sim \\operatorname{Dir}(\\alpha)\\), 作为文本的话题分布 \\(p\\left(z \\mid \\mathb","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:6","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概率计算 LDA 模型整体是由观测变量和隐变量组成的联合概率分布, 可以表为 \\[ p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)=\\prod_{k=1}^K p\\left(\\varphi_k \\mid \\beta\\right) \\prod_{m=1}^M p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m} p\\left(z_{m n} \\mid \\theta_m\\right) p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right) \\] (其中M为文本数，\\(N_m\\)为文档m的长度，K为主题数) 其中观测变量 \\(\\mathrm{w}\\) 表示所有文本中的单词序列, 隐变量 \\(\\mathrm{z}\\) 表示所有文本中的话题序列, 隐变量 \\(\\theta\\) 表示所有文本的话题分布的参数, 隐变量 \\(\\varphi\\) 表示所有话题的单词分布的参 数, \\(\\alpha\\) 和 \\(\\beta\\) 是超参数。 \\(p\\left(\\varphi_k \\mid \\beta\\right)\\) 表示超参数 \\(\\beta\\) 给定条件下第 \\(k\\) 个话题的单词分布的参数 \\(\\varphi_k\\) 的生成概率; \\(p\\left(\\theta_m \\mid \\alpha\\right)\\) 表示超参数 \\(\\alpha\\) 给定条件下第 \\(m\\) 个文本的话题分布的 参数 \\(\\theta_m\\) 的生成概率; \\(p\\left(z_{m n} \\mid \\theta_m\\right)\\) 表示第 \\(m\\) 个文本的话题分布 \\(\\theta_m\\) 给定条件下文本的 第 \\(n\\) 个位置的话题 \\(z_{m n}\\) 的生成概率; \\(p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right)\\) 表示在第 \\(m\\) 个文本的第 \\(n\\) 个位 置的话题 \\(z_{m n}\\) 及所有话题的单词分布的参数 \\(\\varphi\\) 给定条件下第 \\(m\\) 个文本的第 \\(n\\) 个位 置的单词 \\(w_{m n}\\) 的生成概率。 第 \\(m\\) 个文本的联合概率分布可以表为 \\[ p\\left(\\mathbf{w}_m, \\mathbf{z}_m, \\theta_m, \\varphi \\mid \\alpha, \\beta\\right)=\\prod_{k=1}^K p\\left(\\varphi_k \\mid \\beta\\right) p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m} p\\left(z_{m n} \\mid \\theta_m\\right) p\\left(w_{m n} \\mid z_{m n}, \\varphi\\right) \\] 其中 \\(\\mathbf{w}_m\\) 表示该文本中的单词序列, \\(\\mathbf{z}_m\\) 表示该文本的话题序列, \\(\\theta_m\\) 表示该文本的话 题分布参数。 LDA 模型的联合分布含有隐变量, 对隐变量进行积分得到边缘分布。 参数 \\(\\theta_m\\) 和 \\(\\varphi\\) 给定条件下第 \\(m\\) 个文本的生成概率是 \\[ p\\left(\\mathbf{w}_m \\mid \\theta_m, \\varphi\\right)=\\prod_{n=1}^{N_m}\\left[\\sum_{k=1}^K p\\left(z_{m n}=k \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_k\\right)\\right] \\] 超参数 \\(\\alpha\\) 和 \\(\\beta\\) 给定条件下第 \\(m\\) 个文本的生成概率是 \\[ p\\left(\\mathbf{w}_m \\mid \\alpha, \\beta\\right)=\\prod_{k=1}^K \\int p\\left(\\varphi_k \\mid \\beta\\right)\\left[\\int p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m}\\left[\\sum_{l=1}^K p\\left(z_{m n}=l \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_l\\right)\\right] \\mathrm{d} \\theta_m\\right] \\mathrm{d} \\varphi_k \\] 超参数 \\(\\alpha\\) 和 \\(\\beta\\) 给定条件下所有文本的生成概率是 \\[ p(\\mathbf{w} \\mid \\alpha, \\beta)=\\prod_{k=1}^K \\int p\\left(\\varphi_k \\mid \\beta\\right)\\left[\\prod_{m=1}^M \\int p\\left(\\theta_m \\mid \\alpha\\right) \\prod_{n=1}^{N_m}\\left[\\sum_{l=1}^K p\\left(z_{m n}=l \\mid \\theta_m\\right) p\\left(w_{m n} \\mid \\varphi_l\\right)\\right] \\mathrm{d} \\theta_m\\right] \\mathrm{d} \\varphi_k \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:7","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"吉布斯抽样 基本思想 有三个主要目标： - 话题序列的集合\\(z=(z_1, z_2, \\cdots, z_M)\\)的后验概率分布，其中\\(z_m\\)是第m个文本的主题序列，\\(z_m=(z_{m1}, \\cdots, z_{mN_{m}})\\); - 参数\\(\\theta=(\\theta_1, \\cdots, \\theta_{M})\\)，其中\\(\\theta_m\\)是第m个文本的主题分布的参数； - 参数\\(\\varphi=(\\varphi_1, \\cdots, \\varphi_K)\\)，其中\\(\\varphi_k\\)是第k个主题的单词分布的参数。 对\\(p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)\\)进行估计 吉布斯抽样, 这是一种常用的马尔可夫链蒙特卡罗法。为了估计 多元随机变量 \\(x\\) 的联合分布 \\(p(x)\\), 吉布斯抽样法选择 \\(x\\) 的一个分量, 固定其他分量, 按照其条件概率分布进行随机抽样, 依次循环对每一个分量执行这个操作, 得到联合 分布 \\(p(x)\\) 的一个随机样本, 重复这个过程, 在燃烧期之后, 得到联合概率分布 \\(p(x)\\) 的 样本集合。 LDA 模型的学习通常采用收缩的吉布斯抽样 (collapsed Gibbs sampling) , 基本想法是, 通过对隐变量 \\(\\theta\\) 和 \\(\\varphi\\) 积分, 得到边缘概率分布 \\(p(\\mathbf{w}, \\mathbf{z} \\mid \\alpha, \\beta)\\) (也是联合分 布), 其中变量 \\(\\mathbf{w}\\) 是可观测的, 变量 \\(\\mathbf{z}\\) 是不可观测的; 对后验概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 进 行吉布斯抽样, 得到分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本集合; 再利用这个样本集合对参数 \\(\\theta\\) 和 \\(\\varphi\\) 进行估计, 最终得到 LDA 模型 \\(p(\\mathbf{w}, \\mathbf{z}, \\theta, \\varphi \\mid \\alpha, \\beta)\\) 的所有参数估计。 #### 算法流程 输入: 文本的单词序列 \\(\\mathbf{w}=\\{\\mathbf{w}_1, \\cdots, \\mathbf{w}_m, \\cdots, \\mathbf{w}_M\\}, \\mathbf{w}_m=\\left(w_{m 1}, \\cdots, w_{m n}, \\cdots\\right.\\), \\(\\left.w_{m_{N_m}}\\right)\\); 输出: 文本的话题序列 \\(\\mathrm{z}=\\{\\mathbf{z}_1, \\cdots, \\mathbf{z}_m, \\cdots, \\mathbf{z}_M\\}, \\mathbf{z}_m=\\left(z_{m 1}, \\cdots, z_{m n}, \\cdots, z_{m_{N_m}}\\right)\\) 的后验概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本计数, 模型的参数 \\(\\varphi\\) 和 \\(\\theta\\) 的估计值; 参数: 超参数 \\(\\alpha\\) 和 \\(\\beta\\), 话题个数 \\(K\\) 。 设所有计数矩阵的元素 \\(n_{m k}, n_{k v}\\), 计数向量的元素 \\(n_m, n_k\\) 初值为 0 ; 对所有文本 \\(\\mathbf{w}_m, m=1,2, \\cdots, M\\) 对第 \\(m\\) 个文本中的所有单词 \\(w_{m n}, n=1,2, \\cdots, N_m\\) 抽样话题 \\(z_{m n}=z_k \\sim \\operatorname{Mult}\\left(\\frac{1}{K}\\right)\\);(对于文本m，其多项分布的参数为\\(\\frac{1}{K}\\)，由\\(\\alpha\\)生成，即\\(\\theta_m \\sim Dir(\\alpha)\\)，\\(\\theta_m\\)为长度为K的向量。) 增加文本-话题计数 \\(n_{m k}=n_{m k}+1\\), 增加文本-话题和计数 \\(n_m=n_m+1\\), 增加话题-单词计数 \\(n_{k v}=n_{k v}+1\\), 增加话题-单词和计数 \\(n_k=n_k+1\\); （3）循环执行以下操作, 直到进入燃烧期 对所有文本 \\(\\mathbf{w}_m, m=1,2, \\cdots, M\\) 对第 \\(m\\) 个文本中的所有单词 \\(w_{m n}, n=1,2, \\cdots, N_m\\) 当前的单词 \\(w_{m n}\\) 是第 \\(v\\) 个单词, 话题指派 \\(z_{m n}\\) 是第 \\(k\\) 个话题; 减少计数 \\(n_{m k}=n_{m k}-1, n_m=n_m-1, n_{k v}=n_{k v}-1, n_k=n_k-1\\); 按照满条件分布进行抽样 \\[ p\\left(z_i \\mid \\mathbf{z}_{-i}, \\mathbf{w}, \\alpha, \\beta\\right) \\propto \\frac{n_{k v}+\\beta_v}{\\sum_{v=1}^V\\left(n_{k v}+\\beta_v\\right)} \\cdot \\frac{n_{m k}+\\alpha_k}{\\sum_{k=1}^K\\left(n_{m k}+\\alpha_k\\right)} \\] 得到新的第 \\(k^{\\prime}\\) 个话题, 分配给 \\(z_{m n}\\); 增加计数 \\(n_{m k^{\\prime}}=n_{m k^{\\prime}}+1, n_m=n_m+1, n_{k^{\\prime} v}=n_{k^{\\prime} v}+1, n_{k^{\\prime}}=n_{k^{\\prime}}+1\\); 得到更新的两个计数矩阵 \\(N_{K \\times V}=\\left[n_{k v}\\right]\\) 和 \\(N_{M \\times K}=\\left[n_{m k}\\right]\\), 表示后验 概率分布 \\(p(\\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta)\\) 的样本计数; 利用得到的样本计数, 计算模型参数 \\[ \\begin{aligned} \\theta_{m k} \u0026=\\frac{n_{m k}+\\alpha_k}{\\sum_{k=1}^K\\left(n_{m k}+\\alpha_k\\right)} \\\\\\\\ \\varphi_{k v} \u0026=\\frac{n_{k v}+\\beta_v}{\\sum_{v=1}^V\\left(n_{k v}+\\beta_v\\right)} \\end{aligned} \\] ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:8","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"训练与推断 有了LDA模型，我们的目标有两个： 估计模型中的参数\\(\\varphi_1, \\cdots, \\varphi_K\\)和\\(\\theta_1, \\cdots, \\theta_M\\); 对于新来的一篇doc，我们能够计算这篇文档的topic分布\\(\\theta_{new}\\)。 有了吉布斯采样公式就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic语义分析，训练的过程就是通过Gibbs Samping获取语料中的（z,w）样本，而模型中的所有参数可以基于采样的样本进行估计。 训练流程如下： 随机初始化：对语料中的每篇文档的每个词w，随机赋一个topic编号z。 重新扫描语料库，对每个词按照吉布斯采样公式重新采样它的topic，在语料中进行更新。 重复以上语料库的重新采样过程直到吉布斯采样收敛。 统计语料库的topic-word共现频率矩阵，就是LDA的模型 由这个矩阵我们可以计算每一个\\(p(word\\mid topic)\\)概率，从而计算出模型参数\\(\\varphi_1, \\cdots, \\varphi_K\\)，也可以计算另一个参数\\(\\theta_1, \\cdots, \\theta_M\\)，只要在吉布斯抽样收敛后统计每篇文章的topic频率分布，就可以计算每一个\\(p(topic\\mid doc)\\)概率，由于它是和训练语料的每篇文章相关的，对于我们理解新的文档毫无用处，所以一般没有必要保留这个概率。 如何对新的文档进行推断呢？其实和训练过程完全相似，对于新的文档，认为\\(\\varphi_{kt}\\)是稳定不变的，是由训练语料得到的模型提供的。采样过程只估计该文档的topic分布\\(\\theta_{new}\\)就好了。 推断过程如下： 随机初始化：对当前文档的每个词w，随机的赋一个topic编号z； 重新扫描当前文档，按照吉布斯抽样公式，对每个词w，重新采样它的topic； 重复以上过程直到吉布斯采样收敛 统计文档中的topic分布，该分布就是\\(\\theta_{new}\\) ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:9","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"代码 实现了吉布斯推断的python代码： \"\"\" LDA implementation in Python @author: Michael Zhang \"\"\" import matplotlib.pyplot as plt import numpy as np import scipy class LDA(object): def __init__(self, tdm, T, alpha = 1., beta=1., iteration=100): \"\"\" tdm: the copus, of (D, Num_words_in_corpus), the value of each entry is the counts of corresponding words in this the corresponding document. e.g. tdm[d, w] = number of word w appears in document d. T: the number of topics \"\"\" self.tdm = tdm self.D, self.W = self.tdm.shape self.alpha= alpha # count for expected value for hyper parameter alpha of theta, i.e. document-topic distribution. self.beta = beta # count for expected value for hyper parameter beta topic-word distribution. self.T = T self.iteration = iteration # z must take in (d,w,i) as input, corresponding to # topic indicator for i-th obserevation of word w in doc d self.z = {} self.topic_word_matrix = np.zeros((self.T, self.W)) # initialize the topic-word matrix. self.doc_topic_matrix = np.zeros((self.D, self.T)) # initialize the documnet-topic matrix. self.topic_counts = np.zeros(self.T) # initialize the topic counter for after sampling process, should be sum of value in self.topic_word_matrix self.doc_counts = np.zeros(self.D) # initialize the doc counter for after sampling process, should be sum of value in self.doc_topic_matrix self.log_likelihood = np.zeros(self.iteration) # store the value of log likelihood at each iteration self._init_matrix() # @pysnooper.snoop('init.log') def _init_matrix(self): \"\"\" for all words 1. sample a topic randomly from T topics for each word 2. increment topic word count, self.topic_word_matrix 3. increment document topic count, self.doc_topic_matrix 4. update the topic indicator z. \"\"\" for d in range(self.D): doc = scipy.sparse.coo_matrix(self.tdm[d]) word_freq_topic = zip(doc.col, doc.data) for w, frequency in word_freq_topic: # (word, freq) for i in range(frequency): ############ Finish the following initialization steps ############# # 1. sample a topic randomly from T topics for each word topic = np.random.randint(self.T) # 2. increment topic word count, self.topic_word_matrix self.topic_word_matrix[topic, w] += 1 # 3. increment document topic count, self.doc_topic_matrix self.doc_topic_matrix[d, topic] += 1 # 4. update the topic indicator z. self.z[(d, w, i)] = topic # d: document ID; w: word ID: i: instance ID，即在d中第几个w self.topic_counts = self.topic_word_matrix.sum(axis=1) self.doc_counts = self.doc_topic_matrix.sum(axis=1) # @pysnooper.snoop('fit.log') def fit(self): for it in range(self.iteration): # iterate over all the documents for d in range(self.D): # iterate over all the words in d for w in self.tdm[d].indices: # iterate over number of times observed word w in doc d for i in range(self.tdm[d, w]): # we apply the hidden-varible method of Gibbs sampler, the hidden variable is z[(d,w,i)] self.doc_topic_matrix[d,self.z[(d,w,i)]] -= 1 self.doc_counts[d] -= 1 self.topic_word_matrix[self.z[(d,w,i)],w] -= 1 self.topic_counts[self.z[(d,w,i)]] -= 1 # estimation of phi and theta for the current corpus phi_hat = (self.topic_word_matrix[:,w] + self.beta) / (self.topic_counts + self.beta * self.W) theta_hat = (self.doc_topic_matrix[d,:] + self.alpha) / (self.doc_counts[d] + self.alpha * self.T) # calculate the full conditional distribution full_conditional = phi_hat * theta_hat # normalize full_conditional such that it summation equals to 1. full_conditional = full_conditional / full_conditional.sum() # sample a topic for i-th obserevation of word w in doc d based on full_conditional new_topic = np.random.multinomial(1, full_conditional).argmax() # update z, doc_topic_matrix, doc_counts, topic_word_matrix, topic_counts here. self.z[(d,w,i)] = new_topic self.doc_topic_matrix[d,self.z[(d,w,i)]] += 1 self.topic_word_matrix[self.z[(d,w,i)],w] += 1 self.doc_counts[d] += 1 self.topic_counts[self.z[(d,w,i)]] += 1 ############################################################ # Equation 2 log P(w|z) for each itera","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:10","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"本质与使用条件 本质上说，主题模型根本上是实现文本数据的结构化，结构化的文档可以彼此比较和查询，实现传统的任务。 LDA主题模型本质上解决了两类问题： - 文档聚类 - 词汇聚类 主要价值在于： 1）文档的结构化，相比于传统的词袋模型达到了降维的效果 2）完成了文档的聚类和词汇的聚类，实现文本信息的抽象化分析，帮助分析者探索隐含的语义内容。 实践中数据要有以下性质才会有较好的结果： 文档足够多 文档足够长 词汇特征够多 词频足够大 ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:4:11","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"总结 历时好几周，终于完结了主题模型，主要是概率论没有学好，跟着推导的过程过于痛苦，不过也算是稍微理解了一点LDA，复述一下： LDA理解可以类比于PLSA，大体的思想都是根据文档生成主题分布，再根据主题分布和单词分布得到文档中的各个单词。不同的是LDA是贝叶斯派的思想，对于两种分布加入了狄利克雷先验概率。LDA的生成过程可以看成上帝掷骰子，从M个骰子中选取一个作为文本m的主题分布，从K个骰子中选取一个作为主题k的单词分布，（注意这里的多项分布的参数就是多项分布中的概率p，其服从狄利克雷分布，比如对于\\(\\theta_m\\)，它其实就是文本m生成不同主题k的概率\\(p(z\\mid d_m)\\)，是个K维的向量。对于\\(\\varphi_k\\)，是由主题k生成不同单词v的概率\\(p(w\\mid z_k)\\)，是个V维的向量。也就是根据狄利克雷分布采样得到的是一些概率，这些概率也是我们最终要求的参数，这些概率作为多项分布的参数再采样生成主题或者单词，还有就是\\(p(z_k\\mid d_m)\\)与\\(z_{mn}\\)的理解，前者就是相当于\\(\\theta_{mk}\\)，后者肯定是主题集合中的一个，不过是根据参数为\\(\\theta_m\\)的多项分布在位置n采样得到的。这就是LDA的整个的理解，当然模型的求解是使用吉布斯抽样的方法，与上面写的步骤不同。写这些是便于理解）。 由主题分布可以对文本的每个位置赋值一个主题，再根据主题-单词分布可以生成整个文本。一切的一切都是和PLSA一样，求两个分布，以至于可以生成我们的文档。LDA也可以得到文档的主题分布，得到了主题分布和单词分布可以应用于各种任务当中。具体可以参考《LDA漫游指南》。 现在知道了LDA是怎么一回事了，但还是感觉模模糊糊的，感觉如“通俗理解LDA主题模型”这篇文章开头所说的那样陷入了LDA的细枝末节中，所以写了一些主题，加深自己的印象与理解，经过代码的洗礼，又理解深入了一些，但感觉还没有掌握的很好，可能需要消化消化，那就先告一段落了。以后常看看就行。 ## 参考 https://zhuanlan.zhihu.com/p/374924140 https://www.cnblogs.com/gasongjian/p/7631978.html ","date":"2022-06-16","objectID":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/:5:0","tags":["NLP","主题模型"],"title":"主题模型","uri":"/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"Transformer \\[ -\\log \\frac{\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_i^{+}\\right) / \\tau})}{\\sum_{j=1}^N\\left(\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_j^{+}\\right) / \\tau})+\\exp({\\operatorname{sim}\\left(\\mathbf{h}_i, \\mathbf{h}_j^{-}\\right) / \\tau}\\right))} \\] ","date":"2022-06-08","objectID":"/transformer/:0:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"背景 先从word2vec开始说起，word2vec可以看作是一个预训练模型，但是它有个问题就是它没有办法解决一词多义的问题，比如说bank这个词语，有银行的意思，但在某些语义下，它也有河岸的意思，但对于word2vec来说，它区别不了这两种含义，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。 而ELMo就解决了这个问题，它使用了双向的LSTM，具体的可以看ELMo,总之使用RNN作为特征提取器，解决了多义词的问题，但现在来看，RNN的特征提取的能力是远不如本文的Transformer的，为什么要介绍这些东西呢，这就是原因，Transformer出现后，取代了RNN和CNN的地位，成为了最流行的特征提取器，大火的GPT和BERT都与Transformer离不开关系。拿bank为例，RNN在读取整个句子之前不会理解bank的含义，也就是RNN的并行能力比较差，而在Transformer中，token之间会互相交互，也就是所谓的自注意力机制，直观地说，Transformer 的编码器可以被认为是一系列推理步骤（层）。在每一步中，token都会互相看着对方（这是我们需要注意的地方——self-attention），交换信息并尝试在整个句子的上下文中更好地理解对方。这发生在几个层（例如，6 个）中。 在每个解码器层中，前缀标记也通过自注意力机制相互交互。 下面就详细介绍一下。 ","date":"2022-06-08","objectID":"/transformer/:1:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"self-attention 首先介绍一下最主要的self-attention，可以说是self-attention实现了上述的token之间交互的功能。 自注意力是模型的关键组成部分之一。注意 和自注意之间的区别在于，自注意在相同性质的表示之间运行：例如，某个层中的所有编码器状态。 形式上，这种直觉是通过查询键值注意来实现的。self-attention 中的每个输入标记都会收到三种表示，对应于它可以扮演的角色： query key value 进入正题： 作为我们想要翻译的输入语句“The animal didn’t cross the street because it was too tired”。句子中”it”指的是什么呢？“it”指的是”street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。 当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。当模型处理每个位置的词时，self-attention允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。如果你对RNN熟悉，就能想到RNN的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer使用self-attention来将相关词的理解编码到当前词中。 下面看一下self-attention是如何计算的： ","date":"2022-06-08","objectID":"/transformer/:2:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"向量计算 第一步，根据编码器的输入向量，生成三个向量，比如，对每个词向量，生成query-vec, key-vec, value-vec，生成方法为分别乘以三个矩阵，这些矩阵在训练过程中需要学习。【注意：不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；权重矩阵是基于输入位置的转换矩阵；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】 注意到这些新向量的维度比输入词向量的维度要小（512–\u003e64），并不是必须要小的，是为了让多头attention的计算更稳定。 第二步，计算attention就是计算一个分值。对“Thinking Matchines”这句话，对“Thinking”（pos#1）计算attention 分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。 这个分，通过“Thing”对应query-vector与所有词的key-vec依次做点积得到。所以当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积。这也就是所谓的注意力得分. 第三步和第四步，除以8(\\(=\\sqrt{dim_{key}}\\))，这样梯度会更稳定。然后加上softmax操作，归一化分值使得全为正数且加和为1。 softmax分值决定着在这个位置，每个词的表达程度（关注度）。很明显，这个位置的词应该有最高的归一化分数，但大部分时候总是有助于关注该词的相关的词。 第五步，将softmax分值与value-vec按位相乘。保留关注词的value值，削弱非相关词的value值。 第六步，将所有加权向量加和，产生该位置的self-attention的输出结果。 上述就是self-attention的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。 ","date":"2022-06-08","objectID":"/transformer/:2:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"矩阵计算 第一步，计算query/key/value matrix，将所有输入词向量合并成输入矩阵\\(X\\)，并且将其分别乘以权重矩阵\\(W^q, W^k,W^v\\) 最后，鉴于我们使用矩阵处理，将步骤2~6合并成一个计算self-attention层输出的公式。 ","date":"2022-06-08","objectID":"/transformer/:2:2","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"多头注意力机制 论文进一步增加了multi-headed的机制到self-attention上，在如下两个方面提高了attention层的效果： 多头机制扩展了模型集中于不同位置的能力。在上面的例子中，z1只包含了其他词的很少信息，仅由实际自己词决定。在其他情况下，比如翻译 “The animal didn’t cross the street because it was too tired”时，我们想知道单词”it”指的是什么。 多头机制赋予attention多种子表达方式。像下面的例子所示，在多头下有多组query/key/value-matrix，而非仅仅一组（论文中使用8-heads）。每一组都是随机初始化，经过训练之后，输入向量可以被映射到不同的子表达空间中。 如果我们计算multi-headed self-attention的，分别有八组不同的Q/K/V matrix，我们得到八个不同的矩阵。 这会带来点麻烦，前向网络并不能接收八个矩阵，而是希望输入是一个矩阵，所以要有种方式处理下八个矩阵合并成一个矩阵。 上述就是多头自注意机制的内容，我认为还仅是一部分矩阵，下面尝试着将它们放到一个图上可视化如下。 #### 代码 下面实现一下多头注意力机制，在原论文中，实现的方法如下： 也就是对每个W进行多头的设置，即为原维度/head，然后拼接后，再经过\\(hd_v\\times d_{model}\\)的转换又得到原来的维度，代码的实现不太一样，代码是W还是\\(d_{model}\\times d_{model}\\)的矩阵然后得到q,k,v之后再进行截断，实现如下。 class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1) -\u003e None: # h为head，这里为8，d_model为embedding的维度，这里为512 super().__init__() assert d_model % h == 0 self.d_k = d_model // h # 64 self.h = h self.Q_Linear = nn.Linear(d_model, d_model) self.K_Linear = nn.Linear(d_model, d_model) self.V_Linear = nn.Linear(d_model, d_model) self.res_Linear = nn.Linear(d_model, d_model) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) batch_size = query.size(0) query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64) query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64) key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k) # attn为(batch_size, h, seq_len1, seq_len2) x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (batch_size, h, seq_len, d_k) -\u003e (batch_size, seq_len, h, d_k) -\u003e (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512) return self.res_Linear(x) ","date":"2022-06-08","objectID":"/transformer/:2:3","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"Masked self-attention 在训练的时候，主要是消除后面的信息对预测的影响，因为decoder输入的是整个句子，也就是我们所谓的参考答案，而实际预测的时候就是预测后面的token，用不到后面的token，如果不mask掉，当前的token将看到“未来”，这不是我们想要的，因此必须要mask掉。 其实decoder里的sequence mask与encoder里的padding mask异曲同工，padding mask其实很简单，就是为了使句子长度一致进行了padding，而为了避免关注padding的位置，进行了mask，具体的做法就是将这些位置的值变成负无穷，这样softmax之后就接近于0了。 而sequence mask思想也差不多： 假设现在解码器的输入”\u003c s \u003e who am i \u003c e \u003e“在分别乘上一个矩阵进行线性变换后得到了Q、K、V，且Q与K作用后得到了注意力权重矩阵（此时还未进行softmax操作），如图17所示。 此时已经计算得到了注意力权重矩阵。由第1行的权重向量可知，在解码第1个时刻时应该将20%（严格来说应该是经过softmax后的值）的注意力放到’\u003c s \u003e’上，30%的注意力放到’who’上等等。不过此时有一个问题就是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer中的Decoder通过加入注意力掩码机制来解决了这一问题。 当然还要进行softmax等计算。 在网上查了很多资料，说法都很不一样，不过我更倾向于这样的看法。而在预测的时候是用前面的输出结果作为输入的。 几张图帮助理解： 后面还有padding mask，所有的self attention都要用这个，因为pad的位置没有任何意义。 实践一下加深理解： 首先我们来定义模型： # 词典数为10， 词向量维度为8 embedding = nn.Embedding(10, 8) # 定义Transformer，注意一定要改成eval模型，否则每次输出结果不一样 transformer = nn.Transformer(d_model=8, batch_first=True).eval() 接下来定义我们的src和tgt： # Encoder的输入 src = torch.LongTensor([[0, 1, 2, 3, 4]]) # Decoder的输入 tgt = torch.LongTensor([[4, 3, 2, 1, 0]]) 然后我们将[4]送给Transformer进行预测，模拟推理时的第一步： transformer(embedding(src), embedding(tgt[:, :1]), # 这个就是用来生成阶梯式的mask的 tgt_mask=nn.Transformer.generate_square_subsequent_mask(1)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 然后我们将[4, 3]送给Transformer，模拟推理时的第二步： transformer(embedding(src), embedding(tgt[:, :2]), tgt_mask=nn.Transformer.generate_square_subsequent_mask(2)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 出的第一个向量和上面那个一模一样。 最后我们再将tgt一次性送给transformer，模拟训练过程： transformer(embedding(src), embedding(tgt), tgt_mask=nn.Transformer.generate_square_subsequent_mask(5)) tensor([[[ 1.4053, -0.4680, 0.8110, 0.1218, 0.9668, -1.4539, -1.4427, 0.0598], [ 1.2726, -0.3516, 0.6584, 0.3297, 1.1161, -1.4204, -1.5652, -0.0396], [ 1.4799, -0.3575, 0.8310, 0.1642, 0.8811, -1.3140, -1.5643, -0.1204], [ 1.4359, -0.6524, 0.8377, 0.1742, 1.0521, -1.3222, -1.3799, -0.1454], [ 1.3465, -0.3771, 0.9107, 0.1636, 0.8627, -1.5061, -1.4732, 0.0729]]], grad_fn=\u003cNativeLayerNormBackward0\u003e) 可以看到使用mask后就可以保证前面的结果都是不变的，不然如果没有mask则计算attention时因为计算注意力变化所以结果都会变化，这就是Mask self-attention的意义。 到这里self-attention就介绍完了 ","date":"2022-06-08","objectID":"/transformer/:2:4","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"代码 def attention(query, key, value, mask=None, dropout=None): d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # 最后两个维度相乘，即为scores，再scale一下。 if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 将mask的位置的scores置为-1e9 # 实际上pad mask的时候，pad也会作为key与其它token对应的k,v计算score，pad mask只是消除pad作为k,v时候的影响。但在最后softmax的时候，将pad的损失值全部置为0 p_attn = F.softmax(scores, dim=-1) # 将scores进行softmax，得到p_attn，这里是在最后一个维度上softmax，因为对每个query的所有key进行softmax if dropout: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1) -\u003e None: # h为head，这里为8，d_model为embedding的维度，这里为512 super().__init__() assert d_model % h == 0 self.d_k = d_model // h # 64 self.h = h self.Q_Linear = nn.Linear(d_model, d_model) self.K_Linear = nn.Linear(d_model, d_model) self.V_Linear = nn.Linear(d_model, d_model) self.res_Linear = nn.Linear(d_model, d_model) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: mask = mask.unsqueeze(1) batch_size = query.size(0) query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64) query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64) key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k) # attn为(batch_size, h, seq_len1, seq_len2) x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (batch_size, h, seq_len, d_k) -\u003e (batch_size, seq_len, h, d_k) -\u003e (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512) return self.res_Linear(x) ","date":"2022-06-08","objectID":"/transformer/:2:5","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"模型架构 下面是原始论文中的架构： self-attention上面已经讲的比较详细了，下面说一下其余的部分。 ","date":"2022-06-08","objectID":"/transformer/:3:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"FFN(前馈网络) 除了注意力以外，每一层都有一个前馈网络：两个线性层之间具有ReLU非线性： \\[ FFN(x) = max(0, xW_1+b_1)W_2+b_2 \\] 在通过注意力机制查看其他令牌之后，模型使用 FFN 块来处理这些新信息。 class PositionwiseFeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout=0.1): super().__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(p=dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) ","date":"2022-06-08","objectID":"/transformer/:3:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"残差连接 残差连接非常简单（将块的输入添加到其输出），但同时也非常有用：它们缓解了通过网络的梯度流并允许堆叠很多层。解决了网络退化的问题。 在 Transformer 中，在每个注意力和 FFN 块之后使用残差连接。在上图中，残差显示为围绕一个块到黄色 “Add \u0026 Norm”层的箭头。在“Add \u0026 Norm”部分， “Add”部分代表残差连接。 ","date":"2022-06-08","objectID":"/transformer/:3:2","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"Layer Norm “Add \u0026 Norm”层中的“Norm”部分 表示 Layer Normalization。它批量独立地标准化每个示例的向量表示 - 这样做是为了控制“流”到下一层。层归一化提高了收敛稳定性，有时甚至提高了质量。 这里的scale和bias都是可以训练的参数。 注意Layer Norm与Batch Norm是不同的，这里引用一下沐神的视频： 这是Batch Norm的切法，即对每个特征进行norm。 这是Layer norm的切法，即对每个样本进行norm。 为什么用layer norm而不用Batch norm呢？ 当你的样本长度变化比较大的时候，使用batch norm计算的均值和方差波动比较大，而且batch norm需要记录全局的均值和方差，当遇到新的测试样本的时候，由于长度的原因，之前的均值方差可能就效果不太好了。 但是如果使用layer norm 的话就没有那么多的问题，因为它是每个样本自己计算均值方差，不需要存在一个全局的均值方差，所以会稳定一点。 class LayerNorm(nn.Module): def __init__(self, features, eps=1e-6) -\u003e None: super().__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 class SublayerConnection(nn.Module): def __init__(self, size, dropout) -\u003e None: super().__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(p=dropout) def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x))) # 这里和论文不同，先norm再扔给sublayer（比如多头注意力、ffd）,理论上是self.norm(x+self.dropout(sublayer(x))) ","date":"2022-06-08","objectID":"/transformer/:3:3","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"位置编码(position encoding) (Position Embedding是学习式，而Position Encoding为固定式) 请注意，由于 Transformer 不包含递归或卷积，它不知道输入标记(token)的顺序。因此，我们必须让模型明确地知道标记的位置。为此，我们有两组嵌入：用于标记（我们总是这样做）和用于位置（该模型所需的新嵌入）。那么令牌的输入表示是两个嵌入的总和：令牌和位置。 位置嵌入是可以学习的，但作者发现固定的嵌入不会影响质量。Transformer 中使用的固定位置编码是： \\[ PE_{pos,2i} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\] \\[ PE_{pos,2i+1} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\] 可以看到，每个词的维度都是512维，假设句子长度为10，则位置编码的计算如上图所示。 得到位置编码后，将位置编码与词嵌入简单相加即可。 #### 代码 class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000) -\u003e None: super().__init__() self.dropout = nn.Dropout(p=dropout) position_embedding = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) position_embedding[:, 0::2] = torch.sin(position * div_term) position_embedding[:, 1::2] = torch.cos(position * div_term) position_embedding = position_embedding.unsqueeze(0) # 增加一维预留batch size的位置，所以后面forward要在第二维上选取序列长度 self.register_buffer('PositionalEncoding', position_embedding) def forward(self, x): return self.dropout(x + Variable(self.PositionalEncoding[:, :x.size(1)], requires_grad=False)) 这里为了计算做了转换。 ### Padding Mask 对于输入序列一般我们都要进行padding补齐，也就是说设定一个统一长度N，在较短的序列后面填充0到长度为N。对于那些补零的数据来说，我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样经过softmax后，这些位置的权重就会接近0。Transformer的padding mask实际上是一个张量，每个值都是一个Boolean，值为false的地方就是要进行处理的地方。 ","date":"2022-06-08","objectID":"/transformer/:3:4","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"label smoothing(标签平滑) 神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。 label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 增加label smoothing后真实的概率分布有如下改变： 代码 class LabelSmoothing(nn.Module): # [标签平滑](../Deep%20Learning/训练trick/标签平滑.md)损失函数 def __init__(self, size, padding_idx, smoothing=0.0) -\u003e None: super().__init__() self.criterion = nn.KLDivLoss(size_average=False) self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): # x的shape为(batch.size * seq.len, target.vocab.size) # y的shape是(batch.size * seq.len) # x=logits，(seq.len, target.vocab.size) # 每一行，代表一个位置的词 # 类似于：假设seq.len=3, target.vocab.size=5 # x中保存的是log(prob) #x = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) # target 类似于： # target = tensor([2, 1, 0])，torch.size=(3) assert x.size(1) == self.size true_dist = x.data.clone() # true_dist = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) true_dist.fill_(self.smoothing / (self.size - 2)) # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333], #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333], #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]]) # 注意，这里分母target.vocab.size-2是因为 # (1) 最优值 0.6要占一个位置； # (2) 填充词 \u003cblank\u003e 要被排除在外 # 所以被激活的目标语言词表大小就是self.size-2 true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) # target.data.unsqueeze(1) -\u003e # tensor([[2], #[1], #[0]]); shape=torch.Size([3, 1]) # self.confidence = 0.6 # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值 # 填入true_dist: 因为target.data是2,1,0的内容， # 所以，0.6填入第0行的第2列（列号，行号都是0开始） # 0.6填入第1行的第1列 # 0.6填入第2行的第0列： # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333], #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333], #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]]) true_dist[:, self.padding_idx] = 0 # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333], #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]]) # 设置true_dist这个tensor的第一列的值全为0 # 因为这个是填充词'\u003cblank\u003e'所在的id位置，不应该计入 # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表 #中的一个词的id mask = torch.nonzero(target.data == self.padding_idx) # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0， # 因为是'\u003cblank\u003e'的id，所以通过上面的一步，把他们找出来 # 如果不加上nonzero，那么mask的shape就是torch.Size([3]) if mask.dim() \u003e 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) # 当target reference序列中有0这个'\u003cblank\u003e'的时候，则需要把 # 这一行的值都清空。 # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要 # pad '\u003cblank\u003e'来填充，所以会出现类似于(2,1,0)这样的情况 # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333], # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) self.true_dist = true_dist return self.criterion(x, Variable(true_dist, requires_grad=False)) # 这一步就是调用KL loss来计算 # x = tensor([[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233], #[-20.7233, -1.6094, -0.3567, -2.3026, -20.7233]]) # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333], # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) ","date":"2022-06-08","objectID":"/transformer/:3:5","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"预测 预测过程与一般seq2seq不同的是，t时刻是将1到t-1时刻所有的预测结果作为序列进行预测，而seq2seq只是使用前一时刻的输出作为当前时刻的输入，这里困扰了我很久，实现了transformer代码后对比李沐老师的代码才理解。 其中seq2seq: transformer: 注意ys最后与之前的ys使用cat函数合并在一起。 ## 总结 Transformer还有很多的模型细节，以后遇到了再记录一下，在面试中很容易问到这些细节，因此可以参考面经边学习边记录，可以查缺补漏也可以学到新的东西。接下来把代码复现一下可以加深理解，并且提高自己的代码水平和实践能力。 ","date":"2022-06-08","objectID":"/transformer/:4:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"一些问题 Transformer在哪里做了权重共享，为什么可以做权重共享？ Transformer在两个地方进行了权重共享： （1）Encoder和Decoder间的Embedding层权重共享； （2）Decoder中Embedding层和FC层权重共享。 对于（1），《Attention is all you need》中Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，嵌入时都只有对应语言的embedding会被激活，因此是可以共用一张词表做权重共享的。 论文中，Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。 但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。 对于（2），Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。 那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和softmax概率会是最大的。 因此，Embedding层和FC层权重共享，Embedding层中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的Embedding层和FC层有点像互为逆过程。 通过这样的权重共享可以减少参数的数量，加快收敛。 ","date":"2022-06-08","objectID":"/transformer/:5:0","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["NLP"],"content":"为什么除以根号d 论文中的解释是：向量的点积结果会很大，将 softmax 函数 push 到梯度很小的区域，scaled 会缓解这种现象。 \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}=\\mathrm{diag}(\\mathbf{y})-\\mathbf{y}\\mathbf{y}^T\\] 当\\(\\mathbf{y} =\\)softmax\\(( \\mathbf{x} )\\)时，\\(\\mathbf{y}\\)对\\(\\mathbf{x}\\)的梯度为： 这是一个jacobi矩阵\\(^{+}\\),表示y的每一个元素对x每一个元素的导数是什么。 展开： \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}=\\begin{bmatrix}y_1\u00260\u0026\\cdots\u00260\\\\0\u0026y_2\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u0026y_d\\end{bmatrix}-\\begin{bmatrix}y_1^2\u0026y_1y_2\u0026\\cdots\u0026y_1y_d\\\\y_2y_1\u0026y_2^2\u0026\\cdots\u0026y_2y_d\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\y_dy_1\u0026y_dy_2\u0026\\cdots\u0026y_d^2\\end{bmatrix}\\] 根据前面的讨论，当输入 \\(\\mathbf{x}\\) 的某一个元素较大时，softmax 会把大部分概率分布\\(^{+}\\)分配给最大的元 素，假设我们的输入数量级很大，那么就将产生一个接近 one-hot 的向量 \\[\\mathbf{y}\\approx[1,0,\\cdots,0]^\\top \\] 此时上面的矩阵变为如下形式 \\[\\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\\approx\\begin{bmatrix}1\u00260\u0026\\cdots\u00260\\\\0\u00260\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u00260\\end{bmatrix}-\\begin{bmatrix}1\u00260\u0026\\cdots\u00260\\\\0\u00260\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\0\u00260\u0026\\cdots\u00260\\end{bmatrix}=\\mathbf{0}\\] 也就是所有的梯度都接近0 除以\\(\\sqrt{ d }\\)后就使x的分布更加平缓，从而防止梯度消失。 from scipy.special import softmax import numpy as np def test_gradient(dim, time_steps=50, scale=1.0): # Assume components of the query and keys are drawn from N(0, 1) independently q = np.random.randn(dim) ks = np.random.randn(time_steps, dim) x = np.sum(q * ks, axis=1) / scale # x.shape = (time_steps,) y = softmax(x) grad = np.diag(y) - np.outer(y, y)# softmax gradient(dy/dx) return np.max(np.abs(grad)) # the maximum component of gradients NUMBER_OF_EXPERIMENTS = 5 # results of 5 random runs without scaling print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)]) # results of 5 random runs with scaling print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)]) 输出可看到下面的梯度比上面的梯度更大。 这时又有一个问题，为什么多分类的softmax+交叉熵不需要除以东西呢？这是因为交叉熵中有一个log，log_softmax的梯度和刚才算出来的不同，就算输入的某一个x过大也不会梯度消失。所以就又可以推断出softmax+MSE会导致梯度消失，因为MSE中没有Log，这是为什么分类任务不使用MSE损失函数的原因之一。 ### 为什么 Transformer 需要进行 Multi-head Attention 实验证明多头是必要的，8/16个头都可以取得更好的效果，但是超过16个反而效果不好。每个头关注的信息不同，但是头之间的差异随着层数增加而减少。并且不是所有头都有用，有工作尝试剪枝，可以得到更好的表现。 论文中提到模型分为多个头，形成多个子空间，每个头关注不同方面的信息。 那为什么每个头的维度要降呢? 一言蔽之的话，大概是：在不增加时间复杂度的情况下，同时，借鉴CNN多核的思想，在更低的维度，在多个独立的特征空间，更容易学习到更丰富的特征信息。 ### 为什么 Transformer 的 Embedding 最后要乘dmodel 具体的原因是，如果使用 Xavier 初始化，Embedding 的方差为 1/d_model，当d_model非常大时，矩阵中的每一个值都会减小。通过乘一个 dmodel 可以将方差恢复到1。 因为Position Encoding是通过三角函数算出来的，值域为[-1, 1]。所以当加上 Position Encoding 时，需要放大 embedding 的数值，否则规模不一致相加后会丢失信息。 因为 Bert 使用的是学习式的Embedding，所以 Bert 这里就不需要放大。 # 参考 Bert/Transformer 被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com) ","date":"2022-06-08","objectID":"/transformer/:5:1","tags":["NLP","Transformer"],"title":"Transformer","uri":"/transformer/"},{"categories":["算法题"],"content":"最大子序和 https://leetcode-cn.com/problems/maximum-subarray/ 一开始直接暴力，结果tle了最后 class Solution: def maxSubArray(nums): res = -float('inf') for i in range(len(nums)): for j in range(i,len(nums)): res = max(res,sum(nums[i:j+1])) return res 这说明在leetcode尽量不要嵌套循环，大概率Tle class Solution: def maxSubArray(nums): for i in range(1,len(nums)): maxs = max(nums[i-1]+nums[i],nums[i]) nums[i] = maxs return max(nums) 最后巧妙地利用了替换的思想，将每次相加的值和当前比较，并将当前替换为较大的那个值，最后求整个列表的最大值。 ","date":"2022-06-08","objectID":"/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/:0:0","tags":["算法题","最大子序和"],"title":"最大子序和","uri":"/%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%92%8C/"},{"categories":["算法题"],"content":"使用最小花费爬楼梯 每日一题刷到的。 动态规划类型的题目，重点就是找状态转移方程，因为我不太熟练，对动态规划的题目做的比较少，所以WA了好几次。 class Solution: def minCostClimbingStairs(cost): res = [] #res[i]就是到第i阶梯时最小的花费 res.append(cost[0]) #到第一阶梯最小就是0+cost[0] res.append(cost[1]) #第二阶梯最小就是0+cost[1] #状态转移方程:res[i] = min(res[i-1],res[i-2])+cost[i] for i in range(2,len(cost)): res.append(min(res[i-1],res[i-2])+cost[i]) # return min(res[-1],res[-2]) 踏上第i级台阶有两种方法： 先踏上第i-2级台阶（最小总花费dp[i-2]），再直接迈两步踏上第i级台阶（花费cost[i]），最小总花费dp[i-2] + cost[i]； 先踏上第i-1级台阶（最小总花费dp[i-1]），再迈一步踏上第i级台阶（花费cost[i]），最小总花费dp[i-1] + cost[i]； 上述为引用的题解的说明，更加深了对动态规划的理解 ","date":"2022-06-01","objectID":"/%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E8%8A%B1%E8%B4%B9%E7%88%AC%E6%A5%BC%E6%A2%AF/:0:0","tags":["算法题","使用最小花费爬楼梯"],"title":"使用最小花费爬楼梯","uri":"/%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E8%8A%B1%E8%B4%B9%E7%88%AC%E6%A5%BC%E6%A2%AF/"},{"categories":["NLP","概率图模型"],"content":"CRF ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:0:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"概率图模型与无向图 图是由结点和连接结点的边组成的集合。结点和边分别记作v和e，结点和边的集合分别记作V和E，图记作\\(G=(V, E)\\)。 无向图是指没有方向的图。 概率图模型是由图表示的概率分布。设有联合概率分布P(Y), Y是一组随机变量，由无向图\\(G=(V,E)\\)表示概率分布P(Y)，即在图G中，结点\\(v\\in V\\)表示一个随机变量\\(Y_v\\)，\\(Y=(Y_v)\\_{v\\in V}\\)，边e表示随机变量之间的依赖关系。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:1:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"概率无向图模型 设有联合概率分布P(Y)，由无向图\\(G=(V,E)\\)表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布满足成对、局部或全局马尔科夫性，就称此联合概率分布称为概率无向图模型，或马尔科夫随机场。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:2:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"因子分解 首先给出无向图的团和最大团的定义： 无向图G中任何两个结点均有边连接的结点子集称为团。若C是无向图G的一个团，并且不能再加进任何一个G的结点使其成为更大的团，则称此C为最大团。 将无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。 给定概率无向图模型, 设其无向图为 \\(G, C\\) 为 \\(G\\) 上的最大团, \\(Y_C\\) 表示 \\(C\\) 对应的 随机变量。那么概率无向图模型的联合概率分布 \\(P(Y)\\) 可写作图中所有最大团 \\(C\\) 上的 函数 \\(\\Psi_C\\left(Y_C\\right)\\) 的乘积形式, 即 \\[ P(Y)=\\frac{1}{Z} \\prod_C \\Psi_C\\left(Y_C\\right) \\] 其中, \\(Z\\) 是规范化因子 (normalization factor), 由式 \\[ Z=\\sum_Y \\prod_C \\Psi_C\\left(Y_C\\right) \\] 给出。规范化因子保证 \\(P(Y)\\) 构成一个概率分布。函数 \\(\\Psi_C\\left(Y_C\\right)\\) 称为势函数 (potential function)。这里要求势函数 \\(\\Psi_C\\left(Y_C\\right)\\) 是严格正的, 通常定义为指数函数: \\[ \\Psi_C\\left(Y_C\\right)=\\exp \\\\{-E\\left(Y_C\\right)\\\\\\} \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:3:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"条件随机场 条件随机场是指给定随机变量X的条件下，随机变量Y的马尔科夫随机场。一般的条件随机场主要是指线性链条件随机场，可以用于标注等问题。这里的\\(P(Y|X)\\)中，Y是输出变量，表示标注序列，X是输入变量，表示需要标注的观察序列。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"一般的条件随机场 (条件随机场) 设 \\(X\\) 与 \\(Y\\) 是随机变量, \\(P(Y \\mid X)\\) 是在给定 \\(X\\) 的条件 下 \\(Y\\) 的条件概率分布。若随机变量 \\(Y\\) 构成一个由无向图 \\(G=(V, E)\\) 表示的马尔可夫 随机场, 即 \\[ P\\left(Y_v \\mid X, Y_w, w \\neq v\\right)=P\\left(Y_v \\mid X, Y_w, w \\sim v\\right) \\] 对任意结点 \\(v\\) 成立, 则称条件概率分布 \\(P(Y \\mid X)\\) 为条件随机场。式中 \\(w \\sim v\\) 表示在 图 \\(G=(V, E)\\) 中与结点 \\(v\\) 有边连接的所有结点 \\(w, w \\neq v\\) 表示结点 \\(v\\) 以外的所有结 点, \\(Y_v, Y_u\\) 与 \\(Y_w\\) 为结点 \\(v, u\\) 与 \\(w\\) 对应的随机变量。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"线性链条件随机场 设\\(X=(X_1,X_2, \\dots, X_n), \\quad Y=(Y_1, Y_2, \\dots , Y_n)\\)均为线性链表示的随机变量序列，若在给定随机变量序列X的条件下，随机变量Y的条件概率分布\\(P(Y|X)\\)构成条件随机场，即满足马尔科夫性 \\[ P\\left(Y_i \\mid X, Y_1, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_n\\right)=P\\left(Y_i \\mid X, Y_{i-1}, Y_{i+1}\\right) \\] \\(i=1,2, \\cdots, n\\) (在 \\(i=1\\) 和 \\(n\\) 时只考虑单边) 则称 \\(P(Y \\mid X)\\) 为线性链条件随机场。在标注问题中, \\(X\\) 表示输入观测序列, \\(Y\\) 表示对 应的输出标记序列或状态序列。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"线性链条件随机场参数化形式 根据因子分解, 可以给出线性链条件随机场 \\(P(Y \\mid X)\\) 的因子分解式, 各因子是定 义在相邻两个结点 (最大团) 上的势函数。 (线性链条件随机场的参数化形式) 设 \\(P(Y \\mid X)\\) 为线性链条件随机 场, 则在随机变量 \\(X\\) 取值为 \\(x\\) 的条件下, 随机变量 \\(Y\\) 取值为 \\(y\\) 的条件概率具有如下 形式: \\[ P(y \\mid x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_k t_k\\left(y_{i-1}, y_i, x, i\\right)+\\sum_{i, l} \\mu_l s_l\\left(y_i, x, i\\right)\\right) \\] 其中, \\[ Z(x)=\\sum_y \\exp \\left(\\sum_{i, k} \\lambda_k t_k\\left(y_{i-1}, y_i, x, i\\right)+\\sum_{i, l} \\mu_l s_l\\left(y_i, x, i\\right)\\right) \\] 式中, \\(t_k\\) 和 \\(s_l\\) 是特征函数, \\(\\lambda_k\\) 和 \\(\\mu_l\\) 是对应的权值。 \\(Z(x)\\) 是规范化因子, 求和是在所 有可能的输出序列上进行的。 这两个式子是线性链条件随机场模型的基本形式, 表示给定输入序列 \\(x\\), 对输出序列 \\(y\\) 预测的条件概率。\\(t_k\\) 是定义在边上的特 征函数, 称为转移特征, 依赖于当前和前一个位置; \\(s_l\\) 是定义在结点上的特征函数, 称为状态特征, 依赖于当前位置。 \\(t_k\\) 和 \\(s_l\\) 都依赖于位置, 是局部特征函数。通常, 特 征函数 \\(t_k\\) 和 \\(s_l\\) 取值为 1 或 0 ; 当满足特征条件时取值为 1 , 否则为 0 。条件随机场完 全由特征函数 \\(t_k, s_l\\) 和对应的权值 \\(\\lambda_k, \\mu_l\\) 确定。 线性链条件随机场也是对数线性模型 (log linear model)。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:3","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"条件随机场的简化形式 为简便起见, 首先将转移特征和状态特征及其权值用统一的符号表示。设有 \\(K_1\\) 个转移特征, \\(K_2\\) 个状态特征, \\(K=K_1+K_2\\), 记 \\[ f_k\\left(y_{i-1}, y_i, x, i\\right)= \\begin{cases}t_k\\left(y_{i-1}, y_i, x, i\\right), \u0026 k=1,2, \\cdots, K_1 \\\\\\\\ s_l\\left(y_i, x, i\\right), \u0026 k=K_1+l ; l=1,2, \\cdots, K_2\\end{cases} \\] 然后, 对转移与状态特征在各个位置 \\(i\\) 求和, 记作 \\[ f_k(y, x)=\\sum_{i=1}^n f_k\\left(y_{i-1}, y_i, x, i\\right), \\quad k=1,2, \\cdots, K \\] 用 \\(w_k\\) 表示特征 \\(f_k(y, x)\\) 的权值, 即 \\[ w_k= \\begin{cases}\\lambda_k, \u0026 k=1,2, \\cdots, K_1 \\\\\\\\ \\mu_l, \u0026 k=K_1+l ; l=1,2, \\cdots, K_2\\end{cases} \\] 于是, 条件随机场可表示为 \\[ \\begin{aligned} P(y \\mid x) \u0026=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^K w_k f_k(y, x) \\\\\\\\ Z(x) \u0026=\\sum_y \\exp \\sum_{k=1}^K w_k f_k(y, x) \\end{aligned} \\] 若以 \\(w\\) 表示权值向量, 即 \\[ w=\\left(w_1, w_2, \\cdots, w_K\\right)^{\\mathrm{T}} \\] 以 \\(F(y, x)\\) 表示全局特征向量, 即 \\[ F(y, x)=\\left(f_1(y, x), f_2(y, x), \\cdots, f_K(y, x)\\right)^{\\mathrm{T}} \\] 则条件随机场可以写成向量 \\(w\\) 与 \\(F(y, x)\\) 的内积的形式: \\[ P_w(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\] 其中, \\[ Z_w(x)=\\sum_y \\exp (w \\cdot F(y, x)) \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:4","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"矩阵形式 对每个 标记序列引进特殊的起点和终点状态标记 \\(y_0=\\) start 和 \\(y_{n+1}=s t o p\\), 这时标注序列 的概率 \\(P_w(y \\mid x)\\) 可以通过矩阵形式表示并有效计算。 对观测序列 \\(x\\) 的每一个位置 \\(i=1,2, \\cdots, n+1\\), 由于 \\(y_{i-1}\\) 和 \\(y_i\\) 在 \\(m\\) 个标记中 取值, 可以定义一个 \\(m\\) 阶矩阵随机变量 \\[ M_i(x)=\\left[M_i\\left(y_{i-1}, y_i \\mid x\\right)\\right] \\] 矩阵随机变量的元素为 \\[ \\begin{aligned} \u0026M_i\\left(y_{i-1}, y_i \\mid x\\right)=\\exp \\left(W_i\\left(y_{i-1}, y_i \\mid x\\right)\\right) \\\\\\\\ \u0026W_i\\left(y_{i-1}, y_i \\mid x\\right)=\\sum_{k=1}^K w_k f_k\\left(y_{i-1}, y_i, x, i\\right) \\end{aligned} \\] 这里 \\(w_k\\) 和 \\(f_k\\) 分别由前面的式子给出, \\(y_{i-1}\\) 和 \\(y_i\\) 是标记随机变量 \\(Y_{i-1}\\) 和 \\(Y_i\\) 的取值。 这样, 给定观测序列 \\(x\\), 相应标记序列 \\(y\\) 的非规范化概率可以通过该序列 \\(n+1\\) 个矩阵的适当元素的乘积 \\(\\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right)\\) 表示。于是, 条件概率 \\(P_w(y \\mid x)\\) 是 \\[ P_w(y \\mid x)=\\frac{1}{Z_w(x)} \\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right) \\] 其中, \\(Z_w(x)\\) 为规范化因子, 是 \\(n+1\\) 个矩阵的乘积的 (start, stop) 元素, 即 \\[ Z_w(x)=\\left[M_1(x) M_2(x) \\cdots M_{n+1}(x)\\right]_{\\text {start,stop }} \\] 注意, \\(y_0=\\) start 与 \\(y_{n+1}=\\) stop 表示开始状态与终止状态, 规范化因子 \\(Z_w(x)\\) 是以 start 为起点 stop为终点通过状态的所有路径 \\(y_1 y_2 \\cdots y_n\\) 的非规范化概率 \\(\\prod_{i=1}^{n+1} M_i\\left(y_{i-1}, y_i \\mid x\\right)\\) 之和。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:4:5","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"概率计算问题 与HMM类似，引入前向和后向变量，递归的计算概率和一些期望值。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"前向-后向算法 对每个指标 \\(i=0,1, \\cdots, n+1\\), 定义前向向量 \\(\\alpha_i(x)\\) : \\[ \\alpha_0(y \\mid x)= \\begin{cases}1, \u0026 y=\\text { start } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\] 递推公式为 \\[ \\alpha_i^{\\mathrm{T}}\\left(y_i \\mid x\\right)=\\alpha_{i-1}^{\\mathrm{T}}\\left(y_{i-1} \\mid x\\right)\\left[M_i\\left(y_{i-1}, y_i \\mid x\\right)\\right], \\quad i=1,2, \\cdots, n+1 \\] 又可表示为 \\[ \\alpha_i^{\\mathrm{T}}(x)=\\alpha_{i-1}^{\\mathrm{T}}(x) M_i(x) \\] \\(\\alpha_i\\left(y_i \\mid x\\right)\\) 表示在位置 \\(i\\) 的标记是 \\(y_i\\) 并且从 1 到 \\(i\\) 的前部分标记序列的非规范化概 率, \\(y_i\\) 可取的值有 \\(m\\) 个, 所以 \\(\\alpha_i(x)\\) 是 \\(m\\) 维列向量。 同样, 对每个指标 \\(i=0,1, \\cdots, n+1\\), 定义后向向量 \\(\\beta_i(x)\\) : \\[ \\begin{aligned} \\beta_{n+1}\\left(y_{n+1} \\mid x\\right) \u0026= \\begin{cases}1, \u0026 y_{n+1}=\\text { stop } \\\\\\\\ 0, \u0026 \\text { 否则 }\\end{cases} \\\\\\\\ \\beta_i\\left(y_i \\mid x\\right) \u0026=\\left[M_{i+1}\\left(y_i, y_{i+1} \\mid x\\right)\\right] \\beta_{i+1}\\left(y_{i+1} \\mid x\\right) \\end{aligned} \\] 又可表示为 \\[ \\beta_i(x)=M_{i+1}(x) \\beta_{i+1}(x) \\] \\(\\beta_i\\left(y_i \\mid x\\right)\\) 表示在位置 \\(i\\) 的标记为 \\(y_i\\) 并且从 \\(i+1\\) 到 \\(n\\) 的后部分标记序列的非规范化 概率。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"概率计算 按照前向-后向向量的定义, 很容易计算标记序列在位置 \\(i\\) 是标记 \\(y_i\\) 的条件概率 和在位置 \\(i-1\\) 与 \\(i\\) 是标记 \\(y_{i-1}\\) 和 \\(y_i\\) 的条件概率: \\[ P\\left(Y_i=y_i \\mid x\\right)=\\frac{\\alpha_i^{\\mathrm{T}}\\left(y_i \\mid x\\right) \\beta_i\\left(y_i \\mid x\\right)}{Z(x)} \\] \\[ P\\left(Y_{i-1}=y_{i-1}, Y_i=y_i \\mid x\\right)=\\frac{\\alpha_{i-1}^{\\mathrm{T}}\\left(y_{i-1} \\mid x\\right) M_i\\left(y_{i-1}, y_i \\mid x\\right) \\beta_i\\left(y_i \\mid x\\right)}{Z(x)} \\] 其中, \\[ Z(x)=\\alpha_n^{\\mathrm{T}}(x) \\mathbf{1}=1 \\beta_1(x) \\] ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:5:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"预测问题 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"维特比算法 还是使用维特比算法。 \\[ \\begin{aligned} y^* \u0026=\\arg \\max_y P_w(y \\mid x) \\\\\\\\ \u0026=\\arg \\max_y \\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\\\\\\\ \u0026=\\arg \\max_y \\exp (w \\cdot F(y, x)) \\\\\\\\ \u0026=\\arg \\max_y(w \\cdot F(y, x)) \\end{aligned} \\] 于是, 条件随机场的预测问题成为求非规范化概率最大的最优路径问题 \\[ \\max_y(w \\cdot F(y, x)) \\] 这里, 路径表示标记序列。其中, \\[ \\begin{aligned} w \u0026=\\left(w_1, w_2, \\cdots, w_K\\right)^{\\mathrm{T}} \\\\\\\\ F(y, x) \u0026=\\left(f_1(y, x), f_2(y, x), \\cdots, f_K(y, x)\\right)^{\\mathrm{T}} \\\\\\\\ f_k(y, x) \u0026=\\sum_{i=1}^n f_k\\left(y_{i-1}, y_i, x, i\\right), \\quad k=1,2, \\cdots, K \\end{aligned} \\] 注意, 这时只需计算非规范化概率, 而不必计算概率, 可以大大提高效率。为了求解最 优路径, 写成如下形式: \\[ \\max_y \\sum_{i=1}^n w \\cdot F_i\\left(y_{i-1}, y_i, x\\right) \\] 其中, \\[ F_i\\left(y_{i-1}, y_i, x\\right)=\\left(f_1\\left(y_{i-1}, y_i, x, i\\right), f_2\\left(y_{i-1}, y_i, x, i\\right), \\cdots, f_K\\left(y_{i-1}, y_i, x, i\\right)\\right)^{\\mathrm{T}} \\] 是局部特征向量。 下面叙述维特比算法。首先求出位置 1 的各个标记 \\(j=1,2, \\cdots, m\\) 的非规范化概率: \\[ \\delta_1(j)=w \\cdot F_1\\left(y_0=\\text { start, } y_1=j, x\\right), \\quad j=1,2, \\cdots, m \\] 一般地, 由递推公式, 求出到位置 \\(i\\) 的各个标记 \\(l=1,2, \\cdots, m\\) 的非规范化概率的最 大值, 同时记录非规范化概率最大值的路径 \\[ \\begin{gathered} \\delta_i(l)=\\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\\\\\\\ \\Psi_i(l)=\\arg \\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\end{gathered} \\] 直到 \\(i=n\\) 时终止。这时求得非规范化概率的最大值为 \\[ \\operatorname{max}_y(w \\cdot F(y, x))=\\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\] 及最优路径的终点 \\[ y_n^* =\\arg \\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\] 由此最优路径终点返回, \\[ y_i^* =\\Psi_{i+1}\\left(y_{i+1}^* \\right), \\quad i=n-1, n-2, \\cdots, 1 \\] 求得最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)^{\\mathrm{T}}\\) 。 综上所述, 得到条件随机场预测的维特比算法。 (条件随机场预测的维特比算法) 输入: 模型特征向量 \\(F(y, x)\\) 和权值向量 \\(w\\), 观测序列 \\(x=\\left(x_1, x_2, \\cdots, x_n\\right)\\); 输出: 最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)\\) 。 (1) 初始化 \\[ \\delta_1(j)=w \\cdot F_1\\left(y_0=\\operatorname{start}, y_1=j, x\\right), \\quad j=1,2, \\cdots, m \\] 递推。对 \\(i=2,3, \\cdots, n\\) \\[ \\begin{gathered} \\delta_i(l)=\\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\\\\\\\ \\Psi_i(l)=\\arg \\max_{1 \\leqslant j \\leqslant m}\\left\\\\{\\delta_{i-1}(j)+w \\cdot F_i\\left(y_{i-1}=j, y_i=l, x\\right)\\right\\\\\\}, \\quad l=1,2, \\cdots, m \\end{gathered} \\] （3）终止 \\[ \\begin{gathered} \\max_y(w \\cdot F(y, x))=\\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\\\\\\\ y_n^* =\\arg \\max_{1 \\leqslant j \\leqslant m} \\delta_n(j) \\end{gathered} \\] 返回路径 \\[ y_i^* =\\Psi_{i+1}\\left(y_{i+1}^* \\right), \\quad i=n-1, n-2, \\cdots, 1 \\] 求得最优路径 \\(y^* =\\left(y_1^* , y_2^* , \\cdots, y_n^* \\right)\\_{\\text {。 }}\\) 自己的理解就是非规范化概率每个i代表时间步i，要对所有的\\(\\lambda_k t_k\\)和 \\(\\mu_ks_k\\) 进行筛选，找出符合条件的相加，这里要注意下标的理解。括号里的和HMM的类似，代表y的取值。 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:1","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"实例 这里使用维特比算法求解 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:6:2","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"参数学习问题 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:7:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["NLP","概率图模型"],"content":"总结 ","date":"2022-05-12","objectID":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/:8:0","tags":["NLP","概率图模型","马尔科夫网络","条件随机场"],"title":"条件随机场","uri":"/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。 这段叙述里面有很多需要解决的东西，比如如何计算误差率，如何得到弱学习器权重系数，如何更新样本权重，如何结合。 ","date":"2022-04-27","objectID":"/adaboost/:0:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"Adaboost 先看一下adaboost的算法流程 将初始化样本的权重为\\(\\frac{1}{m}\\)，那么这个样本权重到底怎么用呢，主要是用于在构建弱分类器时计算弱学习器在训练集上的误差时使用，也就是计算误差： \\[ \\epsilon_t = \\sum_{i=1}^mP(h_t(x_i) \\neq y_i) =\\sum_{i=1}^m D_{ti}I(h_t(x_i)\\neq y_i) \\] 如果弱分类器为决策树的话，肯定要选择误差最小的划分作为最佳划分，最小的误差最为本轮弱分类器的误差。实际上样本权重不参与训练，只是通过样本权重动态调节模型的关注重点，即通过计算误差，影响决策树的划分，来调节模型的关注重点，这就是样本权重的作用。 然后计算弱学习器权重\\(\\alpha_t\\)，更新样本权重，对于分类错误的样本权重上升，对于分类正确的样本权重下调，再除以规范化因子，确保仍然是一个分布，一般就是\\(D_t\\)的和作为规范化因子，下一轮的模型就更注意划分错误的这些样本点。这样每一轮迭代都会得到弱分类器系数和弱分类器，这里具体的计算过程可以参考李航的《统计学习方法》，迭代完成以后，按照公式将权重与弱分类器相乘后相加，最后经过信号函数得到最终的分类结果。 ## 优点 Adaboost作为分类器时，分类精度很高 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。 作为简单的二元分类器时，构造简单，结果可理解。 不容易发生过拟合 ","date":"2022-04-27","objectID":"/adaboost/:1:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"缺点 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。 ","date":"2022-04-27","objectID":"/adaboost/:2:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"另一种解释 上面叙述的是一般的Adaboost算法的解释，其实还有另外一种解释，就是adaboost为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。 那么什么是前向分步算法？ 输入: 训练数据集\\(T=\\{(x_1, y_1 ),(x_2, y_2), \\cdots,\\left(x_N, y_N\\right)\\}\\); 损失函数 \\(L(y, f(x))\\); 基函数集 \\(\\{b(x ; \\gamma)\\}\\); 输出: 加法模型 \\(f(x)\\) 。 初始化 \\(f_0(x)=0\\); 对 \\(m=1,2, \\cdots, M\\) 极小化损失函数 \\[ \\left(\\beta_m, \\gamma_m\\right)=\\arg \\min_{\\beta, \\gamma} \\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i ; \\gamma\\right)\\right) \\] 得到参数 \\(\\beta_m, \\gamma_m\\) 。 更新 \\[ f_m(x)=f_{m-1}(x)+\\beta_m b\\left(x ; \\gamma_m\\right) \\] 得到加法模型 \\[ f(x)=f_M(x)=\\sum_{m=1}^M \\beta_m b\\left(x ; \\gamma_m\\right) \\] 学过GBDT的话就感觉会很熟悉。实际上提升树和GBDT也都是使用的前向分步算法。 直接上结论吧，推导的感觉没有必要，毕竟都是boosting方法。具体证明的过程可以看李航老师的《统计学习方法》。 Adaboost算法是前向分步加法算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。 ","date":"2022-04-27","objectID":"/adaboost/:3:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"总结 总是要写个总结的，adaboost一种boosting方法，就是不断迭代调整权重，最后将每一步的结果加权求和，可以说是一种典型的boosting的方法。 ","date":"2022-04-27","objectID":"/adaboost/:4:0","tags":["Machine Learning","集成学习","Boosting","Adaboost"],"title":"Adaboost","uri":"/adaboost/"},{"categories":["算法题"],"content":"分割等和子集 ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:0:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/partition-equal-subset-sum/?utm_source=LCUS\u0026utm_medium=ip_redirect\u0026utm_campaign=transfer2china ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:1:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"思路： 典型的01背包问题，利用套路框架做即可 注意做了优化，把原本的二维dp降低了一维 ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:2:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["算法题"],"content":"代码： class Solution: def canPartition(self, nums: List[int]) -\u003e bool: if sum(nums) % 2: return False s = sum(nums) // 2 dp = [False for _ in range(s+1)] dp[0] = True for i in range(1,len(nums)+1): for j in range(s,nums[i-1]-1,-1): # 容量 dp[j] = dp[j] or dp[j-nums[i-1]] # 用了or操作符 return dp[s] 更一般的套路，定义二维数组，然后二维dp # i代表前i个物品,j代表背包容量。 class Solution: def canPartition(self, nums: List[int]) -\u003e bool: if len(nums) \u003c= 1: return False if sum(nums) % 2: return False s = sum(nums) // 2 dp = [[False for _ in range(s+1)] for _ in range(len(nums)+1)] for i in range(len(nums)+1): dp[i][0] = True # 背包容量为0时 永远都是满的 所以为true for i in range(1,len(nums)+1): # 物品个数 for j in range(1,s+1): # 背包容量，最大为总和的一半，也就是需要求的 if j - nums[i-1] \u003c 0: # 如果容量小于当前物品的重量 dp[i][j] = dp[i-1][j] else: dp[i][j] = dp[i-1][j] or dp[i-1][j-nums[i-1]] if dp[i][s]: # 剪枝 return True return dp[len(nums)][s] '''首先，由于i是从 1 开始的，而数组索引是从 0 开始的，所以第i个物品的重量应该是nums[i-1]，这一点不要搞混。 dp[i - 1][j-nums[i-1]]也很好理解：你如果装了第i个物品，就要看背包的剩余重量j - nums[i-1]限制下是否能够被恰好装满。 换句话说，如果j - nums[i-1]的重量可以被恰好装满，那么只要把第i个物品装进去，也可恰好装满j的重量；否则的话，重量j肯定是装不满的。''' ","date":"2022-03-25","objectID":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/:3:0","tags":["算法题","分割等和子集"],"title":"分割等和子集","uri":"/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86/"},{"categories":["面经"],"content":"kd树 knn算法就是用kd树实现的 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:0:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"二分查找 很简单 就不说了 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:1:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"BST 很简单 就不说了 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:2:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"多维数组 假设数组B为\\([[6, 2], [6, 3], [3, 5], [5, 0], [1, 2], [4, 9], [8, 1]]\\)，有一个元素x，我们要找到数组B中距离x最近的元素，应该如何实现呢？比较直接的想法是用数组B中的每一个元素与x求距离，距离最小的那个元素就是我们要找的元素。假设x = [1, 1]，那么用数组B中的所有元素与x求距离得到[5.0, 5.4, 4.5, 4.1, 1.0, 8.5, 7.0]，其中距离最小的是1，对应的元素是数组B中的[1, 2]，所以[1, 2]就是我们的查找结果。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:3:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"kd-tree ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:4:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"如何建立 你 1. 建立根节点； 选取方差最大的特征作为分割特征(或者根据深度选择) 选择该特征的中位数作为分割点； 将数据集中该特征小于中位数的传递给根节点的左儿子，大于中位数的传递给根节点的右儿子； 递归执行步骤2-4，直到所有数据都被建立到KD Tree的节点上为止。 不难看出，KD Tree的建立步骤跟BST是非常相似的，可以认为BST是KD Tree在一维数据上的特例。KD Tree的算法复杂度介于O(Log2(N))和O(N)之间。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:5:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"为什么选择方差最大的维度 数据分割后分散的比较开，主要是为了减少回溯时间，减少子树的访问。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:6:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"为什么选择中位数作为分割点 因为借鉴了BST，选取中位数，让左子树和右子树的数据数量一致，便于二分查找。 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:7:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["面经"],"content":"查找元素 从根节点出发进行查找，根据当前深度计算比较的特征维度，若目标节点的特征值小于当前节点的特征值则遍历左子树，否则遍历右子树 找到叶子结点后，将其暂时标记为当前最邻近的点 递归地向上回退，在回退时需要做： 如果当前节点与目标节点的距离更近，则更新最邻近节点为当前节点 如果当前节点对应特征与目标节点对应特征的值距离小于当前最小值时，进入当前节点的另一个子节点（因为刚刚从一个子节点遍历回来）进行查找（如果存在子节点的话），有可能存在更近的节点。否则的话继续向上回退。 回退到根节点结束。得到最邻近点。 class Node: def __init__(self, data, left=None, right=None): self.val = data self.left = left self.right = right class KDTree: def __init__(self, k): self.k = k def create_Tree(self, dataset, depth): if not dataset: return None mid_index = len(dataset) // 2 # 中位数索引 axis = depth % self.k # 选择的维度 sort_dataset = sorted(dataset, key=(lambda x: x[axis])) # 按照维度排序 mid_data = sort_dataset[mid_index] # 中位数索引对应的数据 cur_node = Node(mid_data) # 创建节点 left_data = sort_dataset[:mid_index] # 左子树数据 right_data = sort_dataset[mid_index+1:] # 右子树数据 cur_node.left = self.create_Tree(left_data, depth+1) # 递归创建左子树 cur_node.right = self.create_Tree(right_data, depth+1) # 递归创建右子树 # print(cur_node.val) return cur_node def search(self, tree, new_data): # kd树的搜索 self.near_node = None # 最近的节点 self.near_val = None # 最近的节点的值 def dfs(node, depth): if not node: return axis = depth % self.k # 当前深度对应选择的维度 if new_data[axis] \u003c node.val[axis]: # 如果新数据的维度值小于当前节点的维度值 dfs(node.left, depth+1) # 递归搜索左子树 else: dfs(node.right, depth+1) # 递归搜索右子树 # 到这就相当于到达了叶子节点 dist = self.distance(new_data, node.val) # 计算新数据与当前节点的距离 if not self.near_val or dist \u003c self.near_val: # 如果当前节点的距离小于最近的节点的距离 self.near_val = dist # 更新最近的节点的距离 self.near_point = node.val # 更新最近的节点的值 #判断是否要进入兄弟节点寻找 if abs(new_data[axis] - node.val[axis]) \u003c self.near_val: # 如果新数据的维度值与当前节点的维度值的差值小于最近的节点的距离，说明兄弟节点区域有可能存在更接近的值。 if new_data[axis] \u003c node.val[axis]: # 控制去兄弟节点而不是刚刚回溯来的节点。 dfs(node.right, depth+1) else: dfs(node.left, depth+1) dfs(tree, 0) return self.near_point def distance(self, point_1, point_2): res = 0 for i in range(self.k): res += (point_1[i] - point_2[i]) ** 2 return res ** 0.5 if __name__ == '__main__': data_set = [[2,3],[5,4],[9,6],[4,7],[8,1],[7,2]] new_data = [1,5] k = len(data_set[0]) kd_tree = KDTree(k) our_tree = kd_tree.create_Tree(data_set, 0) predict = kd_tree.search(our_tree, new_data) print('Nearest Point of {}: {}'.format(new_data,predict)) Nearest Point of [1, 5]: [2, 3] 借用一下别人画的解题过程 ## 参考 https://zhuanlan.zhihu.com/p/499241064#:~:text=kd%E6%A0%91%E7%94%A8%E4%BA%8E%E5%AF%B9k%E7%BB%B4,%E7%9A%84%E6%97%B6%E5%80%99%E9%9D%9E%E5%B8%B8%E8%80%97%E6%97%B6%E3%80%82 ","date":"2022-03-21","objectID":"/kd%E6%A0%91/:8:0","tags":["面经","kd树"],"title":"kd树","uri":"/kd%E6%A0%91/"},{"categories":["算法题"],"content":"最小公众前缀 leetcode上的简单题，最小公众前缀 有三种解法，一种常规，两种巧妙解法 # 最小公共前缀 #解1：常规解法 思路就是一个一个判断 先判断所有字符串第一个是否相同，不相同就返回，否则然后依次往后判断 def longestCommonPrefix1(strs): if len(strs) == 0: return '' if len(strs) == 1: return strs[0] minl=min([len(x) for x in strs]) #求最小长度 end = 0 while end \u003c minl: #判断是否到最小长度 for i in range(1,len(strs)): #以第一个字符串为基准 if strs[i][end] != strs[i-1][end]: #如果到end这里不再相等 则返回到end这里的字符串即最小公共前缀 return strs[0][:end] end+=1 return strs[0][:end] #常规方法容易想到 但是缺点是运行速度慢，从每次判断都要遍历所有字符串就可以看出 #解2: 通过ascii码来判断 #Python里字符串是可以比较的，按照ascII值排 def longestCommonPrefix2(strs): if not strs: return 0 s1 = max(strs) s2 = min(strs) #找出s1 s2的最小公共前缀即为整个列表的最小公共前缀 for i,s in enumerate(s2): if s1[i] != s: return s1[:i] return s2 #通过max 和 min 函数来找到列表里面最大最小的两个字符串 然后找到这两个字符串的最小公共前缀。 #解3：通过python语法糖 将每个字符串的每个对应字符串存为一组，用zip函数，比如说所有的字符串第一个存在一起，然后用set去重，如果留下了一个，则说明都重复了，则就是相同的 def longestCommonPrefix3(strs): if not strs: return 0 cc = list(map(set,zip(*strs))) #为什么用map呢 因为要对zip压缩后的每一个序列去重 res = '' #结果 for i,s in enumerate(cc): x = list(s) if len(x) \u003e 1: #如果长度大于1 说明有不一样的 则直接退出 break res += x[0] return res 如上！ ","date":"2022-03-20","objectID":"/%E6%9C%80%E5%B0%8F%E5%85%AC%E4%BC%97%E5%89%8D%E7%BC%80/:0:0","tags":["算法题","最小公众前缀"],"title":"最小公众前缀","uri":"/%E6%9C%80%E5%B0%8F%E5%85%AC%E4%BC%97%E5%89%8D%E7%BC%80/"},{"categories":["Machine Learning","回归算法"],"content":"参考：https://cuijiahua.com/blog/2017/12/ml_13_regtree_1.html ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"1、ID3算法的弊端 回忆一下，决策树的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。 除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征离散化，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在特性。 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"2、CART算法 与ID3算法相反，CART算法正好适用于连续型特征。CART算法使用二元切分法来处理连续型变量。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。 CART算法有两步： 决策树生成：递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义”最好”： 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。 决策树剪枝我们先不管，我们看下决策树生成。 在决策树的文章中，我们先根据信息熵的计算找到最佳特征切分数据集构建决策树。CART算法的决策树生成也是如此，实现过程如下： 使用CART算法选择特征 根据特征切分数据集合 构建树 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"推导 选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小 值时的(j,s)对。其中Rm是被划分的输入空间， \\(\\mathrm{cm}\\) 是空间Rm对应的固定输出值。 \\[ \\min_{j, s}\\left[\\min_{c_{1}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min_{c_{2}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}\\right] \\] 用选定的(j,s)对，划分区域并决定相应的输出值 \\[ \\begin{gathered} R_{1}(j, s)={x \\mid x^{(j)} \\leq s}, R_{2}(j, s)={x \\mid x^{(j)}\u003es} \\\\\\\\ \\hat{c}_{m}=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}(j, s)} y_{i} \\\\\\\\ x \\in R_{m}, m=1,2 \\end{gathered} \\] 继续对两个子区域调用上述步骤，将输入空间划分为 \\(M\\) 个区域R1,R2,..,Rm，生成决策树。 \\[ f(x)=\\sum_{m=1}^{M} \\hat{c}_{m} I\\left(x \\epsilon R_{m}\\right) \\] 当输入空间划分确定时，可以用平方误差来表示回归树对于训练数据的预测方法，用平方误差最小 的准则求解每个单元上的最优输出值。 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:2:1","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"实例 【机器学习】回归决策树-CSDN博客 ","date":"2022-03-16","objectID":"/%E6%A0%91%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","回归算法","树回归"],"title":"树回归","uri":"/%E6%A0%91%E5%9B%9E%E5%BD%92/"},{"categories":["算法题"],"content":"搜索旋转排序数组 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:0:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/search-in-rotated-sorted-array/ ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:1:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"思路： 明显的二分查找，不过不是有序数组了，而是部分有序，所以需要有判断 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:2:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"代码： class Solution(object): def search(self, nums, target): left, right = 0, len(nums) - 1 while left \u003c= right: mid = left + (right - left) // 2 if nums[mid] == target: return mid if nums[mid] \u003c nums[right]:#右边为升序 if nums[mid] \u003c target \u003c= nums[right]: left = mid + 1 else: right = mid if nums[left] \u003c= nums[mid]:#左边为升序 if nums[left] \u003c= target \u003c nums[mid]: right = mid else: left = mid + 1 return -1 ","date":"2022-03-13","objectID":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/:3:0","tags":["算法题","搜索旋转排序数组"],"title":"搜索旋转排序数组","uri":"/%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84/"},{"categories":["pandas"],"content":"pandas补充学习 推荐网站：http://joyfulpandas.datawhale.club/Content/Preface.html pandas核心操作手册：https://mp.weixin.qq.com/s/l1V5e726XixI0W3EDHx0Nw ","date":"2022-02-22","objectID":"/some_api/:0:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.join和pd.merge 可以说merge包含了join操作，merge支持两个df间行方向或列方向的拼接操作，默认列拼接，取交集，而join只是简化了merge的行拼接的操作 pandas的merge方法提供了一种类似于SQL的内存链接操作，官网文档提到它的性能会比其他开源语言的数据操作（例如R）要高效。 如果对于sql比较熟悉的话，merge也比较好理解。 merge的参数 on：列名，join用来对齐的那一列的名字，用到这个参数的时候一定要保证左表和右表用来对齐的那一列都有相同的列名。 left_on：左表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 right_on：右表对齐的列，可以是列名，也可以是和dataframe同样长度的arrays。 left_index/ right_index: 如果是True的haunted以index作为对齐的key how：数据融合的方法。 sort：根据dataframe合并的keys按字典顺序排序，默认是，如果置false可以提高表现。 简单举一个刚刚在比赛中里面用到的例子 data = pd.merge(data1, data2, on=\"carid\", how=\"inner\") # 根据carid合并两个数据集 可以用pd.merge，也可以用dataframe.merge，更多的信息可以查阅官方API。 ## pd.concat 也是合并dataframe 用法： pd.concat([df1, df2]) # 纵向合并 pd.concat([df1, df2], axis=1) # 横向合并 参数 - ignore_index=True，重新设置合并后的dataframe对象的index值 - sort=False，列的顺序保持原样 - join : {“inner”, “outer”}，默认为outer。 ## pd.append # 语法结构 df.append(self, other, ignore_index=False, verify_integrity=False, sort=False) other 是它要追加的其他 DataFrame 或者类似序列内容 ignore_index 如果为 True 则重新进行自然索引 verify_integrity 如果为 True 则遇到重复索引内容时报错 sort 进行排序 ","date":"2022-02-22","objectID":"/some_api/:1:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"同结构 将同结构的数据追加在原数据后面 ### 不同结构 没有的列会增加，没有的相应内容为空。 ### 可以合并追加多个 result = df1.append([df2, df3]) ","date":"2022-02-22","objectID":"/some_api/:1:1","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"追加序列 s2 = pd.Series(['X0', 'X1', 'X2', 'X3'], index=['A', 'B', 'C', 'D']) result = df1.append(s2, ignore_index=True) ","date":"2022-02-22","objectID":"/some_api/:1:2","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"追加字典列表 dicts = [{'A': 1, 'B': 2, 'C': 3, 'X': 4}, {'A': 5, 'B': 6, 'C': 7, 'Y': 8}] result = df1.append(dicts, ignore_index=True, sort=False) ","date":"2022-02-22","objectID":"/some_api/:1:3","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.rename DataFrame.rename(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors=‘ignore’) 作用就是修改index或者columns的名字。使用时可以指定mapper，然后指定axis，默认axis=0，即修改index 也可以使用index=xxx,columns=xxx，同时进行修改。 ","date":"2022-02-22","objectID":"/some_api/:2:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.get_dummies 用于构造离散数据的独热编码 用法 training = pd.get_dummies(train_data, columns=[\"xxx\", \"xx\", \"x\"]) # 对xxx、xx、x这三列进行onehot编码 ","date":"2022-02-22","objectID":"/some_api/:3:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"pd.melt 直观的看就是将宽数据转化为长数据。转化为variable-value这样的形式。 pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) 参数解释： frame:要处理的数据集。 id_vars:不需要被转换的列名。 value_vars:需要转换的列名，如果剩下的列全部都要转换，就不用写了。 var_name和value_name是自定义设置对应的列名。 col_level :如果列是MultiIndex，则使用此级别。 常常与seaborn的FacetGrid一起进行操作，示例： f = pd.melt(train_data, value_vars=numeric_features) g = sns.FacetGrid(f, col=\"variable\", col_wrap=2, sharex=False, sharey=False) # col_warp限制一行只有两个，sharx、sharey默认为True,与matplotlib.pyplot.subplots相反。 g = g.map(sns.distplot, \"value\") # 第一个参数可以自定义函数。 ","date":"2022-02-22","objectID":"/some_api/:4:0","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["pandas"],"content":"sns.pairplot seaborn一般与pandas的数据结合，因此不分开做笔记了。 根据官方文档，sns.pairplot是在数据集中绘制成对关系。用来展现变量两两之间的关系，比如线性、非线性、相关等等。 hue参数可以指定分类。与其它plot的hue参数一致。 一般都会使用参数diag_kind=\"kde\"，因为对角线上的变量x与y都是一样的，探究关系没有意义，因此展示核密度分布。 seaborn.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='hist', markers=None, size=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None)¶ 数据指定： \u003e vars : 与data使用，否则使用data的全部变量。参数类型：numeric类型的变量list。 {x, y}_vars : 与data使用，否则使用data的全部变量。参数类型：numeric类型的变量list。 dropna : 是否剔除缺失值。参数类型：boolean, optional 特殊参数： \u003e kind : {‘scatter’, ‘reg’}, optional Kind of plot for the non-identity relationships. diag_kind : {‘hist’, ‘kde’}, optional。Kind of plot for the diagonal subplots. 基本参数： \u003e size : 默认 6，图的尺度大小（正方形）。参数类型：numeric hue : 使用指定变量为分类变量画图。参数类型：string (变量名) hue_order : list of strings Order for the levels of the hue variable in the palette palette : 调色板颜色 markers : 使用不同的形状。参数类型：list aspect : scalar, optional。Aspect * size gives the width (in inches) of each facet. {plot, diag, grid}_kws : 指定其他参数。参数类型：dicts ","date":"2022-02-22","objectID":"/some_api/:4:1","tags":["pandas","learn_four"],"title":"learn_four","uri":"/some_api/"},{"categories":["Machine Learning","分类算法"],"content":"线性判别分析(LDA) 线性判别分析，也就是LDA（与主题模型中的LDA区分开），现在常常用于数据的降维中，但从它的名字中可以看出来它也是一个分类的算法，而且属于硬分类，也就是结果不是概率，是具体的类别 ## 主要思想 1. 类内方差小 2. 类间方差大 ## 推导 这里以二类为例，即只有两个类别。 首先是投影，我们假定原来的数据是向量 \\(x\\)，那么顺着 \\(w\\) 方向的投影就是标量： \\[ z=w^T\\cdot x(=|w|\\cdot|x|\\cos\\theta) \\] 对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 \\(N_1\\)和 \\(N_2\\)，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用 \\(S\\) 表示原数据的协方差： \\[ \\begin{align} C_1:Var_z[C_1]\u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(z_i-\\bar{z_{c1}})(z_i-\\bar{z_{c1}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\\frac{1}{N_1}\\sum\\limits_{j=1}^{N_1}w^Tx_j)^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}(x_i-\\bar{x_{c1}})(x_i-\\bar{x_{c1}})^Tw\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^TS_1w\\\\\\\\\\\\\\\\ C_2:Var_z[C_2]\u0026=\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}(z_i-\\bar{z_{c2}})(z_i-\\bar{z_{c2}})^T\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^TS_2w \\end{align} \\] 所以类内距离为： \\[ \\begin{align} Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w \\end{align} \\] 对于第二点类间距离，我们可以用两类的均值表示这个距离： \\[ \\begin{align} (\\bar{z_{c1}}-\\bar{z_{c2}})^2\u0026=(\\frac{1}{N_1}\\sum\\limits_{i=1}^{N_1}w^Tx_i-\\frac{1}{N_2}\\sum\\limits_{i=1}^{N_2}w^Tx_i)^2\\nonumber\\\\\\\\\\\\\\\\ \u0026=(w^T(\\bar{x_{c1}}-\\bar{x_{c2}}))^2\\nonumber\\\\\\\\\\\\\\\\ \u0026=w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw \\end{align} \\] 合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值： \\[ \\begin{align} \\hat{w}=\\mathop{argmax}\\limits_wJ(w)\u0026=\\mathop{argmax}\\limits_w\\frac{(\\bar{z_{c1}}-\\bar{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^T(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw}{w^T(S_1+S_2)w}\\nonumber\\\\\\\\\\\\\\\\ \u0026=\\mathop{argmax}\\limits_w\\frac{w^TS_bw}{w^TS_ww} \\end{align} \\] 这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对w的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了： \\[ \\begin{aligned} \u0026\\frac{\\partial}{\\partial w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\\nonumber\\\\\\\\\\\\\\\\ \u0026\\Longrightarrow w\\propto S_w^{-1}S_bw=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})(\\bar{x_{c1}}-\\bar{x_{c2}})^Tw\\propto S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}}) \\end{aligned} \\] 也就是说最后我们的结果就是\\(w=S_w^{-1}(\\bar{x_{c1}}-\\bar{x_{c2}})\\) 可以归一化求得单位的w值。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:0:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"多类情况 前面的很容易类比二类的情况，现在的目标函数变成了： \\[ \\frac{W^TS_bW}{W^TS_wW} \\] 现在的问题就是这些都是矩阵，不能像上面那样直接优化，需要替换优化目标。 \\[ \\underbrace{arg\\;max}_W\\;\\;J(W) = \\frac{\\prod\\limits_{diag}W^TS_bW}{\\prod\\limits_{diag}W^TS_wW} \\] 其中 \\(\\prod_{diag}A\\)为A的主对角线元素的乘积,W为\\(n \\times d\\)的矩阵，n为原来的维度，d为映射到超平面的维度，则最终的目标就变成了： \\[ J(W) = \\frac{\\prod\\limits_{i=1}^dw_i^TS_bw_i}{\\prod\\limits_{i=1}^dw_i^TS_ww_i} = \\prod\\limits_{i=1}^d\\frac{w_i^TS_bw_i}{w_i^TS_ww_i} \\] 根据广式瑞利商，最大值是矩阵\\(S_w^{-1}S_b\\)的最大特征值,最大的d个值的乘积就是矩阵的 \\(S_w^{-1}S_b\\) 最大的d个特征值的乘积,此时对应的矩阵\\(W\\)为这最大的d个特征值对应的特征向量张成的矩阵。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:1:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"总结 LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。 实际上LDA除了可以用于降维以外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。 我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:2:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"代码 mean_list = [] for i in range(2): mean_list.append(np.mean(X_train[y_train==i], axis=0)) mean_list = np.array(mean_list) S_W = np.zeros((X_train.shape[1], X_train.shape[1])) # 类内散度矩阵 for c, mv in zip(range(2), mean_list): class_scatter = np.zeros((X_train.shape[1], X_train.shape[1])) for row in X_train[y_train==c]: row, mv = row.reshape(X_train.shape[1], -1), mv.reshape(X_train.shape[1], -1) class_scatter += (row-mv).dot((row-mv).T) S_W += class_scatter over_all_mean = np.mean(X_train, axis=0) S_B = np.zeros((X_train.shape[1], X_train.shape[1])) # 类间散度矩阵 for i, mean_vec in enumerate(mean_list): n = X_train[y_train==i, :].shape[0] mean_list_temp = mean_list[i, :].reshape(1, -1) over_all_mean = over_all_mean.reshape(X_train.shape[1], 1) S_B += n*(mean_vec-over_all_mean).dot((mean_vec-over_all_mean).T) eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))] eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True) # eigv_sum = sum(eig_vals) # for i, j in enumerate(eig_pairs): # print('eigenvalue {0:}: {1:.2%}'.format(i + 1, (j[0] / eigv_sum).real)) # 根据百分比显示特征值，从而选取最大的n个特征值 W = np.hstack((eig_pairs[0][1].reshape(X_train.shape[1], 1), eig_pairs[1][1].reshape(X_train.shape[1], 1))) ","date":"2022-02-19","objectID":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/:3:0","tags":["Machine Learning","分类算法","线性判别分析"],"title":"线性判别分析","uri":"/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"},{"categories":["Machine Learning","分类算法"],"content":"条件概率 \\(P(B|A) = \\frac{P(AB)}{P(A)}\\) ","date":"2022-02-16","objectID":"/bayes/:1:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"乘法法则 如果P(A) \u003e 0 \\(P(AB) = P(A)P(B|A)\\) 如果\\(P(A_1 \\dots A_{n-1})\\) \u003e 0 则 \\[ \\begin{aligned} P(A_1A_2\\dots A_n) = P(A_1A_2\\dots A_{n-1})P(A_n | A_1A_2\\dots A_{n-1}) \\\\\\\\ = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\\dots P(A_n|A_1A_2\\dots A_{n-1}) \\end{aligned} \\] 其中第一步使用了乘法公式，然后再对前者继续使用乘法公式，以此类推，就可以得到最后的结果。 ","date":"2022-02-16","objectID":"/bayes/:2:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"全概率公式(加法法则) \\[ P(A) = \\sum_{i=1}^n P(B_i)P(A\\lvert B_i) = \\sum_{i=1}^n P(AB_i) \\] 如果是连续变量则为 \\[ P(A) = \\int P(A,B) \\, dB \\] （加法规则与乘法规则结合是一些推导的基础，注意连续中的积分等同于离散中的连加） 特例为: \\[ P(A)=P(A\\lvert B)P(B) + P(A\\lvert \\bar{B})P(\\bar{B}) \\] 全概率公式的意义： 将复杂的事件A划分为较为简单的事件 \\[ AB_1,AB_2,\\ldots,AB_n \\] 再结合加法公式和乘法公式计算出A的概率 ## 贝叶斯公式 先引入一个小例子。 \\[ P(X=玩LOL)=0.6;\\\\\\\\ P(X=不玩LOL)=0.4 \\] 这个概率是根据统计得到或者根据自身经验给出的一个概率值，我们称之为先验概率(prior probability) 此外 \\[ P(Y=男性\\lvert X=玩LOL)=0.8,\\quad P(Y=小姐姐\\vert X=玩LOL)=0.2\\\\\\\\ P(Y=男性\\lvert X=不玩LOL)=0.2，\\quad P(Y=小姐姐\\vert X=不玩LOL)=0.8 \\] 求在已知玩家为男性的情况下，他是LOL玩家的概率是多少： 根据贝叶斯准则 \\[ P(X=玩LOL\\lvert Y=男性)=P(Y=男性\\lvert X=玩LOL)\\frac{P(X=玩LOL)}{[P(Y=男性\\lvert X=玩LOL)P(X=玩LOL)+P(Y=男性\\lvert X=不玩LOL)]P(X=不玩LOL)} \\] 分母为全概率公式 下面是贝叶斯公式的推导。 \\[ P(B\\lvert A)=\\frac{P(AB)}{P(A)}=\\frac{P(BA)}{P(A)}\\iff \\frac{P(B)P(A\\lvert B)}{\\displaystyle \\sum_{j=1}^n P(B_j)P(A\\lvert B_j)} \\] 贝叶斯公式的意义： 在事件A已经发生的条件下，贝叶斯公式可用来寻找导致A发生各种“原因”Bi的概率。 对于先验概率和后验概率来说， \\[ \\begin{aligned} P(B\\lvert A)为后验概率 \\\\\\\\ P(B)和P(A)为先验概率 \\\\\\\\ P(A\\vert B)为可能性 \\end{aligned} \\] ","date":"2022-02-16","objectID":"/bayes/:3:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"介绍 朴素贝叶斯属于生成式模型， 其主要用于分类，属于是最简单的概率图模型，主要用到概率论中学到的贝叶斯公式，其中需要对模型进行假设，即贝叶斯假设。 ","date":"2022-02-16","objectID":"/bayes/:4:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"贝叶斯假设 条件独立性假设(最简单的概率图模型(有向图))，目的是简化计算 ","date":"2022-02-16","objectID":"/bayes/:5:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"推导 对于数据集\\(\\\\{(x_i, y_i)\\\\}^N_{i=1}\\)，\\(x_i \\in R^p , \\quad y_i \\in \\\\{ 0, 1\\\\}\\) \\[ \\begin{aligned} \\hat{y} \u0026= \\arg \\max(y|X) \\\\\\\\ \u0026 = \\arg \\max\\frac{P(X,y)}{P(X)} \\\\\\\\ \u0026 = \\arg \\max\\frac{P(y)P(X|y)}{P(X)} \\\\\\\\ \u0026 = \\arg \\max(y) P(X|y) \\\\\\\\ \u0026 = \\arg \\max(y)P(x_1,x_2,\\dots x_p| y) \\end{aligned} \\] 其中由于我们的条件独立性假设，因此\\(P(X|y)\\)可以写为\\(\\prod_{j=1}^pP(x_j|y)\\) 即最终的式子就是 \\[ \\hat{y} = \\arg \\max(y)\\prod_{j=1}^p P(x_j|y) \\] 这就是朴素贝叶斯的主要推导。 注意术语： \\(P(y)\\)为先验概率 \\(P(y|X)\\)为后验概率 \\(P(X,y)\\)为联合概率 MAP，即最大后验估计，选择有最高后验概率的类。 ","date":"2022-02-16","objectID":"/bayes/:6:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"后验概率最大化的含义 （来自李航《统计学习方法》,比西瓜书上的更容易理解） ","date":"2022-02-16","objectID":"/bayes/:7:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"极大似然估计 在朴素贝叶斯法中, 学习意味着估计 \\(P\\left(Y=c_{k}\\right)\\) 和 \\(P\\left(X^{(j)}=x^{(j)} \\mid Y=c_{k}\\right)\\) 。可以 应用极大似然估计法估计相应的概率。先验概率 \\(P\\left(Y=c_{k}\\right)\\) 的极大似然估计是 \\[ P\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)}{N}, \\quad k=1,2, \\cdots, K \\] 设第 \\(j\\) 个特征 \\(x^{(j)}\\) 可能取值的集合为 \\(\\left\\{a_{j 1}, a_{j 2}, \\cdots, a_{j S_{j}}\\right\\}\\), 条件概率 \\(P\\left(X^{(j)}=a_{j l} \\mid Y=\\right.\\) \\(c_{k}\\) ) 的极大似然估计是 \\[ \\begin{aligned} \u0026P\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\\right)}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)} \\\\\\\\ \u0026j=1,2, \\cdots, n ; \\quad l=1,2, \\cdots, S_{j} ; \\quad k=1,2, \\cdots, K \\end{aligned} \\] 式中, \\(x_{i}^{(j)}\\) 是第 \\(i\\) 个样本的第 \\(j\\) 个特征; \\(a_{j l}\\) 是第 \\(j\\) 个特征可能取的第 \\(l\\) 个值; \\(I\\) 为指 示函数。 \\(S_j\\)为\\(x^{(j)}\\)的可能取值数，\\(K\\)为类别数。 ","date":"2022-02-16","objectID":"/bayes/:8:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"拉普拉斯平滑 用极大似然估计可能会出现所要估计的概率值为 0 的情况。这时会影响到后验概 率的计算结果, 使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地, 条 件概率的贝叶斯估计是 \\[ P_{\\lambda}\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\\right)+\\lambda}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+S_{j} \\lambda} \\] 式中 \\(\\lambda \\geqslant 0\\) 。等价于在随机变量各个取值的频数上赋予一个正数 \\(\\lambda\u003e0\\) 。当 \\(\\lambda=0\\) 时就 是极大似然估计。常取 \\(\\lambda=1\\), 这时称为拉普拉斯平滑 (Laplacian smoothing)。显然, 对任何 \\(l=1,2, \\cdots, S_{j}, k=1,2, \\cdots, K\\), 有 \\[ \\begin{aligned} \u0026P_{\\lambda}\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)\u003e0 \\\\\\\\ \u0026\\sum_{l=1}^{S_{j}} P\\left(X^{(j)}=a_{j l} \\mid Y=c_{k}\\right)=1 \\end{aligned} \\] 同样, 先验概率的贝叶斯估计是 \\[ P_{\\lambda}\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+\\lambda}{N+K \\lambda} \\] ","date":"2022-02-16","objectID":"/bayes/:9:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"文本分类 接下来是朴素贝叶斯在文本分类中的运用，这里以简单的二分类问题，情感分析为例。 ","date":"2022-02-16","objectID":"/bayes/:10:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"如何定义几个概率？ \\(P(y=k)\\)很容易得到，可以只评估带有标签k的文档比例，即 \\[ P(y=k) = \\frac{N(y=k)}{\\sum_iN(y=i)} \\] \\(P(x|y=k)= P(x_1,x_2,\\dots, x_n | y=k)\\) 这里假设文档x被表示为一组特征，例如一组它的词\\((x_1,x_2,\\dots, x_n)\\) 这里需要两个假设，其中一个是上面提到的贝叶斯假设，即： - 条件独立假设：特征在给定类的情况下是独立的 - Bag of Words假设：词序无关紧要 直观地说，假设 每个单词出现在类别为k的文档中的概率不依赖上下文，因此得到： \\[ P(x|y=k) = P(x_1,x_2,\\dots,x_n|y=k) = \\prod_{t=1}^nP(x_t|y=k) \\] 概率\\(P(x_i|y=k)\\)为单词\\(x_i\\)出现在标签为k的文档中的频率，即 \\[ P(x_i|y=k) = \\frac{N(x_i, y=k)}{\\sum_{t=1}^{|V|}N(x_t,y=k)} \\] 但是有个问题就是有可能会出现\\(N(x_i, y=k)=0\\)的情况 这时就需要拉普拉斯平滑，即在所有的计数中都加入一个新的参数\\(\\delta\\)， \\[ P(x_i|y=k)=\\frac{ {\\delta} + N(x_i, y=k) }{\\sum\\limits_{t=1}^{|V|}( {\\delta} + N(x_t, y=k))} = \\frac{ {\\delta} + N(x_i, y=k) }{ {\\delta\\cdot |V|} + \\sum\\limits_{t=1}^{|V|} N(x_t, y=k)} , \\] 直观地说，朴素贝叶斯期望某些词作为类指示符。例如，对于情感分类标记 awesome、 brilliant、 great 将有更高的概率给定正面类别然后负面类别。 类似地，给定负类比正类 ，标记awful, boring, bad的概率更高。 在实践中，一般都是取log，单调性不变，变为\\(\\log(x, y=k) = \\log P(y=k) + \\sum \\log P(x_i|y=k)\\) ","date":"2022-02-16","objectID":"/bayes/:10:1","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["Machine Learning","分类算法"],"content":"补充：贝叶斯估计 易知\\(P(\\theta \\mid D)\\)称为后验概率，有三种估计\\(\\theta\\)的方法： 使用后验分布的密度函数最大值点作为\\(\\theta\\)的点估计的最大后验估计（MAP）。 使用后验分布的中位数作为\\(\\theta\\)的点估计的后验中位数估计（不常用）。 使用后验分布的均值作为\\(\\theta\\)的点估计的后验期望估计。 其中后验期望估计也就是贝叶斯估计。 贝叶斯估计是在MAP上做进一步拓展，不直接估计参数的值，而是允许参数服从一定的概率密度分布，先求出\\(\\theta\\)的后验分布\\(p(\\theta \\mid x)\\)，然后求出\\(\\theta\\)的期望值。 ","date":"2022-02-16","objectID":"/bayes/:11:0","tags":["Machine Learning","分类算法","bayes"],"title":"bayes","uri":"/bayes/"},{"categories":["pandas","api"],"content":"正为逆时针转，负为顺时针转。 import numpy as np mat = np.array([[1,3,5], [2,4,6], [7,8,9] ]) print mat, \"# orignal\" mat90 = np.rot90(mat, 1) print mat90, \"# rorate 90 \u003cleft\u003e anti-clockwise\" mat90 = np.rot90(mat, -1) print mat90, \"# rorate 90 \u003cright\u003e clockwise\" mat180 = np.rot90(mat, 2) print mat180, \"# rorate 180 \u003cleft\u003e anti-clockwise\" mat270 = np.rot90(mat, 3) print mat270, \"# rorate 270 \u003cleft\u003e anti-clockwise\" 直接复制的代码，python2，能看懂就行。 ","date":"2022-02-12","objectID":"/rot90/:0:0","tags":["pandas","api","rot90"],"title":"rot90","uri":"/rot90/"},{"categories":["比赛相关"],"content":"数据挖掘比赛 ","date":"2022-01-26","objectID":"/eda/:0:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"对赛题进行理解 ","date":"2022-01-26","objectID":"/eda/:1:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"数据分析 ","date":"2022-01-26","objectID":"/eda/:2:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"EDA目标 EDA的价值在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用。 当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系。 引导数据科学从业者进行数据处理以及特征工程的步骤,使数据集的结构和特征集让接下来的预测问题更加可靠。 完成对于数据的探索性分析，并对于数据进行一些图表或者文字总结并打卡。 ","date":"2022-01-26","objectID":"/eda/:2:1","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要操作 载入各种数据科学以及可视化库: 数据科学库 pandas、numpy、scipy； 可视化库 matplotlib、seabon； 其他； 载入数据： 载入训练集和测试集； 简略观察数据(head()+shape)； 数据总览: 通过describe()来熟悉数据的相关统计量 通过info()来熟悉数据类型 判断数据缺失和异常 查看每列的存在nan情况 异常值检测 了解预测值的分布 总体分布概况（无界约翰逊分布等） 查看skewness and kurtosis 查看预测值的具体频数 特征分为类别特征和数字特征，并对类别特征查看unique分布 数字特征分析 相关性分析 查看几个特征得 偏度和峰值 每个数字特征得分布可视化 数字特征相互之间的关系可视化 多变量互相回归关系可视化 类型特征分析 unique分布 类别特征箱形图可视化 类别特征的小提琴图可视化 类别特征的柱形图可视化类别 特征的每个类别频数可视化(count_plot) 用pandas_profiling生成数据报告 ","date":"2022-01-26","objectID":"/eda/:2:2","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要步骤 对于数据的初步分析（直接查看数据，或.sum(), .mean()，.descirbe()等统计函数）可以从：样本数量，训练集数量，是否有时间特征，是否是时许问题，特征所表示的含义（非匿名特征），特征类型（字符类似，int，float，time），特征的缺失情况（注意缺失的在数据中的表现形式，有些是空的有些是”NAN”符号等），特征的均值方差情况。 分析记录某些特征值缺失占比30%以上样本的缺失处理，有助于后续的模型验证和调节，分析特征应该是填充（填充方式是什么，均值填充，0填充，众数填充等），还是舍去，还是先做样本分类用不同的特征模型去预测。 对于异常值做专门的分析，分析特征异常的label是否为异常值（或者偏离均值较远或者事特殊符号）,异常值是否应该剔除，还是用正常值填充，是记录异常，还是机器本身异常等。 对于Label做专门的分析，分析标签的分布情况等。 进步分析可以通过对特征作图，特征和label联合做图（统计图，离散图），直观了解特征的分布情况，通过这一步也可以发现数据之中的一些异常值等，通过箱型图分析一些特征值的偏离情况，对于特征和特征联合作图，对于特征和label联合作图，分析其中的一些关联性。 记录自己之前没用到的东西 ### 数据的偏度和峰度 - 数据的偏度(skewness)：dataframe.skew() - 数据的峰度(kurtosis)：dataframe.kurt() ### log变换 一般要求预测值需要符合正态分布，因此需要先log变换一下 ### sns.pairplot 用来展现变量两两之间的关系，比如线性、非线性、相关 hue参数可以指定分类。 ","date":"2022-01-26","objectID":"/eda/:2:3","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"sns.heatmap 热度图，可以直观感受变量之间的相关程度。 更多的处理方法可以看pandas笔记 ","date":"2022-01-26","objectID":"/eda/:2:4","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"特征工程 ","date":"2022-01-26","objectID":"/eda/:3:0","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["比赛相关"],"content":"主要步骤 异常处理： 通过箱线图（或 3-Sigma）分析删除异常值； BOX-COX 转换（处理有偏分布）； 长尾截断； 特征归一化/标准化： 标准化（转换为标准正态分布）； 归一化（抓换到 [0,1] 区间）； 针对幂律分布，可以采用公式：\\(log(\\frac{1+x}{1+median})\\) 数据分桶： 等频分桶； 等距分桶； Best-KS 分桶（类似利用基尼指数进行二分类）； 卡方分桶； 缺失值处理： 不处理（针对类似 XGBoost 等树模型）； 删除（缺失数据太多）； 插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等； 分箱，缺失值一个箱； 特征构造： 构造统计量特征，报告计数、求和、比例、标准差等； 时间特征，包括相对时间和绝对时间，节假日，双休日等； 地理信息，包括分箱，分布编码等方法； 非线性变换，包括 log/ 平方/ 根号等； 特征组合，特征交叉； 仁者见仁，智者见智。 特征筛选： 过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择发/相关系数法/卡方检验法/互信息法； 包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ； 嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归； 降维： PCA/LDA/LCA;本博客都有介绍 特征选择也是降维 ## 建模调参 ## 模型融合 ","date":"2022-01-26","objectID":"/eda/:3:1","tags":["比赛相关","数据挖掘比赛"],"title":"数据挖掘比赛","uri":"/eda/"},{"categories":["面经"],"content":"什么是前缀树？ 前缀树是N叉树的一种特殊形式。通常来说，一个前缀树是用来存储字符串的。前缀树的每一个节点代表一个字符串（前缀）。每一个节点会有多个子节点，通往不同子节点的路径上有着不同的字符。子节点代表的字符串是由节点本身的原始字符串，以及通往该子节点路径上所有的字符组成的。 在上图示例中，我们在节点中标记的值是该节点对应表示的字符串。例如，我们从根节点开始，选择第二条路径 ‘b’，然后选择它的第一个子节点 ‘a’，接下来继续选择子节点 ‘d’，我们最终会到达叶节点 “bad”。节点的值是由从根节点开始，与其经过的路径中的字符按顺序形成的。 值得注意的是，根节点表示空字符串。 前缀树的一个重要的特性是，节点所有的后代都与该节点相关的字符串有着共同的前缀。这就是前缀树名称的由来。 我们再来看这个例子。例如，以节点 “b” 为根的子树中的节点表示的字符串，都具有共同的前缀 “b”。反之亦然，具有公共前缀 “b” 的字符串，全部位于以 “b” 为根的子树中，并且具有不同前缀的字符串来自不同的分支。 前缀树有着广泛的应用，例如自动补全，拼写检查等等。 ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:1:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["面经"],"content":"代码 import collections class TrieNode(object): # 定义节点 # Initialize your data structure here. def __init__(self): self.node = collections.defaultdict(TrieNode) self.char = \"\" self.is_word = False @property def data(self): return self.node def __getitem__(self, key): return self.node[key] def __str__(self) -\u003e str: return self.char __repr__ = __str__ class Trie(object): def __init__(self): \"\"\" Initialize your data structure here. \"\"\" self.root = TrieNode() def insert(self, word): \"\"\" Inserts a word into the trie. :type word: str :rtype: void \"\"\" node = self.root for chars in word: temp = node.char node = node[chars] # 因为defaultdict，如果不存在则自动生成键 node.char = temp + chars # 键可以视为路径上的字符，节点可以视为从根到当前节点表示的前缀或单词。 node.is_word = True # 这里很重要，用于search判断是否为完整的单词 def search(self, word): \"\"\" Returns if the word is in the trie. :type word: str :rtype: bool \"\"\" node = self.root for chars in word: if chars not in node.data.keys(): # 因为defaultdict所以不能判断value为None，要判断键是否存在 return False node = node[chars] # 判断单词是否是完整的存在在trie树中 return node.is_word def startsWith(self, prefix): \"\"\" Returns if there is any word in the trie that starts with the given prefix. :type prefix: str :rtype: bool \"\"\" node = self.root for chars in prefix: if chars not in node.data.keys(): # 和上面同理 return False node = node[chars] return True def get_all_words(self): # 获取所有的单词 q = [self.root] while q: node = q.pop(0) # 相当于一个队列 for child in node.data.values(): if child.is_word: yield child.char q.append(child) ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:2:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["面经"],"content":"应用 前缀树在中文分词的应用，前缀树的实现和上面的可能不太一样，不过功能都是一样的，主要就是找句子中的词有没有出现在前缀树中。 from trie import Trie import time class TrieTokenizer(Trie): \"\"\" 基于字典树(Trie Tree)的中文分词算法 \"\"\" def __init__(self, dict_path): \"\"\" :param dict_path:字典文件路径 \"\"\" super(TrieTokenizer, self).__init__() self.dict_path = dict_path self.create_trie_tree() self.punctuations = \"\"\"！？｡＂＃＄％＆＇：（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\"\" def load_dict(self): \"\"\" 加载字典文件 词典文件内容如下，每行是一个词： AA制 ABC ABS AB制 AB角 :return: \"\"\" words = [] with open(self.dict_path, mode=\"r\", encoding=\"utf-8\") as file: for line in file: words.append(line.strip().encode('utf-8').decode('utf-8-sig')) return words def create_trie_tree(self): \"\"\" 遍历词典，创建字典树 :return: \"\"\" words = self.load_dict() for word in words: self.insert(word) def mine_tree(self, tree, sentence, trace_index): \"\"\" 从句子第trace_index个字符开始遍历查找词语，返回词语占位个数 :param tree: :param sentence: :param trace_index: :return: \"\"\" if trace_index \u003c= (len(sentence) - 1): if sentence[trace_index] in tree.data: trace_index = trace_index + 1 trace_index = self.mine_tree(tree.data[sentence[trace_index - 1]], sentence, trace_index) return trace_index def tokenize(self, sentence): tokens = [] sentence_len = len(sentence) while sentence_len != 0: trace_index = 0 # 从句子第一个字符开始遍历 trace_index = self.mine_tree(self.root, sentence, trace_index) if trace_index == 0: # 在字典树中没有找到以sentence[0]开头的词语 tokens.append(sentence[0:1]) # 当前字符作为分词结果 sentence = sentence[1:len(sentence)] # 重新遍历sentence sentence_len = len(sentence) else: # 在字典树中找到了以sentence[0]开头的词语，并且trace_index为词语的结束索引 tokens.append(sentence[0:trace_index]) # 命中词语作为分词结果 sentence = sentence[trace_index:len(sentence)] # sentence_len = len(sentence) return tokens def combine(self, token_list): \"\"\" TODO:对结果后处理：标点符号/空格/停用词 :param token_list: :return: \"\"\" flag = 0 output = [] temp = [] for i in token_list: if len(i) != 1: # 当前词语长度不为1 if flag == 0: output.append(i[::]) else: # ['该', '方法'] # temp=['该'] output.append(\"\".join(temp)) output.append(i[::]) temp = [] flag = 0 else: if flag == 0: temp.append(i) flag = 1 else: temp.append(i) return output if __name__ == '__main__': now = lambda: time.time() trie_cws = TrieTokenizer('data/32w_dic.txt') start = now() print(f\"Build Token Tree Time : {now() - start}\") sentence = '该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。' '可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，' '便可以认为此字组可能构成了一个词。该方法又称为无字典分词。' tokens = trie_cws.tokenize(sentence) combine_tokens = trie_cws.combine(tokens) end = now() print(tokens) print(combine_tokens) print(f\"tokenize Token Tree Time : {end - start}\") ","date":"2022-01-19","objectID":"/%E5%89%8D%E7%BC%80%E6%A0%91/:3:0","tags":["面经","前缀树"],"title":"前缀树","uri":"/%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["Machine Learning","分类算法"],"content":"Logistic回归 ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"线性回归 线性回归表达式： \\[ y = w^Tx+b \\] 广义回归模型： \\[ y = g^{-1}(w^Tx+b) \\] ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"Sigmoid函数 在分类任务中，需要找到一个联系函数，即g，将线性回归的输出值与实际的标签值联系起来。因此可以使用Sigmoid函数 即： \\[ \\delta(z) = \\frac{1}{1+e^{-z}} \\] 对数几率其实是一种“sigmoid”函数，它将z值转化为一个接近 0 或 1 的 \\(y\\) 值: \\[ y=\\frac{1}{1+e^{-\\left(w^{T} x+b\\right)}} \\rightarrow \\operatorname{In} \\frac{y}{1-y}=w^{T} x+b \\] 若将y视为样本 \\(x\\) 作为正例的可能性，则1-y是其反例的可能性，两者的比值 \\(\\frac{y}{1-y}\\) 称为“几率”，反映了x作为正例的相对可能性，对几率取对 数则得到 \\(\\operatorname{In} \\frac{y}{1-y}\\) ，可以看出，上式其实是在用线性回归模型的预测结果去逼近真实标记的对数几率。所以该模型也被称作“对数几率回 归”。 ## 损失函数 \\[ J = -\\frac{1}{m}\\sum_{i=1}^my_i\\log(\\hat{y_i})+(1-y_i)\\log(1-\\hat{y}) \\] 实际上可以看作下面交叉熵损失函数形式在二分类问题上的形式： \\[ J = -\\frac{1}{m}\\sum_{i=1}^my_i\\log(\\hat{y_i}) \\] 这里的\\(y_i\\)与\\(\\hat{y_i}\\)都是向量，其长度就是类别的数量。其中\\(y_i\\)代表实际分布，形式上为onehot向量。\\(\\hat{y_i}\\)是概率分布，为预测的值。 其实这里可以想一下神经网络，对于sigmoid来说，输出层的神经元可以是一个，也可以是两个，如果是一个的话就可以用上面的形式，如果是两个的话可以用下面的这种形式。 也可以这样理解，对于softmax的这种形式，对于二分类我们可以拆分成这样 \\[ \\begin{cases} \\log \\hat{y_i}, \\quad y_i=1 \\\\\\\\ \\log (1-\\hat{y_i}), \\quad y_i=0 \\end{cases} \\] 再结合起来，这样就可以得到逻辑回归的损失函数的结果。 ## 与极大似然估计的关系 \\[ h(x;\\theta) = p(y=1|x;\\theta) = \\frac{1}{1+e^{-\\theta x+b}} \\] \\[ p(y=0|x;\\theta) = 1-p(y=1|x;\\theta) \\] 则对于单个样本： \\[ p(y|x;\\theta) = h(x;\\theta)^y(1-h(x;\\theta))^{(1-y)} \\] 接下来用极大似然估计估计出参数\\(\\theta\\) \\[ \\begin{aligned} L(\\theta) = \\prod_{i=1}^mp(y_i|x_i;\\theta)\\\\\\\\ =\\prod_{i=1}^mh(x_i;\\theta)^{y_i}(1-h(x_i;\\theta)) ^{1-y_i} \\end{aligned} \\] 则： \\[ l(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^my_i\\ln (h(x_i;\\theta ))+(1-y_i)ln(1-h(x_i|\\theta)) \\] 极大这个函数，也就是最小化这个函数的负数，也就是上面的损失函数。 ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","分类算法"],"content":"python实现 class LogisticRegression: def __init__(self): pass def sigmoid(self,a): res = [] for x in a: if x \u003e= 0: res.append(1/(1+np.exp(-x))) else: res.append(np.exp(x) / (np.exp(x) + 1)) return np.array(res) def train(self, X, y_true, n_iters=100, learning_rate=1): \"\"\" 根据给定的训练集X和y来训练逻辑回归 \"\"\" # 第零步：初始化参数 n_samples, n_features = X.shape #👆样本数m和特征量数n分别赋值为X的行数和列数 self.weights = np.zeros((n_features,1)) self.bias = 0 costs = [] for i in range(n_iters): # 第一步和第二步：计算输入的特征量和权值的线性组合，使用sigmoid函数 y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias) # 第三步：计算代价值，用于之后计算代价函数值 cost = (-1/n_samples)*np.sum(y_true*np.log(y_predict+1e-5)+(1-y_true)*(np.log(1-y_predict+1e-5))) # 第四步：计算梯度 dw = (1/n_samples)*np.dot(X.T,(y_predict - y_true)) db = (1/n_samples)*np.sum(y_predict-y_true) # 第五步；更新参数 self.weights = self.weights - learning_rate * dw self.bias = self.bias - learning_rate * db costs.append(cost) if i%10 == 0: print(f\"Cost after iteration {i}:{cost}\") # return self.weights,self.bias,costs def predict(self,X): \"\"\" 对于测试集X，预测二元分类标签 \"\"\" y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias) return np.array(y_predict) ","date":"2022-01-15","objectID":"/logistic%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","分类算法","Logistic回归"],"title":"Logistic回归","uri":"/logistic%E5%9B%9E%E5%BD%92/"},{"categories":["pandas","api"],"content":"pd.melt ","date":"2022-01-12","objectID":"/melt/:0:0","tags":["pandas","api","melt"],"title":"melt","uri":"/melt/"},{"categories":["pandas","api"],"content":"用法 直观的看就是将宽数据转化为长数据。转化为variable-value这样的形式。 pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) 参数解释： frame:要处理的数据集。 id_vars:不需要被转换的列名。 value_vars:需要转换的列名，如果剩下的列全部都要转换，就不用写了。 var_name和value_name是自定义设置对应的列名。 col_level :如果列是MultiIndex，则使用此级别。 ## 实例 import pandas as pd df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, 'B': {0: 1, 1: 3, 2: 5}, 'C': {0: 2, 1: 4, 2: 6} }) df ''' A B C 0 a 1 2 1 b 3 4 2 c 5 6 ''' pd.melt(df, id_vars=['A'], value_vars=['B']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 ''' pd.melt(df, id_vars=['A'], value_vars=['B', 'C']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 ''' pd.melt(df, id_vars=['A'], value_vars=['B'], var_name='myVarName', value_name='myValueName') ''' A myVarName myValueName 0 a B 1 1 b B 3 2 c B 5 ''' pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 ''' # 多重索引 df.columns = [list('ABC'), list('DEF')] df ''' A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 ''' # 选择最外层索引 pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B']) ''' A variable value 0 a B 1 1 b B 3 2 c B 5 ''' # 选择内层索引 pd.melt(df, col_level=1, id_vars=['D'], value_vars=['E']) # 选择复合索引 pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')]) ''' (A, D) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 ''' ","date":"2022-01-12","objectID":"/melt/:1:0","tags":["pandas","api","melt"],"title":"melt","uri":"/melt/"},{"categories":["NLP"],"content":"预训练模型 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"概述 预训练模型，则是使自然语言处理由原来的手工调参、依靠 ML 专家的阶段，进入到可以大规模、可复制的大工业施展的阶段。而且预训练模型从单语言、扩展到多语言、多模态任务。一路锐气正盛，所向披靡。 预训练通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。体现某一个词在一个特定上下文中的语义表征。第二个步骤是微调，针对具体的任务修正网络。训练数据可以是文本、文本-图像对、文本-视频对。预训练模型的训练方法可使用自监督学习技术（如自回归的语言模型和自编码技术）。可训练单语言、多语言和多模态的模型。此类模型可经过微调之后，用于支持分类、序列标记、结构预测和序列生成等各项技术，并构建文摘、机器翻译、图片检索、视频注释等应用。 为什么我们要做预训练模型？首先，预训练模型是一种迁移学习的应用，利用几乎无限的文本，学习输入句子的每一个成员的上下文相关的表示，它隐式地学习到了通用的语法语义知识。第二，它可以将从开放领域学到的知识迁移到下游任务，以改善低资源任务，对低资源语言处理也非常有利。第三，预训练模型在几乎所有 NLP 任务中都取得了目前最佳的成果。最后，这个预训练模型+微调机制具备很好的可扩展性，在支持一个新任务时，只需要利用该任务的标注数据进行微调即可，一般工程师就可以实现。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"发展趋势 首先，第一个关键技术是 Transformer。它在 NLP 各个任务中都取得了优异的性能，它是预训练语言模型的核心网络。给定一句话或是一个段落作为输入，首先将输入序列中各个词转换为其对应的词向量，同时加上每一个词的位置向量，体现词在序列的位置。然后将这些词向量输入到多层 Transformer 网络中，通过自注意力（self-attention）机制来学习词与词之间的关系，编码其上下文信息，再通过一个前馈网络经过非线性变化，输出综合了上下文特征的各个词的向量表示。每一层 Transformer 网络主要由 Multi-head self-attention 层（多头自注意力机制）和前馈网络层两个子层构成。Multi-head self-attention 会并行地执行多个不同参数的 self-attention，并将各个 self-attention 的结果拼接作为后续网络的输入，self-attention 机制会在后面中做详细介绍。此后，我们得到了蕴含当前上下文信息的各个词的表示，然后网络会将其输入到前馈网络层以计算非线性层次的特征。 在每一层 Transformer 网络中，会将残差连接（residual connection）把自注意力机制前或者前馈神经网络之前的向量引入进来，以增强自注意力机制或者前馈网络的输出结果向量。并且还做一个 layer normalization，也就是通过归一化把同层的各个节点的多维向量映射到一个区间里面，这样各层节点的向量在一个区间里面。这两个操作加入在每个子层后，可更加平滑地训练深层次网络。 Transformer 可以用于编码，也可以用于解码。所谓解码就是根据一个句子的输入得到一个预想的结果，比如机器翻译（输入源语言句子，输出目标语言句子），或者阅读理解（输入文档和问题，输出答案）。解码时，已经解码出来的词要做一个自注意力机制，之后和编码得到的隐状态的序列再做一个注意力机制。这样可以做 N 层，然后通过一个线性层映射到词表的大小的一个向量。每个向量代表一个词表词的输出可能性，经过一个softmax 层得到每个词的输出概率。 接下来介绍一下 self-attention 机制，以一个 head 作为示例。假定当前输入包含三个词，给定其输入词向量或是其上一层 Transformer 网络的输出，将其通过三组线性变换，转换得到三组 queries、keys 和 values 向量。Query 和 key 向量用来计算两两词之间的得分，也就是其依赖关系，这个得分会同其对应的 value 向量做加权和，以得到每个词综合上下文信息的表示。给定当前第一个词的 query 向量，其首先同各个词的 key 向量通过点积操作得到这两个词的得分，这些得分用来表示这两个词的依赖或是相关程度。这些得分之后会根据 query 等向量的维度做一定比例的缩放，并将这些得分通过 softmax 操作做归一化。之后，各个得分会同其相对应的 value 向量相乘得到针对第一个词加权的各个 value 向量，这些加权的 value 向量最终相加以得到当前第一个词的上下文表示。 在得到第一个词的上下文表示后，给定第二个词的 query 向量，我们会重复之前的操作，计算当前 query 向量同各个词 key 向量的得分，对这些得分做 softmax 归一化处理，并将这些得分同其对应的 value 向量做加权和，以得到其编码上下文信息的表示。 第二个关键技术是自监督学习。在预训练的模型中，AR（自回归）LM 和 AE（自动编码器）是最常用的自监督学习方法，其中，自回归 LM 旨在利用前面的词序列预测下个词的出现概率（语言模型）。自动编码器旨在对损坏的输入句子，比如遮掩了句子某个词、或者打乱了词序等，重建原始数据。通过这些自监督学习手段来学习单词的上下文相关表示。 第三个关键技术就是微调。在做具体任务时，微调旨在利用其标注样本对预训练网络的参数进行调整。以我们使用基于 BERT（一种流行的预训练模型）为例来判断两个句子是否语义相同。输入是两个句子，经过 BERT 得到每个句子的对应编码表示，我们可以简单地用预训练模型的第一个隐节点预测分类标记判断两个句子是同义句子的概率，同时需要额外加一个线性层和 softmax 计算得到分类标签的分布。预测损失可以反传给 BERT 再对网络进行微调。当然也可以针对具体任务设计一个新网络，把预训练的结果作为其输入。 总体来讲，预训练模型发展趋势：第一，模型越来越大。比如 Transformer 的层数变化，从12层的 Base 模型到24层的 Large 模型。导致模型的参数越来越大，比如 GPT 110 M，到 GPT-2 是1.5 Billion，图灵是 17 Billion，而 GPT-3 达到了惊人的 175 Billion。一般而言模型大了，其能力也会越来越强，但是训练代价确实非常大。第二，预训练方法也在不断增加，从自回归 LM，到自动编码的各种方法，以及各种多任务训练等。第三，还有从语言、多语言到多模态不断演进。最后就是模型压缩，使之能在实际应用中经济的使用，比如在手机端。这就涉及到知识蒸馏和 teacher-student models，把大模型作为 teacher，让一个小模型作为 student 来学习，接近大模型的能力，但是模型的参数减少很多。 每个观点都可以看一下参考文章。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"深层表征阶段 在word embedding这篇文章里面，介绍了传统的词向量，也就是固定的词向量。本文将介绍deep contextualized词向量模型。也就是深层表征阶段。 两个伟大的想法： 编码的内容：从单词到上下文中的单词 （从 Word2Vec/GloVe/etc. 到 Cove/ELMo 的过渡） 用于下游任务：从仅替换特定任务模型中的词嵌入到替换整个特定任务模型 （从 Cove/ELMo 到 GPT/BERT 的过渡）。 具体的模型可以看本博客其余内容。 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:3:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"参考 https://www.zhihu.com/question/327642286 https://zhuanlan.zhihu.com/p/115014536 ","date":"2022-01-03","objectID":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/:4:0","tags":["NLP","预训练模型"],"title":"预训练模型","uri":"/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"categories":["pandas","api"],"content":"矩阵的反转，可以按照各个维度很好理解。 例子： cs_matrix = np.array([[ 4, 3, 2, 1, 0], [ 8, 7, 6, 5, 1], [11, 10, 9, 6, 2], [13, 12, 10, 7, 3], [14, 13, 11, 8, 4]]) np.flip(cs_matrix, 0) 变成了： np.flip(cs_matrix, 1) 变成了： ","date":"2022-01-01","objectID":"/flip/:0:0","tags":["pandas","api","flip"],"title":"flip","uri":"/flip/"},{"categories":["NLP"],"content":"ELMo 在Transformer中提到了ELMo解决了word2vec中存在的多义词问题，其使用双向的LSTM作为特征提取器，考虑了上下文的语义，所以可以解决多义词问题。这篇文章就详细介绍一下ELMo。 ElMo与CoVe很类似，不过它不是基于机器翻译模型，而是语言模型。仅仅通过用来自 LM 的嵌入替换词嵌入 (GloVe)，他们就在问答、共指解析、情感分析、命名实体识别等多项任务上获得了巨大的改进。 ","date":"2021-12-12","objectID":"/elmo/:0:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"模型训练(char-CNN 之上的前向和后向 LSTM-LMs) 该模型非常简单，它由两层 LSTM 语言模型组成：前向和后向。使用这两种模型是为了使每个标记都可以具有两个上下文：左和右。 同样有趣的是作者如何获得初始单词表示（然后将其馈送到 LSTM）。让我们回想一下，在标准词嵌入层中，对于词汇表中的每个词，我们训练一个唯一的向量。在这种情况下， - 词嵌入不知道它们所包含的字符（例如，它们不知道单词represent, represents, represented, 和 representation在书面上是接近的） - OOV问题 为了解决这些问题，作者将单词表示为字符级网络的输出。正如我们从插图中看到的，这个 CNN 非常简单，由我们之前已经看到的组件组成：卷积、全局池化、highway connections和线性层。通过这种方式，单词表示通过构造知道它们的字符，我们甚至可以表示那些我们在训练中从未见过的单词。 ","date":"2021-12-12","objectID":"/elmo/:1:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"获取表示 训练模型后，我们可以使用它来获取单词表示。为此，对于每个单词，我们结合来自前向和后向 LSTM 的相应层的表示。通过连接这些前向和后向向量，我们构建了一个“知道”左右上下文的单词表示。 总体而言，ELMo 表示具有三层： 第 0 层（嵌入） - 字符级 CNN 的输出； 第 1 层- 来自前向和后向 LSTM 的第 1 层的连接表示； 第 2 层- 来自前向和后向 LSTM 的第 2 层的连接表示； 这些层中的每一层都对不同类型的信息进行编码：第 0 层 - 仅单词级别，第 1 层和第 2 层 - 上下文中的单词。比较第 1 层和第 2 层，第 2 层可能包含更多高级信息：这些表示来自相应 LM 的更高层。 由于不同的下游任务需要不同种类的信息，ELMo 使用特定于任务的权重来组合来自三层的表示。这些是为每个下游任务学习的标量。得到的向量，即所有层表示的加权和，用于表示一个单词。 ","date":"2021-12-12","objectID":"/elmo/:2:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["NLP"],"content":"总结 CoVe和ELMo都用了上下文单词，解决了word2vec中多义词的问题。但他们主要是替换嵌入层，并保持特定于任务的模型架构几乎完好无损。这意味着例如，对于共指解决，必须使用为此任务设计的特定模型，用于词性标记 - 一些其他模型，用于问答 - 另一个非常特殊的模型等。对于这些任务中的每一个，专门研究它的研究人员不断改进特定于任务的模型架构。 与以前的模型相比，GPT/BERT 不是作为词嵌入的替代品，而是作为特定任务模型的替代品。在这个新设置中，首先使用大量未标记数据（纯文本）对模型进行预训练。然后，该模型在每个下游任务上进行微调。重要的是，现在在微调期间，您必须只使用任务感知输入转换（即以某种方式提供数据）， 而不是 修改模型架构。 ","date":"2021-12-12","objectID":"/elmo/:3:0","tags":["NLP","ELMo"],"title":"ELMo","uri":"/elmo/"},{"categories":["Machine Learning","集成学习"],"content":"Stacking ","date":"2021-11-25","objectID":"/stacking/:0:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","集成学习"],"content":"思想简介 简单得理解，就是对于多个学习器，分别对结果进行预测，然后将预测的结果作为特征，再对结果进行预测。 上一张经典的图： 以这个5折stacking为例： 首先将所有数据集生成测试集和训练集（假如训练集为10000,测试集为2500行），那么上层会进行5折交叉检验，使用训练集中的8000条作为训练集，剩余2000行作为验证集（橙色）。 每次验证相当于使用了蓝色的8000条数据训练出一个模型，使用模型对验证集进行验证得到2000条数据，并对测试集进行预测，得到2500条数据，这样经过5次交叉检验，可以得到中间的橙色的\\(5\\times 2000\\)条验证集的结果(相当于每条数据的预测结果)，\\(5\\times 2500\\)条测试集的预测结果。 接下来会将验证集的\\(5\\times 2000\\)条预测结果拼接成10000行长的矩阵，标记为A1，而对于\\(5\\times 2500\\)行的测试集的预测结果进行加权平均，得到一个2500一列的矩阵，标记为B1。 上面得到一个基模型在数据集上的预测结果A1、B1,这样当我们对3个基模型进行集成的话，相于得到了A1、A2、A3、B1、B2、B3六个矩阵。 之后我们会将A1、A2、A3并列在一起成10000行3列的矩阵作为training data,B1、B2、B3合并在一起成2500行3列的矩阵作为testing data，让下层学习器基于这样的数据进行再训练。 再训练是基于每个基础模型的预测结果作为特征（三个特征），次学习器会学习训练如果往这样的基学习的预测结果上赋予权重w，来使得最后的预测最为准确。 ","date":"2021-11-25","objectID":"/stacking/:1:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","集成学习"],"content":"伪代码 ","date":"2021-11-25","objectID":"/stacking/:2:0","tags":["Machine Learning","集成学习","Stacking"],"title":"Stacking","uri":"/stacking/"},{"categories":["Machine Learning","回归算法"],"content":"参考：https://cuijiahua.com/blog/2017/11/ml_11_regression_1.html ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"什么是回归？ 回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。 HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio 这就是所谓的回归方程（regression equation），其中的0.0015和-0.99称为回归系数（regression weights），求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。 说到回归，一般都是指线性回归（linear regression），所以本文里的回归和线性回归代表同一个意思。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。 这里的线性可以是对变量的线性，也可以是对参数的线性。比如\\(y=x^2\\)可以认为是一个线性函数。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:1:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"正规方程推导 将向量表达形式转为矩阵表达形式，则有 \\(J(\\theta)=\\frac{1}{2}(X \\theta-y)^{2}\\) ，其中 \\(X\\) 为 \\(m\\) 行 \\(n\\) 列的矩阵（ \\(m\\) 为样本个数， \\(n\\) 为特 征个数)， \\(\\theta\\) 为 \\(n\\) 行1列的矩阵， \\(y\\) 为 \\(m\\) 行1列的矩阵，对 \\(J(\\theta)\\) 进行如下变换 \\[ \\begin{aligned} \u0026J(\\theta)=\\frac{1}{2}(X \\theta-y)^{T}(X \\theta-y) \\\\\\\\ \u0026=\\frac{1}{2}\\left(\\theta^{T} X^{T}-y^{T}\\right)(X \\theta-y) \\\\\\\\ \u0026=\\frac{1}{2}\\left(\\theta^{T} X^{T} X \\theta-\\theta^{T} X^{T} y-y^{T} X \\theta-y^{T} y\\right) \\end{aligned} \\] 接下来对 \\(J(\\theta)\\) 偏导，需要用到以下几个矩阵的求导法则: \\[ \\begin{aligned} \u0026\\frac{d A B}{d B}=A^{T} \\\\\\\\ \u0026\\frac{d X^{T} A X}{d X}=2 A X \\end{aligned} \\] 所以有: \\[ \\begin{aligned} \u0026\\frac{\\partial J(\\theta)}{\\partial \\theta}=\\frac{1}{2}\\left(2 X^{T} X \\theta-X^{T} y-\\left(y^{T} X\\right)^{T}-0\\right) \\\\\\\\ \u0026=\\frac{1}{2}\\left(2 X^{T} X \\theta-X^{T} y-X^{T} y-0\\right) \\\\\\\\ \u0026=X^{T} X \\theta-X^{T} y \\\\\\\\ \u0026\\text { 令 } \\frac{\\partial J(\\theta)}{\\partial \\theta}=0 ｝ \\\\\\\\ {\\text { 则有 } \\theta=\\left(X^{T} X\\right)^{-1} X^{T} y} \\end{aligned} \\] 值得注意的是，上述公式中包含逆矩阵，也就是说，这个方程只在逆矩阵存在的时候使用，也即是这个矩阵是一个方阵，并且其行列式不为0。 除了这种方法，也可以使用最小二乘法来解决。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:2:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"局部加权线性回归 线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有小均方误差的无偏估 计。显而易见，如果模型欠拟合将不能取得好的预测效果。所以有些方法允许在估计中引入一 些偏差，从而降低预测的均方误差。 其中的一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。 普通线性回归: \\[ \\min J_R=\\sum_{i=1}^m\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 局部加权线性回归: \\[ \\min J_{L R}=\\sum_{i=1}^m \\theta_i\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 这里唯一的区别是加入了权重 \\(\\theta\\), 采用之前的最小二乘法求解权数 \\(\\mathbf{w}\\) : \\[ \\hat{\\mathbf{w}}^* =\\arg \\min_{\\hat{\\mathbf{w}}} \\theta(\\mathbf{y}-\\mathbf{X} \\hat{\\mathbf{w}})^T(\\mathbf{y}-\\mathbf{X} \\hat{\\mathbf{w}}) \\] 在该方法中，我们给待预测点附近的每个点赋予一定的权重。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解除回归系数W的形式如下： \\[ \\hat{w} = (X^T\\theta X)^{-1}X^T\\theta y \\] 其中W是一个矩阵，这个公式跟我们上面推导的公式的区别就在于W，它用来给每个店赋予权重。 LWLR使用”核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下： \\[ \\theta(i,i) = exp\\left ( \\frac{ (x^{(i)}-x)^2}{-2k^2}\\right) \\] 这样就构造了一个只含对角元素的权重矩阵 \\(w\\) ，并且点xi与 \\(x\\) 越接近， \\(\\theta(i, i)\\) 的值越大，当 \\(x i\\) 与 \\(x\\) 非常接近时， \\(\\theta(i, i)\\) 的值趋于 1 ，我们再回头看之前的优化式: \\[ \\min J_{L R}=\\sum_{i=1}^m \\theta_i\\left(g\\left(x_i\\right)-y_i\\right)^2 \\] 对于一个数据点，与其靠近的点，权重大，与其相距较远的点，权重小，从而优化问题会有所偏倚，靠近的点对该数据点的回归拟合起较大作用，而相距较远的 点由于权数很小，造成的影响基本可以忽略不计，这样就等同于每个点的回归都是基于与其相距较近的点为基础，而忽略了较远的点，这也就是同部加权线性回归局部的由来，因为它着重考虑目标点同部的数据点为回归基础. 可以看到，加权函数只有一个系数，那就是分母上的 \\(k\\) ，当K取很小时， exp得到的很多值均趋于 0 ，此时只有很少一部分样本用于训练，而当k取很大时， exp的值 不会很快趋于 0 ，从而会有一大部分点用于训练，我们可以通过调整k的值，决定这个‘局部’的大小究竟是多大 如果数据的特征比样本点还多应该怎么办？如果矩阵有多重共线性怎么办？很显然，此时我们不能再使用上文的方法进行计算了，因为矩阵X不是满秩矩阵，非满秩矩阵在求逆时会出现问题。 解决的方法就是正则化。在线性回归中，正则化主要有L1正则化与L2正则化，L1正则化对应LASSO回归，L2正则化对应岭回归。 #!/usr/bin/env python #-*- coding:utf-8 -*- from numpy import * import matplotlib.pyplot as plt \"\"\" 打开一个用tab键分隔的文本文件 parameters: fileName -文件名 return: dataMat -数据矩阵 labelMat -目标值向量 \"\"\" def loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t')) - 1 #得到列数，不包括最后一列，默认最后一列值为目标值 dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr =[] curLine = line.strip().split('\\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMat \"\"\" 计算最佳拟合直线 parameters: xArr -给定的输入值 yArr -给定的输出值 return: ws -回归系数 \"\"\" def standRegres(xArr,yArr): xMat = mat(xArr); yMat = mat(yArr).T #将数据保存到矩阵中 #计算x.T *x xTx = xMat.T @ xMat #使用linalg.det()方法来判断它的行列式是否为零，即是否可逆 if linalg.det(xTx) == 0.0: return #使用最小二乘法计算w值 ws = linalg.inv(xTx) @ xMat.T @ yMat return ws \"\"\" 计算局部加权线性回归系数 parameters: testPoint -待预测数据 xArr -给定输入值 yArr -给定输出值 k -高斯核的k值，决定对附近的点赋予多大的权重 return: testPoint * ws -回归预测的估计值 \"\"\" def lwlr(testPoint, xArr, yArr, k=1.0): xMat = mat(xArr); yMat = mat(yArr).T #读入数据到矩阵 m = shape(xMat)[0] #创建对角权重矩阵，该矩阵为方阵，矩阵维数为样本点个数 theta = eye(m, m) #遍历整个数据集 for i, x in enumerate(xMat): #计算待预测数据与每个样本点的差值 x_i = (testPoint - x) @ (testPoint - x).T #计算每个样本点对应的权重值，随着样本点与待预测点距离的递增，权重将以指数级衰减 theta[i][i] = exp(x_i / (-2 * k ** 2)) #计算x.T θ x xT_theta_x = xMat.T @ theta @ xMat #判断矩阵是否可逆 if linalg.det(xT_theta_x) == 0: return #使用最小二乘法计算w值 ws = linalg.inv(xT_theta_x) @ (xMat.T @ (theta @ yMat)) return testPoint*ws \"\"\" 测试函数 parameters: testArr -测试数据集 xArr -给定输入值 yArr -给定输出值 k -高斯核的k值 return: yHat -预测值 \"\"\" def lwlrTest(testArr, xArr, yArr,k=1.0): m = shape(xArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHat \"\"\" 计算预测误差的平方和 parameters: yArr -给定y值 yHatArr -预测y值 return: ((yArr-yHatArr)**2).sum() -误差矩阵 \"\"\" def rssError(yArr,yHatArr): return ((yArr-yHatArr)**2).sum() if __name__=='__main__': abX,abY = loadDataSet('linear_regression_abalone/abalone.txt') yHat01 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],0.1) yHat1 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],1) yHat10 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],10) print(\"使用局部加权线性回归预测误差：\") print(\"核为0.1时：\",rssError(abY[0:99],yHat01.T)) print(\"核为1时：\",rssError(abY[0:99],yHat1.T)) print(\"核为10时：\",rssError(abY[0:99],yHat10.T)) yHat01 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],0.1) yHat1 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],1) yHat10 = lwlrTest(abX[100:199],abX[0:99],abY[0:99],10) print(\"使用局部加权线性回归预测误差在新数据上的表现：","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:3:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"岭回归 岭回归即我们所说的L2正则线性回归，在一般的线性回归最小化均方误差的基础上增加了一个参数w的L2范数的罚项，从而最小化罚项残差平方和： \\[ min\\mid \\mid Xw-y \\mid\\mid_2^2 + \\lambda \\mid\\mid w\\mid\\mid_2^2 \\] 简单说来，岭回归就是在普通线性回归的基础上引入单位矩阵。回归系数的计算公式变形如下： \\[ \\hat{w} = (X^TX+\\lambda I)^{-1}X^Ty \\] 式中，矩阵I是一个mxm的单位矩阵，加上一个λI从而使得矩阵非奇异，进而能对矩阵求逆。 当\\(\\lambda\\)过小，则相当于原来的正规方程，会造成过拟合，而\\(\\lambda\\)过大时，模型的方差会更小，但偏差会变大，所以岭回归的关键是找到一个合理的\\(\\lambda\\)值来平衡模型的方差和偏差。 岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入λ来限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也可以叫做缩减（shrinkage）。 缩减方法可以去掉不重要的参数，因此能更好地裂解数据。此外，与简单的线性回归相比，缩减法能够取得更好的预测效果。 图绘制了回归系数与log(λ)的关系。在最左边，即λ最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减成0；在中间部分的某个位置，将会得到最好的预测结果。想要得到最佳的λ参数，可以使用交叉验证的方式获得，文章的后面会继续讲解。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:4:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"Lasso Lasso对回归系数做了限定，对应的约束条件如下： \\[ \\sum_{k=1}^n|w_k| \\leq \\lambda \\] 而岭回归的约束条件如下： \\[ \\sum_{k=1}^nw_k^2 \\leq \\lambda \\] 唯一不同点在于，Lasso的约束条件用绝对值取代了平方和，虽然约束形式稍作变化，但结果大相径庭，细微的变化极大地增加了计算复杂度。所以用更简单的前向逐步回归来取代。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:5:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"前向逐步回归 前向逐步线性回归算法属于一种贪心算法，即每一步都尽可能减少误差。我们计算回归系数，不再是通过公式计算，而是通过每次微调各个回归系数，然后计算预测误差。那个使误差最小的一组回归系数，就是我们需要的最佳回归系数。 该算法的伪代码如下： 数据标准化，使其满足0均值和单位方差。 在每轮迭代中： 设置当前最小误差lowestError为正无穷 对每个特征 增大或缩小 改变一个系数得到新的W 计算新W下的误差 如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W 将W设置为新的Wbest ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:6:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["Machine Learning","回归算法"],"content":"代码 使用的优化算法是梯度下降法，还有一个随机梯度下降 import numpy as np import matplotlib.pyplot as plt np.random.seed(42) w = np.array([2, 1, 4, 5, 3]) d = len(w) X = [] Y = [] for _ in range(1000000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X.append(x) Y.append(y) X = np.array(X) Y = np.array(Y) def mse(y_true, y_test): return ((y_true - y_test) ** 2) / len(y_true) def gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent(w, alpha, x, y): y_pred = x.dot(w) error = mse(y, y_pred).mean() grad = np.dot(x.T, gradient(y, y_pred)) w = w - alpha * grad return w, error def stochastic_gradient_descent(w, alpha, x, y, epoch): alpha_update = alpha for i in range(len(x)): y_pred = x[i].dot(w) grad = np.dot(x[i].T, (y_pred - y[i])) * 2 / len(x) w = w- alpha_update * grad alpha_update = alpha_update / (epoch+1) error = mse(y, x.dot(w)).mean() return w, error X_test = [] Y_test = [] for _ in range(10000): x = np.random.randn(d) y = w.dot(x) + np.random.randn() X_test.append(x) Y_test.append(y) X_test = np.array(X_test) Y_test = np.array(Y_test) def l2_mse(y_true, y_test, l, w): return ((y_true - y_test) ** 2) / len(y_true) + l * np.sum(w ** 2) def l2_gradient(y_true, y_test): return 2 * (y_test - y_true) / len(y_true) def batch_gradient_descent_with_l2(w, alpha, x, y, l): y_pred = x.dot(w) error = l2_mse(y, y_pred, l, w).mean() grad = np.dot(x.T, l2_gradient(y, y_pred)) w = w - alpha * grad - alpha * l * w *2 return w, error if __name__ == \"__main__\": train_loss = [] test_loss = [] print(\"Batch Gradient Descent\") for epoch in range(1000): w, error = batch_gradient_descent(w, 0.01, X, Y) # train y_pred = X_test.dot(w) error_test = mse(Y_test, y_pred).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-No-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-No-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-No-L2\") plt.plot(test_loss, label=\"Test-No-L2\") plt.legend() plt.show() # ============================================ train_loss = [] test_loss = [] print(\"Batch Gradient Descent with L2\") l = 0.0001 # lambda for epoch in range(1000): w, error = batch_gradient_descent_with_l2(w, 0.01, X, Y, l) # train y_pred = X_test.dot(w) error_test = l2_mse(Y_test, y_pred, l, w).mean() # test if epoch % 100 == 0: print(\"Epoch: {}, TrainError: {}, TestError: {}\".format(epoch, error, error_test)) train_loss.append(error) test_loss.append(error_test) plt.plot(train_loss, label=\"Train-L2\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() plt.plot(test_loss, label=\"Test-L2\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend() plt.show() plt.plot(train_loss, label=\"Train-L2\") plt.plot(test_loss, label=\"Test-L2\") plt.legend() plt.show() 其实很简单，就是数学公式的复现。 ## 总结 缩减方法（逐步线性回归或岭回归），就是将一些系数缩减成很小的值或者直接缩减为0。这样做，就增大了模型的偏差（减少了一些特征的权重），通过把一些特征的回归系数缩减到0，同时也就减少了模型的复杂度。消除了多余的特征之后，模型更容易理解，同时也降低了预测误差。 ","date":"2021-11-25","objectID":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:7:0","tags":["Machine Learning","回归算法","线性回归"],"title":"线性回归","uri":"/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"categories":["算法题"],"content":"计数质数 https://leetcode-cn.com/problems/count-primes/ 一开始直接暴力，隐约感觉会超时，果然不出我所料 class Solution: def countPrimes(self, n: int) -\u003e int: counts = 0 for i in range(n): if self.isprime(i): counts += 1 return counts def isprime(self,n): from itertools import count if n \u003c=1: return False for i in count(2): if i* i \u003e n: return True if n % i == 0: return False 我还特意用了itertools库，没想到还是超时了 ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:0:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["算法题"],"content":"埃氏筛 class Solution: def countPrimes(self, n: int) -\u003e int: res = [1] * n count = 0 for i in range(2, n): if res[i]: count += 1 for j in range(i*i, n, i): res[j] = 0 return count 埃氏筛的原理：从 2 开始，将每个质数的倍数都标记为合数。同样的，标记到 根号n停止。 假设一个数 i 为质数时，那么此时大于 i 且是 i 的倍数的数一定不是质数，例如 2i，3i…。那么我们将这些不是质数的数进行标记。 这里需要注意，标记应该从 i * i 开始，而不是 2 * i 开始。因为对于每个数 i 来说，枚举是从小到大的，此时前面数字的倍数都已经进行了标记。对于 i 而言，2∗i 也肯定会被在枚举数字 2 时进行标记，[2, i) 区间的数同理。 ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:1:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["算法题"],"content":"欧拉筛（线性筛） 具体的证明不说了，背板子就行 class Solution: def countPrimes(self, n: int) -\u003e int: if n \u003c= 1: return 0 is_prime = [True] * n is_prime[0] = is_prime[1] = False res = [] for i in range(2, n): if is_prime[i]: res.append(i) j = 0 while j \u003c len(res) and (tmp:=i*res[j]) \u003c n: is_prime[tmp] = False if i % res[j] == 0: # 如果res[j]为当前数的约数，则i*res[j+1]等后面的数必然会在后面的计算得到。就结束循环。这减少了重复计算。 break j += 1 return len(res) ","date":"2021-11-19","objectID":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/:2:0","tags":["算法题","计数质数"],"title":"计数质数","uri":"/%E8%AE%A1%E6%95%B0%E8%B4%A8%E6%95%B0/"},{"categories":["Machine Learning","分类算法"],"content":"感知机算法 感知机印象中没有系统学习过但是是一个很简单的算法，最近看了一下李航老师的统计学习方法，发现感知机的思想和svm十分类似，并且比svm简单的多，不需要间隔最大，只需要分开就可以。同时老师在课堂上面讲的版本也有点不一样，主要是计算上的不同，本质还是一样的。然后就打算整理一下这一块。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:0:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"感知机模型 假设输入空间（特征空间) 是 \\(\\mathcal{X} \\subseteq \\mathbf{R}^n\\), 输出空间是 \\(\\mathcal{Y}=\\\\{+1,-1\\\\}\\) 。输入 \\(x \\in \\mathcal{X}\\) 表示实例的特征向量, 对应于输入空间 (特征空间 ) 的点; 输出 \\(y \\in \\mathcal{Y}\\) 表示实例的类别。由输入空间到输出空间的如下函数 \\[ f(x)=\\operatorname{sign}(w \\cdot x+b) \\] 称为感知机。其中, \\(w\\) 和 \\(b\\) 为感知机模型参数, \\(w \\in \\mathbf{R}^n\\) 叫作权值 (weight) 或权值向 量 (weight vector), \\(b \\in \\mathbf{R}\\) 叫作偏置 (bias), \\(w \\cdot x\\) 表示 \\(w\\) 和 \\(x\\) 的内积。sign 是符号 函数, 即 \\[ \\operatorname{sign}(x)=\\left\\{\\begin{array}{cc} +1, \u0026 x \\geqslant 0 \\\\\\\\ -1, \u0026 x\u003c0 \\end{array}\\right. \\] ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:1:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"损失函数 假设训练数据集是线性可分的, 感知机学习的目标是求得一个能够将训练集正实 例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面, 即确定感知 机模型参数 \\(w, b\\), 需要确定一个学习策略, 即定义 (经验) 损失函数并将损失函数极 小化。 损失函数的一个自然选择是误分类点的总数。但是, 这样的损失函数不是参数 \\(w\\), \\(b\\) 的连续可导函数, 不易优化。损失函数的另一个选择是误分类点到超平面 \\(S\\) 的总距 离, 这是感知机所采用的。为此, 首先写出输入空间 \\(\\mathbf{R}^n\\) 中任一点 \\(x_0\\) 到超平面 \\(S\\) 的 距离: \\[ \\frac{1}{\\|w\\|}\\left|w \\cdot x_0+b\\right| \\] 这里, \\(\\|w\\|\\) 是 \\(w\\) 的 \\(L_2\\) 范数。 其次, 对于误分类的数据 \\(\\left(x_i, y_i\\right)\\) 来说, \\[ -y_i\\left(w \\cdot x_i+b\\right)\u003e0 \\] 成立。因为当 \\(w \\cdot x_i+b\u003e0\\) 时, \\(y_i=-1\\); 而当 \\(w \\cdot x_i+b\u003c0\\) 时, \\(y_i=+1\\) 。 因此, 误 分类点 \\(x_i\\) 到超平面 \\(S\\) 的距离是 \\[ -\\frac{1}{\\|w\\|} y_i\\left(w \\cdot x_i+b\\right) \\] 这样, 假设超平面 \\(S\\) 的误分类点集合为 \\(M\\), 那么所有误分类点到超平面 \\(S\\) 的总 距离为 \\[ -\\frac{1}{\\|w\\|} \\sum_{x_i \\in M} y_i\\left(w \\cdot x_i+b\\right) \\] 不考虑 \\(\\frac{1}{\\|w\\|}\\), 就得到感知机学习的损失函数。 即 \\[ L(w, b) = -\\sum_{x_i \\in M} y_i\\left(w \\cdot x_i+b\\right) \\] 使用梯度下降算法更新参数，对损失函数求导得到： \\[ \\begin{aligned} \u0026\\nabla_w L(w, b)=-\\sum_{x_i \\in M} y_i x_i \\\\\\\\ \u0026\\nabla_b L(w, b)=-\\sum_{x_i \\in M} y_i \\end{aligned} \\] 随机选取一个误分类点 \\(\\left(x_i, y_i\\right)\\), 对 \\(w, b\\) 进行更新: \\[ \\begin{gathered} w \\leftarrow w+\\eta y_i x_i \\\\\\\\ b \\leftarrow b+\\eta y_i \\end{gathered} \\] ## 算法流程 总结可得，感知机算法流程如下： 输入: 训练数据集 \\(T=\\left\\{\\left(x_1, y_1\\right),\\left(x_2, y_2\\right), \\cdots,\\left(x_N, y_N\\right)\\right\\\\\\}\\), 其中 \\(x_i \\in \\mathcal{X}=\\mathbf{R}^n, y_i \\in\\) \\(\\mathcal{Y}=\\\\{-1,+1\\\\}, i=1,2, \\cdots, N\\); 学习率 \\(\\eta(0\u003c\\eta \\leqslant 1)\\); 输出: \\(w, b\\); 感知机模型 \\(f(x)=\\operatorname{sign}(w \\cdot x+b)\\) 。 (1) 选取初值 \\(w_0, b_0\\); (2) 在训练集中选取数据 \\(\\left(x_i, y_i\\right)\\); (3) 如果 \\(y_i\\left(w \\cdot x_i+b\\right) \\leqslant 0\\), \\[ \\begin{aligned} \u0026w \\leftarrow w+\\eta y_i x_i \\\\\\\\ \u0026b \\leftarrow b+\\eta y_i \\end{aligned} \\] 转至 (2), 直至训练集中没有误分类点。 这很容易理解。就是求解最佳参数\\(w\\)和\\(b\\)，使用梯度下降算法，对于每个样本，如果其真实的标签与预测的结果符号不一致，也就是sign函数之前的结果不同号，则说明分类错误，则就需要更新参数，不断地继续更新直到所有的样本都分类正确。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:2:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"另一种表达方式 感知器: 用数据训练线性模型 \\(g({x})={w}^T {x}+w_0\\) 增广的样本向量: \\[ {y}=\\left(1 ; x_1 ; x_2 ; \\ldots ; x_d\\right) \\] 增广的权向量: \\[ {\\alpha}=\\left(w_0 ; w_1 ; \\ldots ; w_d\\right) \\] 线性判别函数: \\[ g({y})={\\alpha}^T {y} \\] 决策规则: 如果 \\(g({y})\u003e0\\), 则 \\(y \\in \\omega_0\\); 如果 \\(g({y})\u003c0\\), 则 \\(y \\in \\omega_1\\) 若定义新变量 \\(y^{\\prime}\\), 使 \\[ y_i^{\\prime}=\\left\\{\\begin{array}{lll} y_i, \\text { 若 } \u0026 {y}_i \\in \\omega_0 \\\\ -{y}_i, \\text { 若 } \u0026 {y}_i \\in \\omega_1 \\end{array} \\quad i=1,2, \\ldots, m\\right. \\] 样本可分性条件变为：存在 \\(\\alpha\\), 使 \\[ {\\alpha}^T {y}_i^{\\prime}\u003e0, i=1,2, \\ldots, m \\] \\(y^{\\prime}\\) 称作规范化增广样本向量, 仍记作 \\(y\\) 。 可以用这样的形式定义损失函数为： \\[ J(\\alpha) = \\sum_{\\alpha^Ty_k \\leq 0} (-\\alpha^Ty_k) \\] 其中w和b合并为了\\(\\alpha\\)。\\(y_k\\)为原来的x加上了1用于与偏置b对应。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:0","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"梯度下降迭代法求解 \\[ \\boldsymbol{\\alpha}(t+1)=\\boldsymbol{\\alpha}(t)-\\rho_t \\nabla J_P(\\boldsymbol{\\alpha}) \\] 下一时刻的权向量是把当前时刻的权向量向目标函数的负梯度方向调整一个修 正量, \\(\\rho_t\\) 为调整的步长 (“学习率”)。 \\[ \\nabla J_P(\\boldsymbol{\\alpha})=\\frac{\\partial J_P(\\boldsymbol{\\alpha})}{\\partial \\boldsymbol{\\alpha}}=\\sum_{\\alpha^T y_k \\leq 0}\\left(-y_k\\right) \\] 所以 \\[ \\alpha(t+1)=\\alpha(t)+\\rho_t \\sum_{\\alpha^T y_k \\leq 0} y_k \\] 即每次迭代时把错分的样本按照某个系数加到权向量上。 当没有错分样本时, 得到一个合适的解 $^* $ 。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:1","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["Machine Learning","分类算法"],"content":"固定增量法 （1）任意选择初始权向量 \\(\\alpha(0)\\); (2) 对样本 \\(y_j\\), 若 \\(\\alpha(t)^T y_j \\leq 0\\), 则 \\(\\alpha(t+1)=\\alpha(t)+y_j\\) (假设 \\(\\left.\\rho_t=1\\right)\\), 否则继 续; (3) 对所有样本重复 (2), 直至对所有的样本都有 \\(\\alpha(t)^T y_j\u003e0\\), 即 \\(J_P(\\boldsymbol{\\alpha})=0\\) 与梯度下降法的区别就是每次只对一个样本更新，可以这样理解： 原始的数据，对于第二类则增广之后取负数就可以理解为前面第一种表达的\\(y_i\\left(w \\cdot x_i+b\\right)\\)， 大于0则说明分类正确，否则说明分类错误，就需要更新参数。 ","date":"2021-11-16","objectID":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/:3:2","tags":["Machine Learning","分类算法","感知机算法"],"title":"感知机算法","uri":"/%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95/"},{"categories":["算法题"],"content":"最大数 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:0:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/largest-number/ ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:1:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 一开始直接暴力搜索，把所有的情况都列举然后比较，结果超时了，最后利用了自定义排序的方法 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:2:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def largestNumber(self, nums: List[int]) -\u003e str: class Comapre(str): def __lt__(self,other): return int(self+other) \u003e int(other+self) nums.sort(key=Comapre) return str(int(''.join(map(str,nums)))) 注意的是这里利用了自定义的比较类型，继承了str，也可以从functools里导入cmp_to_key方法来实现比较 python3之后不支持cmp，所用key函数并不直接比较任意两个原始元素，而是通过key函数把那些元素转换成一个个新的可比较对象，也就是元素的key，然后用元素的key代替元素去参与比较。如果原始元素本来就是可比较对象，比如数字、字符串，那么不考虑性能优化可以直接sort(key=lambda e: e)。不过这种基于key函数的设计倾向于每个元素的大小有个绝对标准，但有时却会出现单个元素并没有一个绝对的大小的情况，此时可以使用 functools.cmp_to_key构建基于多个元素的比较函数。 ","date":"2021-11-08","objectID":"/%E6%9C%80%E5%A4%A7%E6%95%B0/:3:0","tags":["算法题","最大数"],"title":"最大数","uri":"/%E6%9C%80%E5%A4%A7%E6%95%B0/"},{"categories":["Deep Learning","循环神经网络系列"],"content":" image.png image.png ","date":"2021-11-02","objectID":"/lstm/:0:0","tags":["Deep Learning","循环神经网络系列","LSTM"],"title":"LSTM","uri":"/lstm/"},{"categories":["算法题"],"content":"检查平衡性 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/:0:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/check-balance-lcci/ ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/:1:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/"},{"categories":["算法题"],"content":"思路： 算深度，然后作差是否大于1 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/:2:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/"},{"categories":["算法题"],"content":"代码： # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def isBalanced(self, root: TreeNode) -\u003e bool: if self.maxdepth(root) \u003c 1: return True if abs(self.maxdepth(root.left) - self.maxdepth(root.right)) \u003e 1: return False return self.isBalanced(root.right) and self.isBalanced(root.left) def maxdepth(self,root): if not root: return 0 return 1 + max(self.maxdepth(root.right),self.maxdepth(root.left)) ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/:3:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7-1/"},{"categories":["算法题"],"content":"检查平衡性 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:0:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/check-balance-lcci/ ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:1:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"思路： 算深度，然后作差是否大于1 ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:2:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["算法题"],"content":"代码： # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def isBalanced(self, root: TreeNode) -\u003e bool: if self.maxdepth(root) \u003c 1: return True if abs(self.maxdepth(root.left) - self.maxdepth(root.right)) \u003e 1: return False return self.isBalanced(root.right) and self.isBalanced(root.left) def maxdepth(self,root): if not root: return 0 return 1 + max(self.maxdepth(root.right),self.maxdepth(root.left)) ","date":"2021-10-27","objectID":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/:3:0","tags":["算法题","检查平衡性"],"title":"检查平衡性","uri":"/%E6%A3%80%E6%9F%A5%E5%B9%B3%E8%A1%A1%E6%80%A7/"},{"categories":["NLP"],"content":"语言模型 语言模型是一个很大的主题，很多nlp的任务都是基于语言模型进行的，因此理解语言模型是很重要的。 语言模型简单说就是 计算一个句子在语言中出现的概率。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:0:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"数学表示 一个语言模型通常构建字符串s的概率分布p(s)，这里p(s)反应字符串s作为一个句子出现的概率。 对于一个由m个基元（可以是字、词或者短语）构成的句子\\(s=w_1,w_2, \\dots w_m\\)，其概率计算公式可以表示为(乘法公式，详见bayes)： \\[ p(s) = p(w_1)p(w_2\\mid w_1)p(w_3\\mid w_1, w_2)\\cdots p(w_m\\mid w_1w_2\\dots w_{m-1})= \\prod_{i=1}^mp(w_i\\mid w_1\\cdots,w_{i-1}) \\] 但是很明显这个计算复杂度是极大的。 ## 评价指标 语言模型的常用评价指标是困惑度（perplexity）：在一个测试数据上的perplexity越低，说明建模的效果越好。perplexity计算公式如下： 简单来说，困惑度就是刻画一个语言模型预测一个语言样本的能力，其实际上就是计算每一个词得到的概率倒数的几何平均，即模型预测下一个词的平均可选择数量。 在实际应用中通常使用log的形式，即： ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:1:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"统计语言模型 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:0","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"n-gram 为了解决复杂度高的问题，因此引入马尔科夫假设，即当前词的预测概率只与之前n-1个词相关，基于此，语言模型可以修改如下： \\[ p(s) = p(w_1, w_2, \\dots, w_m) = \\prod_{i=1}^mp(w_i\\mid w_{i-n+1}, w_{i-1}) \\] 当n取1,2,3时，n-gram可以称为unigram、bigram、trigram。n越大复杂度越高。 n-gram model一般采用MLE进行参数估计： \\[ p(w_i\\mid w_{i-n+1}, \\cdots, w_{i-1}) = \\frac{C(w_{i-n+1}, \\cdots,w_{i-1}, w_i)}{C(w_{i-n+1}, \\cdots, w_{i-1})} \\] 即使训练语料再大，也存在参数为0的情况，这时候就需要引入数据平滑策略，其中最为常用的就是拉普拉斯平滑。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:1","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"拉普拉斯平滑 Add one 拉普拉斯平滑，即强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可。这个方法的弊端是，大部分n-gram都是没有出现过的，很容易为他们分配过多的概率空间。 #### Add-K 在Add-one的基础上做了一点小改动，原本是加一，现在加上一个小于1的常数K K。但是缺点是这个常数仍然需要人工确定，对于不同的语料库K可能不同。 k取1时与add one 相同 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:2","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"Kneser-Ney Smoothing 这是目前ngram模型效果最好的平滑方法。 ## 神经网络语言模型 具体可以看本博客有关神经网络语言模型的内容，NNLM是第一个出现的神经网络语言模型。 ","date":"2021-10-26","objectID":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:3","tags":["NLP","语言模型"],"title":"语言模型","uri":"/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"隐马尔科夫模型 ","date":"2021-10-25","objectID":"/hmm/:0:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"介绍 HMM可以看做是处理序列模型的传统方法。 一般来说HMM解决三个问题： 评估观察序列概率。给定模型\\(\\lambda=(A,B,\\prod)\\)和观察序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，计算在模型\\(\\lambda\\)下观测序列O出现的概率\\(P(O\\lvert \\lambda)\\)，这个问题需要用到前向后向算法，属于三个问题中最简单的。 预测问题，也叫解码问题。即给定模型\\(\\lambda = (A,B,\\prod)\\)和观测序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，求在给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，这个问题属于三个问题中复杂度居中的算法。 模型参数学习问题。即给定观测序列\\(O=\\\\{o_1,o_2,\\dots,o_T\\\\}\\)，估计模型\\(\\lambda = (A,B,\\prod)\\)的参数，使得该模型下观测序列的条件概率\\(P(O\\lvert\\lambda)\\)最大，这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。属于三个问题中最复杂的。 ","date":"2021-10-25","objectID":"/hmm/:1:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"定义 设 \\(Q\\) 是所有可能的状态的集合, \\(V\\) 是所有可能的观测的集合: \\[ Q=\\left\\{q_1, q_2, \\cdots, q_N\\right\\}, \\quad V=\\left\\{v_1, v_2, \\cdots, v_M\\right\\} \\] 其中, \\(N\\) 是可能的状态数, \\(M\\) 是可能的观测数。 \\(I\\) 是长度为 \\(T\\) 的状态序列, \\(O\\) 是对应的观测序列: \\[ I=\\left(i_1, i_2, \\cdots, i_T\\right), \\quad O=\\left(o_1, o_2, \\cdots, o_T\\right) \\] \\(A\\) 是状态转移概率矩阵: \\[ A=\\left[a_{i j}\\right]_{N \\times N} \\] 其中, \\[ a_{i j}=P\\left(i_{t+1}=q_j \\mid i_t=q_i\\right), \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, N \\] 是在时刻 \\(t\\) 处于状态 \\(q_i\\) 的条件下在时刻 \\(t+1\\) 转移到状态 \\(q_j\\) 的概率。 \\(B\\) 是观测概率矩阵: \\[ B=\\left[b_j(k)\\right]_{N \\times M} \\] 其中, \\[ b_j(k)=P\\left(o_t=v_k \\mid i_t=q_j\\right), \\quad k=1,2, \\cdots, M ; \\quad j=1,2, \\cdots, N \\] 是在时刻 \\(t\\) 处于状态 \\(q_j\\) 的条件下生成观测 \\(v_k\\) 的概率。 \\(\\pi\\) 是初始状态概率向量: \\[ \\pi=\\left(\\pi_i\\right) \\] 其中, \\[ \\pi_i=P\\left(i_1=q_i\\right), \\quad i=1,2, \\cdots, N \\] 是时刻 \\(t=1\\) 处于状态 \\(q_i\\) 的概率。 隐马尔可夫模型由初始状态概率向量 \\(\\pi\\) 、状态转移概率矩阵 \\(A\\) 和观测概率矩阵 \\(B\\) 决定。 \\(\\pi\\) 和 \\(A\\) 决定状态序列, \\(B\\) 决定观测序列。因此, 隐马尔可夫模型 \\(\\lambda\\) 可以用三元 符号表示, 即 \\[ \\lambda=(A, B, \\pi) \\] \\(A, B, \\pi\\) 称为隐马尔可夫模型的三要素。 状态转移概率矩阵 \\(A\\) 与初始状态概率向量 \\(\\pi\\) 确定了隐藏的马尔可夫链, 生成不 可观测的状态序列。观测概率矩阵 \\(B\\) 确定了如何从状态生成观测, 与状态序列综合确 定了如何产生观测序列。 ","date":"2021-10-25","objectID":"/hmm/:2:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"两个基本假设 齐次马尔可夫性假设, 即假设隐藏的马尔可夫链在任意时刻 \\(t\\) 的状态只依赖 于其前一时刻的状态, 与其他时刻的状态及观测无关, 也与时刻 \\(t\\) 无关: \\[ P\\left(i_t \\mid i_{t-1}, o_{t-1}, \\cdots, i_1, o_1\\right)=P\\left(i_t \\mid i_{t-1}\\right), \\quad t=1,2, \\cdots, T \\] 观测独立性假设, 即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状 态, 与其他观测及状态无关: \\[ P\\left(o_t \\mid i_T, o_T, i_{T-1}, o_{T-1}, \\cdots, i_{t+1}, o_{t+1}, i_t, i_{t-1}, o_{t-1}, \\cdots, i_1, o_1\\right)=P\\left(o_t \\mid i_t\\right) \\] ","date":"2021-10-25","objectID":"/hmm/:3:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"观测序列生成的过程 输入: 隐马尔可夫模型 \\(\\lambda=(A, B, \\pi)\\), 观测序列长度 \\(T\\); 输出: 观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 。 (1) 按照初始状态分布 \\(\\pi\\) 产生状态 \\(i_1\\); (2) 令 \\(t=1\\); (3) 按照状态 \\(i_t\\) 的观测概率分布 \\(b_{i_t}(k)\\) 生成 \\(o_t\\) : (4) 按照状态 \\(i_t\\) 的状态转移概率分布 \\(\\left\\{a_{i_t i_{t+1}}\\right\\}\\) 产生状态 \\(i_{t+1}, i_{t+1}=1,2, \\cdots, N\\); (5) 令 \\(t=t+1\\); 如果 \\(t\u003cT\\), 转步 (3); 否则, 终止。 ","date":"2021-10-25","objectID":"/hmm/:4:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"概率计算问题 ","date":"2021-10-25","objectID":"/hmm/:5:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"直接计算（复杂度太高） 给定模型 \\(\\lambda=(A, B, \\pi)\\) 和观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\), 计算观测序列 \\(O\\) 出现 的概率 \\(P(O \\mid \\lambda)\\) 。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为 \\(T\\) 的状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\), 求各个状态序列 \\(I\\) 与观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 的联合概率 \\(P(O, I \\mid \\lambda)\\), 然后对所有可能的状态序列求和, 得到 \\(P(O \\mid \\lambda)\\) 。 状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\) 的概率是: \\[ P(I \\mid \\lambda)=\\pi_{i_1} a_{i_1 i_2} a_{i_2 i_3} \\cdots a_{i_{T-1} i_T} \\] 对固定的状态序列 \\(I=\\left(i_1, i_2, \\cdots, i_T\\right)\\), 观测序列 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\) 的概率是: \\[ P(O \\mid I, \\lambda)=b_{i_1}\\left(o_1\\right) b_{i_2}\\left(o_2\\right) \\cdots b_{i_T}\\left(o_T\\right) \\] \\(O\\) 和 \\(I\\) 同时出现的联合概率为 \\[ \\begin{aligned} P(O, I \\mid \\lambda) \u0026=P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\\\\ \u0026=\\pi_{i_1} b_{i_1}\\left(o_1\\right) a_{i_1 i_2} b_{i_2}\\left(o_2\\right) \\cdots a_{i_{T-1} i_T} b_{i_T}\\left(o_T\\right) \\end{aligned} \\] 然后, 对所有可能的状态序列 \\(I\\) 求和, 得到观测序列 \\(O\\) 的概率 \\(P(O \\mid \\lambda)\\), 即 \\[ \\begin{aligned} P(O \\mid \\lambda) \u0026=\\sum_I P(O \\mid I, \\lambda) P(I \\mid \\lambda) \\\\\\\\ \u0026=\\sum_{i_1, i_2, \\cdots, i_T} \\pi_{i_1} b_{i_1}\\left(o_1\\right) a_{i_1 i_2} b_{i_2}\\left(o_2\\right) \\cdots a_{i_{T-1} i_T} b_{i_T}\\left(o_T\\right) \\end{aligned} \\] 这种算法复杂度太高，计算量太大，有效算法为前向算法和后向算法。 ","date":"2021-10-25","objectID":"/hmm/:5:1","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"前向算法 首先定义前向概率。 给定隐马尔可夫模型 \\(\\lambda\\), 定义到时刻 \\(t\\) 部分观测序列为 \\(o_1, o_2, \\cdots, o_t\\) 且状态为 \\(q_i\\) 的概率为前向概率, 记作 \\[ \\alpha_t(i)=P\\left(o_1, o_2, \\cdots, o_t, i_t=q_i \\mid \\lambda\\right) \\] 可以递推地求得前向概率 \\(\\alpha_t(i)\\) 及观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (观测序列概率的前向算法) 输入: 隐马尔可夫慔型 \\(\\lambda\\), 观测序列 \\(O\\); 输出: 观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (1) 初值 \\[ \\alpha_1(i)=\\pi_i b_i\\left(o_1\\right), \\quad i=1,2, \\cdots, N \\] （2）递推 对 \\(t=1,2, \\cdots, T-1\\), \\[ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^N \\alpha_t(j) a_{j i}\\right] b_i\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N \\] 终止 \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\alpha_T(i) \\] 前向算法, 步骤 (1) 初始化前向概率, 是初始时刻的状态 \\(i_1=q_i\\) 和观测 \\(o_1\\) 的 联合概率。步骤 (2) 是前向概率的递推公式, 计算到时刻 \\(t+1\\) 部分观测序列为 \\(o_1, o_2, \\cdots, o_t, o_{t+1}\\) 且在时刻 \\(t+1\\) 处于状态 \\(q_i\\) 的前向概率, 如图 \\(10.1\\) 所示。在式 (10.16) 的方括弧里, 既然 \\(\\alpha_t(j)\\) 是到时刻 \\(t\\) 观测到 \\(o_1, o_2, \\cdots, o_t\\) 并在时刻 \\(t\\) 处于状态 \\(q_j\\) 的前向概率, 那么乘积 \\(\\alpha_t(j) a_{j i}\\) 就是到时刻 \\(t\\) 观测到 \\(o_1, o_2, \\cdots, o_t\\) 并在时刻 \\(t\\) 处于 状态 \\(q_j\\) 而在时刻 \\(t+1\\) 到达状态 \\(q_i\\) 的联合概率。对这个乘积在时刻 \\(t\\) 的所有可能的 #### 统计学习方法中前向算法的例子 ### 后向算法 给定隐马尔可夫模型 \\(\\lambda\\), 定义在时刻 \\(t\\) 状态为 \\(q_i\\) 的条件下, 从 \\(t+1\\) 到 \\(T\\) 的部分观测序列为 \\(o_{t+1}, o_{t+2}, \\cdots, o_T\\) 的概率为后向概率, 记作 \\[ \\beta_t(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_T \\mid i_t=q_i, \\lambda\\right) \\] 可以用递推的方法求得后向概率 \\(\\beta_t(i)\\) 及观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (观测序列概率的后向算法) 输入: 隐马尔可夫模型 \\(\\lambda\\), 观测序列 \\(O\\); 输出: 观测序列概率 \\(P(O \\mid \\lambda)\\) 。 (1) \\[ \\beta_T(i)=1, \\quad i=1,2, \\cdots, N \\] 对 \\(t=T-1, T-2, \\cdots, 1\\) \\[ \\beta_t(i)=\\sum_{j=1}^N a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N \\] 后向算法到这一步只用到了第二个观测值，还有第一个观测值没有用到。 因此最后要乘上。 (3) \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\pi_i b_i\\left(o_1\\right) \\beta_1(i) \\] ","date":"2021-10-25","objectID":"/hmm/:5:2","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"结合 利用前向概率和后向概率的定义可以将观测序列概率 \\(P(O \\mid \\lambda)\\) 统一写成 \\[ P(O \\mid \\lambda)=\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1 \\] 也可以写成： \\[ P(O\\mid \\lambda) = \\sum_{i=1}^N[\\alpha_t(i)\\beta_t(i)], \\quad t=1,2,\\cdots, T-1 \\] ","date":"2021-10-25","objectID":"/hmm/:5:3","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"一些概率和期望问题 利用前向概率和后向概率, 可以得到关于单个状态和两个状态概率的计算公式。 1. 给定模型 \\(\\lambda\\) 和观测 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 的概率。记 \\[ \\gamma_t(i)=P\\left(i_t=q_i \\mid O, \\lambda\\right) \\] 可以通过前向后向概率计算。事实上， \\[ \\gamma_t(i)=P\\left(i_t=q_i \\mid O, \\lambda\\right)=\\frac{P\\left(i_t=q_i, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)} \\] 由前向概率 \\(\\alpha_t(i)\\) 和后向概率 \\(\\beta_t(i)\\) 定义可知: \\[ \\alpha_t(i) \\beta_t(i)=P\\left(i_t=q_i, O \\mid \\lambda\\right) \\] 于是得到: \\[ \\gamma_t(i)=\\frac{\\alpha_t(i) \\beta_t(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\] 给定模型 \\(\\lambda\\) 和观测 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 且在时刻 \\(t+1\\) 处于状态 \\(q_j\\) 的概 率。记 \\[ \\xi_t(i, j)=P\\left(i_t=q_i, i_{t+1}=q_j \\mid O, \\lambda\\right) \\] 可以通过前向后向概率计算: \\[ \\xi_t(i, j)=\\frac{P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)}{\\sum_{i=1}^N \\sum_{j=1}^N P\\left(i_t=q_i, i_{t+1}=q_j, O \\mid \\lambda\\right)} \\] 而 \\[ P(i_t=q_i,i_{t+1}=q_j,O | \\lambda) = \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j) \\] 所以 \\[ \\xi_t(i, j)=\\frac{\\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i) a_{i j} b_j\\left(o_{t+1}\\right) \\beta_{t+1}(j)} \\] 将 \\(\\gamma_t(i)\\) 和 \\(\\xi_t(i, j)\\) 对各个时刻 \\(t\\) 求和, 可以得到一些有用的期望值。 在观测 \\(O\\) 下状态 \\(i\\) 出现的期望值: \\[ \\sum_{t=1}^T \\gamma_t(i) \\] （2）在观测 \\(O\\) 下由状态 \\(i\\) 转移的期望值: \\[ \\sum_{t=1}^{T-1} \\gamma_t(i) \\] 在观测 \\(O\\) 下由状态 \\(i\\) 转移到状态 \\(j\\) 的期望值: \\[ \\sum_{t=1}^{T-1} \\xi_t(i, j) \\] ","date":"2021-10-25","objectID":"/hmm/:6:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"预测问题 ","date":"2021-10-25","objectID":"/hmm/:7:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"近似算法 近似算法的想法是, 在每个时刻 \\(t\\) 选择在该时刻最有可能出现的状态 $i_t^* $, 从而得 到一个状态序列 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\), 将它作为预测的结果。 给定隐马尔可夫模型 \\(\\lambda\\) 和观测序列 \\(O\\), 在时刻 \\(t\\) 处于状态 \\(q_i\\) 的概率 \\(\\gamma_t(i)\\) 是 \\[ \\gamma_t(i)=\\frac{\\alpha_t(i) \\beta_t(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\] 在每一时刻 \\(t\\) 最有可能的状态 $i_t^* $ 是 \\[ i_t^* =\\arg \\max_{1 \\leqslant i \\leqslant N}\\left[\\gamma_t(i)\\right], \\quad t=1,2, \\cdots, T \\] 从而得到状态序列 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 近似算法的优点是计算简单, 其缺点是不能保证预测的状态序列整体是最有可能 的状态序列, 因为预测的状态序列可能有实际不发生的部分。事实上, 上述方法得到 的状态序列中有可能存在转移概率为 0 的相邻状态, 即对某些 \\(i, j, a_{i j}=0\\) 时。尽管 如此, 近似算法仍然是有用的。 近似算法就是一种贪心的算法，每个时刻都取最有可能的状态，但整体序列并不一定是最优解。 ","date":"2021-10-25","objectID":"/hmm/:7:1","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"维特比算法 维特比算法实际上是用动态规划解隐马尔科夫模型预测问题，用动态规划求解概率最大路径，一条路径对应着一条状态序列。 首先导入两个变量 \\(\\delta\\) 和 \\(\\Psi\\) 。定义在时刻 \\(t\\) 状态为 \\(i\\) 的所有单个路径 \\(\\left(i_1, i_2, \\cdots, i_t\\right)\\) 中概率最大值为 \\[ \\delta_t(i)=\\max_{i_1, i_2, \\cdots, i_{t-1}} P\\left(i_t=i, i_{t-1}, \\cdots, i_1, o_t, \\cdots, o_1 \\mid \\lambda\\right), \\quad i=1,2, \\cdots, N \\] 后面的部分与前向算法的部分有点类似。 由定义可得变量 \\(\\delta\\) 的递推公式: \\[ \\begin{aligned} \\delta_{t+1}(i) \u0026=\\max_{i_1, i_2, \\cdots, i_t} P\\left(i_{t+1}=i, i_t, \\cdots, i_1, o_{t+1}, \\cdots, o_1 \\mid \\lambda\\right) \\\\\\\\ \u0026=\\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_t(j) a_{j i}\\right] b_i\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} \\] 定义在时刻 \\(t\\) 状态为 \\(i\\) 的所有单个路径 \\(\\left(i_1, i_2, \\cdots, i_{t-1}, i\\right)\\) 中概率最大的路径的 第 \\(t-1\\) 个结点为 \\[ \\Psi_t(i)=\\arg \\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\] 可以简单理解为找到使得从t-1的j到t的i式子\\(\\delta_{t-1}(j)a_{ji}\\)最大的j，也就是说\\(\\Psi_t(i)\\)代表t-1时刻的最佳状态值，如果t时刻的最佳状态值是i的话，那么t-1时刻的最佳状态值就是\\(\\Psi_t(i)\\)，后面回溯要用到 下面介绍维特比算法。 输入: 模型 \\(\\lambda=(A, B, \\pi)\\) 和观测 \\(O=\\left(o_1, o_2, \\cdots, o_T\\right)\\); 输出: 最优路径 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 （1）初始化 \\[ \\begin{gathered} \\delta_1(i)=\\pi_i b_i\\left(o_1\\right), \\quad i=1,2, \\cdots, N \\\\\\\\ \\Psi_1(i)=0, \\quad i=1,2, \\cdots, N \\end{gathered} \\] 前者和前向算法的初始化是一样的。 递推。对 \\(t=2,3, \\cdots, T\\) \\[ \\begin{array}{ll} \\delta_t(i)=\\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_i\\left(o_t\\right), \\quad i=1,2, \\cdots, N \\\\\\\\ \\Psi_t(i)=\\arg \\max_{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} \\] 终止 \\[ \\begin{gathered} P^* =\\max_{1 \\leqslant i \\leqslant N} \\delta_T(i) \\\\\\\\ i_T^* =\\arg \\max_{1 \\leqslant i \\leqslant N}\\left[\\delta_T(i)\\right] \\end{gathered} \\] 这里得到的是最后一个时刻的最佳状态值，然后进行回溯。 最优路径回溯。对 \\(t=T-1, T-2, \\cdots, 1\\) \\[ i_t^* =\\Psi_{t+1}\\left(i_{t+1}^* \\right) \\] 求得最优路径 \\(I^* =\\left(i_1^* , i_2^* , \\cdots, i_T^* \\right)\\) 。 书上的例子 看一个例子就很容易理解了 #### 代码 def viterbi(obs, states, start_p, trans_p, emit_p): V = [{}] # 列表idx代表时间t，字典的键代表状态值，值代表概率 path = {} # 最佳路径 for y in states: V[0][y] = start_p[y] * emit_p[y].get(obs[0], 1e-5) path[y] = [0] # 都初始化为0 for t in range(1, len(obs)): V.append({}) for y in states: em_p = emit_p[y].get(obs[t], 1e-5) # 取出观测值对应的概率 (prob, state) = max([(V[t-1][y0]*trans_p[y0][y]*em_p, y0) for y0 in states]) V[t][y] = prob path[y] = path[y] + [state] # 记录路径，state是当前时间t状态为y时t-1的最佳状态，也就是从state转移到y的概率最大。如果最后时刻的最佳状态是y，则回溯从y开始，最后的状态也是y。 (prob, state) = max((V[len(obs)-1][y], y) for y in states) # 求最后时刻的最大概率和状态。 return path[state][1:] + [state] # 初始状态是0，所以去掉第一个0，再加上最后时刻的最大概率的状态，结果就是最佳路径。这里键对值的过程相当于回溯了。 A = { 0 : {0:0.5, 1:0.2, 2:0.3}, 1 : {0:0.3, 1:0.5, 2:0.2}, 2 : {0:0.2, 1:0.3, 2:0.5} } B = { 0: {'红': 0.5, '白': 0.5}, 1: {'红': 0.4, '白': 0.6}, 2: {'红': 0.7, '白': 0.3} } π = {0:0.2, 1:0.4, 2:0.4} viterbi(['红', '白', '红'], [0, 1, 2], π, A, B) 拿上面的例子进行实验，思路是完全按照李航老师书的思路来的。还是比较容易理解的，只是回溯的实现不太一样，这个严格来说不能叫做回溯。 ","date":"2021-10-25","objectID":"/hmm/:7:2","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"学习算法 ","date":"2021-10-25","objectID":"/hmm/:8:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP","概率图模型","贝叶斯网络"],"content":"参考 《统计学习方法》李航 https://www.52nlp.cn/hmm-learn-best-practices-one-introduction https://www.cnblogs.com/pinard/p/6945257.html ","date":"2021-10-25","objectID":"/hmm/:9:0","tags":["NLP","概率图模型","贝叶斯网络","HMM"],"title":"HMM","uri":"/hmm/"},{"categories":["NLP"],"content":"What? TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。 TF-IDF有两层意思，一层是”词频”（Term Frequency，缩写为TF），另一层是”逆文档频率”（Inverse Document Frequency，缩写为IDF）。 假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。 但这样又会遇到了另一个问题，我们可能发现”量化”、“系统”、“架构”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。 当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。 ","date":"2021-10-24","objectID":"/tf-idf/:1:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"步骤 第一步，计算词频： \\[ 词频(TF) = \\frac{某个词在文章中出现次数}{文章的总词数} \\] 第二步，计算逆文档频率： \\[ 逆文档频率(IDF) = log(\\frac{语料库的文档总数}{包含该词的文档数+1}) \\] 1.为什么+1？是为了处理分母为0的情况。假如所有的文章都不包含这个词，分子就为0，所以+1是为了防止分母为0的情况。 2.为什么要用log函数？log函数是单调递增，求log是为了归一化，保证反文档频率不会过大。 3.会出现负数？肯定不会，分子肯定比分母大。 第三步，计算TF-IDF： \\[ TF-IDF = 词频(TF) \\times 逆文档频率(IDF) \\] ","date":"2021-10-24","objectID":"/tf-idf/:2:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"优缺点 TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。 ","date":"2021-10-24","objectID":"/tf-idf/:3:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"代码 corpus = ['this is the first document', 'this is the second second document', 'and the third one', 'is this the first document'] words_list = list() for i in range(len(corpus)): words_list.append(corpus[i].split(' ')) ","date":"2021-10-24","objectID":"/tf-idf/:4:0","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"手动实现 def manual(): from collections import Counter count_list = list() for i in range(len(words_list)): count = Counter(words_list[i]) count_list.append(count) import math def tf(word, count): return count[word] / sum(count.values()) def idf(word, count_list): n_contain = sum([1 for count in count_list if word in count]) return math.log(len(count_list) / (1 + n_contain)) def tf_idf(word, count, count_list): return tf(word, count) * idf(word, count_list) for i, count in enumerate(count_list): print(\"第 {} 个文档 TF-IDF 统计信息\".format(i + 1)) scores = {word : tf_idf(word, count, count_list) for word in count} sorted_word = sorted(scores.items(), key = lambda x : x[1], reverse=True) for word, score in sorted_word: print(\"\\tword: {}, TF-IDF: {}\".format(word, round(score, 5))) ","date":"2021-10-24","objectID":"/tf-idf/:4:1","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"gensim def gensim_work(): from gensim import corpora # 赋给语料库中每个词(不重复的词)一个整数id dic = corpora.Dictionary(words_list) # 创建词典 new_corpus = [dic.doc2bow(words) for words in words_list] # 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数 from gensim import models tfidf = models.TfidfModel(new_corpus) tfidf.save(\"tfidf.model\") # 载入模型 tfidf = models.TfidfModel.load(\"tfidf.model\") # 使用这个训练好的模型得到单词的tfidf值 res = [tfidf[temp] for temp in new_corpus] print(res) ","date":"2021-10-24","objectID":"/tf-idf/:4:2","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["NLP"],"content":"sklearn def sklearn_work(): from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() tfidf = vectorizer.fit_transform(corpus) print(tfidf.toarray()) # 权重 print(vectorizer.get_feature_names()) # 单词 print(vectorizer.vocabulary_) # 词典 ","date":"2021-10-24","objectID":"/tf-idf/:4:3","tags":["NLP","TF-IDF"],"title":"TF-IDF","uri":"/tf-idf/"},{"categories":["pandas"],"content":"记录一下实训学到的内容 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:0:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"布尔索引 布尔索引不能使用and or not ，只能用\u0026 | ~ 因为只能用位操作符 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"花哨索引 arr = np.arange(32).reshape((8, 4)) arr array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) arr[[1, 5, 7, 2], [0, 3, 1, 2]] array([ 4, 23, 29, 10]) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:1","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"更常用的方式为 arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] # 行，列重置顺序 array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:1:2","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"pandas.cut import pandas as pd import numpy as np cars = pd.read_csv(\"second_cars_info.csv\",encoding=\"gbk\") final_ = [cars.Sec_price.min()] + list(np.linspace(10,100,10)) + [cars.Sec_price.max()] pd.cut(cars[\"Sec_price\"],bins=final_).value_counts().sort_index() # 对区间进行排序 # labels参数给每个区间贴上标签 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:2:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":".str的用法 可以对行列进行python字符串一样的操作 stud_alcoh[[\"Mjob\",'Fjob']].apply(lambda x:x.str.upper()) # 对Mjob Fjob两列变成大写 # 也可以用applymap stud_alcoh[[\"Mjob\",'Fjob']].applymap(lambda x:x.upper()) #区别就在于.str # 因为applymap只能对dataframe进行操作,apply可以对Dataframe和Series进行操作。 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:3:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"groupby与apply灵活运用 coffee = pd.read_excel(\"coffee.xlsx\") coffee.groupby(\"区域\").apply(lambda x:x[\"销售额\"].sum()) # 如果是DataFramegroupby对象调用apply，转换函数处理的就是每一个分组（dataframe） # 等价于 coffee.groupby(\"区域\")['销售额'].sum() 求各个区域的 销售总和、平均销售额、预计销售额与实际销售额的差异总和 def transfer(x_):# x_类型是dataframe res = pd.Series() res[\"销售总和\"] = x_[\"销售额\"].sum() res[\"平均销售额\"] = x_['销售额'].mean() res['差值'] = ( x_[\"销售额\"] - x_['预计销售额']).mean() return res # 返回的是一个series coffee.groupby(\"区域\").apply(transfer) 求出个区域中，高于该区域平均利润的记录 def transfer(x): # x(dataframe) mean_value = x[\"利润额\"].mean() condition = x[\"利润额\"] \u003e mean_value return x[condition] # dataframe c = coffee.groupby(\"区域\").apply(transfer) # 二维索引 # c.loc[(\"Central\",0)] ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:4:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"groupby也可以嵌套 def top_3_data(x):# x 表示DataFrame 每个区域的数据集 res = x.groupby(\"产品名称\").sum().sort_values(\"销售额\",ascending=False).iloc[:3] return res coffee.groupby(\"区域\").apply(top_3_data) ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:4:1","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"nlargest nsmallest 求每一列最大的几个多最小的几个，一般配合groupby和apply使用 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:5:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"pd.Grouper grouper = pd.Grouper(key=\"订单日期\",freq=\"M\") 按月进行分组 。freq=\"Y\"就是按年分组 def top_sale_month(x): #x 表示DataFrame 每个产品的数据集 grouper = pd.Grouper(key=\"订单日期\",freq=\"M\") return x.groupby(grouper).sum().nlargest(1,\"销售额\") coffee.groupby(\"产品名称\").apply(top_sale_month).reset_index() reset_index()方法就是按照原来的index显示，不然就是按照分组的结果展示 ## plt.xticks plt.yticks 可以理解为自定义x轴的坐标 plt.figure(figsize=(12,8)) width = 0.2 plt.bar(np.arange(4)+0,np.random.randint(3,10,(4)),color='r',width=width) plt.bar(np.arange(4)+width*1,np.random.randint(3,10,(4)),color='g',width=width) plt.bar(np.arange(4)+width*2,np.random.randint(3,10,(4)),color='b',width=width) plt.xticks(np.arange(4)+width/2,list(\"abcd\")) # 将0 1 2 3 替换为 a b c d plt.show() ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:6:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["pandas"],"content":"np.where np.where(condition, [x, y]),这里三个参数,其中必写参数是condition(判断条件),后边的x和y是可选参数.那么这三个参数都有怎样的要求呢? condition：array_like，bool,当为True时，产生x，否则产生y 情况1： np.where([[True, False], [True, True]], [[1, 2], [3, 4]], [[9, 8], [7, 6]]) 返回： array([[1, 8], [3, 4]]) 条件中第0个元素中的第0个元素是true,那么取x中的相对应元素1; 条件中第0个元素中的第1个元素是false,那么取y中的相对应元素8; 条件中第1个元素中的第0个元素是ture,那么取x中相对应的元素3; 条件中第1个元素中的第1个元素是ture,那么取x中相对应的元素4; 所以最后的结果中取出的元素是1,8,3,4. 情况2： x = np.arange(9.).reshape(3, 3) np.where(x\u003e5) # 返回的是索引 (array([2, 2, 2], dtype=int64), array([0, 1, 2], dtype=int64)) 第一个array是行坐标，第二个array为列坐标。 不想要索引想要具体的数值也很简单 x[np.where(x\u003e5)] array([6., 7., 8.]) np.where(x \u003c 5, x, -1) array([[ 0., 1., 2.], [ 3., 4., -1.], [-1., -1., -1.]]) 可见小于五的部分不变，大于5的则变成了-1 np.where常用于pandas的Series中。 ","date":"2021-10-24","objectID":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/:7:0","tags":["pandas","learn_three"],"title":"实训学习内容","uri":"/%E5%AE%9E%E8%AE%AD%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"},{"categories":["算法题"],"content":"平衡括号字符串的最少插入次数 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:0:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-insertions-to-balance-a-parentheses-string/ ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:1:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 本题和前面的题属于同一系列的，都是平衡括号字符串，不过这个不是1:1 而是1:2 思路还是差不多，不过判断条件需要改变 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:2:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def minInsertions(self, s: str) -\u003e int: res,temp = 0,0 for i in s: if i == '(': temp += 2 if temp % 2 == 1: res += 1 temp -= 1 if i == ')': temp -= 1 if temp == -1: res += 1 temp = 1 return res + temp 开始还是初始化，temp代表需求的右括号的数量 如果有左括号的话，则让右括号的需求+2 因为一个左对应两个右 这里有个难点，如果需求的是奇数的话，则应添加一个右括号，然后让需求减1 如果是右括号，则需求 减1 如果需求的成了-1 的话 则在左边补上左括号 res++ 此时还需要一个右括号，则temp再初始化为1 最后还是输出 Q:为什么最后不是 temp == -2 res += 1 temp=0 呢？ 看看这个例子: \")))))))\" 这是7个右括号，最后减到最后的话，temp是个负数，影响了最后的结果。 所以还是要用原来的那样，-1的时候就进行判断，不用考虑奇偶的问题了 ","date":"2021-10-19","objectID":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/:3:0","tags":["算法题","平衡括号字符串的最少插入次数"],"title":"平衡括号字符串的最少插入次数","uri":"/%E5%B9%B3%E8%A1%A1%E6%8B%AC%E5%8F%B7%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%B0%91%E6%8F%92%E5%85%A5%E6%AC%A1%E6%95%B0/"},{"categories":["python"],"content":"可变与不可变 “-=”操作符会调用__isub__函数，而”-“操作符会调用__sub__函数，一般对于可变对象来说“-=”操作符会直接改变self自身。 import torch x1 = 1 x2 = 2 params = [x1, x2] for p in params: print(id(p), id(x1), id(x2)) p -= 4 print(id(p), id(x1), id(x2)) print(params) x1 = torch.Tensor([1]) x2 = torch.Tensor([2]) params = [x1, x2] for p in params: print(id(p), id(x1), id(x2)) p -= 4 print(id(p), id(x1), id(x2)) print(params) 9784896 9784896 9784928 9784768 9784896 9784928 9784928 9784896 9784928 9784800 9784896 9784928 [1, 2] 139752445458112 139752445458112 139752445458176 139752445458112 139752445458112 139752445458176 139752445458176 139752445458112 139752445458176 139752445458176 139752445458112 139752445458176 [tensor([-3.]), tensor([-2.])] 可以看到对于int类型，地址变换了，而torch类型，地址却没有变化。 p -= 4等价于p.sub_(4)。这个可变对象改变了自身。写成p = p - 4则会调用构造函数，并返回一个新的变量，也就不可能作用到原先的“可变对象”。 int类没有发生就地变化是因为它是一个不可变对象。 这是python “-” 与”-=“的一个坑 微妙的字符串 a = 'some_thing' b = 'some'+'_'+'thing' id(a),id(b) (1957716471920, 1957716471920) a = 'wtf' b = 'wtf' a is b True a = 'wtf!' b = 'wtf!' a is b False a,b = 'wtf!','wtf!' a is b True 'a'*20 is 'aaaaaaaaaaaaaaaaaaaa','a'*21 is 'aaaaaaaaaaaaaaaaaaaaa' (True, False) Cpython 在编译优化时, 某些情况下会尝试使用已经存在的不可变对象,成为字符串驻留 发生驻留之后, 许多变量可能指向内存中的相同字符串对象 所有长度为 0 和长度为 1 的字符串都被驻留. 字符串在编译时被实现 (‘wtf’ 将被驻留, 但是 ’‘.join([’w’, ’t’, ’f’] 将不会被驻留) 字符串中只包含字母，数字或下划线时将会驻留. 所以 ’wtf!’ 由于包含 ! 而未被驻留。 当在同一行将 a 和 b 的值设置为 “wtf!” 的时候, Python 解释器会创建一个新对象, 然后同时引用第二个变量. 常量折叠(constant folding) 是 Python 中的一种 窥孔优化(peephole optimization) 技术. 这意味着在编译时表达式 ‘a’*20 会被替换为 ‘aaaaaaaaaaaaaaaaaaaa’ 以减少运行时的时钟周期. 只有长度小于 20 的字符串才会发生常量折叠. a = 1 b = 1 a is b,id(a) == id(b) (True, True) is 是比较对象是否相同(is 表示对象标识符即 object identity)，即用 id() 函数查看的地址是否相同，如果相同则返回 True，如果不同则返回 False。is 不能被重载。 == 是比较两个对象的值是否相等，此操作符内部调用的是 _eq() 方法。所以 a==b 等效于a.___eq__(b)，所以 = 可以被重载 ","date":"2021-10-18","objectID":"/wtf/:1:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"是时候来点蛋糕了! some_dict = {} some_dict[5.5] = 'ruby' some_dict[5.0] = 'javascript' some_dict[5] = 'python' print(some_dict[5.0]) python 5 == 5.0,hash(5) == hash(5.0) (True, True) Python 字典通过检查键值是否相等和比较哈希值来确定两个键是否相同. 具有相同值的不可变对象在Python中始终具有相同的哈希值 ","date":"2021-10-18","objectID":"/wtf/:2:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"本质上,我们都一样 class WTF: pass print(WTF() == WTF(),WTF() is WTF()) print(hash(WTF()) == hash(WTF())) print(id(WTF()) == id(WTF())) False False True True 当调用 id 函数时, Python 创建了一个 WTF 类的对象并传给 id 函数. 然后 id 函数获取其id值 (也就是内存地址), 然后丢弃该对象. 该对象就被销毁了. 当我们连续两次进行这个操作时, Python会将相同的内存地址分配给第二个对象. 因为 (在CPython中) id 函数使用对象的内存地址作为对象的id值, 所以两个对象的id值是相同的. print(id(id(WTF())) == id(id(WTF()))) ##无论多少个ID都是True 原因就在上面 ##虽然id(id(WTF())) == id(id(WTF())) 但是id(WTF()) is id(WTF()) 返回True ##原因就是id这个函数调用的过程特殊性 print(id(WTF()) is id(WTF())) True False class WTF(object): def __init__(self): print(\"I\") def __del__(self): print(\"D\") WTF() is WTF() ##这时是两个对象一起创建，然后一起销毁，所以id不一样 I I D D False id(WTF()) == id(WTF()) ##这时候先创建一个销毁，然后再创建。对象销毁的顺序是造成所有不同之处的原因. I D I D True ","date":"2021-10-18","objectID":"/wtf/:3:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"为什么？ some_string = \"wtf\" some_dict = {} for i, some_dict[i] in enumerate(some_string): pass some_dict Python 语法 中对 for 的定义是: {0: 'w', 1: 't', 2: 'f'} for_stmt: 'for' exprlist 'in' testlist ':' suite ['else' ':' suite] 其中 exprlist 指分配目标. 这意味着对可迭代对象中的每一项都会执行类似 {exprlist} = {next_value} 的操作. for i in range(4): print(i) i = 10 0 1 2 3 ","date":"2021-10-18","objectID":"/wtf/:4:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"列表副本 list1 = [1,2,3,4,5] list2 = list1 list2[0] = 6 print(list1,list2) [6, 2, 3, 4, 5] [6, 2, 3, 4, 5] list1 = [1,2,3,4,5] list2 = list1[:] list2[0] = 6 print(list1,list2) [1, 2, 3, 4, 5] [6, 2, 3, 4, 5] ","date":"2021-10-18","objectID":"/wtf/:5:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"执行时机差异 array = [1, 8, 15] g = (x for x in array if array.count(x) \u003e 0) ##这时候x为[1,8,15]的解包 ##而后面的array变成了下面的 array = [2, 8, 22] print(list(g)) [8] 在生成器表达式中, in 子句在声明时执行, 而条件子句则是在运行时执行. 所以在运行前, array 已经被重新赋值为 [2, 8, 22], 因此对于之前的 1, 8 和 15, 只有 count(8) 的结果是大于 0 的, 所以生成器只会生成 8. array_1 = [1,2,3,4] g1 = (x for x in array_1) array_1 = [1,2,3,4,5] array_2 = [1,2,3,4] g2 = (x for x in array_2) array_2[:] = [1,2,3,4,5] print(list(g1)) print(list(g2)) [1, 2, 3, 4] [1, 2, 3, 4, 5] 第二部分中 g1 和 g2 的输出差异则是由于变量 array_1 和 array_2 被重新赋值的方式导致的. 在第一种情况下, array_1 被绑定到新对象 [1,2,3,4,5], 因为 in 子句是在声明时被执行的， 所以它仍然引用旧对象 [1,2,3,4]. 在第二种情况下, 对 array_2 的切片赋值将相同的旧对象 [1,2,3,4] 原地更新为 [1,2,3,4,5]. 因此 g2 和 array_2 仍然引用同一个对象(这个对象现在已经更新为 [1,2,3,4,5]). ","date":"2021-10-18","objectID":"/wtf/:6:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"出人意料的is a = 256 b = 256 a is b True a = 257 b = 257 ##256 是一个已经存在的对象, 而 257 不是 ##当你启动Python 的时候, -5 到 256 的数值就已经被分配好了. ##这些数字因为经常使用所以适合被提前准备好 a is b False a,b = 257,257 ##当 a 和 b 在同一行中使用相同的值初始化时，会指向同一个对象. print(a is b) print(id(a),id(b)) True 1957717387056 1957717387056 [] == [] True [] is [] ##两个空列表位于不同的内存地址 False ","date":"2021-10-18","objectID":"/wtf/:7:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"一蹴即至! row = [\"\"] * 3 board = [row] * 3 board [['', '', ''], ['', '', ''], ['', '', '']] board[0][0] = 'X' board ##这是因为之前对row做乘法导致的 [['X', '', ''], ['X', '', ''], ['X', '', '']] ##如何避免这种情况？ board = [['']*3 for _ in range(3)] board[0][0] = 'X' board [['X', '', ''], ['', '', ''], ['', '', '']] ","date":"2021-10-18","objectID":"/wtf/:8:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"麻烦的输出 funcs = [] res = [] for x in range(7): def func(): return x funcs.append(func) res.append(func()) func_res = [func() for func in funcs] print(func_res,res) [6, 6, 6, 6, 6, 6, 6] [0, 1, 2, 3, 4, 5, 6] power_x = [lambda x:x**i for i in range(11)] print([func(2) for func in power_x]) [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024] 在循环内部定义一个函数时, 如果该函数在其主体中使用了循环变量, 则闭包函数将与循环变量绑定, 而不是它的值. 因此, 所有的函数都是使用最后分配给变量的值来进行计算的. ","date":"2021-10-18","objectID":"/wtf/:9:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"连Python也知道爱是难言的 import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! love = this this is love True love is True False love is False False love is not True or False True love is not True or False;love is love True ","date":"2021-10-18","objectID":"/wtf/:10:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"三个引号 print('wtfpython''') wtfpython print(\"wtf\" \"python\") wtfpython ","date":"2021-10-18","objectID":"/wtf/:11:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"布尔你咋了? mixed_list = [False, 1.0, \"some_string\", 3, True, [], False] integers_found_so_far = 0 booleans_found_so_far = 0 for item in mixed_list: if isinstance(item, int): integers_found_so_far += 1 elif isinstance(item, bool): booleans_found_so_far += 1 integers_found_so_far 4 booleans_found_so_far 0 another_dict = {} another_dict[True] = \"JavaScript\" another_dict[1] = \"Ruby\" another_dict[1.0] = \"Python\" another_dict[True] 'Python' 布尔值是 int 的子类 some_iterable = ('a', 'b') def some_func(val): return \"something\" [x for x in some_iterable] ['a', 'b'] [(yield x) for x in some_iterable] \u003cgenerator object \u003clistcomp\u003e at 0x000001CC6FFC3888\u003e list([(yield x) for x in some_iterable]) ['a', 'b'] list(((yield x) for x in some_iterable)) ['a', None, 'b', None] list(some_func((yield x)) for x in some_iterable) ['a', 'something', 'b', 'something'] ","date":"2021-10-18","objectID":"/wtf/:12:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"消失的外部变量 e = 7 try: raise Exception() except Exception as e: pass print(e) ##error! ","date":"2021-10-18","objectID":"/wtf/:13:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"从有到无 some_list = [1, 2, 3] some_dict = { \"key_1\": 1, \"key_2\": 2, \"key_3\": 3 } some_list = some_list.append(4) some_dict = some_dict.update({\"key_4\": 4}) some_list some_dict 大多数修改序列/映射对象的方法, 比如 list.append, dict.update, list.sort 等等. 都是原地修改对象并返回 None. 这样做的理由是, 如果操作可以原地完成, 就可以避免创建对象的副本来提高性能. ","date":"2021-10-18","objectID":"/wtf/:14:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"迭代列表时删除元素 list_1 = [1, 2, 3, 4] list_2 = [1, 2, 3, 4] list_3 = [1, 2, 3, 4] list_4 = [1, 2, 3, 4] for idx, item in enumerate(list_1): del item for idx, item in enumerate(list_2): list_2.remove(item) for idx, item in enumerate(list_3[:]): list_3.remove(item) for idx, item in enumerate(list_4): list_4.pop(idx) list_1 ##没有修改list_1 [1, 2, 3, 4] list_2 ##每一次删除元素后 迭代的list_2也发生改变 比如第一次删除了1 list_2为[2,3,4]这时idx=1 所以下一个删除了3 [2, 4] list_3 ##迭代副本不会出现上述情况 [] list_4 [2, 4] ","date":"2021-10-18","objectID":"/wtf/:15:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"循环变量泄露 for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') 6 : for x inside loop 6 : x in global ## 这次我们先初始化x x = -1 for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') 6 : for x inside loop 6 : x in global x = 1 print([x for x in range(5)]) print(x, ': x in global') [0, 1, 2, 3, 4] 1 : x in global ","date":"2021-10-18","objectID":"/wtf/:16:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"当心默认的可变参数 def some_func(default_arg=[]): default_arg.append(\"some_string\") return default_arg some_func() ['some_string'] some_func() ['some_string', 'some_string'] some_func([]) ['some_string'] some_func() ['some_string', 'some_string', 'some_string'] Python中函数的默认可变参数并不是每次调用该函数时都会被初始化. 相反, 它们会使用最近分配的值作为默认值. 当我们明确的将 [] 作为参数传递给 some_func 的时候, 就不会使用 default_arg 的默认值, 所以函数会返回我们所期望的结果. some_func.__defaults__ (['some_string', 'some_string', 'some_string'],) 避免可变参数导致的错误的常见做法是将 None 指定为参数的默认值, 然后检查是否有值传给对应的参数. 例: def some_func(default_arg=None): if not default_arg: default_arg = [] default_arg.append(\"some_string\") return default_arg ","date":"2021-10-18","objectID":"/wtf/:17:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"同人不同命 a = [1, 2, 3, 4] b = a a = a + [5, 6, 7, 8] a [1, 2, 3, 4, 5, 6, 7, 8] b [1, 2, 3, 4] a = [1, 2, 3, 4] b = a a += [5, 6, 7, 8] a [1, 2, 3, 4, 5, 6, 7, 8] b [1, 2, 3, 4, 5, 6, 7, 8] a += b 并不总是与 a = a + b 表现相同. 类实现 op= 运算符的方式 也许 是不同的, 列表就是这样做的. 表达式 a = a + [5,6,7,8] 会生成一个新列表, 并让 a 引用这个新列表, 同时保持 b 不变. 表达式 a += [5,6,7,8] 实际上是使用的是 “extend” 函数, 所以 a 和 b 仍然指向已被修改的同一列表. a_var = 'global variable' def a_func(): print(a_var, '[ a_var inside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') global variable [ a_var inside a_func() ] global variable [ a_var outside a_func() ] a_var = 'global value' def a_func(): a_var = 'local value' print(a_var, '[ a_var inside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') local value [ a_var inside a_func() ] global value [ a_var outside a_func() ] a_var = 'global value' def a_func(): global a_var a_var = 'local value' print(a_var, '[ a_var inside a_func() ]') print(a_var, '[ a_var outside a_func() ]') a_func() print(a_var, '[ a_var outside a_func() ]') global value [ a_var outside a_func() ] local value [ a_var inside a_func() ] local value [ a_var outside a_func() ] a_var = 'global value' def outer(): a_var = 'enclosed value' def inner(): a_var = 'local value' print(a_var) inner() outer() local value a_var = 'global variable' def len(in_var): print('called my len() function') l = 0 for i in in_var: l += 1 return l def a_func(in_var): len_in_var = len(in_var) print('Input variable is of length', len_in_var) a_func('Hello, World!') called my len() function Input variable is of length 13 a = 'global' def outer(): def len(in_var): print('called my len() function: ', end=\"\") l = 0 for i in in_var: l += 1 return l a = 'local' def inner(): global len nonlocal a a += ' variable' inner() print('a is', a) print(len(a)) outer() print(len(a)) print('a is', a) a is local variable called my len() function: 14 called my len() function 6 a is global ","date":"2021-10-18","objectID":"/wtf/:18:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["python"],"content":"大海捞针 x, y = (0, 1) if True else None, None x,y ((0, 1), None) ##正确做法 x,y = (0,1) if True else (None,None) x,y (0, 1) t = ('one', 'two') for i in t: print(i) t = ('one') for i in t: print(i) t = () print(t) one two o n e () ##明显上面的把t = ('one') t当成字符串了，正确做法如下 t = ('one',) ##注意逗号 for i in t: print(i) one ","date":"2021-10-18","objectID":"/wtf/:19:0","tags":["python","WTF"],"title":"WTF","uri":"/wtf/"},{"categories":["NLP"],"content":"共现矩阵 主要用于发现主题，解决词向量相近关系的表示 例如，语料库如下： - I like deep learning. - I like NLP. - I enjoy flying. 则共现矩阵如下(窗口大小为1) 例如：“I like”出现在第1，2句话中，一共出现2次，所以=2。 对称的窗口指的是，“like I”也是2次 将共现矩阵行(列)作为词向量表示后，可以知道like，enjoy都是在I附近且统计数目大约相等，他们意思相近。 ","date":"2021-10-15","objectID":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/:0:0","tags":["NLP","共现矩阵"],"title":"共现矩阵","uri":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/"},{"categories":["NLP"],"content":"代码 import numpy as np word2ind = {w:i for i,w in enumerate(words)} # word到key，words就是词汇表 M = np.zeros((num_words, num_words)) # num_words是词汇表的长度 for c in corpus: # 假设语料库是一个列表，元素为一段文本。遍历语料库 for idx, word in enumerate(c): # 遍历文本的每一个词，这里默认空格分词 for i in range(1, window_size+1): # 对窗口大 小进行遍历 left = idx - i # 自己与自己不算共现，所以这里要加减 right = idx + i if left \u003e= 0: # 左边元素 M[word2ind[word], word2ind[c[left]]] += 1 if right \u003c len(c): # 右边元素 M[word2ind[word], word2ind[c[right]]] += 1 ","date":"2021-10-15","objectID":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/:1:0","tags":["NLP","共现矩阵"],"title":"共现矩阵","uri":"/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5/"},{"categories":["python"],"content":"两个数的交换 # a = 1 # b = 2 # temp = b # b = a # a = temp # print(a,b) a = 1 b = 2 a,b = b,a print(a,b) 2 1 ","date":"2021-10-10","objectID":"/skill/:1:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"格式化字符串 a = 17 name = \"wlb\" # print('%s is %d years old' % (name,a)) # print('{} is {} years old'.format(name,a)) print(f'{name} is {a} years old') #明显这个方法更简单 wlb is 17 years old ","date":"2021-10-10","objectID":"/skill/:2:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"yield与yield from def fib(n): a = 0 b = 1 for _ in range(n): yield a a,b = b,a+b for i in fib(10): print(i) #注释的内容与yield a效果相同，yield相当于使其成为一个迭代器 yield一个数后会立马传递出去，而return 要等列表都生成完毕后才会传出去 #他的优势在于一些耗时的操作 # 通过yield来进行dfs，由于没有实现__next__因此是个可迭代对象而不是一个迭代器 class Node: def __init__(self,value) -\u003e None: self._value = value self._node = [] def __repr__(self) -\u003e str: return f'Node({self._value})' def add_children(self,node:'Node') -\u003e 'Node': self._node.append(node) def __iter__(self): return iter(self._node) def dfs(self): yield self for i in self: yield from i.dfs() root = Node(0) children1 = Node(1) children2 = Node(2) root.add_children(children1) root.add_children(children2) children1.add_children(Node(3)) children1.add_children(Node(4)) children11 = Node(5) children2.add_children(children11) children11.add_children(Node(6)) for c in root.dfs(): print(c) from typing import Iterable def test_format(datas: Iterable[str], max_len: int): for data in datas: if len(data) \u003e max_len: yield data[:max_len] + '...' else: yield data print(list(test_format(['vllbc', 'test_for_this_function', 'good'],5))) # 把长度大于5的部分变成省略号 #子生成器 def average_gen(): total = 0 count = 0 average = 0 while True: new_num = yield average if new_num is None: break count += 1 total += new_num average = total/count return total,count,average # 委托生成器 def proxy_gen(): while True: total,count,average = yield from average_gen() # yield from后面是一个可迭代对象,此文后面的将多维数组转化为一维数组中flatten函数就用到了yield from，原理就是如果列表中一个元素是列表就yield from这个列表，否则就直接yield这个元素，也利用了递归的方法。如果子生成器退出while循环了，就执行return以获取返回值。 print(total,count,average) def main(): t = proxy_gen() next(t) print(t.send(10)) print(t.send(15)) print(t.send(20)) t.send(None) main() ","date":"2021-10-10","objectID":"/skill/:3:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"列表解析式 lists = [f\"http://www.baidu.com/page{n}\" for n in range(21)] lists#此方法在爬虫构造urls中非常常用 # lists = [f\"http://www.baidu.com/page{n}\" for n in range(21) if n%2==0] page偶数 # alp = \"abcdefghigklmnopqrstuvwxyz\" # ALP = [n.upper() for n in alp] 将小写转换为大写 ","date":"2021-10-10","objectID":"/skill/:4:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"enumerate lists = ['apple','banana','cat','dog'] for index,name in enumerate(lists): print(index,name) # 手动实现一下enumerate from typing import Iterable def enumerate_(Iterable:Iterable,start=0): yield from zip(range(start,start+len(Iterable)),Iterable) for i,item in enumerate_([1,2,3,4,5,6],9): print(i,item) ","date":"2021-10-10","objectID":"/skill/:5:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"字典的合并 dic1 = {'qq':1683070754, 'phone':123456789 } dic2 = { 'height':180, 'handsome':True } dic3 = {**dic1,**dic2} #合并两个字典 **叫做解包 #或者用dic1.update(dic2) 将dic2合并到dic1 相同键则dic2替代dic1 dic3 {'handsome': True, 'height': 180, 'phone': 123456789, 'qq': 1683070754} ","date":"2021-10-10","objectID":"/skill/:6:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"序列解包 name = \"wang lingbo\" xing,ming = name.split(\" \") #split返回一个序列，分别赋给xing 和ming print(xing,ming) #x,*y,z = [1,2,3,4,5] #x:1 z:5 y:[2,3,4] wang lingbo ","date":"2021-10-10","objectID":"/skill/:7:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"匿名函数lambda lists = [1,2,3,4,5,6] maps = map(lambda x:x*x,lists) print(maps) print(list(maps)) \u003cmap object at 0x000001911C8E03C8\u003e [1, 4, 9, 16, 25, 36] ","date":"2021-10-10","objectID":"/skill/:8:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"装饰器 def logging(level): def wapper(func): def inner_wapper(*args, **wbargs): print(f'{level} enter in {func.__name__}()') return func(*args, **wbargs) #不写return 也可以 return inner_wapper return wapper @logging('inner') def say(a): print('hello! {}'.format(a)) say('wlb') inner enter in say() hello! wlb import time def print_time(func): def wapper(*args,**wbargs): print(f'{func.__name__}()调用于{time.asctime(time.localtime(time.time()))}') return func(*args,**wbargs) #不写return 也可以 return wapper @print_time def my_name(name): print(f'look!{name}') my_name(\"wlb\") my_name()调用于Wed Dec 9 21:21:00 2020 look!wlb ","date":"2021-10-10","objectID":"/skill/:9:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"map、reduce、filter # map print(list(map(abs,[-1,-2,-3,-4,-5]))) #也可以自己定义函数或者用匿名函数 # reduce from functools import reduce #python3中需要从内置库导入 print(reduce(lambda x,y:x+y,list(map(int,str(131351412))))) # filter a = [1,2,3,4,5,6,7,8,9] new_a = filter(lambda x:x%2!=0,a) #filter就是筛选 list(new_a) # 这三个都是函数式编程中常用的函数 ","date":"2021-10-10","objectID":"/skill/:10:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"join() # lists = ['1','2','3','4','5'] # ''.join(lists) lists = [1,2,3,4,5] ''.join(list(map(str,lists))) #join只能是字符串列表，所以要map转换一下 '12345' ","date":"2021-10-10","objectID":"/skill/:11:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"将多维数组转换为一维 ab = [[1, 2, 3], [5, 8], [7, 8, 9]] print([i for item in ab for i in item]) #利用列表解析式 print(sum(ab, [])) # 利用sum函数 from functools import reduce print(reduce(lambda x,y:x+y,ab)) # 利用reduce from itertools import chain print(list(chain(*ab))) # 利用chain def flatten(items,ignore=(str,bytes)): for x in items: if isinstance(x,Iterable) and not isinstance(x,ignore): yield from flatten(x) else: yield x print(list(flatten(ab))) # 利用自己定义的函数 [1, 2, 3, 5, 8, 7, 8, 9] ","date":"2021-10-10","objectID":"/skill/:12:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"将一个列表倒序 lists = [2,4,3,2,5,4] lists[::-1] # list(reversed(lists)) ","date":"2021-10-10","objectID":"/skill/:13:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"随机生成密码 import random b = 8 t = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890' print(''.join(random.sample(t,b))) # 主要就是用sample这个方法来取多个随机值 0KmtEZSU ","date":"2021-10-10","objectID":"/skill/:14:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"断言 assert(True is True) #成功 print('yes') assert(True is False) #报错 print('no') yes ","date":"2021-10-10","objectID":"/skill/:15:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"合并列表 list1 = [1,2,31,13] list2 = [5,2,12,32] # list1.append(list2) # print(list1) #错误方法 list1.extend(list2) print(list1) #正确方法 [1, 2, 31, 13, 5, 2, 12, 32] a = [1,2,3,4,5] b = ['a','b','c','d','e'] fin = dict() for k,i in zip(a,b): fin[k] = i print(fin) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} # 或者 d = {} for i,d[i] in zip(a,b): pass print(d) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} 为什么？在WTFpython中有讲 # 或者 fin = dict(zip(a,b)) print(fin) # {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'} ","date":"2021-10-10","objectID":"/skill/:16:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"对list进行解包 lists = ['dog','cat','you'] print(*lists) #想对一个列表进行zip操作时，可以这样 print(list(zip(*lists))) def test(*args): print(\"args:\",args) test(*lists) dog cat you [('d', 'c', 'y'), ('o', 'a', 'o'), ('g', 't', 'u')] args:('dog','cat','you') ","date":"2021-10-10","objectID":"/skill/:17:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"对类的一些操作 class Test: x = 1 y = 2 print(Test.x,Test.y) #==\u003eprint(Test().x,Test().y) class Test: def __init__(self,x,y): self.x = x self.y = y test = Test(1,2) print(test.x,test.y) 1 2 1 2 class Test: def __init__(self,maxlen): self.maxlen = maxlen self.lists = [] def put(self,*args): for i in args: if len(self.lists) \u003c= self.maxlen: self.lists.append(i) else: break def get(self): return self.lists.pop() def empty(self): if len(self.lists) != 0: return False else: return True def __len__(self): return len(self.lists) def __del__(self): print(\"del this class\") def printfs(self): return self.lists test = Test(10) test.put(1,2,3,4,5,6) print(test.empty()) print(len(test)) print(test.printfs()) test.__del__() #直接调用test还存在，__del__是析构函数，垃圾回收时就会调用a print(test) #del test #print(test) 这时候就会报错，因为del将test这个对象直接删除了 False 6 [1, 2, 3, 4, 5, 6] del this class \u003c__main__.Test object at 0x0000021B7DF33EB0\u003e del this class ","date":"2021-10-10","objectID":"/skill/:18:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"一些内置函数 all([True,True,False]) #False all([True,True,True]) #True any([True,True,False]) #True any([True,False,False])#True any([False,False]) #False import random for i in iter(lambda:random.randint(1,10),5): print(i) #相当于 while True: x = random.randint(1,10) print(x) if x == 5: break iter(object[, sentinel]) sentinel为可选参数，若不传入，则object必须为可迭代对象，传入则必须为可调用对象,当可调用对象的返回值为sentinel抛出异常，但for循环会处理这个异常，这常常用于IO操作 ​ #这是cookbook里面的一个例子 import sys f = open('xxx/xxx.txt') for chunk in iter(lambda:f.read(10),''): n = sys.stdout.write(chunk) #深入理解一下 import random class Test: def __init__(self): self.lists = [1,23,2,4,1,421,412] def __call__(self): return random.choice(self.lists) for i in iter(Test(),1): print(i) #这是可以正常输出的，因为实例化Test后是个可调用对象，返回列表的随机值，当返回1时则循环结束，如果把__call__魔法方法去了后，则会报错，如果想要不使用魔法方法的话可以用匿名函数 import random class Test: def __init__(self): self.lists = [1,23,2,4,1,421,412] # def __call__(self): # return random.choice(self.lists) for i in iter(lambda:random.choice(Test().lists),1): print(i) #总之，吹爆cookbook ","date":"2021-10-10","objectID":"/skill/:19:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"functools.partial #先看演示 from functools import partial def add(a,b): return a + b addOne = partial(add,1) addOne(2) #3 addOne(4) #5 #大概意思就是利用partial将函数的一个参数固定住了 def partial(func,*wargs): def wapper(*kargs): args = list(wargs) print(f\"args:{args}\") print(f\"kargs:{kargs}\") args.extend(kargs) print(f\"last:{args}\") return func(*args) return wapper def add(a,b,c): return a + b + c addone = partial(add,1,2) #此时addone相当于wapper print(addone(3)) #调用wrapper 3为传入的kargs #输出： args:[1, 2] kargs:(3,) last:[1, 2, 3] 6 #上面是partial函数的简化版本 #很明显的闭包操作，很容易就可以理解 #当然也可以转换为装饰器操作 from functools import wraps from functools import wraps,partial def out_wapper(*wargs): def partialout(func): return partial(func,*wargs) # 这是使用partial原理的 # @wraps(func) # def wrapper(*kargs): # args = list(wargs) # print(f\"args:{args}\") # print(f\"kargs:{kargs}\") # args.extend(kargs) # print(f\"last:{args}\") # return func(*args) # return wrapper return partialout @out_wapper(1,2) def add(a,b,c): return a + b + c print(add(3)) #6 #明显装饰器要麻烦一点实现，不过毕竟是封装好的函数，以后直接用就可以，不过了解这些有助于提高思维水平 ","date":"2021-10-10","objectID":"/skill/:20:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"@classmethod和@staticmethod class A(object): bar = 1 def func1(self): print ('foo') @classmethod def func2(cls): print ('func2') print (cls.bar) cls().func1() # 调用 foo 方法 A.func2() # 不需要实例化 func2 1 foo class A(object): # 属性默认为类属性（可以给直接被类本身调用） num = \"类属性\" # 实例化方法（必须实例化类之后才能被调用） def func1(self): # self : 表示实例化类后的地址id print(\"func1\") print(self) # 类方法（不需要实例化类就可以被类本身调用） @classmethod def func2(cls): # cls : 表示没用被实例化的类本身 print(\"func2\") print(cls) print(cls.num) cls().func1() # 不传递传递默认self参数的方法（该方法也是可以直接被类调用的，但是这样做不标准） def func3(): print(\"func3\") print(A.num) # 属性是可以直接用类本身调用的 # A.func1() 这样调用是会报错：因为func1()调用时需要默认传递实例化类后的地址id参数，如果不实例化类是无法调用的 A.func2() A.func3() class A(object): def foo(self, x): print(\"executing foo(%s,%s)\" % (self, x)) print('self:', self) @staticmethod def static_foo(x): print(\"executing static_foo(%s)\" % x) 问题：@staticmethod修饰的方法函数与普通的类外函数，为什么不直接使用普通函数？ @staticmethod是把函数嵌入到类中的一种方式，函数就属于类，同时表明函数不需要访问这个类。通过子类的继承覆盖，能更好的组织代码。 from pydantic import BaseModel from typing import Sequence class Test(BaseModel): text: Sequence[str] @classmethod def create(cls,text: Sequence[str]) -\u003e \"Test\": # classmethod常用构造函数 return cls(text=text) def to_tuple(self) -\u003e \"Test\": return Test(text=tuple(self.text)) @classmethod def join(cls, *Tests): return cls.create(sum([i.text for i in Tests],[])) test = Test.create(list(\"Hello world\")) t2 = Test.create(list(\"NIHAO\")) print(Test.join(test, t2)) # text=['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', 'N', 'I', 'H', 'A', 'O'] ","date":"2021-10-10","objectID":"/skill/:21:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"用类实现装饰器 #先看这样的代码 类实现装饰器要求类必须是可调用的 import time import functools class DelayFunc: def __init__(self, duration, func): self.duration = duration self.func = func def __call__(self, *args, **kwargs): print(f'Wait for {self.duration} seconds...') time.sleep(self.duration) return self.func(*args, **kwargs) def eager_call(self, *args, **kwargs): print('Call without delay') return self.func(*args, **kwargs) def delay(duration): \"\"\"装饰器：推迟某个函数的执行。同时提供 .eager_call 方法立即执行 \"\"\" # 此处为了避免定义额外函数，直接使用 functools.partial 帮助构造 # DelayFunc 实例 return functools.partial(DelayFunc, duration) @delay(2) def add(a,b): print(a,b) add(1,2) #延迟两秒输出3 相当于delay(2)(add)(1,2) add.eager_call(1,2) #不延迟输出3 相当于delay(2)(add).eager_call(1,2) #额，当然，想更深入理解的话，也可以这么写 def delay(duration): def partial(func): return DelayFunc(duration,func) return partial # 上面的就相当于 partial(DelayFunc,duration),缺的func参数就是要修饰的函数 @delay(2) def add(a,b): return a + b print(add(1,2)) 与纯函数相比，我觉得使用类实现的装饰器在特定场景下有几个优势： 实现有状态的装饰器时，操作类属性比操作闭包内变量更符合直觉、不易出错 实现为函数扩充接口的装饰器时，使用类包装函数，比直接为函数对象追加属性更易于维护 更容易实现一个同时兼容装饰器与上下文管理器协议的对象 ","date":"2021-10-10","objectID":"/skill/:22:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"BaseModel from pydantic import BaseModel,AnyUrl class Test(BaseModel): # 继承后可以用类属性创建实例 url: AnyUrl data: str def __str__(self): return self.url + self.data kwargs = { 'url': 'https://www.baidu.com', 'data': '/search' } print(Test(**kwargs)) ","date":"2021-10-10","objectID":"/skill/:23:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"python类型注释 from pydantic import BaseModel from typing import Any class cout(): def __init__(self, cls: \"Test\", text: str) -\u003e None: self.cls = cls self.text = text def __str__(self): return f\"{self.cls} {self.text}\" #程序到cout时 Test类并没有定义，但最后Test在变量空间中，所以加上引号 class Test(BaseModel): def __str__(self) -\u003e str: return \"I am Test Class\" print(cout(cls=Test(), text=\"hello world!\")) ","date":"2021-10-10","objectID":"/skill/:24:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"namedtuple from collections import namedtuple Test = namedtuple(\"Test\", ['name', 'age', 'sex']) def test_for_test(name: str, year: int, sex: str) -\u003e \"Test\": return Test( name=name.title(), age=2021 - year, sex=sex ) name,age,sex = test_for_test('wlb', 2002, 'male') print(name, age, sex) ","date":"2021-10-10","objectID":"/skill/:25:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"@property from pydantic import BaseModel class Test(): def __init__(self, cls, n): self.cls = cls self.n = n @property def to_string_cls(self): return self.cls @property def to_strings(self): return self.n class Test_For(BaseModel): num: int def __str__(self): return str(self.num) __repr__ = __str__ test = Test(Test_For, 22) print(test.to_string_cls(num=1)) # 1 print(test.to_strings) # 22 ","date":"2021-10-10","objectID":"/skill/:26:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"在边界处思考 from typing import Iterable from pydantic import BaseModel,conint,ValidationError class NumberInput(BaseModel): num: conint(ge=0, le=100) def input_a_number(): while True: n = input(\"输入一个数\") try: n = NumberInput(num=n) except ValidationError as e: print(e) continue n = n.num break return n print(input_a_number()) #要求输入一个0-100的数 这样是不是很优雅 ","date":"2021-10-10","objectID":"/skill/:27:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"super()进阶 今天学习cookbook8-8子类中扩展property 先贴一下代码 class Person: def __init__(self, name): self.name = name # 有意思的是 这里的self.name是@property修饰的 这行代码调用name.setter # Getter function @property def name(self): return self._name # Setter function @name.setter def name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._name = value # Deleter function @name.deleter def name(self): raise AttributeError(\"Can't delete attribute\") # 子类 class SubPerson(Person): @property def name(self): print('Getting name') return super().name @name.setter def name(self, value): print('Setting name to', value) super(SubPerson, SubPerson).name.__set__(self, value) @name.deleter def name(self): print('Deleting name') super(SubPerson, SubPerson).name.__delete__(self) 看到super(SubPerson, SubPerson)感到很疑惑，于是搜索资料大致搞明白了 通俗说默认的super(SubPerson,self) (直接写super()也可) 返回的是一个类的实例 \u003e 为了委托给之前定义的setter方法，需要将控制权传递给之前定义的name属性的 __set__() 方法。 不过，获取这个方法的唯一途径是使用类变量而不是实例变量来访问它。 这也是为什么我们要使用 super(SubPerson, SubPerson) 的原因。 从书中这句话可以看出 super(cls,cls)返回的是一个类 不是一个实例，super()的参数的作用就是用于定位位置 第一个cls必须是第二cls的父类或者二者相同，可以通过cls.__mro__查看继承顺序 比如在D里面super(A,D).__init__(self) 而__mro__ 为 (\u003cclass '__main__.D'\u003e, \u003cclass '__main__.A'\u003e, \u003cclass '__main__.B'\u003e, \u003cclass '__main__.C'\u003e, \u003cclass '__main__.Base'\u003e, \u003cclass 'object'\u003e) 那么就调用从A以后的类的__init__() 不过重点不在这里，重点是super(cls,cls)和super(cls,object)的区别 使用super(cls,cls)必须显示的传入self参数，即super(cls,cls).func(self,…)。总之其就是一个定位方法，别的作用我暂且不知。 ### 一个示例 class A: def say(self): print(\"I am A\") class B(A): def say(self): # super(B, B).say(self) super().say() class C(A): def say(self): print(\"I am C\") class D(B, C): def say(self): super().say() D().say() 上述的这段代码怎么修改D输出都是I am C，这是因为在B中super().say()相当于super(B,self).say()，而根据上述内容这是一个实例方法，其中self是D的实例，查看D的mro可以知道C在B的后面，所以根据super的作用，则会调用继承关系中B后面类的say方法，即C的say 方法，所以会得到匪夷所思的结果，将代码修改成注释的那样就可以解决这个问题，希望可以帮助理解。 ## dataclass from dataclasses import dataclass import random @dataclass(order=True) # 等于实现了各种比较方法例如=、\u003e、\u003c,排序函数都依赖比较两个对象 class A: n: int nums = [A(random.randint(1,10)) for _ in range(10)] nums = sorted(nums) print(nums, end='') x = '''hello''' print(x) dataclass可以自动添加__rapr__方法，不必自己实现 @dataclass(init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False) init：默认将生成 __init__ 方法。如果传入 False，那么该类将不会有 __init__ 方法。 repr：__repr__ 方法默认生成。如果传入 False，那么该类将不会有 __repr__ 方法。 eq：默认将生成 __eq__ 方法。如果传入 False，那么 __eq__ 方法将不会被 dataclass 添加，但默认为 object.__eq__。 order：默认将生成 __gt__、__ge__、__lt__、__le__ 方法。如果传入 False，则省略它们。 unsafe_hash：默认生成__hash__方法，用于构建可hashable的类 from dataclasses import dataclass @dataclass(unsafe_hash=True) class VisitRecordDC: first_name: str last_name: str phone_number: str # 跳过“访问时间”字段，不作为任何对比条件 date_visited: str = field(hash=False, compare=False) def find_potential_customers_v4(): return set(VisitRecordDC(**r) for r in users_visited_phuket) - \\ #求差集 set(VisitRecordDC(**r) for r in users_visited_nz) ","date":"2021-10-10","objectID":"/skill/:28:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"自定义format class Student: def __init__(self, name, age): self.name = name self.age = age def __format__(self, format_spec): if format_spec == 'long': return f'{self.name} is {self.age} years old.' elif format_spec == 'simple': return f'{self.name}({self.age})' raise ValueError('invalid format spec') vllbc = Student('vllbc', '18') print(f'{vllbc:simple}') print(f'{vllbc:long}') ","date":"2021-10-10","objectID":"/skill/:29:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"抽象类实践 import collections from abc import ABC, abstractmethod from typing import List customer = collections.namedtuple('customer', ['name', 'points']) class Goods(): def __init__(self, name: str, quantity: float, price: float) -\u003e None: self.name = name self.quantity = quantity self.price = price def total(self) -\u003e float: return self.quantity * self.price class Order(): def __init__(self, customer: customer, cart: List[Goods], prom=None) -\u003e None: self.customer = customer self.cart = cart self.prom = prom def total(self): if not hasattr(self, '__total'): self.__total = sum(i.total() for i in self.cart) return self.__total def due(self): if self.prom is None: discount = 0 else: discount = self.prom.discount(self) return self.total() - discount def __repr__(self) -\u003e str: return f'\u003cOrder total: {self.total():.2f} due: {self.due():.2f}\u003e' class Prom(ABC): # 抽象类 @abstractmethod def discount(self,order) -\u003e float: '''discount''' class discount1(Prom): def discount(self,order) -\u003e float: return order.total() * 0.05 if order.customer.points \u003e= 10000 else 0 john = customer(name='vllbc', points=100000) carts = [Goods(name='apple', quantity=5, price=10), Goods( name='banana', quantity=8, price=5), Goods(name='peach', quantity=4, price=8)] order = Order(customer=john, cart=carts,prom=discount1()) ","date":"2021-10-10","objectID":"/skill/:30:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"accumulate import itertools test_list = [i for i in range(1, 11)] for i in itertools.accumulate(test_list): print(i, end=\",\") # 1,3,6,10,15,21,28,36,45,55, print() for i in itertools.accumulate(test_list, lambda x, y: x * y): print(i, end=',') # 1,2,6,24,120,720,5040,40320,362880,3628800, ","date":"2021-10-10","objectID":"/skill/:31:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"异步装饰器 from functools import wraps import asyncio def decorator(func): @wraps(func) async def hello(*args, **kwargs): await asyncio.sleep(2) return await func(*args,**kwargs) return hello @decorator async def test(): print(\"hello\") asyncio.run(test()) ","date":"2021-10-10","objectID":"/skill/:32:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"bisect import bisect import time # BREAKPOINTS 必须是已经排好序的，不然无法进行二分查找 BREAKPOINTS = (1, 60, 3600, 3600 * 24) TMPLS = ( # unit, template (1, \"less than 1 second ago\"), (1, \"{units} seconds ago\"), (60, \"{units} minutes ago\"), (3600, \"{units} hours ago\"), (3600 * 24, \"{units} days ago\"), ) def from_now(ts): \"\"\"接收一个过去的时间戳，返回距离当前时间的相对时间文字描述 \"\"\" seconds_delta = int(time.time() - ts) unit, tmpl = TMPLS[bisect.bisect(BREAKPOINTS, seconds_delta)] # bisect类似于index方法，要是不存在会选择数值最接近的索引 return tmpl.format(units=seconds_delta // unit) ","date":"2021-10-10","objectID":"/skill/:33:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"contextlib from contextlib import contextmanager,ContextDecorator # contextmanager可以把一个函数变成一个上下文管理器，不需要自己去实现一个定义了__enter__和__exit__方法的类 @contextmanager def open_file(filename, methods=\"r\"): print(f\"打开了文件{filename}\") res_file = open(filename, mode=methods) # __enter__方法 这里也可以是自己定义的类 try: yield res_file # 相当于在__enter__方法里面返回self yield后面为空的话就不用as了 except Exception as e: print(\"有错误发生\", e) # __exit__方法里的错误处理 finally: res_file.close() # __exit__ with open_file(\"testvim.txt\") as fp: print(fp) ","date":"2021-10-10","objectID":"/skill/:34:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"读取大文件 from functools import partial def digits_(file,block_size=1024*8): # 分块读取 _read = partial(file.read, block_size) # 使用partial,也可以使用lambda:file.read(block_size) for line in iter(_read, \"\"): # 当读取完毕时退出 for s in line: if s.isdigit(): yield s # 使用yield def count_digits(fname): \"\"\"计算文件里包含多少个数字字符\"\"\" count = 0 with open(fname) as file: for _ in digits_(file=file): count+=1 return count ","date":"2021-10-10","objectID":"/skill/:35:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__exit__、__enter__ # def __enter__(self): # 该方法将在进入上下文时调用 # return self # def __exit__(self, exc_type, exc_val, exc_tb): # 该方法将在退出上下文时调用 # exc_type, exc_val, exc_tb 分别表示该上下文内抛出的异常类型、异常值、错误栈 # __enter__()：主要执行一些环境准备工作，同时返回一资源对象。如果上下文管理器open(\"test.txt\")的__enter__()函数返回一个文件对象。 # __exit__()：完整形式为__exit__(type, value, traceback),这三个参数和调用sys.exec_info()函数返回值是一样的，分别为异常类型、异常信息和堆栈。如果*执行体语句*没有引发异常，则这三个参数均被设为None。否则，它们将包含上下文的异常信息。__exit__()方法返回True或False,分别指示被引发的异常有没有被处理，如果返回False，引发的异常将会被传递出上下文。如果__exit__()函数内部引发了异常，则会覆盖掉执行体的中引发的异常。处理异常时，不需要重新抛出异常，只需要返回False，with语句会检测__exit__()返回False来处理异常。 ","date":"2021-10-10","objectID":"/skill/:36:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"enum from enum import IntEnum class Test(IntEnum): X = 2 Y = 1 print(2 == Test.X) ","date":"2021-10-10","objectID":"/skill/:37:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"pydantic数据验证 from pydantic import BaseModel, conint, ValidationError # pydantic主要功能就是作数据验证 from typing import ( List, Union, Optional, Dict ) class Test(BaseModel): name: Optional[str] sex: Union[str, List[str]] d: Dict[str, int] id: conint(ge=1,le=10) try: test = Test(name='wlb', sex='male', d={'dict':1}, id=1) print(test.dict(), test.__annotations__) # {'name': 'wlb', 'sex': 'male', 'd': {'dict': 1}, 'id': 1} {'name': typing.Union[str, NoneType], 'sex': typing.Union[str, typing.List[str]], 'd': typing.Dict[str, int], 'id': \u003cclass '__main__.ConstrainedIntValue'\u003e} except ValidationError: print(\"数据错误\") ","date":"2021-10-10","objectID":"/skill/:38:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"islice from itertools import islice def test(): t = 0 while True: yield t t += 1 for i in islice(test(), 10, 21, 2): print(i) ","date":"2021-10-10","objectID":"/skill/:39:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__iter__、__next__ class Range7: # 可迭代类型 只需要实现__iter__即可 def __init__(self,start,end) -\u003e None: self.start = start self.end = end def __iter__(self): return Range7iterator(self) class Range7iterator: #这是迭代器,一般的迭代器只能调用一次 def __init__(self,rangeobj) -\u003e None: self.rangeobj = rangeobj self.cur = rangeobj.start def __iter__(self): return self def __next__(self): while True: if self.cur \u003e self.rangeobj.end: raise StopIteration if self.is_7(self.cur): res = self.cur self.cur += 1 return res self.cur += 1 def is_7(self,num): if num == 0: return False return num%7==0 or \"7\" in str(num) for i in Range7(1,100): print(i,end=\" \") #可迭代对象不一定是迭代器，但迭代器一定是可迭代对象 # 对可迭代对象使用 iter() 会返回迭代器，迭代器则会返回它自身 # 每个迭代器的被迭代过程是一次性的，可迭代对象则不一定 # 可迭代对象只需要实现 __iter__ 方法，而迭代器要额外实现 __next__ 方法 ","date":"2021-10-10","objectID":"/skill/:40:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"求字典的最大值 prices = { 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75 } print(max(zip(prices.values(),prices.keys()))) print(max(prices.items(),key=lambda x:x[1])) print(max(prices,key=lambda k:prices[k])) ","date":"2021-10-10","objectID":"/skill/:41:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"注意循环变量 funcs = [] res = [] for x in range(7): def func(x=x): # 去掉x=x则出现[6,6,6,6,6,6] 在循环内部定义一个函数时, 如果该函数在其主体中使用了循环变量, 则闭包函数将与循环变量绑定, 而不是它的值. 因此, 所有的函数都是使用最后分配给变量的值来进行计算的. return x funcs.append(func) res.append(func()) func_res = [f() for f in funcs] print(func_res) def create_mult(): res = [] for i in range(5): def func(x, i=i): # 去掉i=i则全输出8，原因和上面一样 return x * i res.append(func) return res for cr in create_mult(): print(cr(2)) ","date":"2021-10-10","objectID":"/skill/:42:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"空对象模式 修改前 import decimal class CreateAccountError(Exception): \"\"\"Unable to create a account error\"\"\" class Account: \"\"\"一个虚拟的银行账号\"\"\" def __init__(self, username, balance): self.username = username self.balance = balance @classmethod def from_string(cls, s): \"\"\"从字符串初始化一个账号\"\"\" try: username, balance = s.split() balance = decimal.Decimal(float(balance)) except ValueError: raise CreateAccountError('input must follow pattern \"{ACCOUNT_NAME} {BALANCE}\"') if balance \u003c 0: raise CreateAccountError('balance can not be negative') return cls(username=username, balance=balance) def caculate_total_balance(accounts_data): \"\"\"计算所有账号的总余额 \"\"\" result = 0 for account_string in accounts_data: try: user = Account.from_string(account_string) except CreateAccountError: pass else: result += user.balance return result accounts_data = [ 'piglei 96.5', 'cotton 21', 'invalid_data', 'roland $invalid_balance', 'alfred -3', ] print(caculate_total_balance(accounts_data)) ","date":"2021-10-10","objectID":"/skill/:43:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"空对象模式简介 额外定义一个对象来表示None ","date":"2021-10-10","objectID":"/skill/:43:1","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"好处 它可以加强系统的稳固性，能有有效地防止空指针报错对整个系统的影响，使系统更加稳定。 它能够实现对空对象情况的定制化的控制，能够掌握处理空对象的主动权。 它并不依靠Client来保证整个系统的稳定运行。 它通过isNone对==None的替换，显得更加优雅，更加易懂。 import decimal class Account: \"\"\"一个虚拟的银行账号\"\"\" def __init__(self, username, balance): self.username = username self.balance = balance @classmethod def from_string(cls, s): \"\"\"从字符串初始化一个账号\"\"\" try: username, balance = s.split() balance = decimal.Decimal(float(balance)) except ValueError: # raise CreateAccountError('input must follow pattern \"{ACCOUNT_NAME} {BALANCE}\"') return NullAccount() if balance \u003c 0: return NullAccount() return cls(username=username, balance=balance) def caculate_total_balance(accounts_data): \"\"\"计算所有账号的总余额 \"\"\" return sum(Account.from_string(s).balance for s in accounts_data) class NullAccount: # 要返回的空对象 username = \"\" # 当发生错误时username的值 balance = 0 # 当发生错误时balance的值 def re_Null(): return NotImplementedError accounts_data = [ 'piglei 96.5', 'cotton 21', 'invalid_data', 'roland $invalid_balance', 'alfred -3', ] print(caculate_total_balance(accounts_data)) ","date":"2021-10-10","objectID":"/skill/:43:2","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"pathlib from pathlib import Path # 把txt文件重命名为csv文件 def unify_ext_with_pathlib(path): for fpath in Path(path).glob(\"*.txt\"): fpath.rename(fpath.with_suffix(\".csv\")) print(Path(\".\") / \"test_pathlib.py\") # Path类型可以使用/运算符 print(Path(\"testvim.txt\").read_text()) # 直接读取文件内容 # .resolve() 取绝对路径 # with_name() 修改文件名 with_suffix()修改后缀名 # 把当前目录下的文件批量重命名 # import os # from pathlib import Path # p = Path(\".\") # for filepath in p.glob(\"test_*.py\"): # name = filepath.with_name(str(filepath).replace(\"test_\",\"\")) # filepath.rename(name) ","date":"2021-10-10","objectID":"/skill/:44:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"单元测试 def say_hello(name=None): if name: return f\"hello {name}\" return \"hello world\" import unittest from typing import List class sayhellotest(unittest.TestCase): def setUp(self,nums:List[int] = 0): return super().setUp() def tearDown(self): return super().tearDown() def test_sayhello(self): rv = say_hello() self.assertEqual(rv,\"hello world\") def test_to_name(self): rv = say_hello(\"wlb\") self.assertEqual(rv,\"hello wlb\") if __name__ == '__main__': unittest.main() ","date":"2021-10-10","objectID":"/skill/:45:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"takewhile和dropwhile from itertools import dropwhile,takewhile # 你想遍历一个可迭代对象，但是它开始的某些元素你并不感兴趣，想跳过它们，用dropwhile with open('testvim.txt','r') as fp: for i in dropwhile(lambda i:i.startswith(\"#\"),fp): # 跳过前面#号开头的 print(i) with open(\"testvim.txt\",\"r\") as fp: for i in takewhile(lambda i:i.startswith(\"#\"),fp): # 遍历带#号开头的，遇到不是#号开头的就退出循环，可以当做break使用 # 相当于 if not i.startwith(\"#\"): break print(i) ","date":"2021-10-10","objectID":"/skill/:46:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"装饰器可以装饰方法 import random import wrapt # 为第三方库 def provide_number(min_num, max_num): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): # 参数含义： # # - wrapped：被装饰的函数或类方法 # - instance： # - 如果被装饰者为普通类方法，该值为类实例 # - 如果被装饰者为 classmethod 类方法，该值为类 # - 如果被装饰者为类/函数/静态方法，该值为 None # # - args：调用时的位置参数（注意没有 * 符号） # - kwargs：调用时的关键字参数（注意没有 ** 符号） # num = random.randint(min_num, max_num) # 无需关注 wrapped 是类方法或普通函数，直接在头部追加参数 args = (num,) + args return wrapped(*args, **kwargs) return wrapper @provide_number(1, 100) def print_random_number(num): print(num) class Foo: @provide_number(1, 100) def print_random_number(self, num): print(num) Foo().print_random_number() print_random_number() # 使用 wrapt 模块编写的装饰器，相比原来拥有下面这些优势： # 嵌套层级少：使用 @wrapt.decorator 可以将两层嵌套减少为一层 # 更简单：处理位置与关键字参数时，可以忽略类实例等特殊情况 # 更灵活：针对 instance 值进行条件判断后，更容易让装饰器变得通用 ","date":"2021-10-10","objectID":"/skill/:47:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"___getattribute__ ____getattribute__仅在新式类中可用，重载__getattrbute__方法对类实例的每个属性访问都有效。 class ClassA: x = 'a' def __init__(self): self.y = 'b' def __getattribute__(self, item): return '__getattribute__' if __name__ == '__main__': a = ClassA() # 使用实例直接访问存在的类属性时,会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.x) # 使用实例直接访问实例存在的实例属性时,会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.y) # 使用实例直接访问实例不存在的实例属性时,也会调用__getattribute__方法 # 输出结果 __getattribute__ print(a.z) 由于__getattr__只针对未定义属性的调用，所以它可以在自己的代码中自由地获取其他属性，而__getattribute__针对所有的属性运行，因此要十分注意避免在访问其他属性时，再次调用自身的递归循环。 当在__getattribute__代码块中，再次执行属性的获取操作时，会再次触发__getattribute__方法的调用，代码将会陷入无限递归，直到Python递归深度限制（重载__setter__方法也会有这个问题）。 示例代码（无限递归）： class ClassA: x = 'a' def __getattribute__(self, item): print('__getattribute__') return self.item if __name__ == '__main__': a = ClassA() a.x 运行结果引发异常。 同时，也没办法通过从__dict__取值的方式来避免无限递归 class ClassA: x = 'a' def __getattribute__(self, name): return self.__dict__[name] if __name__ == '__main__': a = ClassA() # 无限递归 a.x 为了避免无限递归，应该把获取属性的方法指向一个更高的超类，例如object（因为__getattribute__只在新式类中可用，而新式类所有的类都显式或隐式地继承自object，所以对于新式类来说，object是所有新式类的超类）。 修改代码（避免无限递归循环）： class ClassA: x = 'a' def __getattribute__(self, item): print('__getattribute__') return super().__getattribute__(self, item) if __name__ == '__main__': a = ClassA() print(a.x) 结果： __getattribute__ a ","date":"2021-10-10","objectID":"/skill/:48:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"___getattr__、__setattr__ 区别 getattribute 和 getattr，前者是任何通过 x.y 访问实例的属性时都会调用的特殊方法，而后者则是在正常访问形式下无法找到的情况下才会被调用。 class Chain(object): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain('%s/%s' % (self._path, path)) def __str__(self): return self._path def users(self,name): return Chain(f\"{self._path}/users/{name}\") __repr__ = __str__ chain = Chain(\"vllbc\") print(chain.x.x.x.x.x) # out: vllbc/x/x/x/x/x 另外，当同时定义__getattribute__和__getattr__时，__getattr__方法不会再被调用，除非显示调用__getattr__方法或引发AttributeError异常。可以在__getattribute__中抛出异常来调用getattr。 ","date":"2021-10-10","objectID":"/skill/:49:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"__getitem__ ","date":"2021-10-10","objectID":"/skill/:50:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"元类 ''' 元类就是控制类的创建的类 ''' class ModelMetaclass(type): def __new__(cls, name, bases, attrs): if name == 'Model': return type.__new__(cls, name, bases, attrs) print(f\"found model {name}\") maps = dict() for k, v in attrs.items(): if isinstance(v, Field): print(f\"Found mapping {k} ==\u003e {v}\") maps[k] = v for k, v in maps.items(): attrs.pop(k) attrs['__mappings__'] = maps attrs['__table__'] = name return type.__new__(cls, name, bases, attrs) class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return '\u003c%s:%s\u003e' % (self.__class__.__name__, self.name) class StringField(Field): def __init__(self, name, column_type='TXT'): super().__init__(name, column_type) class IntegerField(Field): def __init__(self, name, column_type='INT'): super().__init__(name, column_type) class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Model' object has no attribute '%s'\" % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.items(): fields.append(k) params.append('?') args.append(getattr(self, k, None)) sql = 'insert into %s (%s) values (%s)' % ( self.__table__, ','.join(fields), ','.join(params)) print('SQL: %s' % sql) print('ARGS: %s' % str(args)) class User(Model): # 定义类的属性到列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password') # 创建一个实例： u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd') # 保存到数据库： u.save() ","date":"2021-10-10","objectID":"/skill/:51:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"关于hash # set中的元素要求必须是可哈希的，但set本身是不可哈希的 a = set([1,2,3,4]) a.add([1,2,3]) # 会报错，因为列表是不可哈希的 hash(a) # 会报错，因为set本身是不可哈希的 b = tuple([1,2,3,4]) # 元组本身是可哈希的 c = tuple([1,2,3,[4,5]]) # 如果元组里面有不可哈希的元素 那么整个元组也是不可哈希的了 # 在类中定义__hash__方法就可以变成可哈希的类，注意避免返回可能重复的hash值 class My_Hash: def __hash__(self) -\u003e int: return 111 # 还有简便的方法就是使用dataclass类，可以省时省力，本博客也有介绍。 ","date":"2021-10-10","objectID":"/skill/:52:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"处理列表越界 假如你请求的不是某一个元素，而是一段范围的切片。那么无论你指定的范围是否有效，程序都只会返回一个空列表 []，而不会抛出任何错误。 了解了这点后，你会发现像下面这种边界处理代码根本没有必要： def sum_list(l, limit): \"\"\"对列表的前 limit 个元素求和 \"\"\" # 如果 limit 过大，设置为数组长度避免越界 if limit \u003e len(l): limit = len(l) return sum(l[:limit]) 因为做切片不会抛出任何错误，所以不需要判断 limit 是否超出范围，直接做 sum 操作即可： def sum_list(l, limit): return sum(l[:limit]) ","date":"2021-10-10","objectID":"/skill/:53:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"or操作符 在很多场景下，我们可以利用 or 的特点来简化一些边界处理逻辑。看看下面这个例子： context = {} # 仅当 extra_context 不为 None 时，将其追加进 context 中 if extra_context: context.update(extra_context) # 等同于 context.update(extra_context or {}) 因为 a or b or c or … 这样的表达式，会返回这些变量里第一个布尔值为真的值，直到最后一个为止。 含义为当extra_context为None时，会返回{} ## 字典的键 在python里面，Python 字典通过检查键值是否相等和比较哈希值来确定两个键是否相同.具有相同值的不可变对象(可哈希)在Python中始终具有相同的哈希值。 注意: 具有不同值的对象也可能具有相同的哈希值（哈希冲突）。 下面这个例子 some_dict = {} some_dict[5.5] = \"Ruby\" some_dict[5.0] = \"JavaScript\" some_dict[5] = \"Python\" \u003e\u003e\u003e some_dict[5.5] \"Ruby\" \u003e\u003e\u003e some_dict[5.0] \"Python\" \u003e\u003e\u003e some_dict[5] \"Python\" 因为Python将 5 和 5.0 识别为 some_dict 的同一个键, 所以已有值 “JavaScript” 就被 “Python” 覆盖了. ","date":"2021-10-10","objectID":"/skill/:54:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["python"],"content":"类__init__参数快速赋给self def __init__(self, x=1) -\u003e None: super().__init__() self.__dict__.update(locals()) pass 即可，此时不需要self.x=x也可以直接使用self.x ","date":"2021-10-10","objectID":"/skill/:55:0","tags":["python","skill"],"title":"skill","uri":"/skill/"},{"categories":["算法题"],"content":"长度最小的子数组 ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:0:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/minimum-size-subarray-sum/ ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:1:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["算法题"],"content":"思路： ​ 一开始想的是直接排序，然后从后面开始遍历，因为要求最小的 然后出错了，，，，， class Solution: def minSubArrayLen(self, s: int, nums: List[int]) -\u003e int: nums.sort() end = len(nums) - 1 while end \u003e 0: for i in range(end,-1,-1): if sum(nums[i:end+1]) \u003e= s: return len(nums[i:end+1]) end -= 1 return 0 在 213 [12,28,83,4,25,26,25,2,25,25,25,12] 出了错，结果试了一下排序后的列表 [2, 4, 12, 12, 25, 25, 25, 25, 25, 26, 28, 83] 结果居然是对的，说明是我的代码的问题，不应该排序 那么该怎么办呢 class Solution: def minSubArrayLen(self, s: int, nums: List[int]) -\u003e int: if not nums: return 0 left = 0 right = 0 res = float('inf') while right \u003c len(nums): while sum(nums[left:right+1]) \u003e= s: res = min(res, right-left +1) left += 1 else: right += 1 if res == float('inf'): return 0 return res 思路也差不多，也是用到了双指针。不过必须注意要判断最小的这个条件啊。 ","date":"2021-09-30","objectID":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/:2:0","tags":["算法题","长度最小的子数组"],"title":"长度最小的子数组","uri":"/%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/"},{"categories":["Machine Learning","分类算法"],"content":"SVM ","date":"2021-09-20","objectID":"/svm/:0:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"kernel ","date":"2021-09-20","objectID":"/svm/:1:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"介绍 其实核函数和映射关系并不大，kernel可以看作是一个运算技巧。 一般认为，原本在低维线性不可分的数据集在足够高的维度存在线性可分的超平面。 围绕这个，那么我们所做的就是要在Feature Space套用原本在线性可分情况下的Input Space中使用过的优化方法，来找到那个Maximaizing Margin的超平面。原理机制一模一样，是二次规划，唯一不同是代入数据的不同，将原来的\\(x_i\\)替换成了高维空间中的\\(\\phi(x_i)\\)，这就是映射函数，映射到高维空间。 具体的技巧(trick)，就是简化计算二次规划中间的一步内积计算。也即中间步骤有一步必须求得\\(\\phi(x_i) \\phi(x_j)\\)，我们可以定义核函数\\(K(x_i,x_j) = \\phi(x_i)\\phi(x_j)\\)，这样我们不需要显式计算每一个\\(\\phi(x_i)\\)，甚至不需要知道它的形式，就可以直接计算结果出来。 也就是说，核函数、内积、相似度这三个词是等价的。因为inner product其实就是一种similarity的度量。核函数和映射是无关的。 ","date":"2021-09-20","objectID":"/svm/:1:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"例子 举一个例子： 考虑一个带有特征映射的二维输入空间 \\(\\chi \\subseteq \\mathbb{R}^{2}\\) : 特征映射二维到三维: \\(\\quad \\Phi: x=\\left(x_{1}, x_{2}\\right) \\rightarrow \\Phi(x)=\\left(x_{1}^{2}, x_{2}^{2}, \\sqrt{2} x_{1} x_{2}\\right) \\in F=\\mathbb{R}^{3}\\) 特征空间中的内积： \\[ \\begin{aligned} \\langle\\Phi(x), \\Phi(z)\\rangle \u0026=\\left\\langle\\left(x_{1}^{2}, x_{2}^{2}, \\sqrt{2} x_{1} x_{2}\\right),\\left(z_{1}^{2}, z_{2}^{2}, \\sqrt{2} z_{1} z_{2}\\right)\\right\\rangle \\\\\\\\ \u0026=x_{1}^{2} z_{1}^{2}+x_{2}^{2} z_{2}^{2}+2 x_{1} x_{2} z_{1} z_{2} \\\\\\\\ \u0026=\\left\\langle x_{1} z_{1}+x_{2} z_{2}\\right\\rangle^{2} \\\\\\\\ \u0026=\\langle x, z\\rangle^{2} \\end{aligned} \\] 根据上面可得，核函数\\(k(x,z) = \\langle x,z \\rangle^2=\\phi(x)^T \\phi(z)\\) 而这里为什么映射函数是这样的形式呢，其实可以是反推出来的，我也不知道，反正凑巧通过这种映射函数可以得到这个核函数。 ","date":"2021-09-20","objectID":"/svm/:1:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"常用核函数理解 以高斯核函数为例， \\[ \\kappa\\left(x_{1}, x_{2}\\right)=\\exp \\left(-\\frac{\\left|x_{1}-x_{2}\\right|^{2}}{2 \\sigma^{2}}\\right) \\] 我们假设 \\(\\sigma=1\\) ，则 \\[ \\begin{aligned} \\kappa\\left(x_{1}, x_{2}\\right) \u0026=\\exp \\left(-\\frac{\\left|x_{1}-x_{2}\\right|^{2}}{2 \\sigma^{2}}\\right) \\\\\\\\ \u0026=\\exp \\left(-\\left(x_{1}-x_{2}\\right)^{2}\\right) \\\\\\\\ \u0026=\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right) \\exp \\left(2 x_{1} x_{2}\\right) \\\\\\\\ \u0026 \\text { Taylor } \\\\\\\\ \u0026=\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right)\\left(\\sum_{i=0}^{\\infty} \\frac{\\left(2 x_{1} x_{2}\\right)^{i}}{i !}\\right) \\\\\\\\ \u0026=\\sum_{i=0}^{\\infty}\\left(\\exp \\left(-x_{1}^{2}\\right) \\exp \\left(-x_{2}^{2}\\right) \\sqrt{\\left.\\frac{2^{i}}{i !} \\sqrt{\\frac{2^{i}}{i !}} x_{1}^{i} x_{2}^{i}\\right)}\\right.\\\\\\\\ \u0026=\\sum_{i=0}^{\\infty}\\left(\\left[\\exp \\left(-x_{1}^{2}\\right) \\sqrt{\\frac{2^{i}}{i !}} x_{1}^{i}\\right]\\left[\\exp \\left(-x_{2}^{2}\\right) \\sqrt{\\frac{2^{i}}{i !}} x_{2}^{i}\\right]\\right) \\\\\\\\ \u0026=\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) \\end{aligned} \\] w这不，已经有了定义的那种形式，对于 \\(\\phi(x)\\) ，由于 \\[ \\phi(x)=\\exp \\left(-x^{2}\\right) \\cdot\\left(1, \\sqrt{\\frac{2^{1}}{1 !}} x, \\sqrt{\\frac{2^{2}}{2 !}} x^{2}, \\cdots\\right) \\] 所以，可以映射到任何一个维度上。 ","date":"2021-09-20","objectID":"/svm/:1:3","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"核函数类别 其实常用的就那几个，高斯核函数最为常用。 ","date":"2021-09-20","objectID":"/svm/:1:4","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"参考 https://www.cnblogs.com/damin1909/p/12955240.html https://blog.csdn.net/mengjizhiyou/article/details/103437423 ","date":"2021-09-20","objectID":"/svm/:1:5","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性可分支持向量机 ","date":"2021-09-20","objectID":"/svm/:2:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性可分 在二维空间上，两类点被一条直线完全分开叫做线性可分。 ","date":"2021-09-20","objectID":"/svm/:2:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"最大间隔超平面 以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。 ","date":"2021-09-20","objectID":"/svm/:2:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"支持向量 样本中距离超平面最近的一些点，这些点叫做支持向量。 ","date":"2021-09-20","objectID":"/svm/:2:3","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"最优化问题 SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。任意超平面可以用下面这个线性方程来描述： \\[ w^Tx+b=0 \\] 二维空间点(x,y)到直线\\(Ax+By+C=0\\)的距离公式为： \\[ \\frac{|Ax+By+C|}{\\sqrt{A^2+B^2}} \\] 扩展到n维空间中，\\(x=(x_1, x_2,\\dots, x_n)\\)到直线\\(w^Tx+b=0\\)的距离为： \\[ \\frac{|w^Tx+b|}{||w||} \\] 如图所示，根据支持向量的定义我们知道，支持向量到超平面的距离为 d，其他点到超平面的距离大于 d。 于是我们有这样的一个公式： 之后得到: 分母都是正数，因此可以令它为1。 合并得： 至此我们就可以得到最大间隔超平面的上下两个超平面： 每个支持向量到超平面的距离可以写为： 所以我们得到： 最大化这个距离： 这里乘上 2 倍也是为了后面推导，对目标函数没有影响。刚刚我们得到支持向量\\(y(w^Tx+b) = 1\\)，所以我们得到： \\[ \\max\\frac{2}{||w||} \\] 对目标进行转换： \\[ \\min\\frac{1}{2}||w||^2 \\] 所以得到的最优化问题是： ","date":"2021-09-20","objectID":"/svm/:2:4","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"对偶问题 拉格朗日乘数法、拉格朗日对偶和KKT条件 参考：https://zhuanlan.zhihu.com/p/38163970 给定约束优化问题： \\[ \\begin{aligned} \u0026\\min f(x) \\\\\\\\ \u0026 s.t. g(x) = 0 \\end{aligned} \\] 为方便分析，假设 f 与 g 是连续可导函数。Lagrange乘数法是等式约束优化问题的典型解法。定义Lagrangian函数 \\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] 其中 λ 称为Lagrange乘数。Lagrange乘数法将原本的约束优化问题转换成等价的无约束优化问题 计算 L 对 x 与 λ 的偏导数并设为零，可得最优解的必要条件： 接下来是不等式约束： \\[ \\begin{aligned} \u0026 \\min f(x) \\\\ \u0026 s.t. g(x) \\leq 0 \\end{aligned} \\] 据此我们定义可行域(feasible region)\\(K=x\\in R^n | g(x)\\leq 0\\)。设$x^* $为满足条件的最佳解，分情况讨论： \\(g(x^* ) \u003c 0\\)，最佳解位于K的内部，为内部解，这时的约束是无效的。 \\(g(x^* ) = 0\\)，最佳解落在K的边界，称为边界解，此时的约束是有效的。 这两种情况的最佳解具有不同的必要条件。 具有不同的必要条件： 内部解：在约束条件无效的情况下，\\(g(x)\\)不起作用，约束优化问题退化为无约束优化问题，因此$x^* \\(满足\\)= 0$ 边界解：在约束有效的情况下，约束不等式变为等式\\(g(x)=0\\)。此时拉格朗日函数在$x^* \\(的梯度为0，即\\)f = -g\\(，\\)f(x)\\(的极小值在边界取到，那么可行域内部的\\)f(x)\\(应该都是大于这个极小值，则\\)f(x)\\(的方向是可行域内部。而\\)g\\(的方向为可行域外部，因为约束条件是\\)g(x) \\(，也就是可行域外部都是\\)g(x) \u003e 0\\(，所以梯度方向就是指向函数增加的方向。说明两个函数的梯度方向相反，要想上面的等式成立，必须有\\)\\(，这就是对偶可行性。 因此，不论是内部解或边界解， \\)g(x)=0$ 恒成立 整合上述两种情况，最佳解的必要条件包括Lagrangian函数的定常方程式、原始可行性、对偶可行性，以及互补松弛性： 这就是KKT条件。 上面结果可推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题(或称非线性规划)： 定义Lagrangian 函数 则KKT条件为 应用 已知svm优化的主要问题： 那么求解线性可分的 SVM 的步骤为： 步骤1： 构造拉格朗日函数： 步骤2： 利用强对偶性转化： 现对参数 w 和 b 求偏导数： 具体步骤： 在前面的步骤中即为： 我们将这个结果带回到函数中可得： 也就是说： 步骤3： 由上述过程需要满足KKT条件（\\(\\alpha\\)就是本文中的\\(\\lambda\\)）： 易得，当\\(\\lambda_i\\)大于0，则必有\\(y_if(x_i)=1\\),所对应的样本点是一个支持向量，即位于最大间隔边界上。 我们可以看出来这是一个二次规划问题，问题规模正比于训练样本数，我们常用 SMO(Sequential Minimal Optimization) 算法求解。 SMO(Sequential Minimal Optimization)，序列最小优化算法，其核心思想非常简单：每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的应用。 我们刚说了 SMO 算法每次只优化一个参数，但我们的优化目标有约束条件，没法一次只变动一个参数。所以我们选择了一次选择两个参数。具体步骤为： 选择两个需要更新的参数\\(\\lambda _i\\)和\\(\\lambda_j\\)，固定其他参数。于是我们有以下约束： 其中\\(c = -\\sum_{k\\neq i, j}\\lambda_ky_k\\)， 因此可以得出\\(\\lambda_j = \\frac{c-\\lambda_iy_i}{y_j}\\)，这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是\\(\\lambda_i\u003e0\\) 对于仅有一个约束条件的最优化问题，我们完全可以在\\(\\lambda_i\\)上对优化目标求偏导，令导数为零，从而求出变量值\\(\\lambda_{inew}\\)，从而求出\\(\\lambda_{jnew}\\) 多次迭代直至收敛。 通过 SMO 求得最优解$^* $ 步骤4： 我们求偏导数时得到： 由上式可求得 w。 由于所有\\(\\lambda_i\u003e0\\)的点都是支持向量，可以随便找一个支持向量代入\\(y_s(w^Tx_s+b)=1\\)，求出b即可。 两边同时乘以\\(y_s\\)，最后得\\(b = y_s-wx_s\\) 为了更具鲁棒性，我们可以求得支持向量的均值： 步骤5： w 和 b 都求出来了，我们就能构造出最大分割超平面：\\(w^Tx+b=0\\) 分类决策函数：\\(f(x) = sign(w^Tx+b)\\) 将新样本点导入到决策函数中既可得到样本的分类。 ","date":"2021-09-20","objectID":"/svm/:2:5","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"线性支持向量机与软间隔 ","date":"2021-09-20","objectID":"/svm/:3:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"软间隔 在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们应该怎么办？比如下面这个： 于是我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面，即允许出现分类错误的样本： 我们允许部分样本点不满足约束条件： \\[ y_i(w^Tx_i+b) \\geq 1 \\] 则优化目标变成了 \\[ \\min_{w, b} \\frac{1}{2}\\|{w}\\|^{2}+C \\sum_{i=1}^{m} \\ell_{0 / 1}\\left(y_{i}\\left({w}^{\\mathrm{T}} {x}_{i}+b\\right)-1\\right), \\] 其中 \\(C\u003e0\\) 是一个常数, \\(\\ell_{0 / 1}\\) 是 “ \\(0 / 1\\) 损失函数” \\[ \\ell_{0 / 1}(z)= \\begin{cases}1, \u0026 \\text { if } z\u003c0 \\\\\\\\ 0, \u0026 \\text { otherwise. }\\end{cases} \\] 显然, 当 \\(C\\) 为无穷大时, \\(\\xi_i\\)必然无穷小，如此一来线性svm就又变成了线性可分svm，当\\(C\\)为有限值时，才会允许部分样本不遵循约束条件 然而, \\(\\ell_{0 / 1}\\) 非凸、非连续, 数学性质不太好, 使得不易直接求解. 于 是, 人们通常用其他一些函数来代替 \\(\\ell_{0 / 1}\\), 称为 “替代损失” (surrogate loss). 替代损失函数一般具有较好的数学性质, 如它们通常是凸的连续函数且是 \\(\\ell_{0 / 1}\\) 的上界. 给出了三种常用的替代损失函数: hinge 损失: \\(\\ell_{\\text {hinge }}(z)=\\max(0,1-z)\\); 指数损失(exponential loss): \\(\\ell_{\\exp }(z)=\\exp (-z)\\); 对率损失(logistic loss): \\(\\ell_{\\log }(z)=\\log (1+\\exp (-z))\\). 若采用 hinge 损失, 则变成 \\[ \\min_{w, b} \\frac{1}{2}\\|{w}\\|^{2}+C \\sum_{i=1}^{m} \\max\\left(0,1-y_{i}\\left({w}^{\\mathrm{T}} {x}_{i}+b\\right)\\right) \\] 为了度量这个间隔软到何种程度，我们为每个样本引入一个松弛变量\\(\\xi_i\\)，令\\(\\xi_i \\geq 0\\)，且\\(1-y_i(w^Tx_i+b)-\\xi_i\\leq 0\\)，如下图： ","date":"2021-09-20","objectID":"/svm/:3:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"优化目标与求解 优化目标： 步骤1： 构造拉格朗日函数： 步骤2： 分别求导，得出以下关系： 将这些关系带入拉格朗日函数中，得到： 则： 我们可以看到这个和硬间隔的一样，只是多了个约束条件。 然后使用SMO算法求$^* $ 软间隔KKT条件 其中\\(\\alpha\\)对应本文的\\(\\lambda\\)，\\(\\mu\\)对应本文的\\(\\mu\\) 因此由第三个式子得必有\\(\\lambda_i =0\\)或者\\(y_if(x_i) - 1+\\xi_i \\geq 0\\) \\(\\lambda_i=0\\)，则该样本对其没有任何影响。 \\(\\lambda_i \u003e 0\\)，则样本为支持向量。 若\\(\\lambda_i \u003cC\\),则\\(\\mu_i \u003e 0\\)，进而有\\(\\xi_i=0\\)，则样本恰在最大间隔边界上。也是支持向量。 若\\(\\lambda_i=C\\),则有\\(\\mu_i=0\\)，此时若\\(\\xi_i\\leq 1\\)，则样本落在最大间隔内部。若\\(\\xi_i\u003e1\\)则样本被错误分类。 再看一下下面这图就理解了。 步骤3： 然后我们通过上面两个式子求出 w 和 b，最终求得超平面 这边要注意一个问题，在间隔内的那部分样本点是不是支持向量？ 我们可以由求参数 w 的那个式子可看出，只要 \\(\\lambda_i \u003e 0\\)的点都能影响我们的超平面，因此都是支持向量。 ","date":"2021-09-20","objectID":"/svm/:3:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"非线性支持向量机 我们刚刚讨论的硬间隔和软间隔都是在说样本的完全线性可分或者大部分样本点的线性可分。 但我们可能会碰到的一种情况是样本点不是线性可分的，比如： 这种情况的解决方法就是：将二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分，比如： 对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。 我们用 x 表示原来的样本点，用\\(\\phi(x)\\)表示 x 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为: \\(f(x) = w\\phi(x)+b\\) 对于非线性 SVM 的对偶问题就变成了： 区别就在于优化目标中的内积。 ","date":"2021-09-20","objectID":"/svm/:4:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"核函数 我们不禁有个疑问：只是做个内积运算，为什么要有核函数的呢？ 这是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘全部计算好，这样的计算量太大了。 但如果我们有这样的一核函数\\(k(x,y) = (\\phi(x), \\phi(y))\\)，x与y在特征空间中的内积，就等于它们在原始空间中通过函数\\(k(x,y)\\)计算的结果，我们就不需要知道映射函数和计算高维空间中的内积了。 有关内容看本文一开始对kernel的介绍。 ","date":"2021-09-20","objectID":"/svm/:4:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"总结 SVM是深度学习流行之前的首选分类方法，在许多任务上都有很好的效果，稍微修改后可以用于回归任务中。总结一下svm算法的优缺点。 ","date":"2021-09-20","objectID":"/svm/:5:0","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"优点 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题； 能找出对任务至关重要的关键样本（即：支持向量）； 采用核技巧之后，可以处理非线性分类/回归任务； 最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。 ","date":"2021-09-20","objectID":"/svm/:5:1","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["Machine Learning","分类算法"],"content":"缺点 训练时间长。当采用 SMO 算法时，每次都需要挑选一对参数 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为\\(O(N^2)\\) 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。 ## 参考 https://zhuanlan.zhihu.com/p/77750026 ","date":"2021-09-20","objectID":"/svm/:5:2","tags":["Machine Learning","分类算法","SVM"],"title":"SVM","uri":"/svm/"},{"categories":["算法题"],"content":"字符串的排列 ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:0:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/permutation-in-string/ ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:1:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"思路： 滑动窗口加字典 ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:2:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution(object): def checkInclusion(self, s1, s2): counter1 = collections.Counter(s1) N = len(s2) left = 0 right = len(s1) - 1 counter2 = collections.Counter(s2[0:right]) while right \u003c N: counter2[s2[right]] += 1 if counter1 == counter2: return True counter2[s2[left]] -= 1 if counter2[s2[left]] == 0: del counter2[s2[left]] left += 1 right += 1 return False ","date":"2021-09-18","objectID":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:3:0","tags":["算法题","字符串的排列"],"title":"字符串的排列","uri":"/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/"},{"categories":["pandas","api"],"content":"类似于pandas的apply，就是在某一维上进行定义的函数操作 apply_along_axis(func1d, axis, arr, *args, **kwargs) 官网的例子 def my_func(a): return (a[0] + a[-1]) * 0.5 b = np.array([[1,2,3], [4,5,6], [7,8,9]]) np.apply_along_axis(my_func, 0, b) # 结果 array([ 4., 5., 6.]) # 结果 array([ 2., 5., 8.]) ","date":"2021-09-03","objectID":"/apply_along_axis/:0:0","tags":["pandas","api","apply_along_axis"],"title":"apply_along_axis","uri":"/apply_along_axis/"},{"categories":["算法题"],"content":"采购方案 ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:0:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/4xy4Wx/ ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:1:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"思路： 题目很简单，思想就是双指针，感觉是个双指针的典型例子就写了下来 先对数组进行从小到大排序，然后双指针从两边移动，如果一直大于target就一直左移right 然后right - left就是所有成立的数目，再移动left 进行筛选 ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:2:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"代码： class Solution: def purchasePlans(self, nums: List[int], target: int) -\u003e int: nums.sort() left = 0 right = len(nums) - 1 res = 0 while left \u003c right and left \u003c len(nums): while left \u003c right and nums[right] + nums[left] \u003e target: right -= 1 res += right - left left += 1 return res % (1000000007) ","date":"2021-08-30","objectID":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/:3:0","tags":["算法题","采购方案"],"title":"采购方案","uri":"/%E9%87%87%E8%B4%AD%E6%96%B9%E6%A1%88/"},{"categories":["算法题"],"content":"位运算的一个应用 翻了翻以前用python刷leetcode的记录，最后刷的一道题是这样的 https://leetcode-cn.com/problems/single-number/ 叫只出现一次的数字，当时看题感觉非常简单啊！直接搞就行了 当时一开始我的做法是这样的 class Solution: def singleNumber(self, nums): for i in set(nums): if nums.count(i) ==1: return i 信心满满的提交，结果发现TLE了。。 然后看超时案例的输入，没有一个数字重复，也就是说我的set跟没有一样，所以说肯定不能这么做。 然后又想到了哈希表 class Solution: def singleNumber(self, nums): dic={} for num in nums: if num in dic.keys(): dic[num]+=1 else: dic[num]=1 for i in dic.keys(): if dic[i] ==1: return i 这样也算是AC了。本以为这个题就这么结束了，结果无意中看到了别的题解震惊了 代码数量比我短的多得多。 然后就认识到了位运算的魔力。。 先上代码： class Solution: def singleNumber(self, nums): a = 0 for i in nums: a^=i return a 简单的一个异或运算就达到了目的 真是太神奇了！ 找到相关资料 交换律：a ^ b ^ c \u003c=\u003e a ^ c ^ b 任何数于0异或为任何数 0 ^ n =\u003e n 相同的数异或为0: n ^ n =\u003e 0 也就是说相同的数就异或为0了，达到了去重的目的。 ","date":"2021-08-23","objectID":"/%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/:0:0","tags":["算法题","只出现一次的数字"],"title":"只出现一次的数字","uri":"/%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/"},{"categories":["NLP","概率图模型"],"content":"概率图模型概述 ","date":"2021-08-13","objectID":"/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/:0:0","tags":["NLP","概率图模型","概率图模型概述"],"title":"概率图模型概述","uri":"/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"},{"categories":["面经"],"content":"B树 B树就是B-树，以前还以为这是两种树，现在才知道这俩就是一个东西。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:0:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"基本概念 所有的叶子结点都出现在同一层上，并且不带信息(可以看做是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空)。 每个结点包含的关键字个数有上界和下界。用一个被称为 B-树的 最小度数 的固定整数 t≥2 来表示这些界 ，其中 t 取决于磁盘块的大小： a.除根结点以外的每个结点必须至少有 t−1 个关键字。因此，除了根结点以外的每个内部结点有 t 个孩子。如果树非空，根结点至少有一个关键字。 每个结点至多包含 2t−1 个关键字。 一个包含x个关键字的结点有x+1个孩子。 一个结点中所有的关键字升序排列，两个关键字\\(k_1\\)和\\(k_2\\)之间的孩子结点的所有关键字key在\\((k_1, k_2)\\)的范围内。 其中最小度数和B树的阶不一样： 度：一个结点含有的子结点的个数称为该结点的度 阶：一棵树的最大孩子数 最小度minimum degree（t）：用来衡量结点的关键字数量范围 阶 order（m）：衡量B树中的结点的最大孩子数 关系如下： \\[ t = ceil(\\frac{m}{2}) \\quad m = 2t \\] 可以简单理解为最小度是孩子数的最小值，阶是孩子数的最大值。最小度-1是节点关键字数的最小值，阶-1是节点关键字数的最大值。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:1:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"查找 查找很简单，对每个键中的索引进行比较然后查找就可以。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:2:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"插入 伪代码： 初始化 x 作为根结点 当 x 不是叶子结点，执行如下操作： 找到 x 的下一个要被访问的孩子结点 y 如果 y 没有满，则将结点 y 作为新的 x 如果 y 已经满了，拆分 y ，结点 x 的指针指向结点 y 的两部分。 如果 k 比 y 中间的关键字小， 则将 y 的第一部分作为新的 x ，否则将 y 的第二部分作为新的 x ，当将 y 拆分后，将 y 中的一个关键字移动到它的父结点 x 当中。 当 x 是叶子结点时，第二步结束； 由于我们已经提前查分了所有结点，x 必定至少有一个额外的关键字空间，进行简单的插入即可。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:3:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["面经"],"content":"删除 待删除的关键字 k 在结点 x 中，且 x 是叶子结点，删除关键字k 待删除的关键字 k 在结点 x 中，且 x 是内部结点，分一下三种情况 1). 如果位于结点 x 中的关键字 k 之前的第一个孩子结点 y 至少有 t 个关键字，则在孩子结点 y 中找到 k 的前驱结点 k0 ，递归地删除关键字 k0 ，并将结点 x 中的关键字 k 替换为 k0 直接前驱：当前关键字左侧指针 所指子树中“最右下”的元素 删除 B-树中的关键字 G ，G 的前一个孩子结点 y 为 [D、E、F] ，包含 3个关键字，满足情况一，关键字 G 的直接前驱为关键 F ，删除 F ，然后将 G 替换为 F . 2).y 所包含的关键字少于 t 个关键字，则检查结点 x 中关键字 k 的后一个孩子结点 z 包含的关键字的个数，如果 z 包含的关键字的个数至少为 t 个，则在 z 中找到关键字 k 的直接后继 K0 ,然后删除 K0 ，并将关键 k 替换为 K0 . 直接后继：当前关键字右侧指针 所指子树中“最左下”的元素 删除 B-树中的关键字 C , y 中包含的关键字的个数为 2 个，小于 t = 3 ,结点 [C、G、L] 中的 关键字 C 的后一个孩子 z 为 [D、E、F] 包含 3 个关键字，关键字 C 的直接后继为 D ，删除 D ，然后将 C 替换为 D . 3). 如果 y 和 z 都只包含 t -1 个关键字，合并关键字 k 和所有 z 中的关键字到 结点 y 中，结点 x 将失去关键字 k 和孩子结点 z，y 此时包含 2t -1 个关键字，释放结点 z 的空间并递归地从结点 y 中删除关键字 k. 删除关键字 C , 结点 y 包含 2 个关键字 ，结点 z 包含 2 个关键字，均等于 t - 1 = 2 个， 合并关键字 C 和结点 z 中的所有关键字到结点 y 当中： 之后直接删除C即可。 如果关键字 k 不在当前在内部结点 x 中，则确定必包含 k 的子树的根结点 x.c(i) （如果 k 确实在 B-树中）。如果 x.c(i) 只有 t - 1 个关键字，必须执行下面两种情况进行处理： 首先我们得确认什么是当前内部结点 x ，什么是 x.c(i) ,如下图所示， P 现在不是根结点，而是完整 B-树的一个子树的根结点： 1). x.c(i) 仅包含 t - 1 个关键字且 x.c(i) 的一个兄弟结点包含至少 t 个关键字，则将 x 的某一个关键字下移到 x.c(i) 中，将 x.c(i) 的相邻的左兄弟或右兄弟结点中的一个关键字上移到 x 当中，将该兄弟结点中相应的孩子指针移到 x.c(i) 中，使得 x.c(i) 增加一个额外的关键字。 我们以删除结点 [A、B] 中的结点 B 为例，上图中 x.c(i) 包含 2 个关键字，即 t - 1 个关键字， x.c(i) 的一个兄弟结点 [H、J、K] 包含 3 个关键字（满足至少 t 个关键字的要求），则将兄弟结点 [H、J、K] 中的关键字 H 向上移动到 x 中， 将 x 中的关键字 C 下移到 x.c(i) 中；删除关键字 B . 2).如果 x.c(i) 及 x.c(i) 的所有相邻兄弟都只包含 t - 1 个关键字，则将 x.c(i) 与 一个兄弟合并，即将 x 的一个关键字移动至新合并的结点，使之成为该结点的中间关键字，将合并后的结点作为新的 x 结点 . 以此图为例： 上面的图标明了相应的 x 及 x.c(i) ，我们以删除关键字 D 为例，此时当前内部结点 x 不包含关键字 D , 确定是第三种情况，我们可以确认关键 D 一定在结点 x 的第一个孩子结点所在的子树中，结点 x 的第一个孩子结点所在子树的跟结点为 x.c(i) 即 [C、L] . 其中 结点 [C、L] 及其相邻的兄弟结点 [T、W] 都只包含 2 个结点（即 t - 1) ，则将 [C、L] 与 [T、W] 合并，并将结点 x 当中仅有的关键字 P 合并到新结点中；然后将合并后的结点作为新的 x 结点，递归删除关键字 D ，发现D 此时在叶子结点 y 中，直接删除，就是 1. 的情况。 ","date":"2021-08-07","objectID":"/b%E6%A0%91/:4:0","tags":["面经","B树"],"title":"B树","uri":"/b%E6%A0%91/"},{"categories":["算法题"],"content":"和为s的连续正数序列 ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:0:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"题目: https://leetcode-cn.com/problems/he-wei-sde-lian-xu-zheng-shu-xu-lie-lcof/ ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:1:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"思路： 滑动窗口即可，滑动窗口就是选取数组的一部分来进行操作，left 和 right只能向右移动 ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:2:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def findContinuousSequence(self, target: int) -\u003e List[List[int]]: res = [] left,right = 1,2 while left \u003c= target // 2: # 优化 减少时间复杂度 if sum(range(left,right+1)) \u003c target: # 小于target 右指针移动 right += 1 elif sum(range(left,right+1)) \u003e target: # 大于target 左指针移动 left += 1 else: res.append(list(range(left,right+1))) # 相等的话 两个指针都移动 right += 1 left += 1 return res ","date":"2021-08-06","objectID":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:3:0","tags":["算法题","和为s的连续正数序列"],"title":"和为s的连续正数序列","uri":"/%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"合并区间 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:0:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/merge-intervals/ ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:1:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"思路： ​ 一开始思路想的是，根据每一个区间的left排序后，然后比较每一个数，再向前更新，然后写了半天，一直WA，感觉这个思路不太行了 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:2:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"代码： ​ 先贴上错误的代码： class Solution: def merge(self, res: List[List[int]]) -\u003e List[List[int]]: if not res: return [] res.sort(key=lambda i:i[0]) n = len(res) - 1 for i in range(0,n): if res[i][1] \u003c res[i+1][0]: continue else: res[i+1] = [res[i][0],max(res[i][1],res[i+1][1])] res[i] = res[i-1] ress = [] for i in res: if i not in ress: ress.append(i) return ress 在[[1,4],[0,2],[3,5]] ​ 出错了 输出： [[3,5],[0,5]] 预期结果： [[0,5]] 应该是思路的错误 后来觉得不应该在原数组上操作 又改了如下，终于过了 class Solution: def merge(self, intervals: List[List[int]]) -\u003e List[List[int]]: if not intervals: return [] intervals.sort(key=lambda i:i[0]) res = [] for i in intervals: if len(res) == 0 or res[-1][1] \u003c i[0]: res.append(i) else: res[-1][1] = max(res[-1][1],i[1]) return res 这个思路就是先创造一个空数组res 然后如果数组为空或者题设的条件不成立的时候，把原数组的值加进去，要是条件成立的话，则将目前区间的right改为目前区间的right和原数组的right之间的最大值，预防[[1,4],[2,3]]这种情况。注意这个也是按left排序的 我上面代码的思路和这个是一样的，看来类似的题目尽量不要在原数组上面操作，除非题目要求 ","date":"2021-08-03","objectID":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/:3:0","tags":["算法题","合并区间"],"title":"合并区间","uri":"/%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"},{"categories":["Machine Learning","降维算法"],"content":"SVD奇异值分解 参考：https://www.cnblogs.com/pinard/p/6251584.html ","date":"2021-07-16","objectID":"/svd/:0:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["Machine Learning","降维算法"],"content":"特征值与特征向量 首先回顾特征值与特征向量\\(Ax=\\lambda x\\) \\(\\lambda\\) 是矩阵A的一个特征值，x是矩阵A的特征值\\(\\lambda\\)对应的特征向量。 求出特征值与特征向量可以将矩阵A进行特征分解。如果求出了A的n个特征值，以及这n个特征值所对应的特征向量\\({w_1,w_2,\\dots,w_n}\\)，如果这n个特征向量线性无关，则矩阵A就可以用下式进行表示： \\[ A = W\\sum W^{-1} \\] 其中W为这n个特征向量所张成的\\(n\\times n\\)维矩阵，\\(\\sum\\)为这n个特征值为主对角线的矩阵。 我们一般会把n个特征向量标准化，即\\(w_i^Tw_i=1\\)，此时W的n个特征向量为标准正交基，满足\\(W^TW=I\\)，即\\(W^T=W^{-1}\\)，也就是说W为酉矩阵。 这样特征分解表达式可以写为\\(A=W\\sum W^T\\) 特征分解要求A必须为方阵，如果行列不相同则使用SVD进行分解。 ","date":"2021-07-16","objectID":"/svd/:1:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["Machine Learning","降维算法"],"content":"SVD 假设A为一个\\(m\\times n\\)的矩阵，那么定义A的SVD为： \\[ A = U\\sum V^T \\] 其中U是一个\\(m\\times n\\)的矩阵，\\(\\sum\\)是一个\\(m\\times n\\)的矩阵，除了主对角线上的元素全为0，主对角线上的每个元素成为奇异值，V是一个\\(n\\times n\\)的矩阵，U和V都是酉矩阵。 先得到\\(m\\times m\\)的方阵\\(AA^T\\),然后进行特征值分解，\\((AA^T)u_i=\\lambda_iu_i\\) 将\\(AA^T\\)的所有特征向量张成一个\\(m\\times m\\)的矩阵U，就是SVD公式里面的U矩阵了。 后得到\\(n\\times n\\)的方阵\\(A^TA\\),然后进行特征值分解，\\((A^TA)v_i=\\lambda_iv_i\\) 将\\(A^TA\\)的所有特征向量张成一个\\(n\\times n\\)的矩阵V，就是SVD公式里面的V矩阵了。 特征值和奇异值满足以下关系\\(\\sigma_i = \\sqrt{\\lambda_i}\\)，意思就是我们可以通过求\\(A^TA\\)的特征值取平方根来求奇异值。 ","date":"2021-07-16","objectID":"/svd/:2:0","tags":["Machine Learning","降维算法","SVD"],"title":"SVD","uri":"/svd/"},{"categories":["NLP"],"content":"词嵌入 ","date":"2021-07-16","objectID":"/word-embedding/:0:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"介绍 词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。 词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。 在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中语法分析器和文本情感分析等的效果。 以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word representation)，可以粗略得把它分为三个阶段： 一、特征工程阶段，以词袋模型为典型代表。 二、浅层表证阶段，以word2vec为典型代表。 三、深层表征阶段，以基于transformer的Bert为典型代表。 本文介绍了一、二两部分内容 ","date":"2021-07-16","objectID":"/word-embedding/:1:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"语言模型 一句话，语言模型是这样一个模型：对于任意的词序列，它能够计算出这个序列是一句话的概率。 具体的详见本博客有关语言模型的文章。 为什么要介绍语言模型，因为NLP的做预训练一般选择用语言模型任务来做的。 语言模型主要有： N-gram LM、FeedForward Neural Network LM、RNN LM和GPT系列 本文会涉及到Neural Network LM，其本质是一个语言模型，词向量只是其在训练过程中的一个副产物。 ","date":"2021-07-16","objectID":"/word-embedding/:2:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"特征工程阶段(基于计数的方法) ","date":"2021-07-16","objectID":"/word-embedding/:3:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"One-Hot 最简单的方法是将单词表示为 one-hot 向量：对于词汇表中的第 i 个单词，向量在第 i 个维度上具有 1，在其余维度上具有 0。在机器学习中，这是表示分类特征的最简单方法。 One-Hot的缺点很明显，它对自己代表的词一无所知，没有捕捉到词的意义。 ","date":"2021-07-16","objectID":"/word-embedding/:3:1","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"词袋模型 即无视词语的顺序，只关心出现的次数，以下都属于词袋模型。在文本匹配领域也有类似的字面意义的匹配。本博客有相关内容。 ### TFIDF 权重也可以作为词向量表示。也可以计算文本相似度等，本博客有相关内容。 ","date":"2021-07-16","objectID":"/word-embedding/:3:2","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"共现矩阵 详见本博客共现矩阵 ### PPMI 详见本博客互信息相关博文。 中文名字为正点互信息，公式如下 \\[ PPMI(w, c) = max(0, PMI(w, c)) \\\\\\\\ PMI(w, c) = log\\frac{P(w,c)}{P(w)P(c)} = log \\frac{N(w,c)|(w,c)|}{N(w)N(c)} \\] 事实证明，word2vec被证明可以隐式逼近PMI矩阵的因式分解。 『Neural Word Embedding as Implicit Matrix Factorization』这篇论文就详细讨论了这个问题。 ","date":"2021-07-16","objectID":"/word-embedding/:3:3","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"主题模型 这个更是重量级，见本博客。 ","date":"2021-07-16","objectID":"/word-embedding/:3:4","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"浅层表征阶段(基于推理的方法) ","date":"2021-07-16","objectID":"/word-embedding/:4:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"NNLM 介绍 先来张经典的图片 这就是大名鼎鼎的神经网络语言模型（其它的语言模型详见本博客语言模型部分） 学习任务是输入某个句中单词\\(W_t=i\\)前面句子的t-1个单词，要求网络正确预测单词\\(W_t=i\\)，即最大化： \\[ P(W_t=i | W_1, W_2, \\dots W_{(t-1)}; \\theta) \\] 前面任意单词 \\(W_i\\)用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 \\(C(W_i)\\)，每个单词的 \\(C(W_i)\\) 拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。 这个\\(C(W_i)\\)是什么？ 这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。 所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。 参数解释 训练样本(Context(w), w),w是语料C的每一个词，Context(w)为取其前n-1个词 投影层向量\\(X_w\\): 将该训练样本(Context(w), w)的前n-1个词的词向量首尾拼接在一起，这里的词向量可以用独热编码表示。\\(X_w\\)的形状为 \\((n-1)\\times m\\)，这里的m为词汇表的所有词的个数。 隐藏层向量\\(Z_w\\): \\[ Z_w = tanh(WX_w+p) \\] 输出层向量\\(y_w\\): 维度为N=|D|,即词典D中词的个数。 \\[ y_w = Uz_w + q \\] 在对\\(y_w\\)做softmax后，\\(y_w\\)的分量就表示当前词是w的概率 \\[ p(w|Context(w)) = \\frac{e^{y_w,iw}}{\\sum_{i-1}^N e^{y^{w, i}} } \\] 优点与缺点 NNLM相对于N-grams语言模型，有以下优点： 词语与词语间的相似度可以通过词向量来体现 基于词向量的模型自带『平滑化』功能，无需额外处理。 当然也有缺点，缺点就是计算量太大。 下面就重点介绍一下word2vec，相比于NNLM来说这是专门训练词向量的一种工具，而词向量对于NNLM来说只是一个副产物，其本质还是一个语言模型。 代码 代码来自https://github.com/graykode/nlp-tutorial import torch import torch.nn as nn import torch.optim as optim def make_batch(): input_batch = [] target_batch = [] for sen in sentences: word = sen.split() # space tokenizer input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input target = word_dict[word[-1]] # create (n) as target, We usually call this 'casual language model' input_batch.append(input) target_batch.append(target) return input_batch, target_batch # Model class NNLM(nn.Module): def __init__(self): super(NNLM, self).__init__() self.C = nn.Embedding(n_class, m) self.H = nn.Linear(n_step * m, n_hidden, bias=False) self.d = nn.Parameter(torch.ones(n_hidden)) self.U = nn.Linear(n_hidden, n_class, bias=False) self.W = nn.Linear(n_step * m, n_class, bias=False) self.b = nn.Parameter(torch.ones(n_class)) def forward(self, X): X = self.C(X) # X : [batch_size, n_step, m] X = X.view(-1, n_step * m) # [batch_size, n_step * m] tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden] output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class] return output if __name__ == '__main__': n_step = 2 # number of steps, n-1 in paper n_hidden = 2 # number of hidden size, h in paper m = 2 # embedding size, m in paper sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"] word_list = \" \".join(sentences).split() # ['i', 'like', 'dog', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk'] word_list = list(set(word_list)) # ['i', 'like', 'dog', 'love', 'coffee', 'hate', 'milk'] word_dict = {w: i for i, w in enumerate(word_list)} # {'i':0, 'like':1, 'dog':2, 'love':3, 'coffee':4, 'hate':5, 'milk':6} number_dict = {i: w for i, w in enumerate(word_list)} # {0:'i', 1:'like', 2:'dog', 3:'love', 4:'coffee', 5:'hate', 6:'milk'} n_class = len(word_dict) # number of Vocabulary, just like |V|, in this task n_class=7 model = NNLM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) input_batch, target_batch = make_batch() input_batch = torch.LongTensor(input_batch) target_batch = torch.LongTensor(target_batch) # Training for epoch in range(5000): optimizer.zero_grad() output = model(input_batch) # output : [batch_size, n_class], target_batch : [batch_size] loss = criterion(output, target_batch) if (epoch + 1) % 1000 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() # Predict predict = model(input_batch).data.argmax(1, keepdim=True) # Test print([sen.split()[:2] for sen in sentences], '-\u003e', [number_dict[n.item()] for n in predict.squeeze()]) ","date":"2021-07-16","objectID":"/word-embedding/:4:1","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"word2vec Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。 Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。 (skip-gram与CBOW只是word2vec的变体。) 训练思路 Word2Vec 是一个模型，其参数是词向量。这些参数针对某个目标进行迭代优化。目标迫使词向量“知道”一个词可能出现的上下文：训练向量以预测相应词的可能上下文。正如您从分布假设中所记得的那样，如果向量“知道”上下文，它们就“知道”单词的含义。 获取一个巨大的文本语料库； 使用滑动窗口浏览文本，一次移动一个单词。在每一步，都有一个中心词和上下文词（此窗口中的其他词）； 对于中心词，计算上下文词的概率； 调整向量以增加这些概率。 推导 以Skip-grams模型为例，首先清楚几个概念，中心词、背景词（上下文词）、负采样词。中心词就是我们的输入，因为skip-grams相当于在一句话中扣去一个词，然后用这个词预测这句话的其余词。形式上给人的感觉就是一对多，这里的一句话其实不是一句话，是我们设定的窗口大小，比如一句话”I miss you very much”， 设置中心词为you，窗口大小为1，那么背景词就是”miss”和”very”。那么对于我们的模型来说，miss和very就是正例，就是我们的预测值(sigmoid后)的值接近于1的，而其余的词就是负例，就是使其值接近于0的。所以负采样就是从这些负例中随机抽取一些负例，不然每次都要计算单词表中所有单词的sigmoid值，这个计算量很大，而使用负采样就大大缩小了计算量。 目标函数：负似然对数 对于每个位置\\(t=1,\\dots, T\\)，在文本语料库中，Word2Vec在给定中心词的m大小窗口内预测上下文词\\(w_t\\): \\[ Likelihood=L(\\theta)=\\prod_{t=1}^T\\prod_{-m\\leq j\\leq m, j\\neq0} P(w_{t+j} | w_t, \\theta) \\] 目标函数\\(J(\\theta)\\)为平均负对数似然 \\[ Loss = J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta) \\] 对于每个单词w我们都有两个向量： \\(v_w\\)当w是中心词 \\(u_w\\)当w时背景词 因为word2vec是一个专门训练词向量的神经网络，输入为onehot向量，经过第一层W的计算得到隐藏层，就相当于查表，数值为中心词的词向量，然后再与第二层的W计算得到输出，输出的维度和输入的维度相同，因此输出层的结果在计算上就是\\(u_c^Tv_w\\)。即输入层对应的中心词的词向量*输出层对应背景词的词向量，再经过softmax就是对应背景词的概率。 训练完毕后，我们只使用\\(v_w\\)，即只使用从输入层到隐藏层的权重。 对于中心词w，上下文词c的概率为： \\[ P(c|w) = \\frac{exp(u_c^Tv_w)}{\\sum_{o\\in V}exp(u_o^Tv_w)} \\] 这就是softmax函数。 训练 \\(\\theta^{new} = \\theta^{old} - \\alpha \\nabla_\\theta J(\\theta)\\) 一次进行一次更新，每次更新都是针对一对中心词和其中一个背景词。损失函数： \\[ Loss = J(\\theta) = -\\frac{1}{T}logL(\\theta)= -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta)\\\\\\\\=\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0}J_{t,j}(\\theta). \\] 其中\\(J_{t,j}(\\theta) = -logP(w_{t+j}|w_t, \\theta)\\) 以”I miss you very much”这句话为例子，中心词为”you”，其中一个背景词为miss，则损失项为 \\[ J_{t,j}(\\theta) = -logP(miss|you) = -log\\frac{exp(u_{miss}^Tv_{you})}{\\sum_{o\\in V}exp(u_o^Tv_{you})} = \\\\\\\\-u_{miss}^Tv_{you}+log\\sum_{o\\in V} exp(u_o^Tv_{you}) \\] 这是中心词对应其中一个背景词的损失函数，如果要求总的损失，则将所有背景词的损失相加，然后将所有样本的损失求平均，就是上面的Loss。其实这就是交叉熵损失函数，是个多分类问题，预测的target相当于miss对应的数字，也就是序列值，具体的pytorch代码为 loss = (-output_layer[:, Y] + torch.log(torch.sum(torch.exp(output_layer), dim=1))).mean() 这里的output_layer就是神经网络的输出层，注意这里的都是对batch操作，这里求出的loss是这个batch上最终的loss，而不是一对中心词与背景词的loss。理解了这个简单的word2vec模型就算理解了。 下面求一下梯度： 注意这里的c为center word, o为上下文词 \\[ \\frac{\\partial logP(w_o|w_c)}{\\partial v_c} = \\frac{\\partial}{\\partial v_c}log\\frac{exp(u_0^Tv_c)}{\\sum_{i=1}^{|V|}exp(u_i^Tv_c)}\\\\\\\\= \\frac{\\partial}{\\partial v_c}log\\, exp(u_o^Tv_c) - \\frac{\\partial}{\\partial v_c}log\\sum_{i=1}^{|V|}exp(u_i^Tv_c) \\] 左边的： \\[ \\frac{\\partial}{\\partial v_c}log\\, exp(u_o^Tv_c)=\\frac{\\partial}{\\partial v_c}u_o^Tv_c=u_o \\] 第二部分推导 \\[ \\begin{aligned} \\frac{\\partial}{\\partial {v}_{c}} \\log \\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right) \u0026=\\frac{1}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} \\cdot \\frac{\\partial}{\\partial {v}_{c}} \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\\\\\\\ \u0026=\\frac{1}{A} \\cdot \\sum_{x=1}^{|V|} \\frac{\\partial}{\\partial {v}_{c}} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\\\\\\\ \u0026=\\frac{1}{A} \\cdot \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) \\frac{\\partial}{\\partial {v}_{c}} {u}_{x}^{T} {v}_{c} \\\\\\\\ \u0026=\\frac{1}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} \\sum_{x=1}^{|V|} \\exp \\left({u}_{x}^{T} {v}_{c}\\right) {u}_{x} \\\\\\\\ \u0026=\\sum_{x=1}^{|V|} \\frac{\\exp \\left({u}_{x}^{T} {v}_{c}\\right)}{\\sum_{i=1}^{|V|} \\exp \\left({u}_{i}^{T} {v}_{c}\\right)} {u}_{x} \\\\\\\\ \u0026=\\sum_{x=1}^{|V|} P\\left(w_{x} \\mid w_{c}\\right) {u}_{x} \\end{aligned} \\] 综上所述 \\[ \\frac{\\partial \\log P\\left(w_{o} \\mid w_{c}\\right)}{\\partial {v}_{c}}={u}_{o}-\\sum_{j \\in V} P\\left(w_{j} \\mid w_{c}\\right) {u}_{j} \\] 通过上面计算得到梯度后，我们可以使用随机梯度下降来不断迭代模型参数\\(v_c\\)","date":"2021-07-16","objectID":"/word-embedding/:4:2","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"GloVe GloVe 模型是基于计数的方法和预测方法（例如 Word2Vec）的组合。模型名称 GloVe 代表“Global Vectors”，体现了它的思想：该方法利用 语料库中的全局信息来学习向量。 正如我们之前看到的，最简单的基于计数的方法使用共现计数来衡量单词 w 和上下文c之间的关联：\\(N(w,c)\\)。 GloVe 也使用这些计数来构建损失函数： \\[ J(\\theta) = \\sum_{w,c\\in V}f(N(w,c)) \\cdot(u_c^Tv_w+b_c+\\bar{b_w}-logN(w,c)^2) \\] 其中： \\[ f(x) = \\begin{cases} (\\frac{x}{x_{max}})^{0.75}, \\quad if \\;x \u003c x_{max} \\\\\\\\ 1, \\quad if \\; x\\geq x_{max} \\end{cases} \\] 具体的推导可以看这个博客：https://www.cnblogs.com/Lee-yl/p/11172255.html 可以看到GloVe并没有使用神经网络的方法。 与 Word2Vec 类似，我们也有不同的 中心词和上下文词向量——这些是我们的参数。此外，该方法对每个词向量都有一个标量偏置项。 特别有趣的是 GloVe 控制稀有词和频繁词影响的方式：每对 ( w , c ) 的损失以如下方式加权 罕见事件受到惩罚， 非常频繁的事件不会被过度加权。 ","date":"2021-07-16","objectID":"/word-embedding/:4:3","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["NLP"],"content":"关于word embedding word embedding只能通过语言模型的训练获取吗？ 不是的；事实上任何NLP任务都可以在训练过程中获取word embedding，甚至特定任务下的word embedding在特定任务中的使用效果还会更好（如基于fasttext的文本分类任务）。之所以我们平时提及的word embedding均是在语言模型任务中产生，私以为主要有这么几个原因：a).语言模型是无监督任务，存在海量训练语料，无需标注成本；b).语言模型任务本身要求较高，训练过程中可以学习大量的语义知识，进而生成高质的word representation。 如何评估word embedding好坏？ 有两种方式：第一种，把word embedding融入现有系统中，看其对系统性能的提升；第二，从语言学的角度对word embedding进行分析，如相似度、语义偏移等。更细节的可以参考这里。 ## 参考 参考： https://blog.csdn.net/malefactor/article/details/83961886 https://www.zybuluo.com/Dounm/note/591752 https://lena-voita.github.io/nlp_course/word_embeddings.html https://wmathor.com/index.php/archives/1430/ https://zhuanlan.zhihu.com/p/27234078 Rong X . word2vec Parameter Learning Explained[J]. Computer Science, 2014. ","date":"2021-07-16","objectID":"/word-embedding/:5:0","tags":["NLP","Word Embedding"],"title":"Word Embedding","uri":"/word-embedding/"},{"categories":["算法题"],"content":"三数之和 ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:0:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/3sum/solution/ ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:1:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"思路： ​ 第一眼看就想到了用双指针，注意重复数值的处理问题，算是一个滑动窗口问题 ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:2:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["算法题"],"content":"代码： class Solution: def threeSum(self, nums: List[int]) -\u003e List[List[int]]: res = [] if len(nums) \u003c 3: return [] nums.sort() for i, num in enumerate(nums): if num \u003e 0: return res if i \u003e 0 and nums[i] == nums[i-1]: continue left, right = i+1, len(nums) - 1 while left \u003c right: temp = nums[i] + nums[left] + nums[right] if temp == 0: res.append([nums[i], nums[left], nums[right]]) while left \u003c right and nums[right-1] == nums[right]: right -= 1 while left \u003c right and nums[left+1] == nums[left]: left += 1 left += 1 right -= 1 if temp \u003e 0: right -=1 if temp \u003c 0: left += 1 return res ","date":"2021-07-16","objectID":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/:3:0","tags":["算法题","三数之和"],"title":"三数之和","uri":"/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"},{"categories":["Machine Learning","性能指标"],"content":"# ROC曲线 了解什么是ROC曲线和AUC之前，要先了解什么是混淆矩阵。 混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下： 称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。 预测正确的为True（真），预测错误的为False（伪）。 因此有了True Postive Rate、False Postive Rate两个概念 \\(TPRate = \\frac{TP}{TP+FN}\\) \\(FPRate = \\frac{FP}{FP+TN}\\) TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。 FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。 按照定义，AUC即ROC曲线下的面积，而ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图: 这样的分类器和瞎猜没啥区别，我们可以看成AUC的最小值为0.5。 而我们希望分类器达到的效果是：对于真实类别为1的样本，分类器预测为1的概率（即TPRate），要大于真实类别为0而预测类别为1的概率（即FPRate），即y＞x。这是理所当然的，分类器肯定要分对的嘛。 我们知道，在二分类（0，1）的模型中，一般我们最后的输出是一个概率值，表示结果是1的概率。那么我们最后怎么决定输入的x是属于0或1呢？我们需要一个 阈值，超过这个阈值则归类为1，低于这个阈值就归类为0。 所以，不同的阈值会导致分类的结果不同，也就是混淆矩阵不一样了，FPR和TPR也就不一样了。所以当阈值从0开始慢慢移动到1的过程，就会形成很多对(FPR, TPR)的值，将它们画在坐标系上，就是所谓的ROC曲线了。 看这张图： 当阈值选取为0.5时，阈值右边的视为 预测为正例 ，阈值左边的视为 预测为负例 。由于准确度为\\(\\frac{TP+TN}{all}\\)，因此可得此时的准确度为90%。 看这张图： 阈值设定为0.6，即右边视为 预测为正例 ，红色的为实际为正例，蓝色的为实际为负例，因此很容易得到FP=0，因为阈值右边没有蓝色区域，可以这么理解。 当蓝色区域与红色区域基本重叠时，ROC曲线就和接近y=x这条线了。 其实，AUC表示的是正例排在负例前面的概率 我们知道阈值可以取不同，也就是说，分类的结果会受到阈值的影响。如果使用AUC的话，因为阈值变动考虑到了，所以评估的效果更好。 最后说说AUC的优势，AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。 ","date":"2021-06-26","objectID":"/roc%E6%9B%B2%E7%BA%BF/:1:0","tags":["Machine Learning","性能指标","ROC曲线"],"title":"ROC曲线","uri":"/roc%E6%9B%B2%E7%BA%BF/"},{"categories":["Machine Learning","性能指标"],"content":"AUC计算 ","date":"2021-06-26","objectID":"/roc%E6%9B%B2%E7%BA%BF/:2:0","tags":["Machine Learning","性能指标","ROC曲线"],"title":"ROC曲线","uri":"/roc%E6%9B%B2%E7%BA%BF/"},{"categories":["算法题"],"content":"删除排序数组中的重复项 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:0:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"删除排序数组中的重复项1 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/ ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:1","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"思路： ​ 双指针，定义 nums[0...i] 为为非重复数列，遍历整个数列不断的维护这个定义 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:2","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"代码： ​ class Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: start = 0 for i in range(len(nums)): if nums[i] != nums[start]: start += 1 nums[i],nums[start] = nums[start],nums[i] return start + 1 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:1:3","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"删除排序数组中的重复项2 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:0","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii/ ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:1","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"思路： ​ 也是利用双指针，一个指针用于遍历数组元素，一个指针指向要拷贝赋值的索引位置 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:2","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["算法题"],"content":"代码： ​ class Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: if len(nums) \u003c= 2: #长度小于等于2时 return len(nums) count = 1 #用于重复的计数 j = 1 #指向多余重复的元素 for i in range(1,len(nums)): if nums[i] == nums[i-1]: count += 1 #重复了就加一 if count \u003e 2: #如果重复两次以上就pass掉，等着被替换 pass else: nums[j] = nums[i] j += 1 else: nums[j] = nums[i] #如果不相等了 把多余重复的那个替换掉了 count = 1 #重置计数 j += 1 return j 这是一种思路比较清晰的写法 ","date":"2021-06-22","objectID":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/:2:3","tags":["算法题","删除排序数组中的重复项"],"title":"删除排序数组中的重复项","uri":"/%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/"},{"categories":["python"],"content":"import threading import time ","date":"2021-06-13","objectID":"/asyncio/:0:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"多线程例子 def spider(): #..... time.sleep(0.02) def main1(): for i in range(100): spider() def main2(): thread_list = [] for i in range(100): thread = threading.Thread(target = spider) thread.start() thread_list.append(thread) for t in thread_list: t.join() if __name__ == \"__main__\": start = time.time() main1() end = time.time() print(\"time1 :{:.4f}\".format(end-start)) start = time.time() main2() end = time.time() print(\"time2 :{:4f}\".format(end-start)) time1 :2.0523 time2 :0.037929 ","date":"2021-06-13","objectID":"/asyncio/:1:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"yield def fib(n): a,b = 0,1 while b\u003cn: a,b = b,a+b yield a print(fib(100)) for i in fib(100): print(i) \u003cgenerator object fib at 0x000002B1A7AA1E60\u003e 1 1 2 3 5 8 13 21 34 55 89 ","date":"2021-06-13","objectID":"/asyncio/:2:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"协程 GEN_CREATED 创建完成，等待执行 GEN_RUNNING 解释器正在执行 GEN_SUSPENDED 在 yield 表达式处暂停 GEN_CLOSE 执行结束，生成器停止 import inspect def generator(): i = \"激活生成器\" while True: try: value = yield i except ValueError: print(\"OVER\") i = value g = generator() print(inspect.getgeneratorstate(g)) #查看状态 next(g) # next(g)相当于g.send(None) 可以用后面的语句来预缴携程 GEN_CREATED '激活生成器' inspect.getgeneratorstate(g) #查看生成器状态 'GEN_SUSPENDED' g.send(\"hello world\") 'hello world' 暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值，也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i g.throw(ValueError) #抛出异常 结束 OVER 'hello world' g.close() inspect.getgeneratorstate(g) #关闭了 'GEN_CLOSED' ","date":"2021-06-13","objectID":"/asyncio/:3:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"预激协程 from functools import wraps def corcut(func): @wraps(func) def wrapper(*args,**kw): g = func(*args,**kw) next(g) return g return wrapper @corcut #装饰器 def generator(): i = \"激活生成器\" while True: try: value = yield i except ValueError: print(\"OVER\") i = value g = generator() print(inspect.getgeneratorstate(g)) #此时已经用装饰器将生成器激活了 GEN_SUSPENDED @corcut def generator(): l = [] while True: value = yield if value == \"CLOSE\": break l.append(value) return l g = generator() for i in ['a','b','CLOSE']: try: g.send(i) except StopIteration as e: value = e.value value ['a', 'b'] ","date":"2021-06-13","objectID":"/asyncio/:4:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"yield from用法 from itertools import chain c = chain({'one','two','three'},list(\"abc\")) for i in c: print(i) three two one a b c def chains1(*args): for i in args: for n in i: yield n def chains2(*args): for i in args: yield from i #i为可迭代对象，避免嵌套循环 c1 = chains1({\"one\",\"two\",\"three\"},list(\"abc\")) for i in c1: print(i) print(\"\\n\") c2 = chains2({\"one\",\"two\",\"three\"},list(\"abc\")) for i in c2: print(i) three two one a b c three two one a b c ","date":"2021-06-13","objectID":"/asyncio/:5:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"转移控制权 from functools import wraps from faker import Faker import time def corout(func): @wraps(func) def wapper(*args,**kw): g = func(*args,**kw) next(g) return g return wapper # 子生成器 def generator(): l = [] while True: i = yield if i == \"CLOSE\": break l.append(i) return sorted(l) # 委托生成器 @corout def generator2(): while True: l = yield from generator() print(\"排序后的列表\",l) print(\"-----------------\") # 客户端 if __name__ == \"__main__\": fake = Faker().country_code nest_country = [[fake() for i in range(3)] for j in range(3)] for country in nest_country: print('国家代号列表：', country) c = generator2() for i in country: c.send(i) c.send(\"CLOSE\") 国家代号列表： ['AM', 'ZA', 'BG'] 排序后的列表 ['AM', 'BG', 'ZA'] ----------------- 国家代号列表： ['UG', 'BE', 'SI'] 排序后的列表 ['BE', 'SI', 'UG'] ----------------- 国家代号列表： ['SC', 'KI', 'KI'] 排序后的列表 ['KI', 'KI', 'SC'] ----------------- yield显然不只是用来减小循环次数的，引用一下《流畅的python》中关于yield from 的意义： - 子生成器产出的值都直接传给委派生成器的调用方（即客户端代码）。 - 使用 send() 方法发给委派生成器的值都直接传给子生成器。如果发送的值是 None，那么会调用子生成器的 next() 方法。如果发送的值不是 None，那么会调用子生成器的 send() 方法。如果调用的方法抛出 StopIteration 异常，那么委派生成器恢复运行。任何其他异常都会向上冒泡，传给委派生成器。 - 生成器退出时，生成器（或子生成器）中的 return expr 表达式会触发 StopIteration(expr) 异常抛出。 - yield from 表达式的值是子生成器终止时传给 StopIteration异常的第一个参数。 为什么yield可以转移控制权，可以看一下这一段伪代码： 注意这里的6是委托生成器向子生成器发送_s，而_s是调用方向委托生成器发送的，发送后得到结果_y，并在下一个循环yield即抛出给调用方。 ## asyncio模块 import time import asyncio def one(): start = time.time() @asyncio.coroutine #1 def do_something(): #2 print(\"start ------\") time.sleep(0.1) #3 print(\"doing something\") loop = asyncio.get_event_loop() #4 coroutine = do_something() #5 loop.run_until_complete(coroutine) #6 end = time.time() print(\"消耗时间:{:.4f}\".format(end-start))#7 one() start ------ doing something 消耗时间:0.1012 代码说明： 1、使用协程装饰器创建协程函数 2、协程函数 3、模拟 IO 操作 4、创建事件循环。每个线程中只能有一个事件循环，get_event_loop 方法会获取当前已经存在的事件循环，如果当前线程中没有，新建一个 5、调用协程函数获取协程对象 6、将协程对象注入到事件循环，协程的运行由事件循环控制。事件循环的 run_until_complete 方法会阻塞运行，直到任务全部完成。协程对象作为 run_until_complete 方法的参数，loop 会自动将协程对象包装成任务来运行。后面我们会讲到多个任务注入事件循环的情况 7、打印程序运行耗时 import time import asyncio def two(): start = time.time() @asyncio.coroutine def do_something(): print(\"start ------\") time.sleep(0.1) print(\"doing something\") loop = asyncio.get_event_loop() coroutine = do_something() task = loop.create_task(coroutine) #1 print(\"task是不是Task的示例？\",isinstance(task,asyncio.Task)) #2 print(\"task state\",task._state) #3 loop.run_until_complete(task) #4 print(\"take state\",task._state) end = time.time() print(\"消耗时间:{:.4f}\".format(end-start)) two() task是不是Task的示例？ True task state PENDING start ------ doing something take state FINISHED 消耗时间:0.1013 1、事件循环的 create_task 方法可以创建任务，另外 asyncio.ensure_future 方法也可以创建任务，参数须为协程对象 2、task 是 asyncio.Task 类的实例，为什么要使用协程对象创建任务？因为在这个过程中 asyncio.Task 做了一些工作，包括预激协程、协程运行中遇到某些异常时的处理 **3、task 对象的 _state 属性保存当前任务的运行状态，任务的运行状态有 PENDING 和 FINISHED 两种** 4、将任务注入事件循环，阻塞运行 ","date":"2021-06-13","objectID":"/asyncio/:6:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"async / await import functools def three(): start = time.time() #@asyncio.coroutine async def do_something(): #1 print(\"start doing\") time.sleep(0.1) print(\"done\") def callback(name,task): #2 print(\"call back:{}\".format(name)) print(\"call back:{}\".format(task._state)) loop = asyncio.get_event_loop() coroutine = do_something() task = loop.create_task(coroutine) task.add_done_callback(functools.partial(callback, 'vllbc')) #3 loop.run_until_complete(task) end = time.time() print(\"total time {:.4f}\".format(end-start)) three() start doing done call back:vllbc call back:FINISHED total time 0.1013 代码说明： 1、使用 async 关键字替代 asyncio.coroutine 装饰器创建协程函数 2、回调函数，协程终止后需要顺便运行的代码写入这里，回调函数的参数有要求，最后一个位置参数须为 task 对象 3、task 对象的 add_done_callback 方法可以添加回调函数，注意参数必须是回调函数，这个方法不能传入回调函数的参数，这一点需要通过 functools 模块的 partial 方法解决，将回调函数和其参数 name 作为 partial 方法的参数，此方法的返回值就是偏函数，偏函数可作为 task.add_done_callback 方法的参数 def four(): start = time.time() async def do_something(name,t): print(\"start !\u003e\u003e\",name) await asyncio.sleep(t) #1 print('Stop coroutine', name) return 'Coroutine {} OK'.format(name) #2 loop = asyncio.get_event_loop() coroutine1 = do_something('wlb',3) #3 coroutine2 = do_something('yyh',1) task1 = loop.create_task(coroutine1) #4 task2 = loop.create_task(coroutine2) gather = asyncio.gather(task1,task2) #5 loop.run_until_complete(gather) print(\"task1\",task1.result()) print(\"task2\",task2.result()) #result = loop.run_until_complete(gather) #这里result就是两个返回值组成的列表 即['task1 Coroutine wlb OK','task2 Coroutine yyh OK'] end = time.time() print(\"total time:{:.4f}\".format(end-start)) four() start !\u003e\u003e wlb start !\u003e\u003e yyh Stop coroutine yyh Stop coroutine wlb task1 Coroutine wlb OK task2 Coroutine yyh OK total time:3.0022 代码说明： 1、await 关键字等同于 Python 3.4 中的 yield from 语句，后面接协程对象。asyncio.sleep 方法的返回值为协程对象，这一步为阻塞运行。asyncio.sleep 与 time.sleep 是不同的，前者阻塞当前协程，即 corowork 函数的运行，而 time.sleep 会阻塞整个线程，所以这里必须用前者，阻塞当前协程，CPU 可以在线程内的其它协程中执行 2、协程函数的 return 值可以在协程运行结束后保存到对应的 task 对象的 result 方法中 3、创建两个协程对象，在协程内部分别阻塞 3 秒和 1 秒 4、创建两个任务对象 5、将任务对象作为参数，asyncio.gather 方法创建任务收集器。注意，asyncio.gather 方法中参数的顺序决定了协程的启动顺序 6、将任务收集器作为参数传入事件循环的 run_until_complete 方法，阻塞运行，直到全部任务完成 7、任务结束后，事件循环停止，打印任务的 result 方法返回值，即协程函数的 return 值 到这一步，大家应该可以看得出，上面的代码已经是异步编程的结构了，在事件循环内部，两个协程是交替运行完成的。简单叙述一下程序协程部分的运行过程： -\u003e 首先运行 task1 -\u003e 打印 [corowork] Start coroutine ONE -\u003e 遇到 asyncio.sleep 阻塞 -\u003e 释放 CPU 转到 task2 中执行 -\u003e 打印 [corowork] Start coroutine TWO -\u003e 再次遇到 asyncio.sleep 阻塞 -\u003e 这次没有其它协程可以运行了，只能等阻塞结束 -\u003e task2 的阻塞时间较短，阻塞 1 秒后先结束，打印 [corowork] Stop coroutine TWO -\u003e 又过了 2 秒，阻塞 3 秒的 task1 也结束了阻塞，打印 [corowork] Stop coroutine ONE -\u003e 至此两个任务全部完成，事件循环停止 -\u003e 打印两个任务的 result -\u003e 打印程序运行时间 -\u003e 程序全部结束 ","date":"2021-06-13","objectID":"/asyncio/:7:0","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"await的理解 从上文中也可以看到await其实是从yield from中转变过来的，当在代码中看到await时，可以知道当前协程要去运行await后面的任务，此时控制权回到了event loop手中，去执行其它的任务，当前面的任务完成了以后，则转去执行前面await后面的代码。注意当await直接跟一个coroutline时，此时相当于去yield from，会卡在那里，并不会实现真正的异步，所以要先将coroutline变为task或者future就可以直接await。 ## 异步编程 ","date":"2021-06-13","objectID":"/asyncio/:7:1","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"一个买土豆的例子 import asyncio import random # potato类 class Potato: # 生成土豆 @classmethod def make(cls, num, *args, **kws): potatos = [] for i in range(num): potatos.append(cls.__new__(cls, *args, **kws)) return potatos all_potatos = Potato.make(5) ## 这是一个异步生成器，可以用async for迭代，nums为想买的数量。 async def take_photos(nums): count = 0 while True: # 如果没有土豆了，挂起当前任务请求生成土豆任务。 if len(all_potatos) == 0: await askfor_photos() else: photo = all_potatos.pop() # 如果有土豆将土豆抛出去 yield photo count += 1 if count == nums : break async def askfor_photos(): await asyncio.sleep(2) all_potatos.append(Potato.make(5)) async def buy_photos(): bucket = [] async for p in take_photos(50): bucket.append(p) print(f\"Go photo {id(p)}\") loop = asyncio.get_event_loop() loop.run_until_complete(buy_photos()) ","date":"2021-06-13","objectID":"/asyncio/:7:2","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["python"],"content":"requests例子 import asyncio import requests import time # 相当于委托生成器 async def result(url): res = await request_url(url) print(url, res) # 相当于子生成器 async def request_url(url): res = requests.get(url) print(url) await asyncio.sleep(2) print(\"execute_time:\", time.time() - start) return res url_list = [\"https://www.csdn.net/\", \"https://vllbc.top/\", \"https://www.baidu.com/\", ] # 以下相当于调用方 start = time.time() print(f\"start_time:{start}\\n\") task = [result(url) for url in url_list] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(task)) endtime = time.time() - start print(\"\\nendtime:\", time.time()) print(\"all_execute_time:\", endtime) ","date":"2021-06-13","objectID":"/asyncio/:7:3","tags":["python","asyncio"],"title":"asyncio","uri":"/asyncio/"},{"categories":["算法题"],"content":"移除元素 还是以前刷过的题 https://leetcode-cn.com/problems/remove-element/ 以前的思路早忘了 然后我重新做了一下，一开始就一行代码 class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: return len(list(filter(lambda x:x!=val,nums))) ​ 然后发现输出和正确输出不一样。于是看了了下面的提示，然后改了改 class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: for i in range(nums.count(val)): nums.remove(val) return len(nums) 但这算不上叫算法，利用双指针做法如下： class Solution: def removeElement(self, nums: List[int], val: int) -\u003e int: left = 0 for i in range(len(nums)): if nums[i] != val: nums[left], nums[i] = nums[i], nums[left] left += 1 return left ","date":"2021-06-12","objectID":"/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/:0:0","tags":["算法题","移除元素"],"title":"移除元素","uri":"/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/"},{"categories":["NLP"],"content":"参考：https://blog.csdn.net/asialee_bird/article/details/96894533 TextRank算法是一种基于图的用于关键词抽取和文档摘要的排序算法，由谷歌的网页重要性排序算法PageRank算法改进而来，它利用一篇文档内部的词语间的共现信息(语义)便可以抽取关键词，它能够从一个给定的文本中抽取出该文本的关键词、关键词组，并使用抽取式的自动文摘方法抽取出该文本的关键句。 ","date":"2021-05-20","objectID":"/textrank/:0:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"PageRank算法 PageRank算法通过计算网页链接的数量和质量来粗略估计网页的重要性，算法创立之初即应用在谷歌的搜索引擎中，对网页进行排名。 PageRank算法的核心思想如下： （1）链接数量：如果一个网页被越多的其他网页链接，说明这个网页越重要，即该网页的PR值（PageRank值）会相对较高； （2）链接质量：如果一个网页被一个越高权值的网页链接，也能表明这个网页越重要，即一个PR值很高的网页链接到一个其他网页，那么被链接到的网页的PR值会相应地因此而提高。 ","date":"2021-05-20","objectID":"/textrank/:1:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"TextRank算法 TextRank算法的基本思想是将文档看作一个词的网络，该网络中的链接表示词与词之间的语义关系。 TextRank算法主要包括：关键词抽取、关键短语抽取、关键句抽取。 ","date":"2021-05-20","objectID":"/textrank/:2:0","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（1）关键词抽取（keyword extraction） 关键词抽取是指从文本中确定一些能够描述文档含义的术语的过程。对关键词抽取而言，用于构建顶点集的文本单元可以是句子中的一个或多个字；根据这些字之间的关系（比如：在一个框中同时出现）构建边。根据任务的需要，可以使用语法过滤器（syntactic filters）对顶点集进行优化。语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集。 ","date":"2021-05-20","objectID":"/textrank/:2:1","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（2）关键短语抽取（keyphrase extration） 关键词抽取结束后，我们可以得到的N个关键词，在原始文本中相邻的关键词构成关键短语。因此，从get_keyphrases函数的源码中我们可以看到，它先调用get_keywords抽取关键词，然后分析关键词是否存在相邻的情况，最后确定哪些是关键短语。 ","date":"2021-05-20","objectID":"/textrank/:2:2","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["NLP"],"content":"（3）关键句抽取（sentence extraction） 句子抽取任务主要针对的是自动摘要这个场景，将每一个sentence作为一个顶点，根据两个句子之间的内容重复程度来计算他们之间的“相似度”，以这个相似度作为联系，由于不同句子之间相似度大小不一致，在这个场景下构建的是以相似度大小作为edge权重的有权图。 ","date":"2021-05-20","objectID":"/textrank/:2:3","tags":["NLP","TextRank"],"title":"TextRank","uri":"/textrank/"},{"categories":["python"],"content":"matplotlib.pyplot学习 ","date":"2021-05-02","objectID":"/pyplot/:0:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"绘图标记 import matplotlib.pyplot as plt import numpy as np ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4]) plt.plot(ypoints, marker = 'o') # \"o\"代表实心圆 plt.show() maker可用的符号如下： ","date":"2021-05-02","objectID":"/pyplot/:1:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"fmt参数 fmt = '[marker][line][color]' 例如 o:r，o 表示实心圆标记，: 表示虚线，r 表示颜色为红色。 ","date":"2021-05-02","objectID":"/pyplot/:2:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线类型 线的类型可以使用 linestyle 参数来定义，简写为 ls。 ","date":"2021-05-02","objectID":"/pyplot/:3:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线的宽度 线的宽度可以使用 linewidth 参数来定义，简写为 lw，值可以是浮点数，如：1、2.0、5.67 等。 ","date":"2021-05-02","objectID":"/pyplot/:3:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"颜色类型 ","date":"2021-05-02","objectID":"/pyplot/:4:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"线的颜色 线的颜色可以使用 color 参数来定义，简写为 c。 ","date":"2021-05-02","objectID":"/pyplot/:4:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"标记大小与颜色 我们可以自定义标记的大小与颜色，使用的参数分别是： markersize，简写为 ms：定义标记的大小。 markerfacecolor，简写为 mfc：定义标记内部的颜色。 markeredgecolor，简写为 mec：定义标记边框的颜色。 import matplotlib.pyplot as plt import numpy as np ypoints = np.array([6, 2, 13, 10]) plt.plot(ypoints, marker = 'o', ms = 20) plt.show() ","date":"2021-05-02","objectID":"/pyplot/:4:2","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 轴标签和标题 我们可以使用 xlabel() 和 ylabel() 方法来设置 x 轴和 y 轴的标签。 import numpy as np import matplotlib.pyplot as plt x = np.array([1, 2, 3, 4]) y = np.array([1, 4, 9, 16]) plt.plot(x, y) plt.xlabel(\"x - label\") plt.ylabel(\"y - label\") plt.show() ","date":"2021-05-02","objectID":"/pyplot/:5:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"标题 我们可以使用 title() 方法来设置标题 也可以自定义字体样式，通过传入fontdict参数 import matplotlib.pyplot as plt import numpy as np font = {\"color\":\"blue\",\"size\":20} ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4]) plt.plot(ypoints,marker=\"o\",color=\"r\",linestyle=\"-.\") plt.title(\"test\",fontdict=font) plt.xlabel(\"x\",fontdict=font) plt.ylabel(\"y\",fontdict=font) plt.show() ","date":"2021-05-02","objectID":"/pyplot/:5:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 网格线 我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。 grid() 方法语法格式如下： matplotlib.pyplot.grid(b=None, which='major', axis='both', ) 参数说明： b：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。 which：可选，可选值有 ‘major’、‘minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。 axis：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），‘x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。 **kwargs：可选，设置网格样式，可以是 color=‘r’, linestyle=‘-’ 和 linewidth=2，分别表示网格线的颜色，样式和宽度。 以下实例添加一个简单的网格线，并设置网格线的样式，格式如下： grid(color = 'color', linestyle = 'linestyle', linewidth = number) 参数说明： color：’b’ 蓝色，‘m’ 洋红色，‘g’ 绿色，‘y’ 黄色，‘r’ 红色，‘k’ 黑色，‘w’ 白色，‘c’ 青绿色，‘#008000’ RGB 颜色符串。 linestyle：’‐’ 实线，‘‐‐’ 破折线，‘‐.’ 点划线，‘:’ 虚线。 linewidth：设置线的宽度，可以设置一个数字。 ","date":"2021-05-02","objectID":"/pyplot/:6:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 绘制多图 我们可以使用 pyplot 中的 subplot() 和 subplots() 方法来绘制多个子图。 subpot() 方法在绘图时需要指定位置，subplots() 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。 ","date":"2021-05-02","objectID":"/pyplot/:7:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"subplot subplot(nrows, ncols, index, **kwargs) subplot(pos, **kwargs) subplot(**kwargs) subplot(ax) 以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 1…N ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 index 来设置。 设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为： (1, 1), (1, 2) 设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为： (1, 1), (1, 2) (2, 1), (2, 2) ","date":"2021-05-02","objectID":"/pyplot/:7:1","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"subplots() subplots() 方法语法格式如下： matplotlib.pyplot.subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) 参数说明： nrows：默认为 1，设置图表的行数。 ncols：默认为 1，设置图表的列数。 sharex、sharey：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、‘all’、‘row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，‘row’ 设置每个子图行共享一个 x 轴或 y 轴，‘col’：设置每个子图列共享一个 x 轴或 y 轴。 squeeze：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N1 或 1N 个子图，返回一个 1 维数组，对于 N*M，N\u003e1 和 M\u003e1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。 subplot_kw：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。 gridspec_kw：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。 **fig_kw：把详细的关键字参数传给 figure() 函数。 常用技巧 x = np.linspace(0, 2*np.pi, 400) y = np.sin(x**2) fig, axs = plt.subplots(2, 2) for i in axs.flatten(): # axs.flatten()将二维数组变为一维，方便循环。 i.plot(x, y) ","date":"2021-05-02","objectID":"/pyplot/:7:2","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 散点图 我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。 scatter() 方法语法格式如下： matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs) 参数说明： x，y：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。 s：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。 c：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。 marker：点的样式，默认小圆圈 ‘o’。 cmap：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。 norm：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。 vmin，vmax：：亮度设置，在 norm 参数存在时会忽略。 alpha：：透明度设置，0-1 之间，默认 None，即不透明。 linewidths：：标记点的长度。 edgecolors：：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。 plotnonfinite：：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。 **kwargs：：其他参数。 设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组。 如果要显示颜色条，需要使用 plt.colorbar() 方法： ","date":"2021-05-02","objectID":"/pyplot/:8:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 柱形图 我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。 bar() 方法语法格式如下： matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs) 参数说明： x：浮点型数组，柱形图的 x 轴数据。 height：浮点型数组，柱形图的高度。 width：浮点型数组，柱形图的宽度。 bottom：浮点型数组，底座的 y 坐标，默认 0。 align：柱形图与 x 坐标的对齐方式，‘center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align=‘edge’。 **kwargs：：其他参数。 垂直方向的柱形图可以使用 barh() 方法来设置： ","date":"2021-05-02","objectID":"/pyplot/:9:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["python"],"content":"Matplotlib 饼图 我们可以使用 pyplot 中的 pie() 方法来绘制饼图。 pie() 方法语法格式如下： matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0, frame=False, rotatelabels=False, *, normalize=None, data=None)[source] 参数说明： x：浮点型数组，表示每个扇形的面积。 explode：数组，表示各个扇形之间的间隔，默认值为0。 labels：列表，各个扇形的标签，默认值为 None。 colors：数组，表示各个扇形的颜色，默认值为 None。 autopct：设置饼图内各个扇形百分比显示格式，%d%% 整数百分比，%0.1f 一位小数， %0.1f%% 一位小数百分比， %0.2f%% 两位小数百分比。 labeldistance：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 \u003c1则绘制在饼图内侧。 pctdistance：：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。 shadow：：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。 radius：：设置饼图的半径，默认为 1。 startangle：：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 =90 则从 y 轴正方向画起。 counterclock：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。 wedgeprops ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops={‘linewidth’:5} 设置 wedge 线宽为5。 textprops ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。 center ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。 frame ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。 rotatelabels ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。 ","date":"2021-05-02","objectID":"/pyplot/:10:0","tags":["python","pyplot"],"title":"pyplot","uri":"/pyplot/"},{"categories":["Machine Learning"],"content":"范数的定义 \\[ {\\lVert x \\rVert}_p := \\left(\\sum_{i=1}^n{\\lvert x_i\\rvert}^p\\right)^{\\frac{1}{p}} \\] ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:1:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"标准化与归一化 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"定义 归一化和标准化都是对数据做变换的方式，将原始的一列数据转换到某个范围，或者某种形态。 \u003e归一化：将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]； 标准化：将数据变换为均值为0，标准差为1的分布。切记，并非一定是正态的； 中心化：还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:1","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"差异 归一化会严格限定变换后的数据的范围，标准化没有严格的区间，变换后的数据没有范围，只是其均值为0，标注差为1. 归一化对数据的缩放比例只与极值有关，而标准化缩放比例与所有的数据都有关。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:2","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"标准化、归一化的好处 统计建模中，如回归模型，自变量\\(X\\)的量纲不一致导致了回归系数无法直接解读或者错误解读；需要将\\(X\\)都处理到统一量纲下，这样才可比； 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果； 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:2:3","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"决策边界 所谓决策边界就是能够把样本正确分类的一条边界，主要有线性决策边界(linear decision boundaries)和非线性决策边界(non-linear decision boundaries)。注意：决策边界是假设函数的属性，由参数决定，而不是由数据集的特征决定。下面主要举一些例子，形象化的来说明线性决策边界和非线性决策边界。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:3:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"方差与偏差 当我们的模型表现不佳时，通常是出现两种问题，一种是 高偏差 问题，另一种是 高方差 问题。识别它们有助于选择正确的优化方式，所以我们先来看下 偏差 与 方差 的意义。 偏差: 描述模型输出结果的期望与样本真实结果的差距。 方差: 描述模型对于给定值的输出稳定性。 就像打靶一样，偏差描述了我们的射击总体是否偏离了我们的目标，而方差描述了射击准不准。接下来让我们通过各种情况下 训练集 和 交叉验证集 的 误差 曲线来直观地理解 高偏差 与 高方差 的意义。 对于 多项式回归，当次数选取较低时，我们的 训练集误差 和 交叉验证集误差 都会很大；当次数选择刚好时，训练集误差 和 交叉验证集误差 都很小；当次数过大时会产生过拟合，虽然 训练集误差 很小，但 交叉验证集误差 会很大（ 关系图如下 ） 对于 正则化 参数，使用同样的分析方法，当参数比较小时容易产生过拟合现象，也就是高方差问题。而参数比较大时容易产生欠拟合现象，也就是高偏差问题。 偏差和方差与数据噪声之和就是模型的泛化能力 模型的期望预测（这里x指所有的样本，期望预测为该模型的所有预测结果的期望。也可以表示有多个模型同时对x一个样本进行预测，期望预测为所有模型预测的期望）： 样本数相同的不同训练集产生的方差（可以理解为测试集预测结果与训练集输出期望之间的方差，也可以直接理解为一个模型中所有的预测与预测期望之间的平方差）： 噪声（这里的噪声为人工标注的错误。）： 期望输出与真实标记的差别称为偏差(也有两种理解，一种是多模型的预测期望与真实值之间的偏差，还有一种就直接是单模型的预测输出（因为单模型的预测期望就是它的输出了）与真实值之间的平方差就可以记为偏差的平方，其实这里应理解为多模型的情况 泛化误差也就是期望风险。 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"参考 参考1 参考2 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:1","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"改进策略 [高方差] 采集更多的样本数据，(参考正规方程) [高方差] 减少特征数量，去除非主要的特征 [高偏差] 引入更多的相关特征 [高偏差] 采用多项式特征 [高偏差] 减小正则化参数 λ [高方差] 增加正则化参数 λ ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:4:2","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Machine Learning"],"content":"学习曲线 无论你是要检查你的学习算法是否正常工作或是要改进算法的表现，学习曲线 都是一个十分直观有效的工具。学习曲线 的横轴是样本数，纵轴为 训练集 和 交叉验证集 的 误差 ","date":"2021-05-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/:5:0","tags":["Machine Learning","机器学习的一些概念"],"title":"机器学习的一些概念","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"categories":["Deep Learning","损失函数"],"content":"Softmax理解 主要记录了在使用softmax这个函数中遇到的一些问题，比较基础，但确实困扰了一段时间。 在学习word2vec中, 使用的一般都是如下的损失函数： \\[ \\begin{aligned} Loss = J(\\theta) = -\\frac{1}{T}logL(\\theta)= -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0} logP(w_{t+j}|w_t, \\theta) \\\\\\\\ =\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m, j\\neq0}J_{t,j}(\\theta). \\end{aligned} \\] \\[ \\begin{aligned} J_{t,j}(\\theta) = -logP(miss|xwh) = -log\\frac{exp(u_{miss}^Tv_{xwh})}{\\sum_{o\\in V}exp(u_o^Tv_{xwh})} = \\\\\\\\ -u_{miss}^Tv_{xwh}+log\\sum_{o\\in V} exp(u_o^Tv_{xwh}) \\end{aligned} \\] 但是说起交叉熵往往是下面的式子： \\[ L = -\\sum_{c=1}^Cy_clog(p_c) \\] 在学习的时候就疑惑，这两种形式有什么区别与联系呢，最近看到一篇文章正好解答了这个疑惑。 下面给出结论： 第一种形式是只针对正确类别的对应点输出，将这个位置的softmax即概率最大化，而第二种形式是直接衡量真实分布和实际输出之间的距离，因为交叉熵就是由KL散度变形得来的。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:0:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"交叉熵 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"信息量 一条信息的信息量大小和它的不确定性有很大的关系。一句话如果需要很多外部信息才能确定，我们就称这句话的信息量比较大。 将信息的定义为下： \\(I(x_0) = -log(p(x_0))\\) ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:1","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"熵 信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。公式如下： \\[ H(X) = -\\sum_{i=1}^np(x_i)log(p(x_i)) \\] ### 相对熵 相对熵也称之为KL散度，用于衡量对于同意随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)通常描述样本的真实分布，例如[1, 0, 0, 0]表示样本属于第一类，而q(x)常常用于表示预测的分布，例如[0.7, 0.1, 0.1, 0.1] KL散度的定义公式如下： \\[ D_{KL}(p||q) = \\sum_{i=1}^np(x_i)log(\\frac{p(x_i)}{q(x_i)}) \\] KL越小则说明二者越接近 ### 交叉熵 将KL散度变形 \\[ D_{KL}(p||q) = \\sum_{i=1}^np(x_i)log(p(x_i)) -\\sum_{i=1}^np(x_i)log(q(x_i)) = -H(p(x)) - \\sum_{i=1}^np(x_i)log(q(x_i)) \\] 后半部分就是我们的交叉熵，常常用于评估predict和label之间的差别。 ## 理解 以一个三分类的问题来说，假设真实分布为[0, 1, 0]，则对于第二个式子来说 \\[ L = -\\sum_{c=1}^Cy_clog(p_c) = -0 \\times log(p_0) - 1\\times log(p_1) - 0\\times log(p_3) = -log(p_1) \\] 对于第一个式子就是 \\[ loss_1 = -log(\\frac{e^{z_1}}{\\sum_{c=1}^C e^{z_c}}) = -log(p_1) = -z_1+log\\sum_{c=1}^Ce^{z_c} \\] 所以说实际上这俩是一样的，只是出发点不一样。在skip-gram中，使用的就是第一种式子，对它来说，正确的类别就是背景词，就直接将背景词的概率最大，即损失函数最小。 loss = (-output_layer[:, Y] + torch.log(torch.sum(torch.exp(output_layer), dim=1))).mean() 这行代码就是对第一个式子的实现，需要注意的是所有的操作都是基于batch的。负采样的形式有一些不同，这里不再讨论，到这里我才终于稍稍理解了softmax这个函数。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:1:2","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"与二分类的联系 具体的说二分类就是两个类别，可以使用上述的方法定义，对于正类和负类都用不同的概率表示，也可以只计算正类，负类就等于1-正类，上述出现的y都是向量的形式，对于二分类，向量的长度就是2，就可以直接展开，最后的结果就是熟知的损失函数的形式。 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:2:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["Deep Learning","损失函数"],"content":"参考 https://zhuanlan.zhihu.com/p/105722023 ","date":"2021-04-30","objectID":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/:3:0","tags":["Deep Learning","损失函数","交叉熵损失函数"],"title":"交叉熵损失函数","uri":"/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"categories":["算法题"],"content":"验证二叉搜索树 https://leetcode-cn.com/problems/validate-binary-search-tree/ # Definition for a binary tree node. # class TreeNode: # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution: def isValidBST(self, root: TreeNode) -\u003e bool: return self.search(root,-(232),232) def search(self,root,mins,maxs): if root == None: return True if root.val \u003e mins and root.val \u003c maxs: pass else: return False return all([self.search(root.left,mins,root.val),self.search(root.right,root.val,maxs)]) 最后用了个all 也是简洁了代码 ","date":"2021-04-22","objectID":"/%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:0:0","tags":["算法题","验证二叉搜索树"],"title":"验证二叉搜索树","uri":"/%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["Deep Learning","循环神经网络系列"],"content":"回看博客，发现深度学习的笔记空荡荡，才发觉一直没有详细得进行笔记，但也感觉确实没有什么可以记录的东西，都是一些网络和模型，具体的trick倒是记录了一些，不过为了博客的美观，还是写一些东西吧，最主要的还是一些小知识。 (MLP就不记录了) RNN其实很类似于一个自回归模型，用历史观测预测下一个观测，我们可以假设不适用所有的历史预测下一个，而是部分的历史，也就是说序列满足马尔科夫条件，而RNN，即循环神经网络，就符合一阶马尔科夫模型，即当前观测仅仅与前一个的观测有关，模型图如下： ","date":"2021-04-21","objectID":"/rnn/:0:0","tags":["Deep Learning","循环神经网络系列","RNN"],"title":"RNN","uri":"/rnn/"},{"categories":["工具"],"content":"范围+文本对象 ","date":"2021-04-20","objectID":"/vim/:0:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"范围 内部：i ，意指 inner 外部：a ，英文单词 a，一个的意思 ","date":"2021-04-20","objectID":"/vim/:1:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"文本对象 ( 或 ) ：一对 () b ：一对 () { 或 } ：一对 {} B ：一对 {} [ 或 ] ：一对 [] \u003c 或 \u003e ：一对 \u003c\u003e t ：tag （HTML 或 XML）标签 ' 或 ' ：一对 '' \" 或 \" ：一对 \"\" ` 或 ` ：一对 `` w ：一个单词 s ：一个句子；以 . ! ? 结尾即为一个句子 p ：一个段落；以一个换行符间隔即为一个段落(一般用于对函数体操作) 比如i'代表'内部，iw代表光标所在单词，注意到，加上了范围i或者a，则原本的w含义也发生了变化，注意这是两个东西，原本的w为一个移动，而这里的w代表文本对象即单词。 move ","date":"2021-04-20","objectID":"/vim/:2:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"some easy hjkl，左下上右 wbeWBE，单词级别 0和$，行首和行尾 ^和g _，行首和行尾，但不算空白字符 H和L，页面相关，基本就是映射到^和g _ %括号匹配 ","date":"2021-04-20","objectID":"/vim/:3:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"search 光标移到要搜索的单词，*向下查找，#向上查找 /向下模糊搜索，?向上模糊搜索，配合n和N f + 字符：自左往右移动光标到下一个匹配的字符中 F + 字符：自右往左移动光标到下一个匹配的字符中 t + 字符：自左往右移动光标到下一个匹配的字符的前一个字符中 T + 字符：自右往左移动光标到下一个匹配的字符的后一个字符中 ;：重复执行上一个搜索命令 ,： 与上一个命令方向相反地执行上一个搜索命令 动词 ","date":"2021-04-20","objectID":"/vim/:4:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"some easy d: 删除 c: 删除并进入insert模式 y: 复制 ys, ds, cs，vim-surround插件，对应添加括号，删除括号，修改括号 .重复上一个命令 u撤销上条命令 p粘贴剪切板内容 r替换 ","date":"2021-04-20","objectID":"/vim/:5:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"g+？整理 g : 定位到上次编辑位置 g c 注释 g h 弹出变量或者函数的详细信息 gu和gU 小写或大写，g u u和g U U整行小写大写 g b 多选相同的word gg文件首，G文件尾 3G或3gg或:3，跳到第3行 数量词 1234567...表示命令执行的次数。 结合起来吧 ","date":"2021-04-20","objectID":"/vim/:6:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"动词 + move d h，删除前一个字符 d w，删除字符到下一个单词开头 d tV，删除字符到下一个v字符前面 d 2fb，删除字符到第二个b，且包括这个b，等同于2 dfb ","date":"2021-04-20","objectID":"/vim/:7:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["工具"],"content":"动词 + （范围） +文本对象 ys w'，给当前到下一个单词前添加'号 ds '，删除包裹的符号 cs [(，将[]修改为() yssb，整行添加括号，重复两次代表对行操作，b在前面介绍说了是文本对象，代表括号。 ysiw'，出现了两个文本对象，第一个与前面的i结合为iw代表单词，第二个文本对象代表即位要添加的引号，注意和第一条的区别，w所代表的不同。 gUw，大写直到下一个单词前，是动词+move，gUiw，大写光标所在单词，是动词+范围+文本对象。 ","date":"2021-04-20","objectID":"/vim/:8:0","tags":["vim","工具"],"title":"vim","uri":"/vim/"},{"categories":["Deep Learning","损失函数"],"content":"L1 Loss 也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。 优点： 对离群点（Outliers）或者异常值更具有鲁棒性。 缺点： 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:1:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["Deep Learning","损失函数"],"content":"L2 Loss 也称为Mean Squred Error，即均方差（MSE），它衡量的是预测值与真实1值之间距离的平方和，作用范围同为0到正无穷。 优点： 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。 缺点： 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。 提一嘴，真的不理解鲁棒性这个翻译的含义，后来才知道是音译，真的是，毒害了多少人。。理解成稳定性就行。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:2:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["Deep Learning","损失函数"],"content":"Smooth L1 Loss 以上两种方法都有各自的优缺点，大部分的情况下其实二者都不使用，因此需要新的损失函数。即平滑的L1损失（SLL)。 SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|\u003e1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。 通过对这三个损失函数进行求导可以发现，L1损失的导数为常数，如果不及时调整学习率，那么当值过小时，会导致模型很难收敛到一个较高的精度，而是趋向于一个固定值附近波动。反过来，对于L2损失来说，由于在训练初期值较大时，其导数值也会相应较大，导致训练不稳定。最后，可以发现Smooth L1在训练初期输入数值较大时能够较为稳定在某一个数值，而在后期趋向于收敛时也能够加速梯度的回传，很好的解决了前面两者所存在的问题。 ","date":"2021-04-19","objectID":"/smooth-l1-loss/:3:0","tags":["Deep Learning","损失函数","Smooth L1 Loss"],"title":"Smooth L1 Loss","uri":"/smooth-l1-loss/"},{"categories":["算法题"],"content":"无重叠区间 https://leetcode-cn.com/problems/non-overlapping-intervals/ 利用了贪心 移除的数目就是总数目减去条件成立的数目 class Solution: def eraseOverlapIntervals(self, intervals: List[List[int]]) -\u003e int: if len(intervals) == 0: return 0 res = 0 mins = -float(\"inf\") for i in sorted(intervals,key=lambda i:i[1]): if i[0] \u003e= mins: res += 1 mins = i[1] return len(intervals) - res 注意是根据end进行排序的，引用别人的解释@HONGYANG 比如你一天要参加几个活动，这个活动开始的多早其实不重要，重要的是你结束的多早，早晨7点就开始了然后一搞搞一天，那你今天也就只能参加这一个活动；但如果这个活动开始的不早，比如9点才开始，但是随便搞搞10点就结束了，那你接下来就还有大半天的时间可以参加其他活动。 这就是为啥要着眼于end，而不是start。 贪心就是考虑当前最优解 ","date":"2021-04-16","objectID":"/%E6%97%A0%E9%87%8D%E5%8F%A0%E5%8C%BA%E9%97%B4/:0:0","tags":["算法题","无重叠区间"],"title":"无重叠区间","uri":"/%E6%97%A0%E9%87%8D%E5%8F%A0%E5%8C%BA%E9%97%B4/"},{"categories":["算法题"],"content":"对角线遍历 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:0:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"题目： ​ https://leetcode-cn.com/problems/diagonal-traverse/ ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:1:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"思路： ​ 每个对角线的两索引之和是一样的 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:2:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["算法题"],"content":"代码： class Solution: def findDiagonalOrder(self, matrix: List[List[int]]) -\u003e List[int]: if not matrix: return [] hashs = collections.defaultdict(list) row, col = len(matrix), len(matrix[0]) for i in range(row): for j in range(col): hashs[j + i].append(matrix[i][j]) res = [] flag = True for k, v in sorted(hashs.items()): if flag: res.extend(v[::-1]) else: res.extend(v) flag = not flag return res 注意flag的作用 ","date":"2021-04-13","objectID":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/:3:0","tags":["算法题","对角线遍历"],"title":"对角线遍历","uri":"/%E5%AF%B9%E8%A7%92%E7%BA%BF%E9%81%8D%E5%8E%86/"},{"categories":["Machine Learning","降维算法"],"content":"主成分分析(PCA) 主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。注意的是PCA属于无监督学习。 PCA降维的原则是投影方差最大。 使用PCA时如果有不同种类的数据，PCA会把这些数据混合在一起降维。 PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据(x(1),x(2),…,x(m))。我们希望将这m个数据的维度从n维降到n’维，希望这m个n’维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n’维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这n’维的数据尽可能表示原来的数据呢？ ","date":"2021-04-12","objectID":"/pca/:0:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"协方差矩阵 在统计学中，方差是用来度量单个随机变量的离散程度，而协方差则一般用来刻画两个随机变量的相似程度，其中，方差的计算公式为: \\[ \\sigma_x^2 = \\frac{1}{n}\\sum_{i=1}^n(x-\\bar{x})^2 \\] 协方差的公式为: \\[ \\sigma(x,y) = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) \\] 根据方差的定义，给定d个随机变量\\(x_k,k=1,2,\\dots,d\\)，这些随机变量的方差为 \\[ \\sigma(x_k,x_k) = \\frac{1}{n-1}\\sum_{i=1}^n(x_{ki}-\\bar{x_k})^2,k=1,2,\\dots,d \\] 因此可以求出两两之间的协方差 \\[ \\sigma(x_m,x_k) = \\frac{1}{n-1}\\sum_{i=1}^n(x_{mi}-\\bar{x_m})(x_{ki}-\\bar{x_k}) \\] 因此，协方差矩阵为 \\[ \\sum = \\begin{bmatrix} \\sigma(x_1,x_1) \u0026 \\cdots \u0026 \\sigma(x_1,x_d) \\\\\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\\\\ \\sigma(x_d,x_1) \u0026 \\cdots \u0026 \\sigma(x_d,x_d) \\end{bmatrix} \\] ","date":"2021-04-12","objectID":"/pca/:1:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"拉格朗日乘数法优化 设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系： \\[ D = \\frac{1}{m}YY^T \\\\\\\\ =\\frac{1}{m}(PX)(PX)^T \\\\\\\\ =\\frac{1}{m}PXX^TP^T \\\\\\\\ =P(\\frac{1}{m}XX^T)P^T \\\\\\\\ =PCP^T \\] 我们令P=\\(w^T\\),令原本的协方差为A，于是我们有优化目标如下： \\[ \\begin{cases} \\max{w^TAw} \\\\\\\\ s.t. w^Tw = 1 \\end{cases} \\] 然后构造拉格朗日函数： \\[ L(w) = w^TAw + \\lambda(1-w^Tw) \\] 对w求导： \\[ Aw = \\lambda w \\] 则方差\\(D(x) = w^TAw = \\lambda w^Tw = \\lambda\\) 于是我们发现，x 投影后的方差就是协方差矩阵的特征值。我们要找到最大方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量，次佳就是第二大特征值对应的特征向量，以此类推。 ","date":"2021-04-12","objectID":"/pca/:2:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"对角矩阵 由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质： 实对称矩阵不同特征值对应的特征向量必然正交。 设特征向量\\(\\lambda\\) 重数为 r，则必然存在 r 个线性无关的特征向量对应于 \\(\\lambda\\) ，因此可以将这 r 个特征向量单位正交化。 实对称矩阵一定可以对角化 由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 \\(e_1,e_2,\\cdots,e_n\\)，我们将其按列组成矩阵： \\(E=(e_1,e_2,\\cdots,e_n)\\)。 对于协方差矩阵C有以下结论: \\[ E^TCE = \\begin{bmatrix} \\lambda_1 \\\\\\\\ \u0026\\lambda_2 \\\\\\\\ \u0026\u0026 \\ddots \\\\\\\\ \u0026\u0026\u0026 \\lambda_n \\end{bmatrix} \\] 注：因为E为正交矩阵，则\\(E^{-1}\\) = \\(E^T\\) ,这个过程成为相似对角化，\\(\\lambda_n\\)为C的特征值，对应的特征向量为\\(e_n\\) 这都是线代的基础知识。 ","date":"2021-04-12","objectID":"/pca/:3:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"SVD 复制一下将协方差矩阵写成中心化的形式： \\[ \\begin{align}S\u0026=\\frac{1}{N}\\sum\\limits_{i=1}^N(x_i-\\overline{x})(x_i-\\overline{x})^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}(x_1-\\overline{x},x_2-\\overline{x},\\cdots,x_N-\\overline{x})(x_1-\\overline{x},x_2-\\overline{x},\\cdots,x_N-\\overline{x})^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}(X^T-\\frac{1}{N}X^T\\mathbb{I}_{N1}\\mathbb{I}_{N1}^T)(X^T-\\frac{1}{N}X^T\\mathbb{I}_{N1}\\mathbb{I}_{N1}^T)^T\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^T(E_N-\\frac{1}{N}\\mathbb{I}_{N1}\\mathbb{I}_{1N})(E_N-\\frac{1}{N}\\mathbb{I}_{N1}\\mathbb{I}_{1N})^TX\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^TH_NH_N^TX\\nonumber\\\\\\\\ \u0026=\\frac{1}{N}X^TH_NH_NX=\\frac{1}{N}X^THX \\end{align} \\] 对中心化后的数据集进行奇异值分解： \\[ HX=U\\Sigma V^T,U^TU=E_N,V^TV=E_p,\\Sigma:N\\times p \\] 于是： \\[ S=\\frac{1}{N}X^THX=\\frac{1}{N}X^TH^THX=\\frac{1}{N}V\\Sigma^T\\Sigma V^T \\] 因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值\\(\\Sigma^2\\)和特征向量 \\(V\\)，在新坐标系中的坐标就是： \\[ HX\\cdot V \\] ## 步骤 总结一下 PCA 的算法步骤： 设有 m 条 n 维数据。 对所有的样本进行中心化： \\(x^{(i)} = x^{(i)}-\\frac{1}{m}\\sum_{j=1}^mx^{(j)}\\) 计算样本的协方差矩阵 \\(XX^T\\) 对矩阵\\(XX^T\\)进行特征值分解 4）取出最大的n’个特征值对应的特征向量\\((w_1,w_2,...,w_{n′})\\), 将所有的特征向量标准化后，组成特征向量矩阵W。 5）对样本集中的每一个样本\\(x^{(i)}\\),转化为新的样本\\(z^{(i)}=W^Tx^{(i)}\\) 6) 得到输出样本集\\(D^′=(z^{(1)},z^{(2)},...,z^{(m)})\\) ","date":"2021-04-12","objectID":"/pca/:4:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"实例 假设我们的数据集有10个二维数据(2.5,2.4), (0.5,0.7), (2.2,2.9), (1.9,2.2), (3.1,3.0), (2.3, 2.7), (2, 1.6), (1, 1.1), (1.5, 1.6), (1.1, 0.9)，需要用PCA降到1维特征。 首先我们对样本中心化，这里样本的均值为(1.81, 1.91),所有的样本减去这个均值向量后，即中心化后的数据集为(0.69, 0.49), (-1.31, -1.21), (0.39, 0.99), (0.09, 0.29), (1.29, 1.09), (0.49, 0.79), (0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)。 现在我们开始求样本的协方差矩阵。 然后求出特征值\\((0.0490833989,1.28402771)\\)，对应的特征向量为 \\((0.735178656,0.677873399)^T\\),\\((−0.677873399,−0.735178656)^T\\)，,由于最大的k=1个特征值为1.28402771,对于的k=1个特征向量为\\((−0.677873399,−0.735178656)^T\\). 则我们的W=\\((−0.677873399,−0.735178656)^T\\)。 们对所有的数据集进行投影\\(z^{(i)}=W^Tx^{(i)}\\)，得到PCA降维后的10个一维数据集为：(-0.827970186， 1.77758033， -0.992197494， -0.274210416， -1.67580142， -0.912949103， 0.0991094375， 1.14457216, 0.438046137， 1.22382056) ","date":"2021-04-12","objectID":"/pca/:5:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"性质 缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段； 降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果； 过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合； 特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立； ","date":"2021-04-12","objectID":"/pca/:6:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["Machine Learning","降维算法"],"content":"代码 # 手动实现PCA算法 import numpy as np def pca(data): \"\"\" 主成分分析 :param data: 数据集 :return: \"\"\" # 数据集的行数 num_data, num_feat = data.shape # 对每一列的数据进行平均值的计算 mean_vec = np.mean(data, axis=0) # 对数据集中每一行的数据进行平均值的计算 data_mean_centered = data - mean_vec # 计算协方差矩阵 sigma = np.dot(data_mean_centered.T, data_mean_centered) / num_data # 计算特征值和特征向量 eig_val, eig_vec = np.linalg.eig(sigma) # 对特征值进行排序 eig_pairs = [(np.abs(eig_val[i]), eig_vec[:, i]) for i in range(len(eig_val))] eig_pairs.sort(key=lambda x: x[0], reverse=True) # 要降维的维数，这里以2为例。将特征向量以列向量形式拼合。 matrix_w = np.hstack([eig_pairs[i][1].reshape(-1, 1) for i in range(2)]) # 将原始数据进行投影。 res = data.dot(matrix_w) print(res) pca(np.array([[1, 2, 5], [3, 4, 6], [5, 6, 9], [3, 2 ,5]])) [[ 4.78637704 -1.46926342] [ 7.62278435 -0.82094287] [11.68194836 -0.79767573] [ 5.77746434 0.2656909 ]] 参考文章 https://zhuanlan.zhihu.com/p/77151308 https://www.cnblogs.com/pinard/p/6239403.html ","date":"2021-04-12","objectID":"/pca/:7:0","tags":["Machine Learning","降维算法","PCA"],"title":"PCA","uri":"/pca/"},{"categories":["NLP"],"content":"Bert BERT 的模型架构非常简单，你已经知道它是如何工作的：它只是 Transformer 的编码器。新的是训练目标和 BERT 用于下游任务的方式。 我们如何使用纯文本训练（双向）编码器？我们只知道从左到右的语言建模目标，但它仅适用于每个标记只能使用以前的标记（并且看不到未来）的解码器。BERT 的作者提出了其他未标记数据的训练目标。在讨论它们之前，让我们先看看 BERT 作为 Transformer 编码器的输入。 训练输入：带有特殊标记的句子对 在训练中，BERT 看到用特殊的标记分隔符 [SEP] 分隔的句子对。为了让模型轻松区分这些句子，除了标记和位置嵌入之外，它还使用了段嵌入。 另一个特殊标记是 [CLS] 。顾名思义，它就是表示整个句子的类别的token。在训练中，它用于我们接下来会看到的 NSP 目标。一旦模型被训练，它就会被用于下游任务。 ","date":"2021-04-08","objectID":"/bert/:0:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"预训练目标：NSP Next Sentence Prediction (NSP) 目标是一个二元分类任务。根据特殊标记[CLS] 的最后一层表示 ，该模型预测这两个句子是否是某些文本中的连续句子。 输入： [CLS] 这个人去了 [MASK] 商店 [SEP] 他买了一加仑 [MASK] 牛奶 [SEP] 标签： isNext 输入： [CLS] 男子去了 [MASK] 商店 [SEP] 企鹅 [MASK] 正在飞行##less 鸟 [SEP] 标签： notNext 该任务教模型理解句子之间的关系。正如我们稍后将看到的，这将使 BERT 能够用于需要某种推理的复杂任务。 ","date":"2021-04-08","objectID":"/bert/:1:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"预训练目标：MLM（掩蔽语言模型） BERT 有两个训练目标，其中最重要的是 Masked Language Modeling (MLM) 目标。对于 MLM 目标，在步骤中会发生以下情况： 选择一些标记 （每个标记以 15% 的概率被选中） 替换这些选定的标记 （使用特殊标记 [MASK] (p=80%)，随机标记 (p=10%)，原始标记（保持不变）(p=10%)） 预测原始标记（计算损失） 其思想来自于完形填空，也借鉴了CBOW的思想。 MLM 仍然是语言建模：目标是根据文本的某些部分预测句子/文本中的一些标记。为了更清楚，让我们将 MLM 与标准的从左到右的语言建模目标进行比较 在每一步，标准的从左到右的 LMs 根据之前的标记预测下一个标记。这意味着最终表示，即来自最终层的用于预测的表示，仅编码先前的上下文，即它们 看不到未来。 不同的是，MLM可以一次看到整个文本，但有些标记被破坏了：这就是 BERT 是双向的原因。请注意，为了让 ELMo 知道左右上下文，作者必须训练两个不同的单向 LM(即双向LSTM)，然后将它们的表示连接起来。在 BERT 中，我们不需要这样做：一个模型就足够了。 注意一些细节，在代码实现的时候，注意特殊的标记如[SEP][CLS] 等不要替换， 还有[PAD] ","date":"2021-04-08","objectID":"/bert/:2:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"数据集构建代码 class BERTDataset(Dataset): def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True): self.vocab = vocab self.seq_len = seq_len self.on_memory = on_memory self.corpus_lines = corpus_lines self.corpus_path = corpus_path self.encoding = encoding with open(corpus_path, \"r\", encoding=encoding) as f: if self.corpus_lines is None and not on_memory: for _ in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines): self.corpus_lines += 1 if on_memory: self.lines = [line[:-1].split(\"\\t\") for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)] # 一行有两个句子，分隔符是\\t self.corpus_lines = len(self.lines) if not on_memory: self.file = open(corpus_path, \"r\", encoding=encoding) self.random_file = open(corpus_path, \"r\", encoding=encoding) for _ in range(random.randint(self.corpus_lines if self.corpus_lines \u003c 1000 else 1000)): self.random_file.__next__() def __len__(self): return self.corpus_lines def __getitem__(self, item): t1, t2, is_next_label = self.random_sent(item) # is_next_label: 1 or 0，1代表t2是相邻句子，0代表不是相邻句子 t1_random, t1_label = self.random_word(t1) # mlm任务 t2_random, t2_label = self.random_word(t2) # [CLS] tag = SOS tag, [SEP] tag = EOS tag t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index] t2 = t2_random + [self.vocab.eos_index] t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index] t2_label = t2_label + [self.vocab.pad_index] segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len] bert_input = (t1 + t2)[:self.seq_len] # 截断 bert_label = (t1_label + t2_label)[:self.seq_len] padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))] #pad bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding) output = {\"bert_input\": bert_input, \"bert_label\": bert_label, \"segment_label\": segment_label, \"is_next\": is_next_label} return {key: torch.tensor(value) for key, value in output.items()} def random_word(self, sentence): # 对sent token进行mask并返回mask后的label tokens = sentence.split() output_label = [] for i, token in enumerate(tokens): prob = random.random() if prob \u003c 0.15: prob /= 0.15 # 80% randomly change token to mask token if prob \u003c 0.8: tokens[i] = self.vocab.mask_index # 10% randomly change token to random token elif prob \u003c 0.9: tokens[i] = random.randrange(len(self.vocab)) # 10% randomly change token to current token else: tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index) output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index)) # 被mask掉的token作为label else: tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index) output_label.append(0) # label为0，这样计算loss时不用考虑，因为可以设置nn.NLLLoss(ignore_index=0) return tokens, output_label def random_sent(self, index): # 根据idx选择某对句子并随机返回相邻或不相邻的句子。 t1, t2 = self.get_corpus_line(index) # output_text, label(isNotNext:0, isNext:1) if random.random() \u003e 0.5: return t1, t2, 1 else: return t1, self.get_random_line(), 0 def get_corpus_line(self, item): # 通过item idx选择某对句子 if self.on_memory: return self.lines[item][0], self.lines[item][1] else: line = self.file.__next__() if line is None: self.file.close() self.file = open(self.corpus_path, \"r\", encoding=self.encoding) line = self.file.__next__() t1, t2 = line[:-1].split(\"\\t\") return t1, t2 def get_random_line(self): # 随机选一行 if self.on_memory: return self.lines[random.randrange(len(self.lines))][1] line = self.file.__next__() if line is None: self.file.close() self.file = open(self.corpus_path, \"r\", encoding=self.encoding) for _ in range(random.randint(self.corpus_lines if self.corpus_lines \u003c 1000 else 1000)): self.random_file.__next__() line = self.random_file.__next__() return line[:-1].split(\"\\t\")[1] ","date":"2021-04-08","objectID":"/bert/:3:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"微调 ","date":"2021-04-08","objectID":"/bert/:4:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"分类 对于分类任务直接取第一个[CLS] token的final hidden state，然后加一层权重后softmax输出。 \\[ P = softmax(CW^T) \\] ","date":"2021-04-08","objectID":"/bert/:4:1","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"其它任务 其它任务需要一些调整 ","date":"2021-04-08","objectID":"/bert/:4:2","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"适配器(Adapter) 到目前为止，我们只考虑了将知识从预训练模型（例如 BERT）转移到下游任务的标准方法：微调。“微调”意味着您采用预训练模型并以相当小的学习率训练您感兴趣的任务（例如，情感分类）。这意味着首先，您更新整个（大型）模型，其次，对于每个任务，您需要微调预训练模型的单独副本。最后，对于几个下游任务，您最终会得到很多大型模型 - 这是非常低效的！ Apdater-Bert的想法是将task-specific layer放在预训练模型中间，也就是加入Adapter结构，然后冻结住预训练模型参数，最后我们fientuning的时候，只更新Apdater、layerNorm以及与具体任务相关的layer的参数。具体结构图如下： 左图是Adapter-BERT中的transformer layer，我们可以看到每一个transformer layer增加了两个Adapter layer，分别加在LayerNorm之前，当然了，在进行LayerNorm之前，我们需要进行讲Apdater layer的输出进行残差连接。 右图是Adapter layer的具体结构示意 \u003e这里为什么要用残差连接？主要是因为当初始化的时候，权重都很小，残差连接可以保证模型输出与预训练模型相同。 ","date":"2021-04-08","objectID":"/bert/:5:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"总结 总之BERT就只有这么多新的特性，或者说创新，但是它一经问世就成为了新的霸主，可见效果之好，BERT还有很多细节上的问题，后面看到或者学习到的时候会继续记录下来。 ## 一些问题 ","date":"2021-04-08","objectID":"/bert/:6:0","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"为什么 Bert 的三个 Embedding 可以进行相加？ 因为三个 embedding 相加等价于三个原始 one-hot 的拼接再经过一个全连接网络。和拼接相比，相加可以节约模型参数。 引用苏建林老师的话： \u003e Embedding的数学本质，就是以one hot为输入的单层全连接。 也就是说，世界上本没什么Embedding，有的只是one hot。 假设 token Embedding 矩阵维度是 [4,768]；position Embedding 矩阵维度是 [3,768]；segment Embedding 矩阵维度是 [2,768]。 对于一个字，假设它的 token one-hot 是[1,0,0,0]；它的 position one-hot 是[1,0,0]；它的 segment one-hot 是[1,0]。 那这个字最后的 word Embedding，就是上面三种 Embedding 的加和。 如此得到的 word Embedding，和concat后的特征：[1,0,0,0,1,0,0,1,0]，再过维度为 [4+3+2,768] = [9, 768] 的全连接层，得到的向量其实就是一样的。 ","date":"2021-04-08","objectID":"/bert/:6:1","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["NLP"],"content":"transformers中bert模型的输出 ","date":"2021-04-08","objectID":"/bert/:6:2","tags":["NLP","BERT"],"title":"BERT","uri":"/bert/"},{"categories":["工具"],"content":"常用命令 ","date":"2021-04-04","objectID":"/git/:1:0","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"diff image.png git diff：当工作区有改动，临时区为空，diff的对比是“工作区”与最后一次commit提交的仓库的共同文件”；当工作区有改动，临时区不为空，diff对比的是“工作区”与“暂存区”的共同文件”。 git diff --cached：显示暂存区与最后一次commit的改动。 git diff \u003c分支1\u003e \u003c分支2\u003e 显示两个分支最后一次commit的改动。 ","date":"2021-04-04","objectID":"/git/:1:1","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"init git init初始化一个仓库 ","date":"2021-04-04","objectID":"/git/:1:2","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"add git add .将除.gitignore中声明的所有文件加入暂存区 也可以git add 特定文件，将文件加入暂存区。 ","date":"2021-04-04","objectID":"/git/:1:3","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"commit image.png git commit -m \"提交说明\" 提交到工作区 git commit --amend 修改上次的提交记录 ","date":"2021-04-04","objectID":"/git/:1:4","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"push git push 提交到远程库。可以指定分支提交 比如：git push origin main 指定为main分支 ","date":"2021-04-04","objectID":"/git/:1:5","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"pull git pull 将改动拉倒本地库，并合并，默认为Merge合并 如果加上rebase参数则合并方式变为rebase合并。 ","date":"2021-04-04","objectID":"/git/:1:6","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"log git log 查看提交历史 git log --oneline简洁输出 ","date":"2021-04-04","objectID":"/git/:1:7","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"branch git branch xxx 建立分支xxx 然后用git checkout xxx 切换到分支xxx 或者用git checkout -b xxx 完成同样的操作 ","date":"2021-04-04","objectID":"/git/:1:8","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"checkout image.png git checkout 回退版本即head分离 但master不变 checkout可以切换分支，也可以head分离，但是分支的位置不变，但后面的reset和revert都会改变分支 ","date":"2021-04-04","objectID":"/git/:1:9","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"reset git reset --hard HEAD^回退一个版本（master也改变） git reset --hard HEAD~n 回退n个版本 git reset --hard xxxxxx xxxxxx为版本号 即为git log显示的每一个版本号 一般为前六位 reset的参数有三种，其作用如下： 最危险但最常用的就是hard。soft也常用来修改某个提交，只修改提交，之后再进行提交即可达到修改的目的。 ","date":"2021-04-04","objectID":"/git/:1:10","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"revert git revert命令用于撤消对仓库提交历史的更改。其他“撤消”命令，例如 git checkout 和 git reset，将HEAD和分支引用指针移动到指定的提交。git revert也需要一个指定的提交，但是，它并不会将 ref 指针移动到这个提交。revert 操作将采用反转指定的提交的更改，并创建一个新的“还原提交”。然后更新 ref 指针以指向新的还原提交，使其成为分支的HEAD。(创建一个新的commit结点) ","date":"2021-04-04","objectID":"/git/:1:11","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"merge git merge 比如在master分支里 执行git merge xxx 将xxx分支合并到master中，一般项目开发，一人一个分支，最后提交的时候合并再提交。不过更推荐用git rebase方法，这样合并后的分支更加直观。一般是进行三方合并。 ","date":"2021-04-04","objectID":"/git/:1:12","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"branch 最常用的就是创建和删除分支 git branch -f master~3将分支强制回退三个版本，但head不动 ","date":"2021-04-04","objectID":"/git/:1:13","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"cherry-pick 当需要另一个分支的所有改动时，用git merge，但当需要部分改动时候，要用git cherry-pick xxx xxx为哈希值或者分支名，指定为分支名时候，将分支的最新改动合并过来 ","date":"2021-04-04","objectID":"/git/:1:14","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"rebase 当不知道提交的哈希值时，可以用git rebase -i HEAD~x 来可视化管理，可以调整提交的顺序，可以删除不想要的提交，或合并提交 这是线性化的自动的 cherry-pick git rebase xx1 xx2将xx2分支上的提交记录放到xx1后面 此外活用git rebase -i ,进行可视化的操作。 ### fetch git fetch获取远程仓库的数据，不会改变你本地仓库的状态，不会更新你的master,也没有更改你的本地磁盘文件，可以理解为单纯的下载操作。而git pull相当于git fetch + git merge即抓取后合并 ","date":"2021-04-04","objectID":"/git/:1:15","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"reflog git reflog 查看操作记录，这个操作可以撤销不小心用git reset回退版本的操作 ","date":"2021-04-04","objectID":"/git/:1:16","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"stash 它会保存当前工作进度，会把暂存区和工作区的改动保存到一个未完结变更的堆栈中；执行完这个命令后，在运行 git status 命令，就会发现当前是一个干净的工作区，没有任何改动。 git stash 是本地的，不会上传到服务器上； 可以通过使用git stash save 'message...'可以添加一些注释。 image.png ","date":"2021-04-04","objectID":"/git/:1:17","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"底层内容 这几天系统学习了一下git的底层内容，通透了很多，记录一些。贴一下原博客：https://www.lzane.com/tech/git-internal/ 首先看一个图 首先要明确有四种object，第一种是记录文件内容，第二种是记录目录结构，第三种是记录提交信息，第四种是记录tag信息，第四种无关紧要。 从下面开始看，最下面记录的文件内容，注意只记录文件内容，不包括文件名等其它内容。是一个blob类型的节点，将文件的内容信息经过SHA1哈希算法得到对应的哈希值作为这个object在Git仓库中的唯一身份证。 然后再往上的三角形记录的是仓库的目录结构，它将当前的目录结构打了一个快照。从它储存的内容来看可以发现它储存了一个目录结构（类似于文件夹），以及每一个文件（或者子文件夹）的权限、类型、对应的身份证（SHA1值）、以及文件名。 再往上就是记录的提交的信息，它储存的是一个提交的信息，包括对应目录结构的快照tree的哈希值，上一个提交的哈希值，提交的作者以及提交的具体时间，最后是该提交的信息。 还有分支的信息和Head。HEAD、分支和普通的Tag可以理解为一个指针，指向对应commit的sha1值。 仓库有三个分区：工作目录、index索引区域、Git仓库。 当文件被修改后，只是工作目录发生了改变，其余两个是没有任何变化的。当运行git add xxx命令后，即将xxx文件加入了索引区域，此时新建了一个blob object，并且将原来指向xxx指向了新建的blob Object，记住索引索引的是add的所有文件，这时运行git commit，会生成一个tree object，然后创建commit object，将分支等信息指向新的commit。注意每次commit都是储存的全新的文件快照而不是变更部分。 ","date":"2021-04-04","objectID":"/git/:2:0","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"合并策略 ","date":"2021-04-04","objectID":"/git/:3:0","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"Fast-forward(没有分叉) image.png ","date":"2021-04-04","objectID":"/git/:3:1","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["工具"],"content":"Recursive(最常用最重要) Recursive是Git分支合并策略中最重要也是最常用的策略，是Git在合并两个有分叉的分支时的默认行为。其算法可以简单描述为：递归寻找路径最短的唯一共同祖先节点，然后以其为base节点进行递归三向合并。 那怎么保证Git能够找到正确的合并base节点，尽可能的减少冲突呢？答案就是，Git在寻找路径最短的共同祖先节点时，如果满足条件的祖先节点不唯一，那么Git会继续递归往下寻找直至唯一。不唯一的多个祖先节点做一个临时的三向合并节点。 Recursive策略已经被大量的场景证明它是一个尽量减少冲突的合并策略，我们可以看到有趣的一点是，对于两个合并分支的中间节点（如上图节点4，5），只参与了base的计算，而最终真正被三向合并拿来做合并的节点，只包括末端以及base节点。 需要注意Git只是使用这些策略尽量的去帮你减少冲突，如果冲突不可避免，那Git就会提示冲突，需要手工解决。（也就是真正意义上的冲突）。 ### Ours \u0026 Theirs ### Octopus ## 参考 图解Git (marklodato.github.io) 这才是真正的Git——Git内部原理 - LZANE | 李泽帆（靓仔） 这才是真正的Git——分支合并 - LZANE | 李泽帆（靓仔） ","date":"2021-04-04","objectID":"/git/:3:2","tags":["git","工具"],"title":"git","uri":"/git/"},{"categories":["算法题"],"content":"编辑距离 ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:0:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"定义 编辑距离(Edit Distance)是针对两个字符串S1和S2的差异程度进行量化，计算方式是看至少需要多少次的处理才能将S1变成S2（和S2变成S1是等价的），用 EditDis(S1,S2)表示。 其中处理的方式有三种： 1.插入一个字符 2.删除一个字符 3.替换一个字符 这是严格意义上的距离，满足”距离三公理”： 1.对称性，EditDis(S1,S2) = EditDis(S2,S1) 2.非负性，EditDis(S1,S2) \u003e= 0, 当且仅当S1=S2时，等号成立 3.三角不等式，EditDis(S1,S2) + EditDis(S1,S3) \u003e= EditDis(S2,S3) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:1:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"动态规划求解 令 S1.substr(i)表示 S1前i个字符构成的子串，S2.substr(j)表示S2前j个字符构成的子串 \\(dp[i][j]\\)表示S1.substr(i)和S2.substr(j)的编辑距离。 注意，这句话的另一层含义是：当我们计算出了EditDis(S1,S2)则默认了S1.substr(i)已经通过三种处理方式变成了S2.substr(j)。 所以，当我们计算\\(dp[i+1][j+1]\\)时，我们可以利用\\(dp[i][j]\\)，\\(dp[i+1][j]\\)，\\(dp[i][j+1]\\)的信息。 ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:2:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"过程 1.确定最后一步： 令，S1的长度为len1，S2的长度为len2 \\(dp[len1][len2]\\)有三种方式可以实现，一种是S1.substr(len1-1)插入一个字符使之等于S2（\\(dp[len1][len2-1] + 1\\)），一种是S2.substr(len2)删除一个字符使之等于S1（\\(dp[len1-1][len2] + 1\\)），另一种是替换最后一个字符使S1和S2相等（\\(dp[len1-1][len2-1] + 1\\)） 确定转移方程: 3.确定边界和初始状态 我们设定Dp二维数组大小是（Len+1） * （Len2+1），第0行代表 S1为空串，第0列代表S2为空串。 显然，S1变成为空串需要的每次操作是\\(dp[i][0]=i\\) S2变成为空串需要的每次操作是\\(dp[0][j] = j\\) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:3:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["算法题"],"content":"代码 class Solution: def minDistance(self, word1: str, word2: str) -\u003e int: import numpy as np len1, len2 = len(word1), len(word2) dp = [[0]*(len2+1) for _ in range(len1+1)] # 定义 for i in range(1,len1+1): dp[i][0] = i # 边界 for j in range(1,len2+1): dp[0][j] = j # 边界 for i in range(1,len1+1): for j in range(1,len2+1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+1) return int(dp[len1][len2]) ","date":"2021-04-03","objectID":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/:4:0","tags":["算法题","编辑距离"],"title":"编辑距离","uri":"/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/"},{"categories":["Machine Learning","聚类算法"],"content":"通过sklearn模块实现 import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.datasets import load_iris %matplotlib inline X,y = make_blobs(n_samples=100,n_features=2,centers=[[-1,-1],[0,0],[1,1],[2,2]],cluster_std=[0.4,0.2,0.2,0.2])#使用make_blobs生成训练数据,生成100个样本,每个样本2个特征,共4个聚类,聚类中心分别为[-1,-1],[0,0],[1,1],[2,2],聚类方差分别为0.4,0.2,0.2,0.2 plt.scatter(X[:,0],X[:,1])#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征 plt.show() kmeans = KMeans(n_clusters=3)#生成kmeans分类器,聚类数量为3,其余参数使用默认值。 y_pred = kmeans.fit_predict(X)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(X[:,0],X[:,1],c=y_pred)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征,将各聚类结果显示为不同的颜色 plt.show() kmeans = KMeans(n_clusters=4)#生成kmeans分类器,聚类数量为4,其余参数使用默认值。 y_pred = kmeans.fit_predict(X)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(X[:,0],X[:,1],c=y_pred)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第二维特征,将各聚类结果显示为不同的颜色 plt.show() iris = load_iris() #导入iris数据集,iris数据集包含了150个样本,分别属于3类,每个样本包含4个特征 data_train=iris.data #iris样本集的样本特征 label_train=iris.target #iris样本集的样本标签 kmeans = KMeans(n_clusters=3)#生成kmeans分类器,聚类数量为3,其余参数使用默认值。 y_predict = kmeans.fit_predict(data_train)#使用fit_predict方法计算聚类中心并且预测每个样本的聚类索引。 plt.scatter(data_train[:,0],data_train[:,2],c=y_predict)#画出训练样本的散点图,散点图的横坐标为样本的第一维特征,纵坐标为样本的第三维特征,将各聚类结果显示为不同的颜色 plt.show() ","date":"2021-03-28","objectID":"/kmeans/:1:0","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["Machine Learning","聚类算法"],"content":"手动实现 import numpy as np import matplotlib.pyplot as plt center = np.array([[0,0],[0,1]]) cls_num = 2 X = np.array([[0,0],[0,1],[2,1],[2,3],[3,4],[1,0]]) max_iter = 10 cls = np.zeros(X.shape[0]) run = True while run and max_iter \u003e 0: for i,x in enumerate(X): tmp = np.argmin(np.sum(np.power(x - center,2),axis=1)) cls[i] = tmp run = False # 重新计算聚类中心 for i in range(cls_num): data = X[cls==i] # 取相同类别的样本 new_center = np.mean(data,axis=0) # 对相同类别的x和y取平均值 if np.sum(np.abs(center[i]-new_center),axis=0) \u003e 1e-4: center[i] = new_center # 更新中心 run = True max_iter -= 1 plt.scatter(X[:,0],X[:,1],c=cls) plt.show() ## kernel kmeans 已知kmeans的核心公式为下： 这里可以看到,实际上就是计算每个样本点簇中心的距离,然后判断出到哪个簇中心距离最短,然后分给那个簇。然后下次迭代时,簇中心就按照新分配的点重新进行计算了,然后所有的点再同样计算样本点到簇中心的距离,重新分配到不同的簇中。所以这样不断迭代下去,就能够收敛了,得到最后的聚类效果。 核k-means,概括地来说,就是将数据点都投影到了一个高维的特征空间中（为啥要这么做呢,主要是突显出不同样本中的差异）,然后再在这个高维的特征空间中,进行传统的k-means聚类。主要的思想就是这么简单,比起传统普通的k-means就多了一个步核函数的操作。所以它的公式也与传统k-means很相近： 但实际中很难计算,因此需要改造, 改造成为 ","date":"2021-03-28","objectID":"/kmeans/:2:0","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["Machine Learning","聚类算法"],"content":"代码 class KernelKmeans: def __init__(self, n_clusters, max_iter, kernel): self.n_clusters = n_clusters self.max_iter = max_iter self.kernel = kernel def fit(self, X): self.centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)] for i in range(self.max_iter): distances = np.array([self.kernel(X, centroid) for centroid in self.centroids]) self.labels = np.argmin(distances, axis=0) self.centroids = np.array([X[self.labels == j].mean(axis=0) for j in range(self.n_clusters)], dtype=np.float32) def predict(self, X): distances = np.array([self.kernel(X, centroid) for centroid in self.centroids]) return np.argmin(distances, axis=0) def gaussian_kernel(X, y): return np.exp(-np.linalg.norm(X - y, axis=1)**2 / 2) ","date":"2021-03-28","objectID":"/kmeans/:2:1","tags":["Machine Learning","聚类算法","kmeans"],"title":"kmeans","uri":"/kmeans/"},{"categories":["算法题"],"content":"翻转二叉树 开始学习二叉树了 先来个简单题 https://leetcode-cn.com/problems/invert-binary-tree/ 很简单 # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def invertTree(self, root: TreeNode) -\u003e TreeNode: if root == None: return None temp = root.left root.left = root.right root.right = temp self.invertTree(root.left) self.invertTree(root.right) return root ","date":"2021-03-24","objectID":"/%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91/:0:0","tags":["算法题","翻转二叉树"],"title":"翻转二叉树","uri":"/%E7%BF%BB%E8%BD%AC%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"对称二叉树 ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/symmetric-tree/ ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:1:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"思路： 利用双向队列，每次把对称的两个对应的节点放入队列中，然后取出来比较，如果值不相等则返回false,如果一边为空 一边不为空也返回false 符合条件的话就继续搜索 ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:2:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"代码： # Definition for a binary tree node. # class TreeNode: # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution: def isSymmetric(self, root: TreeNode) -\u003e bool: from collections import deque d = deque() d.append((root,root)) while d: left,right = d.popleft() if not left and not right: continue elif not left or not right: return False elif left.val != right.val: return False else: d.append((left.left,right.right)) d.append((left.right,right.left)) return True ","date":"2021-03-21","objectID":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:3:0","tags":["算法题","对称二叉树"],"title":"对称二叉树","uri":"/%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法题"],"content":"合并两个有序列表 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:0:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/merge-two-sorted-lists/ ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:1:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"思路： 利用递归的思想，比较两个当前值，因为是有序链表 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:2:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"代码： # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -\u003e ListNode: if l1 == None: return l2 if l2 == None: return l1 if l1.val \u003c= l2.val: l1.next = self.mergeTwoLists(l1.next,l2) return l1 else: l2.next = self.mergeTwoLists(l1,l2.next) return l2 ","date":"2021-03-18","objectID":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/:3:0","tags":["算法题","合并两个有序列表"],"title":"合并两个有序列表","uri":"/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8/"},{"categories":["算法题"],"content":"可获得的最大点数 ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:0:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/maximum-points-you-can-obtain-from-cards/ ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:1:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 滑动窗口题目，限定窗口大小然后滑动即可 ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:2:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def maxScore(self, cardPoints: List[int], k: int) -\u003e int: n = len(cardPoints) # 滑动窗口大小为 n-k windowSize = n - k # 选前 n-k 个作为初始值 s = sum(cardPoints[:windowSize]) minSum = s for i in range(windowSize, n): # 滑动窗口每向右移动一格，增加从右侧进入窗口的元素值，并减少从左侧离开窗口的元素值 s += cardPoints[i] - cardPoints[i - windowSize] minSum = min(minSum, s) return sum(cardPoints) - minSum ","date":"2021-03-03","objectID":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/:3:0","tags":["算法题","可获得的最大点数"],"title":"可获得的最大点数","uri":"/%E5%8F%AF%E8%8E%B7%E5%BE%97%E7%9A%84%E6%9C%80%E5%A4%A7%E7%82%B9%E6%95%B0/"},{"categories":["算法题"],"content":"滑动窗口中位数 ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:0:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/sliding-window-median/ ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:1:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"思路： 很明显的滑动窗口，首先定义一个求中位数的匿名函数，然后一点一点求出来 ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:2:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["算法题"],"content":"代码： class Solution: def medianSlidingWindow(self, nums: List[int], k: int) -\u003e List[float]: median = lambda a: (a[(len(a)-1)//2] + a[len(a)//2]) / 2 res = [] for i in range(len(nums)-k+1): res.append(median(sorted(nums[i:i+k]))) return res ","date":"2021-02-25","objectID":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/:3:0","tags":["算法题","滑动窗口中位数"],"title":"滑动窗口中位数","uri":"/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/"},{"categories":["NLP"],"content":"Seq2Seq中的Attention ","date":"2021-02-23","objectID":"/attention/:0:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"缺陷 在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。 不仅编码器很难将所有信息放入一个向量中——这对解码器来说也很困难。解码器只看到源的一种表示。但是，在每个生成步骤中，源的不同部分可能比其他部分更有用。但在目前的情况下，解码器必须从相同的固定表示中提取相关信息——这不是一件容易的事。 这个时候就需要引入注意力机制了，注意这里的注意力机制和transformer中的self-attention是不一样的。下面详细介绍一下。注意几个名词：注意力得分、注意力权重。其中注意力得分即score的计算有多种方法，权重就是对得分进行softmax归一化。 ","date":"2021-02-23","objectID":"/attention/:1:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"attention 注意机制是神经网络的一部分。在每个解码器步骤中，它决定哪些源部分更重要。在此设置中，编码器不必将整个源压缩为单个向量 - 它为所有源标记提供表示（例如，所有 RNN 状态而不是最后一个）。 步骤： - 接受注意输入：解码器状态\\(h_t\\)以及所有编码器状态\\(s_1,s_2,\\dots,s_m\\) - 计算每个编码器状态的注意力分数\\(s_k\\)，注意力分数表示它对解码器状态\\(h_t\\)的相关性，使用注意力函数，接收一个解码器状态和一个编码器状态并返回一个标量分数，即图中的\\(score(h_t,s_k)\\) - 计算注意力权重：即概率分布- 使用Softmax函数 - 计算注意力输出：具有注意力机制的编码器状态的加权和 即为如图所示内容为如何计算注意力。 注意我们提到的注意力函数，这里的注意力分数的计算有很多种方法，下面介绍几种比较常见的办法： - 点积： 最简单的办法。 - 双线性函数 - 多层感知机 注意后两者都有要优化的参数的，第一个点积是直接运算，因此很简单。 在应用时可以直接将注意力的结果传输到最后的softmax，也可以将原始的\\(h_t\\)合并，下面介绍几种变体。 ","date":"2021-02-23","objectID":"/attention/:2:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"Bahdanau Model - 编码器使用双向的RNN - 利用上一时刻的隐层状态计算注意力输出c，然后和隐层状态一起作为当前时刻的输入，再得到结果\\(\\hat{y}\\)。这里再说一下训练的过程中当前步的输入使用的是真实的\\(y\\)，测试的时候才会使用上一步的输出作为输入。可以将上下文向量c（也就是注意力输出）与\\(x\\)拼接后作为输入。 - 注意力得分使用的是感知机。 这里引用一下李沐大佬的代码： class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention = d2l.AdditiveAttention( num_hiddens, num_hiddens, num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU( embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): # outputs的形状为(batch_size，num_steps，num_hiddens). # hidden_state的形状为(num_layers，batch_size，num_hiddens) outputs, hidden_state = enc_outputs return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs的形状为(batch_size,num_steps,num_hiddens). # hidden_state的形状为(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # 输出X的形状为(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) # 转换是为了方便后面循环计算。 outputs, self._attention_weights = [], [] for x in X: # query的形状为(batch_size,1,num_hiddens) query = torch.unsqueeze(hidden_state[-1], dim=1) # -1是指在最后一层最后时刻的隐藏状态，作为query # context的形状为(batch_size,1,num_hiddens) context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # 在特征维度上连结 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # 将x变形为(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # 全连接层变换后，outputs的形状为 # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights ","date":"2021-02-23","objectID":"/attention/:3:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"Luong Model 这个模型的编码器比较常规，使用当前状态计算注意力输出，然后解码器中将隐层状态与注意力输出做一步结合，这样得到了新的隐层状态，然后再传递得到预测结果。 ","date":"2021-02-23","objectID":"/attention/:4:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"注意力对齐 可以看到解码器关注的源token。 到此seq2seq中的attention就介绍完毕了，其实还有很多细节，以后遇到了会持续补充。 Self-attention 移步transformer。Transformer MQA image.png 标准的mha中，KV heads的数量和Query heads的数量相同，每一个q head对应一个独立的kv head，但这样的开销比较大。 MQA (Multi Queries Attention): MQA比较极端，只保留一个KV Head，多个Query Heads共享相同的KV Head。这相当于不同Head的Attention差异，全部都放在了Query上，需要模型仅从不同的Query Heads上就能够关注到输入hidden states不同方面的信息。这样做的好处是，极大地降低了KV Cache的需求，但是会导致模型效果有所下降。（层内共享） # GQA 如上图所示，GQA就是在MHA和MQA之间做了一个平衡。对query heads进行分组，分成几组就对应多少个kv heads，然后每一组内的query Heads共享相同的KV head。 GQA可以在减少计算量和KV Cache同时确保模型效果不受到大的影响。 # online attention ### 3-pass \\(\\mathsf{NO}\\)TATIONS \\(\\{m_i\\}{:}\\max_{j=1}^i\\left\\{x_j\\right\\}\\), with initial value \\(m_0=-\\infty.\\) \\(\\{d_i\\}{:}\\sum_{j=1}^ie^{x_j-m_N}\\), with initial value \\(d_0=0,d_N\\) is the denominator of safe softmax. \\(\\{a_i\\}{:\\text{ the final softmax value}}.\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[m_i\\leftarrow\\max\\left(m_{i-1},x_i\\right)\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[d_i\\leftarrow d_{i-1}+e^{x_i-m_N}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[a_i\\leftarrow\\frac{e^{x_i-m_N}}{d_N}\\] \\(\\mathbf{end}\\) 这是3step计算attention的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于sram中没有足够的存储空间，因此需要多次访存。 ### online attention \\[\\begin{aligned} d_i^{\\prime}\u0026 =\\sum_{j=1}^ie^{x_j-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_i}\\right)+e^{x_i-m_i} \\\\ \u0026= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_{i-1}}\\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\\\ \u0026= d_{i-1}' e^{m_{i-1}-m_i}+e^{x_i-m_i} \\end{aligned}\\] 找到迭代式之后就可以从3step降到2step \\[\\begin{aligned}\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026m_i\u0026\u0026\\leftarrow\u0026\\max\\left(m_{i-1},x_i\\right)\\\\\u0026\u0026\u0026d_i^{\\prime}\u0026\u0026\\leftarrow\u0026d_{i-1}^{\\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\\\\u0026\\mathbf{end}\\\\\u0026\\mathbf{for~}i\\leftarrow1,N\\textbf{ do}\\\\\u0026\u0026\u0026a_i\\leftarrow\u0026\u0026\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026\\mathbf{end}\\end{aligned}\\] 好像FLOPs计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的scale x值，也就是pre-softmax logits，由于需要O(N^2)的显存无法放在SRAM中。因此： 1. 要么提前计算好x，保存在全局显存中，需要O(N^2)的显存，容易爆显存。 2. 要么在算法中online计算，每次循环中去load一部分Q，K到片上内存，计算得到x。 Attention优化的目标就是避开第一种情况，尽可能节省显存，否则，LLM根本无法处理类似100K以上这种long context的情况。而对于第二种情况，我们不需要保存中间矩阵x，节省了显存，但是计算没有节省，并且增加了HBM IO Accesses（需要不断地load Q, K）。此时，2-pass算法相对于3-pass算法，可以减少一次整体的load Q, K以及减少一次对 xi 的online recompute，因为在2-pass的第一个pass中， xi 是被两次计算共享的。类似online-softmax这种算法，对应到Attention中的应用，就是Memory Efficient Attention（注意不是FlashAttention）。 flash attention safe softmax并没有1-pass算法，那么Attention会不会有呢？有！这就是FlashAttention！ 在使用online attention的情况下，从头开始计算attention score的过程如下： \\(\\operatorname{NOTATIONS}\\) \\(Q[k,:]:\\)the \\(k\\)-th row vector of \\(Q\\) matrix. \\(\\begin{aligned}O[k,:]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\\\\\mathbf{V}[i,i]:\\mathrm{~the~}k\\text{-th row of output }O\\mathrm{~matrix.}\\end{aligned}\\) \\(V[i,:]{:\\text{ the }i\\text{-th row of }V\\text{ matrix}}.\\) \\(\\{\\boldsymbol{o}_i\\}{:}\\sum_{j=1}^ia_jV[j,:]\\), a row vector storing partial aggregation result \\(A[k,:i]\\times V[:i,:]\\) BODY \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}x_i\u0026\\leftarrow\\quad Q[k,:]\\:K^T[:,i]\\\\m_i\u0026\\leftarrow\\quad\\max\\left(m_{i-1},x_i\\right)\\\\d_i'\u0026\\leftarrow\\quad d_{i-1}'e^{m_{i-1}-m_i}+e^{x_i-m_i}\\end{aligned}\\] \\(\\mathbf{end}\\) \\(\\textbf{for }i\\leftarrow 1, N\\textbf{ do}\\) \\[\\begin{aligned}\u0026a_i\\:\\leftarrow\\:\\frac{e^{x_i-m_N}}{d_N^{\\prime}}\\\\\u0026o_i\\:\\leftarrow\\:o_{i-1}+a_i\\:V[i,:\\:]\\end{aligned}\\] \\(\\mathbf{end}\\) \\[O[k,:]\\leftarrow\\boldsymbol{o}_N\\] 优化思路和online attention一样，将\\(o_{i}\\)的计算简化以便于可以写成迭代式。 原来的\\(o_{i}\\)使用以下方式计算，依赖于全局的\\(m_{N}\\)和\\(d_{N}\\)。 \\[\\boldsymbol{o}_i:=\\sum_{j=1}^i\\left(\\frac{e^{x_j-m_N}}{d_N^{\\prime}}V[j,:]\\right)\\] 将其改写成如下形式： \\[\\boldsymbol{o}_i^{\\prime}:=\\left(\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:]\\right)\\] 这样按照上面的方式拓展下去，可以找到一个循环迭代式。 \\[\\begin{aligned} \\mathbf{o}_i^{\\prime}\u0026 =\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d'}V[j,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:] \\right)+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}}\\frac{e^{x_","date":"2021-02-23","objectID":"/attention/:5:0","tags":["NLP","Attention"],"title":"Attention","uri":"/attention/"},{"categories":["NLP"],"content":"PMI 点互信息 对于两个单词之间的PMI来说，可以这样计算： \\[ PMI(w,c) = \\log \\frac{p(w,c)}{p(w)p(c)} = \\log \\frac{N(w,c) |w,c|}{N(w)N(c)} \\] ## MI 在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息(Mutual Information)是度量两个事件集合之间的相关性(mutual dependence)。互信息最常用的单位是bit。 ","date":"2021-02-22","objectID":"/ppmi/:1:0","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["NLP"],"content":"定义 正式地，两个离散随机变量 X 和 Y 的互信息可以定义为： 其中 p(x,y) 是 X 和 Y 的联合概率分布函数，而p(x)和p(y)分别是 X 和 Y 的边缘概率分布函数。 在结果上互信息与信息增益是一样的，下面是详细的推导。 应用到文本特征选择： U、C都是二值随机变量，当文档包含词项t时，U的取值为1，否则0；当文档属于类别c时，C的取值1，否则0。简单的理解就是对于文本来说，每一个token就是它的特征，取值只有有或者没有，也就是0或者1，互信息常用于文本特征的选择，也就 是选择有价值的token。在贝叶斯文本分类中用到了，特此记录。 \\[ I_k = \\sum_{\\tilde{y}=0}^1 \\bigg(p(X = k | Y = \\tilde{y})p(Y = \\tilde{y}) \\log\\frac{p(X = k | Y = \\tilde{y})}{p(X=k)} + (1-p(X = k | Y = \\tilde{y}))p(Y = \\tilde{y}) \\log\\frac{1 - p(X = k | Y = \\tilde{y})}{1 - p(X = k)}\\bigg), \\] 公式如上。\\(I_k\\)意味着单词k与Y之间的互信息。 示例代码如下： def calculateMI(dtm_ham_train, dtm_spam_train): ham_sums = np.sum(dtm_ham_train, axis=0) ham_probs = ham_sums / np.sum(ham_sums) spam_sums = np.sum(dtm_spam_train, axis=0) spam_probs = spam_sums / np.sum(spam_sums) all_sums = ham_sums + spam_sums all_probs = all_sums / sum(all_sums) mi = [] for i in range(len(all_probs)): if all_probs[i] == 0 or np.isnan(all_probs[i]): mi.append(0) else: mi.append(.5 * ham_probs[i] * np.log(ham_probs[i] / all_probs[i]) + .5 * (1 - ham_probs[i]) * np.log((1 - ham_probs[i])/(1 - all_probs[i])) + .5 * spam_probs[i] * np.log(spam_probs[i] / all_probs[i]) + .5 * (1 - spam_probs[i]) * np.log((1 - spam_probs[i])/(1 - all_probs[i]))) mi = np.array(mi) mi = np.where(np.isnan(mi), 0, mi) return mi ","date":"2021-02-22","objectID":"/ppmi/:1:1","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["NLP"],"content":"PPMI ","date":"2021-02-22","objectID":"/ppmi/:2:0","tags":["NLP","PPMI"],"title":"PPMI","uri":"/ppmi/"},{"categories":["算法题"],"content":"至少有k个重复字符的最长字串 ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:0:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/longest-substring-with-at-least-k-repeating-characters/ ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:1:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"思路： 利用递归，如果s中字符c的数目小于k,则以c作分割，分成的字串再次调用函数形成递归，然后从众多结果中找寻最大长度的。 ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:2:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"代码： class Solution(object): def longestSubstring(self, s, k): if len(s) \u003c k: return 0 for c in set(s): if s.count(c) \u003c k: return max(self.longestSubstring(t, k) for t in s.split(c)) return len(s) ","date":"2021-02-18","objectID":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/:3:0","tags":["算法题","至少有k个重复字符的最长字串"],"title":"至少有k个重复字符的最长字串","uri":"/%E8%87%B3%E5%B0%91%E6%9C%89k%E4%B8%AA%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%97%E4%B8%B2/"},{"categories":["算法题"],"content":"最长递增子序列 ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:0:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/longest-increasing-subsequence/ ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:1:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"思路： 动态规划 定义dp[i]为到nums[i]的最长递增子序列的长度，全部都初始化为1,因为本身就是长度为1的递增子序列 ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:2:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def lengthOfLIS(self, nums: List[int]) -\u003e int: dp = [1 for _ in range(len(nums))] for i in range(1,len(nums)): for j in range(i): if nums[j] \u003c nums[i]: dp[i] = max(dp[i],dp[j]+1) return max(dp) ","date":"2021-02-17","objectID":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:3:0","tags":["算法题","最长递增子序列"],"title":"最长递增子序列","uri":"/%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["算法题"],"content":"外观数列 https://leetcode-cn.com/problems/count-and-say/ 这题有意思 可以打表，不过打表的过程也相当于做出来了 class Solution: def countAndSay(self,n: int) -\u003e str: if n == 1: return '1' s = self.countAndSay(n - 1) n,res = 0,'' for ii,ss in enumerate(s): if ss != s[n]: res += str(ii-n) + s[n] n = ii res += str(len(s) - n) + s[-1] return res print(Solution().countAndSay(3)) 思路： ​ 递归，将上一层计算出来的东西作为迭代对象。 ","date":"2021-02-16","objectID":"/%E5%A4%96%E8%A7%82%E6%95%B0%E5%88%97/:0:0","tags":["算法题","外观数列"],"title":"外观数列","uri":"/%E5%A4%96%E8%A7%82%E6%95%B0%E5%88%97/"},{"categories":["pandas","api"],"content":"调用为np.lib.stride_tricks.as_strided() 可以分割一个数组为不同的shape块，有个问题就是什么是strides呢？可以看个例子： a = np.arange(9, dtype=np.int32).reshape(3,3) print(a) ''' [[0 1 2] [3 4 5] [6 7 8]] ''' print(a.strides) ''' (12, 4) ''' 这里（12， 4）中的12表示在内存中a[n, 0]到a[n+1, 0]跨过多少byte，4表示在内存中a[n, 0]到a[n, 1]跨过多少byte。 32int需要4byte是众所周知。 看一下函数的参数： numpy.lib.stride_tricks.as_strided(x, shape=None, strides=None, subok=False, writeable=True) x就是我们要分割的矩阵，可以当做是一个蓝图，shape，strides都是新矩阵的属性，也就是说这个函数按照给定的shape和strides来划分x，返回一个新的矩阵。 对于X： 如果卷积核的大小是2x2，stride为1，那么就需要把矩阵X转换为包含如下4个小矩阵的新矩阵A： 很明显A的维度为(2,2,2,2)。 所以shape可以确定，但strides还不确定： A = as_strided(X, shape=(2,2,2,2), strides) 下面确定strides，从图中可以确定最低维的为4，因为所有数据都在X上，所以A的各个维度的跨度都要根据X来确定，而不是A中，以1和4为例子，在X中的距离为12字节，所以现在可以确定后两维：（?,?,12,4）。 再看更高维度: 从X中可以看到，第二维的距离为4。 第一维也不多说，是12。 最后可以strides =（12，4，12，4）。 这就是整个分析的过程，可以方便卷积操作，不是嘛。 再看一个例子就结束了，估计以后会忘，记录下来。 意思就是将一个向量拓展成这样的形式，用循环的方法很容易实现： def sliding_stack_py(v, k): \"Stack sliding windows of v of length k.\" rows = [] for i in range(len(v) - k + 1): rows.append(v[i : (i + k)]) return np.array(rows) 但如果不能使用循环呢，就可以用刚说的这个函数了: def sliding_stack_np(v, k): return np.lib.stride_tricks.as_strided(v, shape=(len(v) - k + 1, k), strides=(v.strides[0], v.strides[0])) 因为原向量是1维的，所以转换后的strides为[4,4]。希望可以帮助理解。 ","date":"2021-02-14","objectID":"/as_strided/:0:0","tags":["pandas","api","as_strided"],"title":"as_strided","uri":"/as_strided/"},{"categories":["NLP"],"content":"NER(命名实体识别) 参考：https://www.jianshu.com/p/16e1f6a7aaef 命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。 举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息 人名：小明，时间：早上8点，地点：学校。 ","date":"2021-02-06","objectID":"/ner/:0:0","tags":["NLP","NER"],"title":"NER","uri":"/ner/"},{"categories":["算法题"],"content":"阶乘函数后K个零(首个困难题) ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:0:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/preimage-size-of-factorial-zeroes-function/ ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:1:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"思路： 首先先写个判断阶乘后有多少个零的函数，思路就是找所有相乘的数中因数有5的个数。 然后再用二分查找，找到有K个0的左界和右界，然后相减即可，就是要找的数目 class Solution: def preimageSizeFZF(self, K: int) -\u003e int: return self.findright(K) - self.findleft(K) def whatzero(self,n): dis = 5 res = 0 while dis \u003c= n: res += n // dis dis *= 5 return res def findleft(self,K): mins,maxs = 0,sys.maxsize while (mins \u003c maxs): mid = mins + (maxs-mins) // 2 if self.whatzero(mid) \u003c K: mins = mid + 1 elif self.whatzero(mid) \u003e K: maxs = mid else: maxs = mid return mins def findright(self,K): mins,maxs = 0,sys.maxsize while (mins \u003c maxs): mid = mins + (maxs-mins) // 2 if self.whatzero(mid) \u003c K: mins = mid + 1 elif self.whatzero(mid) \u003e K: maxs = mid else: mins = mid + 1 return maxs 注意这里的最大值要初始化为sys库里的maxsize 用float(“inf”)会返回nan值 ","date":"2021-02-05","objectID":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/:2:0","tags":["算法题","阶乘函数后K个零(首个困难题)"],"title":"阶乘函数后K个零(首个困难题)","uri":"/%E9%98%B6%E4%B9%98%E5%87%BD%E6%95%B0%E5%90%8Ek%E4%B8%AA%E9%9B%B6%E9%A6%96%E4%B8%AA%E5%9B%B0%E9%9A%BE%E9%A2%98/"},{"categories":["算法题"],"content":"打家劫舍 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:0:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"打家劫舍I ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/house-robber/ ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:1","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"思路: 一个简单题，不过踩了特例的坑。。可以暴力解决 也可以动态规划 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:2","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"代码: 暴力解决 class Solution: def rob(nums): if nums == []: return 0 if len(nums) == 1: return nums[0] if len(nums) == 2: return max(nums[0],nums[1]) maxs = [] #max[i]代表到i+1家的最大价钱 maxs.append(nums[0]) maxs.append(nums[1]) for i in range(2,len(nums)): maxs.append(max(maxs[:i-1])+nums[i]) #从头到这家前面的第二家最大的价钱加上这一家的价钱 return max(maxs) 动态规划 class Solution: def rob(self, nums: List[int]) -\u003e int: if len(nums) \u003c= 2: return max(nums) dp = [0] * len(nums) dp[0], dp[1] = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): dp[i] = max(dp[i-1], dp[i-2] + nums[i]) return max(dp) ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:1:3","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"打家劫舍II ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:0","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/house-robber-ii/ ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:1","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"思路： 跟上面的题目非常类似，只是加了一个限制条件，就是第一家和最后一家不能同时打劫。 这里先写一个函数，表示从start 到end 范围里面的最大值，然后在主函数里面进行选择 如果打劫第一家，就不能打劫最后一家以及不打劫第一家去打劫最后一家，这两者之间的最大值 ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:2","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["算法题"],"content":"代码： class Solution: def rob(self, nums: List[int]) -\u003e int: if len(nums) == 1: return nums[0] return max(self.dp(0,len(nums)-2,nums),self.dp(1,len(nums)-1,nums)) def dp(self,start,end,nums): dp = [0 for _ in range(len(nums)+2)] for i in range(end,start-1,-1): dp[i] = max(dp[i+1],dp[i+2]+nums[i]) return dp[start] ","date":"2021-02-04","objectID":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/:2:3","tags":["算法题","打家劫舍"],"title":"打家劫舍","uri":"/%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"},{"categories":["python"],"content":"import threading import time 简单的创建 def run(n): print(\"task\", n) time.sleep(1) print('2s') time.sleep(1) print('1s') time.sleep(1) print('0s') time.sleep(1) if __name__ == '__main__': t1 = threading.Thread(target=run, args=(\"t1\",)) t2 = threading.Thread(target=run, args=(\"t2\",)) t1.start() t2.start() 通过类创建 class MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"task\", self.n) time.sleep(1) print('2s') time.sleep(1) print('1s') time.sleep(1) print('0s') time.sleep(1) if __name__ == \"__main__\": t1 = MyThread(\"t1\") t2 = MyThread(\"t2\") t1.start() t2.start() 对比没有join()和join()的区别 def run(n): print(\"task\", n) time.sleep(1) #此时子线程停1s print('3') time.sleep(1) print('2') time.sleep(1) print('1') if __name__ == '__main__': t = threading.Thread(target=run, args=(\"t1\",)) t.setDaemon(True) #把子进程设置为守护线程，必须在start()之前设置 t.start() print(\"end\") def run(n): print(\"task\", n) time.sleep(1) #此时子线程停1s print('3') time.sleep(1) print('2') time.sleep(1) print('1') if __name__ == '__main__': t = threading.Thread(target=run, args=(\"t1\",)) t.setDaemon(True) #把子进程设置为守护线程，必须在start()之前设置 t.start() t.join() # 设置主线程等待子线程结束 print(\"end\") 锁的应用 def run(n, semaphore): semaphore.acquire() #加锁 time.sleep(1) print(\"run the thread:%s\\n\" % n) semaphore.release() #释放 if __name__ == '__main__': num = 0 semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行 for i in range(22): t = threading.Thread(target=run, args=(\"t-%s\" % i, semaphore)) t.start() while threading.active_count() != 1: pass # print threading.active_count() else: print('--') 事件类 event = threading.Event() def lighter(): count = 0 event.set() #初始值为绿灯 while True: if 5 \u003c count \u003c=10 : event.clear() # 红灯，清除标志位 print(\"\\33[41;1mred light is on...\\033[0m\") elif count \u003e 10: event.set() # 绿灯，设置标志位 count = 0 else: print(\"\\33[42;1mgreen light is on...\\033[0m\") time.sleep(1) count += 1 def car(name): while True: if event.is_set(): #判断是否设置了标志位（绿灯） print(\"[%s] running...\"%name) time.sleep(1) else: print(\"[%s] sees red light,waiting...\"%name) event.wait()#如果变为绿灯 print(\"[%s] green light is on,start going...\"%name) light = threading.Thread(target=lighter,) light.start() car = threading.Thread(target=car,args=(\"MINI\",)) car.start() queue队列 import threading import queue,time q=queue.Queue(maxsize=10) def Producer(name): count=1 while True: q.put(\"骨头 %s\"%count) print(\"{}生产了骨头\".format(name),count) count+=1 time.sleep(1) def Consumer(name): while True: print(\"[%s] 取到 [%s] 并且吃了它。。。\"%(name,q.get())) time.sleep(1) p=threading.Thread(target=Producer,args=('wlb',)) c=threading.Thread(target=Consumer,args=(\"dog\",)) c1=threading.Thread(target=Consumer,args=(\"cat\",)) p.start() c.start() c1.start() 互斥锁 由于线程之间是进行随机调度，并且每个线程可能只执行n条执行之后，当多个线程同时修改同一条数据时可能会出现脏数据，所以，出现了线程锁，即同一时刻允许一个线程执行操作。线程锁用于锁定资源，你可以定义多个锁, 像下面的代码, 当你需要独占某一资源时，任何一个锁都可以锁这个资源，就好比你用不同的锁都可以把相同的一个门锁住是一个道理。 由于线程之间是进行随机调度，如果有多个线程同时操作一个对象，如果没有很好地保护该对象，会造成程序结果的不可预期，我们也称此为“线程不安全”。 为了方式上面情况的发生，就出现了互斥锁(Lock) from threading import Thread,Lock import os,time def work(): global n lock.acquire() temp=n time.sleep(0.1) n=temp-1 lock.release() if __name__ == '__main__': lock=Lock() n=100 l=[] for i in range(100): p=Thread(target=work) l.append(p) p.start() for p in l: p.join() 信号量 互斥锁同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去。 import threading import time def run(n, semaphore): semaphore.acquire() #加锁 time.sleep(1) print(\"run the thread:%s\\n\" % n) semaphore.release() #释放 if __name__ == '__main__': num = 0 semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行 for i in range(22): t = threading.Thread(target=run, args=(\"t-%s\" % i, semaphore)) t.start() while threading.active_count() != 1: pass # print threading.active_count() else: print('--') GIL（Global Interpreter Lock）全局解释器锁 在非python环境中，单核情况下，同时只能有一个任务执行。多核时可以支持多个线程同时执行。但是在python中，无论有多少核，同时只能执行一个线程。究其原因，这就是由于GIL的存在导致的。 GIL的全称是Global Interpreter Lock(全局解释器锁)，来源是python设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到GIL，我们可以把GIL看作是“通行证”，并且在","date":"2021-01-21","objectID":"/thread/:0:0","tags":["python","thread"],"title":"thread","uri":"/thread/"},{"categories":["算法题"],"content":"丑数系列 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:0:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"1.丑数 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ugly-number/ ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:1","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"思路： 就是让这个数字不断地除以2.3.5 如果最后变成了1 就说明是个丑数 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:2","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def isUgly(self, num: int) -\u003e bool: if num\u003c=-231 or num\u003e=231-1: return False while num \u003e1: if num %2 == 0: num=int(num/2) elif num %3 ==0: num =int(num/3) elif num %5 ==0: num=int(num/5) else: break if num == 1: return True else: return False ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:1:3","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"丑数II ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:0","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/ugly-number-ii/ ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:1","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"思路： 利用三指针，维护i2 i3 i5三个指针分别指向2 3 5 ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:2","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["算法题"],"content":"代码： class Solution: def nthUglyNumber(self, n: int) -\u003e int: res = [1] # 先初始化为1 i2 = i3 = i5 = 0 # 初始化为0 for i in range(1,n): mins = min(res[i2]*2,res[i3]*3,res[i5]*5) # 从小到大找 res.append(mins) if res[i] == res[i2]*2: i2 += 1 if res[i] == res[i3]*3: i3 += 1 if res[i] == res[i5]*5: i5 += 1 return res[n-1] ","date":"2021-01-18","objectID":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/:2:3","tags":["算法题","丑数系列"],"title":"丑数系列","uri":"/%E4%B8%91%E6%95%B0%E7%B3%BB%E5%88%97/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"梯度提升决策树(GBDT) GBDT(Gradient Boosting Decision Tree)是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案。 ","date":"2021-01-06","objectID":"/gbdt/:0:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"回归树 选择最优切分变量j与切分点s：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小 值时的(j,s)对。其中Rm是被划分的输入空间， \\(\\mathrm{cm}\\) 是空间Rm对应的固定输出值。 \\[ \\min_{j, s}\\left[\\min_{c_{1}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min_{c_{2}} \\sum_{x_{i} \\in R_{i}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}\\right] \\] 用选定的(j,s)对，划分区域并决定相应的输出值 \\[ \\begin{gathered} R_{1}(j, s)=\\{x \\mid x^{(j)} \\leq s\\}, R_{2}(j, s)=\\{x \\mid x^{(j)}\u003es\\} \\\\\\\\ \\hat{c}_{m}=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}(j, s)} y_{i} \\\\\\\\ x \\in R_{m}, m=1,2 \\end{gathered} \\] 继续对两个子区域调用上述步骤，将输入空间划分为 \\(M\\) 个区域R1,R2,..,Rm，生成决策树。 \\[ f(x)=\\sum_{m=1}^{M} \\hat{c}_{m} I\\left(x \\epsilon R_{m}\\right) \\] 当输入空间划分确定时，可以用平方误差来表示回归树对于训练数据的预测方法，用平方误差最小 的准则求解每个单元上的最优输出值。 ","date":"2021-01-06","objectID":"/gbdt/:1:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"提升树 梯度提升树是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。 提升树算法: (1) 初始化 \\(f_{0}(x)=0\\) (2) 对 \\(m=1,2, \\ldots, M\\) (a) 计算残差 \\[ r_{m i}=y_{i}-f_{m-1}(x), i=1,2, \\ldots, N \\] 为什么残差是这种形式？ 当采用平方误差损失函数时, \\[ L(y, f(x))=(y-f(x))^2 \\] 其损失变为 \\[ \\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_m\\right)\\right) \u0026=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_m\\right)\\right]^2 \\\\\\\\ \u0026=\\left[r-T\\left(x ; \\Theta_m\\right)\\right]^2 \\end{aligned} \\] 这里, \\[ r=y-f_{m-1}(x) \\] 也可以用上一步的残差（定义的上一步的标签）减去拟合上一步残差的回归树。即： \\[ r_{mi} = r_{(m-1)i} - h_{m-1}(x) \\] 易证明这两种形式等价。 (b) 拟合残差 \\(r_{m i}\\) 学习一个回归树，得到 \\(h_{m}(x)\\) (c) 更新 \\(f_{m}(x)=f_{m-1}+h_{m}(x)\\) (3) 得到回归问题提升树 \\[ f_{M}(x)=\\sum_{m=1}^{M} h_{m}(x) \\] ","date":"2021-01-06","objectID":"/gbdt/:2:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"GBDT GBDT与提升树不同的是GBDT使用负梯度来近似残差。 GBDT算法: (1) 初始化弱学习器 \\[ f_{0}(x)=\\arg \\min_{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right) \\] 对 \\(m=1,2, \\ldots, M\\) 有: 对每个样本 \\(i=1,2, \\ldots, N\\) ，计算负梯度，即残差 \\[ r_{i m}=-\\left[\\frac{\\left.\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)} \\] 一般回归的损失函数就是均方误差，但缺点是对于outlier比较敏感。 因此可以选择使用MAE或者Huber loss。所以说负梯度并不等于残差，损失函数选择MSE的时候才可以划等号。 将上步得到的残差作为样本新的真实值，并将数据 \\(\\left(x_{i}, r_{i m}\\right), i=1,2, . . N\\) 作为下棵树的训练数据，得到一颗新的回归树 \\(f_{m}(x)\\) 其对应的叶子节点区域为 \\(R_{j m}, j=1,2, \\ldots, J\\) 。其中 \\(J\\) 为回归树的叶子节点的个数。 对叶子区域 \\(j=1,2, . . J\\) 计算最佳拟合值 \\[ \\Upsilon_{j m}=\\underbrace{\\arg \\min}_{\\Upsilon} \\sum_{x_{i} \\in R_{j m}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\Upsilon\\right) \\] 也可以理解为： \\[ \\Upsilon_{jm} = \\underbrace{\\arg \\min}_{\\Upsilon} \\sum_{x_i \\in R_{jm}} L(r_{im}, \\Upsilon) \\] 损失函数L为MSE时，就与回归树的构建类似，最佳拟合值就是划分的叶子节点的均值。可以简单理解为提升树和GBDT的区别就是计算残差的方式不同。 更新强学习器 \\[ f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} \\Upsilon_{j m} I\\left(x \\in R_{j m}\\right) \\] 得到最终学习器 \\[ f(x)=f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M} \\sum_{j=1}^{J} \\Upsilon_{j m} I\\left(x \\in R_{j m}\\right) \\] 实例可以看参考里面。 ","date":"2021-01-06","objectID":"/gbdt/:3:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"与梯度下降算法的关系 ","date":"2021-01-06","objectID":"/gbdt/:4:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["Machine Learning","集成学习","Boosting"],"content":"代码 import numpy as np class RegressionTree: def __init__(self, max_depth=2, min_samples_split=2): self.max_depth = max_depth self.min_samples_split = min_samples_split self.tree = {} def fit(self, X, y): self.X = X self.y = y self.n_features = X.shape[1] self.n_samples = X.shape[0] self.tree = self._build_tree(X, y) def predict(self, X): return np.array([self._predict(inputs) for inputs in X]) def _build_tree(self, X, y, depth=0): m = X.shape[0] n = X.shape[1] # 1. 终止条件 if m \u003c= self.min_samples_split or depth \u003e= self.max_depth: return self._leaf(y) # 2. 找到最优分裂特征和特征值 feature, value = self._best_split(X, y) # 3. 构建子树 left_idx, right_idx = self._split(X, feature, value) left = self._build_tree(X[left_idx, :], y[left_idx], depth + 1) right = self._build_tree(X[right_idx, :], y[right_idx], depth + 1) return {\"feature\": feature, \"value\": value, \"left\": left, \"right\": right} def _leaf(self, y): return np.mean(y) def _best_split(self, X, y): m = X.shape[0] n = X.shape[1] min_mse = np.inf best_feature = None best_value = None for feature in range(n): values = np.unique(X[:, feature]) for value in values: y1 = y[X[:, feature] \u003c value] y2 = y[X[:, feature] \u003e= value] mse = np.mean(y1) - np.mean(y2) if mse \u003c min_mse: min_mse = mse best_feature = feature best_value = value return best_feature, best_value def _split(self, X, feature, value): left_idx = np.argwhere(X[:, feature] \u003c value).flatten() right_idx = np.argwhere(X[:, feature] \u003e= value).flatten() return left_idx, right_idx class GBDT: def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3): self.n_estimators = n_estimators self.learning_rate = learning_rate self.max_depth = max_depth self.trees = [] def fit(self, X, y): y_pred = np.zeros_like(y, dtype=np.float) for i in range(self.n_estimators): tree = RegressionTree(self.max_depth) tree.fit(X, -self.gradient(y, y_pred)) y_pred += self.learning_rate * tree.predict(X) self.trees.append(tree) def predict(self, X): y_pred = np.zeros((X.shape[0], ), dtype=np.float) for tree in self.trees: y_pred += self.learning_rate * tree.predict(X) return y_pred def gradient(self, y_true, y_pred): return y_true - y_pred 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？ Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。 对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近。所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。 对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。 ## 参考 https://blog.csdn.net/zpalyq110/article/details/79527653 https://zhuanlan.zhihu.com/p/280222403 ","date":"2021-01-06","objectID":"/gbdt/:5:0","tags":["Machine Learning","集成学习","Boosting","GBDT"],"title":"GBDT","uri":"/gbdt/"},{"categories":["算法题"],"content":"两两交换链表中的节点 ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:0:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"题目： https://leetcode-cn.com/problems/swap-nodes-in-pairs/ ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:1:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"思路: 先把第二位储存起来，然后将后面的递归操作后，再把第二位指向第一位，完成换位 ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:2:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"代码： # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: #假设为[1,2,3,4] def swapPairs(self, head: ListNode) -\u003e ListNode: if not head or not head.next: #递归出口 return head newnode = head.next #储存第二位2 head.next = self.swapPairs(head.next.next) #此时为[1,4,3] newnode.next = head #[2,1,4,3] return newnode ","date":"2021-01-03","objectID":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/:3:0","tags":["算法题","两两交换链表中的节点"],"title":"两两交换链表中的节点","uri":"/%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9/"},{"categories":["算法题"],"content":"种花问题（新年快乐!2021第一题） 新年快乐！2021年第一题，每日一题！希望2021年LC和github可以全绿！加油！ https://leetcode-cn.com/problems/can-place-flowers/ 代码如下： class Solution: def canPlaceFlowers(self, flowerbed: List[int], n: int) -\u003e bool: flowerbed = [0] + flowerbed + [0] for i in range(1,len(flowerbed)-1): if flowerbed[i-1] == 0 and flowerbed[i] == 0 and flowerbed[i+1] == 0: n -= 1 flowerbed[i] = 1 return n \u003c= 0 思路很暴力，就是三个0在一起就可以插进去。。 主要是边界问题，这里构造了两个边界 新的一年开开心心，完成自己的目标，让自己更优秀！ ","date":"2021-01-01","objectID":"/%E7%A7%8D%E8%8A%B1%E9%97%AE%E9%A2%98%E6%96%B0%E5%B9%B4%E5%BF%AB%E4%B9%902021%E7%AC%AC%E4%B8%80%E9%A2%98/:0:0","tags":["算法题","种花问题（新年快乐!2021第一题）"],"title":"种花问题（新年快乐!2021第一题）","uri":"/%E7%A7%8D%E8%8A%B1%E9%97%AE%E9%A2%98%E6%96%B0%E5%B9%B4%E5%BF%AB%E4%B9%902021%E7%AC%AC%E4%B8%80%E9%A2%98/"},{"categories":["算法题"],"content":"零钱兑换 https://leetcode-cn.com/problems/coin-change/ 以我目前的水平做出来有点吃力，看了思路才做出来 class Solution: def coinChange(self, coins: List[int], amount: int) -\u003e int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(amount+1): for coin in coins: # if i \u003e= coin: dp[i] = min(dp[i],dp[i-coin]+1) return -1 if (dp[-1] == float(\"inf\")) else dp[-1] 伪代码如下 # 伪码框架 def coinChange(coins: List[int], amount: int): # 定义：要凑出金额 n，至少要 dp(n) 个硬币 def dp(n): # 做选择，选择需要硬币最少的那个结果 for coin in coins: res = min(res, 1 + dp(n - coin)) return res # 题目要求的最终结果是 dp(amount) return dp(amount) ","date":"2020-10-10","objectID":"/%E9%9B%B6%E9%92%B1%E5%85%91%E6%8D%A2/:0:0","tags":["算法题","零钱兑换"],"title":"零钱兑换","uri":"/%E9%9B%B6%E9%92%B1%E5%85%91%E6%8D%A2/"},{"categories":["算法题"],"content":"去除重复字母 一开始看到题目感觉挺简单的，没想到对现在的我挺有难度。。 https://leetcode-cn.com/problems/remove-duplicate-letters/ #1 class Solution: def removeDuplicateLetters(s: str): res = \"\" while s: #用递归也可以 loc = min(map(s.rindex,s)) #s.rindex是返回列表各值最后出现的索引 求这个最小的索引 a = min(s[:loc+1]) #求字典序最小的 res += a s = s[s.index(a):].replace(a,\"\") #把已经加入的和与其重复的都去掉了 return res #2 #遍历字符串，压入栈，如果遇到比栈顶小的元素且当前字符后面还有与栈顶相同的元素时，移除栈顶元素 class Solution: def removeDuplicateLetters(s: str) -\u003e str: stack = [] for i, t in enumerate(s): if t in stack: continue while stack !=[] and t \u003c stack[-1] and s[i:].find(stack[-1]) != -1: stack.pop() stack.append(t) return \"\".join(stack) 两个方法，第二个方法更好想点。第一个方法是copy的 ","date":"2020-09-02","objectID":"/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%AD%97%E6%AF%8D/:0:0","tags":["算法题","去除重复字母"],"title":"去除重复字母","uri":"/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E5%AD%97%E6%AF%8D/"},{"categories":null,"content":" 好久没折腾博客了，总觉得mkdocs的可定义功能太少了，而且不太美观，于是将博客从mkdocs迁移到Hugo了，原本是想用hexo的，但是图片的引用改过来的话太麻烦了，所以选择了hugo。花费了一点时间迁移，主要是工程量有点大，之前写的博文内容不符合hugo的规范，所以写了一些脚本用于迁移，总体还是比较顺利，本来是想主要是想创造一个舒服的写作环境。目前是obsidian+hugo，然后部署在github pages上面。 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"关于本站 从大一开始就折腾博客了，从一开始只会python语言，到现在会了一点点机器学习，把博客当做自己的成长过程吧，其实大部分都是直接照搬网上的东西，不过自己复现了一些代码，我认为理论与实践要统一，不能只有理论知识，但没有理论去实践，就算照着写出来也一头雾水，做调参侠，但要知道每个参数的意义。 不知道写啥了，先空着。 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":" Table file.mtime as \"最后修改时间\" WHERE tags[0] = \"Reading\" and date(today) - file.mtime \u003c= dur(7 days) Sort file.mtime desc ","date":"0001-01-01","objectID":"/reading_for_oneweek/:0:0","tags":null,"title":"","uri":"/reading_for_oneweek/"},{"categories":null,"content":" Table rows.file.link as filename WHERE todo and title != \"\u003c% tp.file.title %\u003e\" Sort file.ctime desc GROUP BY tags[1] as \"category\" ","date":"0001-01-01","objectID":"/to_writing/:0:0","tags":null,"title":"","uri":"/to_writing/"},{"categories":null,"content":" Table file.mtime as \"最后修改时间\" WHERE !todo and date(today) - file.mtime \u003c= dur(7 days) and tags[0] != \"Reading\" and title Sort file.mtime desc ","date":"0001-01-01","objectID":"/writing_for_oneweek/:0:0","tags":null,"title":"","uri":"/writing_for_oneweek/"},{"categories":null,"content":"论文阅读情况 Table file.mtime as \"最后修改时间\" WHERE tags[0] = \"Reading\" and this.file.ctime - file.mtime \u003c= dur(7 days) Sort file.mtime desc ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:1:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":"PlAN-AND-ACT PLAN-AND-ACT：Improving Planning of Agents for Long-Horizon Tasks 这篇论文解答了我心中一直存在的一个想法，就是计划和行动是分离的。在agent的执行过程中，要先制定计划，然后按照计划一步步执行，计划并不是可以执行的动作，因此需要一个转换。即分离规划器和执行器。 此外这篇论文还提出了一种数据合成的方法： 行动轨迹生成（Action Trajectory Generation）：首先，利用一个强大的“教师 LLM”（如 GPT-4 o），在真实或模拟环境（如 WebArena）中执行任务，收集成功的“行动轨迹”（即一系列成功的底层操作序列）。 接地规划生成（Grounded Plan Generation）：接着，让另一个“教师 LLM”扮演“事后诸葛亮”的角色，对上一步收集到的成功行动轨迹进行“逆向工程”。它会分析这些具体的行动序列，反推出一个合乎逻辑的高层计划，并将每个计划步骤与具体的行动序列进行关联。这一步至关重要，因为它确保了生成的计划是**“接地的”（Grounded）**，即与真实世界的可行操作紧密相连，而非凭空想象。 合成计划扩展（Synthetic Plan Expansion）：最后，将上一步生成的“计划-行动”对作为“种子”，利用 LLM 强大的泛化能力，生成大量结构相似但内容多样的新查询和计划。例如，从一个关于“查找商品”的计划，可以衍生出关于“查找不同商品”、“比较价格”等多种新计划。 此文还提出了动态重规划，也就是发生意外时（和我的第一个科研点很像）会重新调整原来的计划，而不是计划一成不变，这样确保了一定的泛化性能。 这篇论文验证了我一直以来的想法，就是计划和行动分离，计划和行动之间是有gap的。在这基础上可以做的有： 1. ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:2:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":"Routine Routine：A Structural Planning Framework for LLM Agent System in Enterprise 这篇论文沿用了PLAN-AND-ACT的想法，将行动和规划分离，与上面论文的区别在于： 与之前依赖模型在运行时进行“黑盒”规划的Plan-and-Act模式相比，Routine将“Plan”过程前置和显式化，使得整个工作流变得透明、可调试、可维护。开发者可以轻易地修改、增加或删除Routine中的步骤。 此外最大的创新就是双重内存模块（Memory Module）。 程序内存（Procedure Memory）：用于存储和检索整个Routine库。当用户提出请求时，系统能快速匹配到最合适的Routine。 变量内存（Variable Memory）：当工具调用返回过长的结果（如一整篇文章）时，系统会将其存入变量内存，并只在上下文中保留一个简短的键（如VAR_AI_ETHICS_TEXT_001）。这极大地压缩了上下文长度，降低了token消耗，并减少了长文本可能导致的模型“分心”或语法错误。 从论文的图中也可以看到，routine生成的计划不是纯粹的自然语言，而是一种结构化文本，还告知了使用哪个工具，这减小了从计划到行动的gap。 ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:3:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":"笔记整理情况 未完成笔记： Table rows.file.link as filename WHERE todo and title != \"2025_07_27_周日\" Sort file.ctime desc GROUP BY tags[1] as \"category\" 已完成笔记： Table file.mtime as \"最后修改时间\" WHERE !todo and this.file.ctime - file.mtime \u003c= dur(7 days) and tags[0] != \"Reading\" and title Sort file.mtime desc ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:4:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":"科研情况 准备复现Text-Based World Simulators作为下一篇文章的一个测试集 ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:5:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":"其它学习情况 为verl中的agent_loop修复了一个bug并提交了pr。 复现了zerosearch，但没有跑完 更深入阅读了verl的源码，接下来整理关于device mesh的内容，下周需要学习完fsdp和zero，这是继续深入阅读的基础。 准备培养写周报的习惯，并且利用dataview高效整理一周做了什么事情。 ","date":"0001-01-01","objectID":"/2025_07_27_%E5%91%A8%E6%97%A5/:6:0","tags":null,"title":"","uri":"/2025_07_27_%E5%91%A8%E6%97%A5/"},{"categories":null,"content":" World model：预测执行 action 后 state 的转换，从而选择最合适的 action，对 agent 的应用很重要。World model 也能对应我的第一个科研点，即意外事件，意外事件即世界状态的突然转变，如果有 world model ，可以通过执行正确的 action 来解决意外事件。 Tool-use RL：通过强化学习增强模型调用工具的能力，deepresearch 属于一个细分的方向 Reasoning 相关：熵、思维链压缩，都有可以做的东西 ","date":"0001-01-01","objectID":"/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/:0:0","tags":null,"title":"","uri":"/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"},{"categories":null,"content":"目前想通过对 verl 项目做贡献来做为以后的项目经历，可以贡献的地方： 将多轮 RL 引入到 vllm，目前只有 sglang 支持多轮 RL 建立一个 tiny-verl 仓库，抽离 verl 的核心逻辑，提供学习。 ","date":"0001-01-01","objectID":"/%E7%AE%80%E5%8E%86%E4%B8%8A%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%80%9D%E8%80%83/:0:0","tags":null,"title":"","uri":"/%E7%AE%80%E5%8E%86%E4%B8%8A%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%80%9D%E8%80%83/"}]