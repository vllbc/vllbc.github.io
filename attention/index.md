# Attention



# Seq2Seqä¸­çš„Attention

## ç¼ºé™·
åœ¨seq2seqè¿™ç¯‡æ–‡ç« ä¸­è¯¦ç»†ä»‹ç»äº†seq2seqæ¨¡å‹çš„ç»†èŠ‚ï¼Œä½†æ˜¯ä»…ä»…ç”¨ä¸€ä¸ªè¯­ä¹‰ç¼–ç cæ˜¯å®Œå…¨ä¸èƒ½å¤Ÿè¡¨ç¤ºç¼–ç å™¨çš„è¾“å…¥çš„ï¼Œæºçš„å¯èƒ½å«ä¹‰çš„æ•°é‡æ˜¯æ— é™çš„ã€‚å½“ç¼–ç å™¨è¢«è¿«å°†æ‰€æœ‰ä¿¡æ¯æ”¾å…¥å•ä¸ªå‘é‡ä¸­æ—¶ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šå¿˜è®°ä¸€äº›ä¸œè¥¿ã€‚

ä¸ä»…ç¼–ç å™¨å¾ˆéš¾å°†æ‰€æœ‰ä¿¡æ¯æ”¾å…¥ä¸€ä¸ªå‘é‡ä¸­â€”â€”è¿™å¯¹è§£ç å™¨æ¥è¯´ä¹Ÿå¾ˆå›°éš¾ã€‚è§£ç å™¨åªçœ‹åˆ°æºçš„ä¸€ç§è¡¨ç¤ºã€‚ä½†æ˜¯ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæºçš„ä¸åŒéƒ¨åˆ†å¯èƒ½æ¯”å…¶ä»–éƒ¨åˆ†æ›´æœ‰ç”¨ã€‚ä½†åœ¨ç›®å‰çš„æƒ…å†µä¸‹ï¼Œè§£ç å™¨å¿…é¡»ä»ç›¸åŒçš„å›ºå®šè¡¨ç¤ºä¸­æå–ç›¸å…³ä¿¡æ¯â€”â€”è¿™ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹ã€‚

![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220727180013.png)

è¿™ä¸ªæ—¶å€™å°±éœ€è¦å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶äº†ï¼Œæ³¨æ„è¿™é‡Œçš„æ³¨æ„åŠ›æœºåˆ¶å’Œtransformerä¸­çš„self-attentionæ˜¯ä¸ä¸€æ ·çš„ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»ä¸€ä¸‹ã€‚æ³¨æ„å‡ ä¸ªåè¯ï¼šæ³¨æ„åŠ›å¾—åˆ†ã€æ³¨æ„åŠ›æƒé‡ã€‚å…¶ä¸­æ³¨æ„åŠ›å¾—åˆ†å³scoreçš„è®¡ç®—æœ‰å¤šç§æ–¹æ³•ï¼Œæƒé‡å°±æ˜¯å¯¹å¾—åˆ†è¿›è¡Œsoftmaxå½’ä¸€åŒ–ã€‚

## attention
æ³¨æ„æœºåˆ¶æ˜¯ç¥ç»ç½‘ç»œçš„ä¸€éƒ¨åˆ†ã€‚åœ¨æ¯ä¸ªè§£ç å™¨æ­¥éª¤ä¸­ï¼Œå®ƒå†³å®šå“ªäº›æºéƒ¨åˆ†æ›´é‡è¦ã€‚åœ¨æ­¤è®¾ç½®ä¸­ï¼Œç¼–ç å™¨ä¸å¿…å°†æ•´ä¸ªæºå‹ç¼©ä¸ºå•ä¸ªå‘é‡ - å®ƒä¸ºæ‰€æœ‰æºæ ‡è®°æä¾›è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰ RNN çŠ¶æ€è€Œä¸æ˜¯æœ€åä¸€ä¸ªï¼‰ã€‚


![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180414.png)
æ­¥éª¤ï¼š
- æ¥å—æ³¨æ„è¾“å…¥ï¼šè§£ç å™¨çŠ¶æ€$h_t$ä»¥åŠæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€$s_1,s_2,\dots,s_m$
- è®¡ç®—æ¯ä¸ªç¼–ç å™¨çŠ¶æ€çš„æ³¨æ„åŠ›åˆ†æ•°$s_k$ï¼Œæ³¨æ„åŠ›åˆ†æ•°è¡¨ç¤ºå®ƒå¯¹è§£ç å™¨çŠ¶æ€$h_t$çš„ç›¸å…³æ€§ï¼Œä½¿ç”¨æ³¨æ„åŠ›å‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªè§£ç å™¨çŠ¶æ€å’Œä¸€ä¸ªç¼–ç å™¨çŠ¶æ€å¹¶è¿”å›ä¸€ä¸ªæ ‡é‡åˆ†æ•°ï¼Œå³å›¾ä¸­çš„$score(h_t,s_k)$
- è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼šå³æ¦‚ç‡åˆ†å¸ƒ- ä½¿ç”¨Softmaxå‡½æ•°
- è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼šå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨çŠ¶æ€çš„åŠ æƒå’Œ
![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180444.png)

å³ä¸ºå¦‚å›¾æ‰€ç¤ºå†…å®¹ä¸ºå¦‚ä½•è®¡ç®—æ³¨æ„åŠ›ã€‚

æ³¨æ„æˆ‘ä»¬æåˆ°çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œè¿™é‡Œçš„æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—æœ‰å¾ˆå¤šç§æ–¹æ³•ï¼Œä¸‹é¢ä»‹ç»å‡ ç§æ¯”è¾ƒå¸¸è§çš„åŠæ³•ï¼š
- ç‚¹ç§¯ï¼š æœ€ç®€å•çš„åŠæ³•ã€‚
- åŒçº¿æ€§å‡½æ•°
- å¤šå±‚æ„ŸçŸ¥æœº
![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180511.png)

æ³¨æ„åä¸¤è€…éƒ½æœ‰è¦ä¼˜åŒ–çš„å‚æ•°çš„ï¼Œç¬¬ä¸€ä¸ªç‚¹ç§¯æ˜¯ç›´æ¥è¿ç®—ï¼Œå› æ­¤å¾ˆç®€å•ã€‚
åœ¨åº”ç”¨æ—¶å¯ä»¥ç›´æ¥å°†æ³¨æ„åŠ›çš„ç»“æœä¼ è¾“åˆ°æœ€åçš„softmaxï¼Œä¹Ÿå¯ä»¥å°†åŸå§‹çš„$h_t$åˆå¹¶ï¼Œä¸‹é¢ä»‹ç»å‡ ç§å˜ä½“ã€‚

## Bahdanau Model
![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180459.png)
- ç¼–ç å™¨ä½¿ç”¨åŒå‘çš„RNN
- åˆ©ç”¨ä¸Šä¸€æ—¶åˆ»çš„éšå±‚çŠ¶æ€è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºcï¼Œç„¶åå’Œéšå±‚çŠ¶æ€ä¸€èµ·ä½œä¸ºå½“å‰æ—¶åˆ»çš„è¾“å…¥ï¼Œå†å¾—åˆ°ç»“æœ$\hat{y}$ã€‚è¿™é‡Œå†è¯´ä¸€ä¸‹è®­ç»ƒçš„è¿‡ç¨‹ä¸­å½“å‰æ­¥çš„è¾“å…¥ä½¿ç”¨çš„æ˜¯çœŸå®çš„$y$ï¼Œæµ‹è¯•çš„æ—¶å€™æ‰ä¼šä½¿ç”¨ä¸Šä¸€æ­¥çš„è¾“å‡ºä½œä¸ºè¾“å…¥ã€‚å¯ä»¥å°†ä¸Šä¸‹æ–‡å‘é‡cï¼ˆä¹Ÿå°±æ˜¯æ³¨æ„åŠ›è¾“å‡ºï¼‰ä¸$x$æ‹¼æ¥åä½œä¸ºè¾“å…¥ã€‚
- æ³¨æ„åŠ›å¾—åˆ†ä½¿ç”¨çš„æ˜¯æ„ŸçŸ¥æœºã€‚
è¿™é‡Œå¼•ç”¨ä¸€ä¸‹ææ²å¤§ä½¬çš„ä»£ç ï¼š
```python
class Seq2SeqAttentionDecoder(AttentionDecoder):

Â  Â  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,

Â  Â  Â  Â  Â  Â  Â  Â  Â dropout=0, **kwargs):

Â  Â  Â  Â  super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)

Â  Â  Â  Â  self.attention = d2l.AdditiveAttention(

Â  Â  Â  Â  Â  Â  num_hiddens, num_hiddens, num_hiddens, dropout)

Â  Â  Â  Â  self.embedding = nn.Embedding(vocab_size, embed_size)

Â  Â  Â  Â  self.rnn = nn.GRU(

Â  Â  Â  Â  Â  Â  embed_size + num_hiddens, num_hiddens, num_layers,

Â  Â  Â  Â  Â  Â  dropout=dropout)

Â  Â  Â  Â  self.dense = nn.Linear(num_hiddens, vocab_size)

  

Â  Â  def init_state(self, enc_outputs, enc_valid_lens, *args):

Â  Â  Â  Â  # outputsçš„å½¢çŠ¶ä¸º(batch_sizeï¼Œnum_stepsï¼Œnum_hiddens).

Â  Â  Â  Â  # hidden_stateçš„å½¢çŠ¶ä¸º(num_layersï¼Œbatch_sizeï¼Œnum_hiddens)

Â  Â  Â  Â  outputs, hidden_state = enc_outputs

Â  Â  Â  Â  return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

  

Â  Â  def forward(self, X, state):

Â  Â  Â  Â  # enc_outputsçš„å½¢çŠ¶ä¸º(batch_size,num_steps,num_hiddens).

Â  Â  Â  Â  # hidden_stateçš„å½¢çŠ¶ä¸º(num_layers,batch_size,

Â  Â  Â  Â  # num_hiddens)

Â  Â  Â  Â  enc_outputs, hidden_state, enc_valid_lens = state

Â  Â  Â  Â  # è¾“å‡ºXçš„å½¢çŠ¶ä¸º(num_steps,batch_size,embed_size)

Â  Â  Â  Â  X = self.embedding(X).permute(1, 0, 2) # è½¬æ¢æ˜¯ä¸ºäº†æ–¹ä¾¿åé¢å¾ªç¯è®¡ç®—ã€‚

Â  Â  Â  Â  outputs, self._attention_weights = [], []

Â  Â  Â  Â  for x in X:

Â  Â  Â  Â  Â  Â  # queryçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)

Â  Â  Â  Â  Â  Â  query = torch.unsqueeze(hidden_state[-1], dim=1) # -1æ˜¯æŒ‡åœ¨æœ€åä¸€å±‚æœ€åæ—¶åˆ»çš„éšè—çŠ¶æ€ï¼Œä½œä¸ºquery

Â  Â  Â  Â  Â  Â  # contextçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)

Â  Â  Â  Â  Â  Â  context = self.attention(

Â  Â  Â  Â  Â  Â  Â  Â  query, enc_outputs, enc_outputs, enc_valid_lens)

Â  Â  Â  Â  Â  Â  # åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿ç»“

Â  Â  Â  Â  Â  Â  x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)

Â  Â  Â  Â  Â  Â  # å°†xå˜å½¢ä¸º(1,batch_size,embed_size+num_hiddens)

Â  Â  Â  Â  Â  Â  out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)

Â  Â  Â  Â  Â  Â  outputs.append(out)

Â  Â  Â  Â  Â  Â  self._attention_weights.append(self.attention.attention_weights)

Â  Â  Â  Â  # å…¨è¿æ¥å±‚å˜æ¢åï¼Œoutputsçš„å½¢çŠ¶ä¸º

Â  Â  Â  Â  # (num_steps,batch_size,vocab_size)

Â  Â  Â  Â  outputs = self.dense(torch.cat(outputs, dim=0))

Â  Â  Â  Â  return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  enc_valid_lens]

  

Â  Â  @property

Â  Â  def attention_weights(self):

Â  Â  Â  Â  return self._attention_weights
```
## Luong Model
![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180540.png)
è¿™ä¸ªæ¨¡å‹çš„ç¼–ç å™¨æ¯”è¾ƒå¸¸è§„ï¼Œä½¿ç”¨å½“å‰çŠ¶æ€è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼Œç„¶åè§£ç å™¨ä¸­å°†éšå±‚çŠ¶æ€ä¸æ³¨æ„åŠ›è¾“å‡ºåšä¸€æ­¥ç»“åˆï¼Œè¿™æ ·å¾—åˆ°äº†æ–°çš„éšå±‚çŠ¶æ€ï¼Œç„¶åå†ä¼ é€’å¾—åˆ°é¢„æµ‹ç»“æœã€‚

## æ³¨æ„åŠ›å¯¹é½
![](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180549.png)
å¯ä»¥çœ‹åˆ°è§£ç å™¨å…³æ³¨çš„æºtokenã€‚

åˆ°æ­¤seq2seqä¸­çš„attentionå°±ä»‹ç»å®Œæ¯•äº†ï¼Œå…¶å®è¿˜æœ‰å¾ˆå¤šç»†èŠ‚ï¼Œä»¥åé‡åˆ°äº†ä¼šæŒç»­è¡¥å……ã€‚

# Self-attention
ç§»æ­¥transformerã€‚[Transformer](../basic/Transformer.md)

# MQA
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916123443.png)

æ ‡å‡†çš„mhaä¸­ï¼ŒKV headsçš„æ•°é‡å’ŒQuery headsçš„æ•°é‡ç›¸åŒï¼Œæ¯ä¸€ä¸ªq headå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„kv headï¼Œä½†è¿™æ ·çš„å¼€é”€æ¯”è¾ƒå¤§ã€‚
**MQA (Multi Queries Attention): MQAæ¯”è¾ƒæç«¯ï¼Œåªä¿ç•™ä¸€ä¸ªKV Headï¼Œå¤šä¸ªQuery Headså…±äº«ç›¸åŒçš„KV Head**ã€‚è¿™ç›¸å½“äºä¸åŒHeadçš„Attentionå·®å¼‚ï¼Œå…¨éƒ¨éƒ½æ”¾åœ¨äº†Queryä¸Šï¼Œéœ€è¦æ¨¡å‹ä»…ä»ä¸åŒçš„Query Headsä¸Šå°±èƒ½å¤Ÿå…³æ³¨åˆ°è¾“å…¥hidden statesä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæå¤§åœ°é™ä½äº†KV Cacheçš„éœ€æ±‚ï¼Œä½†æ˜¯ä¼šå¯¼è‡´æ¨¡å‹æ•ˆæœæœ‰æ‰€ä¸‹é™ã€‚ï¼ˆå±‚å†…å…±äº«ï¼‰
# GQA
å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒGQAå°±æ˜¯åœ¨MHAå’ŒMQAä¹‹é—´åšäº†ä¸€ä¸ªå¹³è¡¡ã€‚å¯¹query headsè¿›è¡Œåˆ†ç»„ï¼Œåˆ†æˆå‡ ç»„å°±å¯¹åº”å¤šå°‘ä¸ªkv headsï¼Œç„¶åæ¯ä¸€ç»„å†…çš„query Headså…±äº«ç›¸åŒçš„KV headã€‚
GQAå¯ä»¥åœ¨å‡å°‘è®¡ç®—é‡å’ŒKV CacheåŒæ—¶ç¡®ä¿æ¨¡å‹æ•ˆæœä¸å—åˆ°å¤§çš„å½±å“ã€‚
# online attention
### 3-pass 
$\mathsf{NO}$TATIONS

$\{m_i\}{:}\max_{j=1}^i\left\{x_j\right\}$, with initial value $m_0=-\infty.$
$\{d_i\}{:}\sum_{j=1}^ie^{x_j-m_N}$, with initial value $d_0=0,d_N$ is the denominator of safe softmax.
$\{a_i\}{:\text{ the final softmax value}}.$

BODY
$\textbf{for }i\leftarrow 1, N\textbf{ do}$
$$m_i\leftarrow\max\left(m_{i-1},x_i\right)$$
$\mathbf{end}$

$\textbf{for }i\leftarrow 1, N\textbf{ do}$
$$d_i\leftarrow d_{i-1}+e^{x_i-m_N}$$
$\mathbf{end}$

$\textbf{for }i\leftarrow 1, N\textbf{ do}$
$$a_i\leftarrow\frac{e^{x_i-m_N}}{d_N}$$
$\mathbf{end}$

è¿™æ˜¯3stepè®¡ç®—attentionçš„æ–¹æ³•ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦ä¸Šä¸€æ­¥çš„ç»“æœæ‰å¯ä»¥ç»§ç»­è®¡ç®—ã€‚è¿™æ ·çš„è¯ç”±äºsramä¸­æ²¡æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ï¼Œå› æ­¤éœ€è¦å¤šæ¬¡è®¿å­˜ã€‚
### online attention
$$\begin{aligned}
d_i^{\prime}& =\sum_{j=1}^ie^{x_j-m_i} \\
&= \left(\sum_{j=1}^{i-1} e^{x_j-m_i}\right)+e^{x_i-m_i} \\
&= \left(\sum_{j=1}^{i-1} e^{x_j-m_{i-1}}\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
&= d_{i-1}' e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}$$
æ‰¾åˆ°è¿­ä»£å¼ä¹‹åå°±å¯ä»¥ä»3stepé™åˆ°2step
$$\begin{aligned}&\mathbf{for~}i\leftarrow1,N\textbf{ do}\\&&&m_i&&\leftarrow&\max\left(m_{i-1},x_i\right)\\&&&d_i^{\prime}&&\leftarrow&d_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\&\mathbf{end}\\&\mathbf{for~}i\leftarrow1,N\textbf{ do}\\&&&a_i\leftarrow&&\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&\mathbf{end}\end{aligned}$$
å¥½åƒFLOPsè®¡ç®—é‡å¹¶æ²¡æœ‰å‡å°‘ï¼Œç”šè‡³è¿˜ç•¥æœ‰å¢åŠ ï¼Œå› ä¸ºç°åœ¨æ¯æ¬¡éƒ½éœ€è¦è®¡ç®—é¢å¤–çš„scale

> xå€¼ï¼Œä¹Ÿå°±æ˜¯pre-softmax logitsï¼Œç”±äºéœ€è¦O(N^2)çš„æ˜¾å­˜æ— æ³•æ”¾åœ¨SRAMä¸­ã€‚å› æ­¤ï¼š  
> 1. è¦ä¹ˆæå‰è®¡ç®—å¥½xï¼Œä¿å­˜åœ¨å…¨å±€æ˜¾å­˜ä¸­ï¼Œéœ€è¦O(N^2)çš„æ˜¾å­˜ï¼Œå®¹æ˜“çˆ†æ˜¾å­˜ã€‚  
> 2. è¦ä¹ˆåœ¨ç®—æ³•ä¸­onlineè®¡ç®—ï¼Œæ¯æ¬¡å¾ªç¯ä¸­å»loadä¸€éƒ¨åˆ†Qï¼ŒKåˆ°ç‰‡ä¸Šå†…å­˜ï¼Œè®¡ç®—å¾—åˆ°xã€‚

Attentionä¼˜åŒ–çš„ç›®æ ‡å°±æ˜¯é¿å¼€ç¬¬ä¸€ç§æƒ…å†µï¼Œå°½å¯èƒ½èŠ‚çœæ˜¾å­˜ï¼Œå¦åˆ™ï¼ŒLLMæ ¹æœ¬æ— æ³•å¤„ç†ç±»ä¼¼100Kä»¥ä¸Šè¿™ç§long contextçš„æƒ…å†µã€‚è€Œå¯¹äºç¬¬äºŒç§æƒ…å†µï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¿å­˜ä¸­é—´çŸ©é˜µxï¼ŒèŠ‚çœäº†æ˜¾å­˜ï¼Œä½†æ˜¯è®¡ç®—æ²¡æœ‰èŠ‚çœï¼Œå¹¶ä¸”å¢åŠ äº†HBM IO Accessesï¼ˆéœ€è¦ä¸æ–­åœ°load Q, Kï¼‰ã€‚æ­¤æ—¶ï¼Œ2-passç®—æ³•ç›¸å¯¹äº3-passç®—æ³•ï¼Œå¯ä»¥å‡å°‘ä¸€æ¬¡æ•´ä½“çš„load Q, Kä»¥åŠå‡å°‘ä¸€æ¬¡å¯¹Â xiÂ çš„online recomputeï¼Œå› ä¸ºåœ¨2-passçš„ç¬¬ä¸€ä¸ªpassä¸­ï¼ŒÂ xiÂ æ˜¯è¢«ä¸¤æ¬¡è®¡ç®—å…±äº«çš„ã€‚ç±»ä¼¼online-softmaxè¿™ç§ç®—æ³•ï¼Œå¯¹åº”åˆ°Attentionä¸­çš„åº”ç”¨ï¼Œå°±æ˜¯Memory Efficient Attentionï¼ˆæ³¨æ„ä¸æ˜¯FlashAttentionï¼‰ã€‚

# flash attention
safe softmaxå¹¶æ²¡æœ‰1-passç®—æ³•ï¼Œé‚£ä¹ˆAttentionä¼šä¸ä¼šæœ‰å‘¢ï¼Ÿæœ‰ï¼è¿™å°±æ˜¯FlashAttentionï¼

åœ¨ä½¿ç”¨online attentionçš„æƒ…å†µä¸‹ï¼Œä»å¤´å¼€å§‹è®¡ç®—attention scoreçš„è¿‡ç¨‹å¦‚ä¸‹ï¼š
$\operatorname{NOTATIONS}$

$Q[k,:]:$the $k$-th row vector of $Q$ matrix.
$\begin{aligned}O[k,:]:\mathrm{~the~}k\text{-th row of output }O\mathrm{~matrix.}\\\mathbf{V}[i,i]:\mathrm{~the~}k\text{-th row of output }O\mathrm{~matrix.}\end{aligned}$
$V[i,:]{:\text{ the }i\text{-th row of }V\text{ matrix}}.$
$\{\boldsymbol{o}_i\}{:}\sum_{j=1}^ia_jV[j,:]$, a row vector storing partial aggregation result $A[k,:i]\times V[:i,:]$
BODY

$\textbf{for }i\leftarrow 1, N\textbf{ do}$
$$\begin{aligned}x_i&\leftarrow\quad Q[k,:]\:K^T[:,i]\\m_i&\leftarrow\quad\max\left(m_{i-1},x_i\right)\\d_i'&\leftarrow\quad d_{i-1}'e^{m_{i-1}-m_i}+e^{x_i-m_i}\end{aligned}$$
$\mathbf{end}$

$\textbf{for }i\leftarrow 1, N\textbf{ do}$
$$\begin{aligned}&a_i\:\leftarrow\:\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&o_i\:\leftarrow\:o_{i-1}+a_i\:V[i,:\:]\end{aligned}$$
$\mathbf{end}$
$$O[k,:]\leftarrow\boldsymbol{o}_N$$


ä¼˜åŒ–æ€è·¯å’Œonline attentionä¸€æ ·ï¼Œå°†$o_{i}$çš„è®¡ç®—ç®€åŒ–ä»¥ä¾¿äºå¯ä»¥å†™æˆè¿­ä»£å¼ã€‚

åŸæ¥çš„$o_{i}$ä½¿ç”¨ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼Œä¾èµ–äºå…¨å±€çš„$m_{N}$å’Œ$d_{N}$ã€‚
$$\boldsymbol{o}_i:=\sum_{j=1}^i\left(\frac{e^{x_j-m_N}}{d_N^{\prime}}V[j,:]\right)$$
å°†å…¶æ”¹å†™æˆå¦‚ä¸‹å½¢å¼ï¼š
$$\boldsymbol{o}_i^{\prime}:=\left(\sum_{j=1}^i\frac{e^{x_j-m_i}}{d_i^{\prime}}V[j,:]\right)$$
è¿™æ ·æŒ‰ç…§ä¸Šé¢çš„æ–¹å¼æ‹“å±•ä¸‹å»ï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå¾ªç¯è¿­ä»£å¼ã€‚

$$\begin{aligned}
\mathbf{o}_i^{\prime}& =\sum_{j=1}^i\frac{e^{x_j-m_i}}{d'}V[j,:] \\
&= \left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_i}}{d_i^{\prime}}V[j,:] \right)+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,:] \\
&= \left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}}\frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}}\frac{d_{i-1}^{\prime}}{d_i^{\prime}}V[j,:]\right)+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,:] \\
&= \left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}}V[j,.]\right)\frac{d_{i-1}^{\prime}}{d_i^{\prime}}e^{m_{i-1}-m_i}+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,.] \\
&= \boldsymbol{o}_{i-1}'\frac{d_{i-1}'e^{m_{i-1}-m_i}}{d_i'}+\frac{e^{x_i-m_i}}{d_i'}V[i,:]
\end{aligned}$$

è¿™æ ·å°±æ‰¾åˆ°äº†$o_{i}$çš„é€’æ¨è¡¨è¾¾å¼ã€‚

ä¹‹åå¯¹Q,Kè¿›è¡Œtilingåè®¡ç®—ï¼Œå¾—åˆ°å¦‚ä¸‹ï¼š
$$\begin{aligned}&\textbf{for }i\leftarrow1,\#\text{tiles do}\\&&&\boldsymbol{x}_i\quad\leftarrow\quad Q[k;\cdot] K^T[\cdot,(i-1) b; i b]\\&&&m_i^{(\mathrm{local})}=\begin{array}{c}\overset{b}{\operatorname*{max}}\left(\boldsymbol{x}_i[j]\right)\\\end{array}\\&&&m_i \leftarrow \max\left(m_{i-1},m_i^{(\mathrm{local})}\right)\\&&&a_i^{\prime} \leftarrow d_{i-1}^{\prime}e^{m_{i-1}-m_i}+\sum_{j=1}^be^{\boldsymbol{x}_i[j]-m_i}\\&&&\boldsymbol{o}_i^{\prime} \leftarrow \boldsymbol{o}_{i-1}^{\prime}\frac{d_{i-1}^{\prime}e^{m_{i-1}-m_i}}{d_i^{\prime}}+\sum_{j=1}^b\frac{e^{\boldsymbol{x}_i[j]-m_i}}{d_i^{\prime}}V[(i-1) b+j,:]\\&\text{end}\\&&&O[k,:]\leftarrow\boldsymbol{o}_{N/b}^{\prime}\end{aligned}$$
å¯¹äºtilesï¼Œç¤ºæ„å›¾å¦‚ä¸‹ï¼š
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916201336.png)

å¯ä»¥ç†è§£æˆæ»‘åŠ¨çª—å£ï¼Œ$K^{T}$ä»å·¦å‘å³æ»‘åŠ¨ï¼ˆæŒ‰åˆ—è¯»å–ï¼‰ï¼Œ$V$ä»ä¸Šå‘ä¸‹æ»‘åŠ¨ï¼ˆæŒ‰è¡Œè¯»å–ï¼‰ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç†è§£æˆåˆ†å—çŸ©é˜µï¼Œå…·ä½“ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Œå‚è€ƒï¼š[Cuda ç¼–ç¨‹ä¹‹ Tiling - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/342103911)
# å‚è€ƒ

[[KV Cacheä¼˜åŒ–]ğŸ”¥MQA/GQA/YOCO/CLA/MLKVç¬”è®°: å±‚å†…å’Œå±‚é—´KV Cacheå…±äº« - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/697311739)

[Transformers KV Caching Explained | by JoÃ£o Lages | Medium](https://medium.com/@joaolages/kv-caching-explained-276520203249)

From Online Softmax to FlashAttention.Â [https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)
