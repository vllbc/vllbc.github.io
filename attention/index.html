<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Attention - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/attention/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Attention">
  <meta property="og:description" content="Seq2Seq中的Attention 缺陷 在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-02-23T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-03-24T00:00:00+00:00">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Attention">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Attention">
  <meta name="twitter:description" content="Seq2Seq中的Attention 缺陷 在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/attention/" /><link rel="prev" href="https://blog.vllbc.top/ppmi/" /><link rel="next" href="https://blog.vllbc.top/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Attention",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/attention\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "NLP, Attention","wordcount":  3305 ,
        "url": "https:\/\/blog.vllbc.top\/attention\/","datePublished": "2021-02-23T00:00:00+00:00","dateModified": "2023-03-24T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Attention</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2021-02-23">2021-02-23</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 3305 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id="/attention/" class="leancloud_visitors" data-flag-title="Attention">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="content" id="content"><h1 id="seq2seq中的attention">Seq2Seq中的Attention</h1>
<h2 id="缺陷">缺陷</h2>
<p>在seq2seq这篇文章中详细介绍了seq2seq模型的细节，但是仅仅用一个语义编码c是完全不能够表示编码器的输入的，源的可能含义的数量是无限的。当编码器被迫将所有信息放入单个向量中时，它很可能会忘记一些东西。</p>
<p>不仅编码器很难将所有信息放入一个向量中——这对解码器来说也很困难。解码器只看到源的一种表示。但是，在每个生成步骤中，源的不同部分可能比其他部分更有用。但在目前的情况下，解码器必须从相同的固定表示中提取相关信息——这不是一件容易的事。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220727180013.png" /></p>
<p>这个时候就需要引入注意力机制了，注意这里的注意力机制和transformer中的self-attention是不一样的。下面详细介绍一下。注意几个名词：注意力得分、注意力权重。其中注意力得分即score的计算有多种方法，权重就是对得分进行softmax归一化。</p>
<h2 id="attention">attention</h2>
<p>注意机制是神经网络的一部分。在每个解码器步骤中，它决定哪些源部分更重要。在此设置中，编码器不必将整个源压缩为单个向量
- 它为所有源标记提供表示（例如，所有 RNN 状态而不是最后一个）。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180414.png" />
步骤： - 接受注意输入：解码器状态<span
class="math inline">\(h_t\)</span>以及所有编码器状态<span
class="math inline">\(s_1,s_2,\dots,s_m\)</span> -
计算每个编码器状态的注意力分数<span
class="math inline">\(s_k\)</span>，注意力分数表示它对解码器状态<span
class="math inline">\(h_t\)</span>的相关性，使用注意力函数，接收一个解码器状态和一个编码器状态并返回一个标量分数，即图中的<span
class="math inline">\(score(h_t,s_k)\)</span> -
计算注意力权重：即概率分布- 使用Softmax函数 -
计算注意力输出：具有注意力机制的编码器状态的加权和 <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180444.png" /></p>
<p>即为如图所示内容为如何计算注意力。</p>
<p>注意我们提到的注意力函数，这里的注意力分数的计算有很多种方法，下面介绍几种比较常见的办法：
- 点积： 最简单的办法。 - 双线性函数 - 多层感知机 <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180511.png" /></p>
<p>注意后两者都有要优化的参数的，第一个点积是直接运算，因此很简单。
在应用时可以直接将注意力的结果传输到最后的softmax，也可以将原始的<span
class="math inline">\(h_t\)</span>合并，下面介绍几种变体。</p>
<h2 id="bahdanau-model">Bahdanau Model</h2>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180459.png" />
- 编码器使用双向的RNN -
利用上一时刻的隐层状态计算注意力输出c，然后和隐层状态一起作为当前时刻的输入，再得到结果<span
class="math inline">\(\hat{y}\)</span>。这里再说一下训练的过程中当前步的输入使用的是真实的<span
class="math inline">\(y\)</span>，测试的时候才会使用上一步的输出作为输入。可以将上下文向量c（也就是注意力输出）与<span
class="math inline">\(x\)</span>拼接后作为输入。 -
注意力得分使用的是感知机。 这里引用一下李沐大佬的代码：</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2SeqAttentionDecoder(AttentionDecoder):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_size, num_hiddens, num_layers,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                 dropout<span class="op">=</span><span class="dv">0</span>, <span class="op">**</span>kwargs):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Seq2SeqAttentionDecoder, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> d2l.AdditiveAttention(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            num_hiddens, num_hiddens, num_hiddens, dropout)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_size)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.GRU(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            embed_size <span class="op">+</span> num_hiddens, num_hiddens, num_layers,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span>dropout)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> nn.Linear(num_hiddens, vocab_size)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_state(<span class="va">self</span>, enc_outputs, enc_valid_lens, <span class="op">*</span>args):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        outputs, hidden_state <span class="op">=</span> enc_outputs</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (outputs.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>), hidden_state, enc_valid_lens)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, state):</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden_state的形状为(num_layers,batch_size,</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># num_hiddens)</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        enc_outputs, hidden_state, enc_valid_lens <span class="op">=</span> state</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.embedding(X).permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>) <span class="co"># 转换是为了方便后面循环计算。</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        outputs, <span class="va">self</span>._attention_weights <span class="op">=</span> [], []</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># query的形状为(batch_size,1,num_hiddens)</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            query <span class="op">=</span> torch.unsqueeze(hidden_state[<span class="op">-</span><span class="dv">1</span>], dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># -1是指在最后一层最后时刻的隐藏状态，作为query</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># context的形状为(batch_size,1,num_hiddens)</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> <span class="va">self</span>.attention(</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>                query, enc_outputs, enc_outputs, enc_valid_lens)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 在特征维度上连结</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat((context, torch.unsqueeze(x, dim<span class="op">=</span><span class="dv">1</span>)), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            out, hidden_state <span class="op">=</span> <span class="va">self</span>.rnn(x.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>), hidden_state)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>            outputs.append(out)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._attention_weights.append(<span class="va">self</span>.attention.attention_weights)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 全连接层变换后，outputs的形状为</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (num_steps,batch_size,vocab_size)</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.dense(torch.cat(outputs, dim<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>), [enc_outputs, hidden_state,</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>                                          enc_valid_lens]</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    <span class="op">@</span>property</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention_weights(<span class="va">self</span>):</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._attention_weights</span></code></pre></div>
<h2 id="luong-model">Luong Model</h2>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180540.png" />
这个模型的编码器比较常规，使用当前状态计算注意力输出，然后解码器中将隐层状态与注意力输出做一步结合，这样得到了新的隐层状态，然后再传递得到预测结果。</p>
<h2 id="注意力对齐">注意力对齐</h2>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020221111180549.png" />
可以看到解码器关注的源token。</p>
<p>到此seq2seq中的attention就介绍完毕了，其实还有很多细节，以后遇到了会持续补充。</p>
<h1 id="self-attention">Self-attention</h1>
<p>移步transformer。<a
href="../model/Transformer.md">Transformer</a></p>
<h1 id="mqa">MQA</h1>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916123443.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>标准的mha中，KV heads的数量和Query heads的数量相同，每一个q
head对应一个独立的kv head，但这样的开销比较大。 <strong>MQA (Multi
Queries Attention): MQA比较极端，只保留一个KV Head，多个Query
Heads共享相同的KV
Head</strong>。这相当于不同Head的Attention差异，全部都放在了Query上，需要模型仅从不同的Query
Heads上就能够关注到输入hidden
states不同方面的信息。这样做的好处是，极大地降低了KV
Cache的需求，但是会导致模型效果有所下降。（层内共享） # GQA
如上图所示，GQA就是在MHA和MQA之间做了一个平衡。对query
heads进行分组，分成几组就对应多少个kv heads，然后每一组内的query
Heads共享相同的KV head。 GQA可以在减少计算量和KV
Cache同时确保模型效果不受到大的影响。 # online attention ### 3-pass
<span class="math inline">\(\mathsf{NO}\)</span>TATIONS</p>
<p><span
class="math inline">\(\{m_i\}{:}\max_{j=1}^i\left\{x_j\right\}\)</span>,
with initial value <span class="math inline">\(m_0=-\infty.\)</span>
<span class="math inline">\(\{d_i\}{:}\sum_{j=1}^ie^{x_j-m_N}\)</span>,
with initial value <span class="math inline">\(d_0=0,d_N\)</span> is the
denominator of safe softmax. <span class="math inline">\(\{a_i\}{:\text{
the final softmax value}}.\)</span></p>
<p>BODY <span class="math inline">\(\textbf{for }i\leftarrow 1,
N\textbf{ do}\)</span> <span
class="math display">\[m_i\leftarrow\max\left(m_{i-1},x_i\right)\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span class="math display">\[d_i\leftarrow
d_{i-1}+e^{x_i-m_N}\]</span> <span
class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span
class="math display">\[a_i\leftarrow\frac{e^{x_i-m_N}}{d_N}\]</span>
<span class="math inline">\(\mathbf{end}\)</span></p>
<p>这是3step计算attention的方法，每一步都需要上一步的结果才可以继续计算。这样的话由于sram中没有足够的存储空间，因此需要多次访存。
### online attention <span class="math display">\[\begin{aligned}
d_i^{\prime}&amp; =\sum_{j=1}^ie^{x_j-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1} e^{x_j-m_i}\right)+e^{x_i-m_i} \\
&amp;= \left(\sum_{j=1}^{i-1}
e^{x_j-m_{i-1}}\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\
&amp;= d_{i-1}&#39; e^{m_{i-1}-m_i}+e^{x_i-m_i}
\end{aligned}\]</span> 找到迭代式之后就可以从3step降到2step <span
class="math display">\[\begin{aligned}&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;m_i&amp;&amp;\leftarrow&amp;\max\left(m_{i-1},x_i\right)\\&amp;&amp;&amp;d_i^{\prime}&amp;&amp;\leftarrow&amp;d_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\&amp;\mathbf{end}\\&amp;\mathbf{for~}i\leftarrow1,N\textbf{
do}\\&amp;&amp;&amp;a_i\leftarrow&amp;&amp;\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&amp;\mathbf{end}\end{aligned}\]</span>
好像FLOPs计算量并没有减少，甚至还略有增加，因为现在每次都需要计算额外的scale</p>
<blockquote>
<p>x值，也就是pre-softmax
logits，由于需要O(N^2)的显存无法放在SRAM中。因此：<br />
1.
要么提前计算好x，保存在全局显存中，需要O(N^2)的显存，容易爆显存。<br />
2.
要么在算法中online计算，每次循环中去load一部分Q，K到片上内存，计算得到x。</p>
</blockquote>
<p>Attention优化的目标就是避开第一种情况，尽可能节省显存，否则，LLM根本无法处理类似100K以上这种long
context的情况。而对于第二种情况，我们不需要保存中间矩阵x，节省了显存，但是计算没有节省，并且增加了HBM
IO Accesses（需要不断地load Q,
K）。此时，2-pass算法相对于3-pass算法，可以减少一次整体的load Q,
K以及减少一次对 xi 的online
recompute，因为在2-pass的第一个pass中， xi 是被两次计算共享的。类似online-softmax这种算法，对应到Attention中的应用，就是Memory
Efficient Attention（注意不是FlashAttention）。</p>
<h1 id="flash-attention">flash attention</h1>
<p>safe
softmax并没有1-pass算法，那么Attention会不会有呢？有！这就是FlashAttention！</p>
<p>在使用online attention的情况下，从头开始计算attention
score的过程如下： <span
class="math inline">\(\operatorname{NOTATIONS}\)</span></p>
<p><span class="math inline">\(Q[k,:]:\)</span>the <span
class="math inline">\(k\)</span>-th row vector of <span
class="math inline">\(Q\)</span> matrix. <span
class="math inline">\(\begin{aligned}O[k,:]:\mathrm{~the~}k\text{-th row
of output }O\mathrm{~matrix.}\\\mathbf{V}[i,i]:\mathrm{~the~}k\text{-th
row of output }O\mathrm{~matrix.}\end{aligned}\)</span> <span
class="math inline">\(V[i,:]{:\text{ the }i\text{-th row of }V\text{
matrix}}.\)</span> <span
class="math inline">\(\{\boldsymbol{o}_i\}{:}\sum_{j=1}^ia_jV[j,:]\)</span>,
a row vector storing partial aggregation result <span
class="math inline">\(A[k,:i]\times V[:i,:]\)</span> BODY</p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span
class="math display">\[\begin{aligned}x_i&amp;\leftarrow\quad
Q[k,:]\:K^T[:,i]\\m_i&amp;\leftarrow\quad\max\left(m_{i-1},x_i\right)\\d_i&#39;&amp;\leftarrow\quad
d_{i-1}&#39;e^{m_{i-1}-m_i}+e^{x_i-m_i}\end{aligned}\]</span> <span
class="math inline">\(\mathbf{end}\)</span></p>
<p><span class="math inline">\(\textbf{for }i\leftarrow 1, N\textbf{
do}\)</span> <span
class="math display">\[\begin{aligned}&amp;a_i\:\leftarrow\:\frac{e^{x_i-m_N}}{d_N^{\prime}}\\&amp;o_i\:\leftarrow\:o_{i-1}+a_i\:V[i,:\:]\end{aligned}\]</span>
<span class="math inline">\(\mathbf{end}\)</span> <span
class="math display">\[O[k,:]\leftarrow\boldsymbol{o}_N\]</span></p>
<p>优化思路和online attention一样，将<span
class="math inline">\(o_{i}\)</span>的计算简化以便于可以写成迭代式。</p>
<p>原来的<span
class="math inline">\(o_{i}\)</span>使用以下方式计算，依赖于全局的<span
class="math inline">\(m_{N}\)</span>和<span
class="math inline">\(d_{N}\)</span>。 <span
class="math display">\[\boldsymbol{o}_i:=\sum_{j=1}^i\left(\frac{e^{x_j-m_N}}{d_N^{\prime}}V[j,:]\right)\]</span>
将其改写成如下形式： <span
class="math display">\[\boldsymbol{o}_i^{\prime}:=\left(\sum_{j=1}^i\frac{e^{x_j-m_i}}{d_i^{\prime}}V[j,:]\right)\]</span>
这样按照上面的方式拓展下去，可以找到一个循环迭代式。</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{o}_i^{\prime}&amp; =\sum_{j=1}^i\frac{e^{x_j-m_i}}{d&#39;}V[j,:]
\\
&amp;= \left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_i}}{d_i^{\prime}}V[j,:]
\right)+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,:] \\
&amp;=
\left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}}\frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}}\frac{d_{i-1}^{\prime}}{d_i^{\prime}}V[j,:]\right)+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,:]
\\
&amp;=
\left(\sum_{j=1}^{i-1}\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\prime}}V[j,.]\right)\frac{d_{i-1}^{\prime}}{d_i^{\prime}}e^{m_{i-1}-m_i}+\frac{e^{x_i-m_i}}{d_i^{\prime}}V[i,.]
\\
&amp;=
\boldsymbol{o}_{i-1}&#39;\frac{d_{i-1}&#39;e^{m_{i-1}-m_i}}{d_i&#39;}+\frac{e^{x_i-m_i}}{d_i&#39;}V[i,:]
\end{aligned}\]</span></p>
<p>这样就找到了<span
class="math inline">\(o_{i}\)</span>的递推表达式。</p>
<p>之后对Q,K进行tiling后计算，得到如下： <span
class="math display">\[\begin{aligned}&amp;\textbf{for
}i\leftarrow1,\#\text{tiles
do}\\&amp;&amp;&amp;\boldsymbol{x}_i\quad\leftarrow\quad Q[k;\cdot]
K^T[\cdot,(i-1) b; i
b]\\&amp;&amp;&amp;m_i^{(\mathrm{local})}=\begin{array}{c}\overset{b}{\operatorname*{max}}\left(\boldsymbol{x}_i[j]\right)\\\end{array}\\&amp;&amp;&amp;m_i
\leftarrow
\max\left(m_{i-1},m_i^{(\mathrm{local})}\right)\\&amp;&amp;&amp;a_i^{\prime}
\leftarrow
d_{i-1}^{\prime}e^{m_{i-1}-m_i}+\sum_{j=1}^be^{\boldsymbol{x}_i[j]-m_i}\\&amp;&amp;&amp;\boldsymbol{o}_i^{\prime}
\leftarrow
\boldsymbol{o}_{i-1}^{\prime}\frac{d_{i-1}^{\prime}e^{m_{i-1}-m_i}}{d_i^{\prime}}+\sum_{j=1}^b\frac{e^{\boldsymbol{x}_i[j]-m_i}}{d_i^{\prime}}V[(i-1)
b+j,:]\\&amp;\text{end}\\&amp;&amp;&amp;O[k,:]\leftarrow\boldsymbol{o}_{N/b}^{\prime}\end{aligned}\]</span>
对于tiles，示意图如下： <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240916201336.png"
alt="image.png" /></p>
<p>可以理解成滑动窗口，<span
class="math inline">\(K^{T}\)</span>从左向右滑动（按列读取），<span
class="math inline">\(V\)</span>从上向下滑动（按行读取）。也可以直接理解成分块矩阵，具体为什么这么做，参考：<a
href="https://zhuanlan.zhihu.com/p/342103911">Cuda 编程之 Tiling - 知乎
(zhihu.com)</a> # 参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/697311739">[KV
Cache优化]🔥MQA/GQA/YOCO/CLA/MLKV笔记: 层内和层间KV Cache共享 - 知乎
(zhihu.com)</a></p>
<p><a
href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Transformers
KV Caching Explained | by João Lages | Medium</a></p>
<p>From Online Softmax to FlashAttention. <a
href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-03-24</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/attention/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/attention/" data-title="Attention" data-hashtags="NLP,Attention"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/attention/" data-hashtag="NLP"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/attention/" data-title="Attention"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/attention/" data-title="Attention"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/attention/" data-title="Attention"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/attention/">Attention</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/ppmi/" class="prev" rel="prev" title="PPMI"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>PPMI</a>
            <a href="/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%AD%E4%BD%8D%E6%95%B0/" class="next" rel="next" title="滑动窗口中位数">滑动窗口中位数<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>

    </div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
