<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Word Embedding - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:url" content="https://blog.vllbc.top/word-embedding/">
  <meta property="og:site_name" content="vllbc02&#39;s blogs">
  <meta property="og:title" content="Word Embedding">
  <meta property="og:description" content="词嵌入 介绍 词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。
词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。
在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中语法分析器和文本情感分析等的效果。
以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word representation)，可以粗略得把它分为三个阶段：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-07-16T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-04-20T00:00:00+00:00">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Word Embedding">
    <meta property="og:image" content="https://blog.vllbc.top/images/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.vllbc.top/images/logo.png">
  <meta name="twitter:title" content="Word Embedding">
  <meta name="twitter:description" content="词嵌入 介绍 词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。
词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。
在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中语法分析器和文本情感分析等的效果。
以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word representation)，可以粗略得把它分为三个阶段：">
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.vllbc.top/word-embedding/" /><link rel="prev" href="https://blog.vllbc.top/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/" /><link rel="next" href="https://blog.vllbc.top/svd/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Word Embedding",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.vllbc.top\/word-embedding\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/blog.vllbc.top\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "NLP, Word Embedding","wordcount":  7434 ,
        "url": "https:\/\/blog.vllbc.top\/word-embedding\/","datePublished": "2021-07-16T00:00:00+00:00","dateModified": "2023-04-20T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.vllbc.top\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Word Embedding</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2021-07-16">2021-07-16</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 7434 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 15 分钟&nbsp;<span id="/word-embedding/" class="leancloud_visitors" data-flag-title="Word Embedding">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="content" id="content"><h1 id="词嵌入">词嵌入</h1>
<h2 id="介绍">介绍</h2>
<blockquote>
<p><strong>词嵌入</strong>是<a
href="https://baike.baidu.com/item/自然语言处理">自然语言处理</a>（NLP）中<a
href="https://baike.baidu.com/item/语言模型">语言模型</a>与<a
href="https://baike.baidu.com/item/表征学习">表征学习</a>技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续<a
href="https://baike.baidu.com/item/向量空间">向量空间</a>中，每个单词或词组被映射为实数<a
href="https://baike.baidu.com/item/域">域</a>上的向量。</p>
<p>词嵌入的方法包括<a
href="https://baike.baidu.com/item/人工神经网络">人工神经网络</a>、对词语同现矩阵<a
href="https://baike.baidu.com/item/降维">降维</a>、<a
href="https://baike.baidu.com/item/概率模型">概率模型</a>以及单词所在上下文的显式表示等。</p>
<p>在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中<a
href="https://baike.baidu.com/item/语法分析器">语法分析器</a>和<a
href="https://baike.baidu.com/item/文本情感分析">文本情感分析</a>等的效果。</p>
</blockquote>
<p>以上是百度百科中对词嵌入的定义。本文只介绍传统的词向量，也就是固定的词向量。deep
contextualized词向量模型在本博客预训练模型内容里面。词嵌入也可以称为词表征(word
representation)，可以粗略得把它分为三个阶段：</p>
<p><strong>一、特征工程阶段，以词袋模型为典型代表。</strong></p>
<p><strong>二、浅层表证阶段，以word2vec为典型代表。</strong></p>
<p><strong>三、深层表征阶段，以基于transformer的Bert为典型代表。</strong></p>
<p>本文介绍了一、二两部分内容</p>
<h2 id="语言模型">语言模型</h2>
<p>一句话，语言模型是这样一个模型：<strong>对于任意的词序列，它能够计算出这个序列是一句话的概率</strong>。</p>
<p>具体的详见本博客有关语言模型的文章。</p>
<p>为什么要介绍语言模型，因为NLP的做预训练一般选择用语言模型任务来做的。</p>
<p>语言模型主要有： N-gram LM、FeedForward Neural Network LM、RNN
LM和GPT系列</p>
<p>本文会涉及到Neural Network
LM，其本质是一个语言模型，词向量只是其在训练过程中的一个副产物。</p>
<h2 id="特征工程阶段基于计数的方法">特征工程阶段(基于计数的方法)</h2>
<h3 id="one-hot">One-Hot</h3>
<p>最简单的方法是将单词表示为 one-hot 向量：对于词汇表中的第 i
个单词，向量在第 i 个维度上具有 1，在其余维度上具有
0。在机器学习中，这是表示分类特征的最简单方法。</p>
<p>One-Hot的缺点很明显，它对自己代表的词一无所知，没有捕捉到词的意义。</p>
<h3 id="词袋模型">词袋模型</h3>
<p>即无视词语的顺序，只关心出现的次数，以下都属于词袋模型。在文本匹配领域也有类似的字面意义的匹配。本博客有相关内容。
### TFIDF</p>
<p>权重也可以作为词向量表示。也可以计算文本相似度等，本博客有相关内容。</p>
<h3 id="共现矩阵">共现矩阵</h3>
<p>详见本博客共现矩阵 ### PPMI 详见本博客互信息相关博文。</p>
<p>中文名字为正点互信息，公式如下</p>
<p><span class="math display">\[
PPMI(w, c) = max(0, PMI(w, c)) \\\\
PMI(w, c) = log\frac{P(w,c)}{P(w)P(c)} = log
\frac{N(w,c)|(w,c)|}{N(w)N(c)}
\]</span></p>
<p>事实证明，word2vec被证明可以隐式逼近PMI矩阵的因式分解。</p>
<pre class="text"><code>『Neural Word Embedding as Implicit Matrix Factorization』这篇论文就详细讨论了这个问题。</code></pre>
<h3 id="主题模型">主题模型</h3>
<p>这个更是重量级，见本博客。</p>
<h2 id="浅层表征阶段基于推理的方法">浅层表征阶段(基于推理的方法)</h2>
<h3 id="nnlm">NNLM</h3>
<h4 id="介绍-1">介绍</h4>
<p>先来张经典的图片</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/NNLM.jpg" /></p>
<p>这就是大名鼎鼎的神经网络语言模型（其它的语言模型详见本博客语言模型部分）</p>
<p>学习任务是输入某个句中单词<span
class="math inline">\(W_t=i\)</span>前面句子的t-1个单词，要求网络正确预测单词<span
class="math inline">\(W_t=i\)</span>，即最大化：</p>
<p><span class="math display">\[
P(W_t=i | W_1, W_2, \dots W_{(t-1)}; \theta)
\]</span></p>
<p>前面任意单词 <span
class="math inline">\(W_i\)</span>用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量
<span class="math inline">\(C(W_i)\)</span>，每个单词的 <span
class="math inline">\(C(W_i)\)</span>
拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。</p>
<p>这个<span class="math inline">\(C(W_i)\)</span>是什么？</p>
<p>这其实就是单词对应的Word
Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word
embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word
embedding值。</p>
<p>所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word
Embedding是被如何学会的。</p>
<h4 id="参数解释">参数解释</h4>
<ul>
<li>训练样本(Context(w),
w),w是语料C的每一个词，Context(w)为取其前n-1个词</li>
<li>投影层向量<span class="math inline">\(X_w\)</span>:
将该训练样本(Context(w),
w)的前n-1个词的词向量首尾拼接在一起，这里的词向量可以用独热编码表示。<span
class="math inline">\(X_w\)</span>的形状为 <span
class="math inline">\((n-1)\times
m\)</span>，这里的m为词汇表的所有词的个数。</li>
<li>隐藏层向量<span class="math inline">\(Z_w\)</span>:</li>
</ul>
<p><span class="math display">\[
Z_w = tanh(WX_w+p)
\]</span></p>
<ul>
<li>输出层向量<span class="math inline">\(y_w\)</span>:
维度为N=|D|,即词典D中词的个数。</li>
</ul>
<p><span class="math display">\[
y_w = Uz_w + q
\]</span> 在对<span class="math inline">\(y_w\)</span>做softmax后，<span
class="math inline">\(y_w\)</span>的分量就表示当前词是w的概率</p>
<p><span class="math display">\[
p(w|Context(w)) = \frac{e^{y_w,iw}}{\sum_{i-1}^N e^{y^{w, i}} }
\]</span></p>
<h4 id="优点与缺点">优点与缺点</h4>
<p>NNLM相对于N-grams语言模型，有以下优点：</p>
<ol type="1">
<li>词语与词语间的相似度可以通过词向量来体现</li>
<li>基于词向量的模型自带『平滑化』功能，无需额外处理。</li>
</ol>
<p>当然也有缺点，缺点就是计算量太大。</p>
<p>下面就重点介绍一下word2vec，相比于NNLM来说这是专门训练词向量的一种工具，而词向量对于NNLM来说只是一个副产物，其本质还是一个语言模型。</p>
<h4 id="代码">代码</h4>
<p>代码来自<a
href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_batch():</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    input_batch <span class="op">=</span> []</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    target_batch <span class="op">=</span> []</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sen <span class="kw">in</span> sentences:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        word <span class="op">=</span> sen.split() <span class="co"># space tokenizer</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> [word_dict[n] <span class="cf">for</span> n <span class="kw">in</span> word[:<span class="op">-</span><span class="dv">1</span>]] <span class="co"># create (1~n-1) as input</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> word_dict[word[<span class="op">-</span><span class="dv">1</span>]] <span class="co"># create (n) as target, We usually call this &#39;casual language model&#39;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        input_batch.append(<span class="bu">input</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        target_batch.append(target)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> input_batch, target_batch</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NNLM(nn.Module):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NNLM, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> nn.Embedding(n_class, m)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> nn.Linear(n_step <span class="op">*</span> m, n_hidden, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d <span class="op">=</span> nn.Parameter(torch.ones(n_hidden))</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.U <span class="op">=</span> nn.Linear(n_hidden, n_class, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> nn.Linear(n_step <span class="op">*</span> m, n_class, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> nn.Parameter(torch.ones(n_class))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.C(X) <span class="co"># X : [batch_size, n_step, m]</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> X.view(<span class="op">-</span><span class="dv">1</span>, n_step <span class="op">*</span> m) <span class="co"># [batch_size, n_step * m]</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        tanh <span class="op">=</span> torch.tanh(<span class="va">self</span>.d <span class="op">+</span> <span class="va">self</span>.H(X)) <span class="co"># [batch_size, n_hidden]</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.b <span class="op">+</span> <span class="va">self</span>.W(X) <span class="op">+</span> <span class="va">self</span>.U(tanh) <span class="co"># [batch_size, n_class]</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    n_step <span class="op">=</span> <span class="dv">2</span> <span class="co"># number of steps, n-1 in paper</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    n_hidden <span class="op">=</span> <span class="dv">2</span> <span class="co"># number of hidden size, h in paper</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="dv">2</span> <span class="co"># embedding size, m in paper</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> [<span class="st">&quot;i like dog&quot;</span>, <span class="st">&quot;i love coffee&quot;</span>, <span class="st">&quot;i hate milk&quot;</span>]</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    word_list <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(sentences).split() <span class="co"># [&#39;i&#39;, &#39;like&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;i&#39;, &#39;love&#39;, &#39;coffee&#39;, &#39;i&#39;, &#39;hate&#39;, &#39;milk&#39;]</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    word_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(word_list)) <span class="co"># [&#39;i&#39;, &#39;like&#39;, &#39;dog&#39;, &#39;love&#39;, &#39;coffee&#39;, &#39;hate&#39;, &#39;milk&#39;]</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    word_dict <span class="op">=</span> {w: i <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(word_list)} <span class="co"># {&#39;i&#39;:0, &#39;like&#39;:1, &#39;dog&#39;:2, &#39;love&#39;:3, &#39;coffee&#39;:4, &#39;hate&#39;:5, &#39;milk&#39;:6}</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    number_dict <span class="op">=</span> {i: w <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(word_list)} <span class="co"># {0:&#39;i&#39;, 1:&#39;like&#39;, 2:&#39;dog&#39;, 3:&#39;love&#39;, 4:&#39;coffee&#39;, 5:&#39;hate&#39;, 6:&#39;milk&#39;}</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    n_class <span class="op">=</span> <span class="bu">len</span>(word_dict)  <span class="co"># number of Vocabulary, just like |V|, in this task n_class=7</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> NNLM()</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    input_batch, target_batch <span class="op">=</span> make_batch()</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    input_batch <span class="op">=</span> torch.LongTensor(input_batch)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    target_batch <span class="op">=</span> torch.LongTensor(target_batch)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5000</span>):</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(input_batch)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output : [batch_size, n_class], target_batch : [batch_size]</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target_batch)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;Epoch:&#39;</span>, <span class="st">&#39;</span><span class="sc">%04d</span><span class="st">&#39;</span> <span class="op">%</span> (epoch <span class="op">+</span> <span class="dv">1</span>), <span class="st">&#39;cost =&#39;</span>, <span class="st">&#39;</span><span class="sc">{:.6f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(loss))</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>    predict <span class="op">=</span> model(input_batch).data.argmax(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>([sen.split()[:<span class="dv">2</span>] <span class="cf">for</span> sen <span class="kw">in</span> sentences], <span class="st">&#39;-&gt;&#39;</span>, [number_dict[n.item()] <span class="cf">for</span> n <span class="kw">in</span> predict.squeeze()])</span></code></pre></div>
<h3 id="word2vec">word2vec</h3>
<p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p>
<p>Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word
embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word
embedding的，这是主产品，所以它完全可以随性地这么去训练网络。
(skip-gram与CBOW只是word2vec的变体。)</p>
<h4 id="训练思路">训练思路</h4>
<p>Word2Vec
是一个模型，其参数是词向量。这些参数针对某个目标进行迭代优化。目标迫使词向量“知道”一个词可能出现的上下文：训练向量以预测相应词的可能上下文。正如您从分布假设中所记得的那样，如果向量“知道”上下文，它们就“知道”单词的含义。</p>
<ul>
<li>获取一个巨大的文本语料库；</li>
<li>使用滑动窗口浏览文本，一次移动一个单词。在每一步，都有一个中心词和上下文词（此窗口中的其他词）；</li>
<li>对于中心词，计算上下文词的概率；</li>
<li>调整向量以增加这些概率。</li>
</ul>
<h4 id="推导">推导</h4>
<p>以Skip-grams模型为例，首先清楚几个概念，中心词、背景词（上下文词）、负采样词。中心词就是我们的输入，因为skip-grams相当于在一句话中扣去一个词，然后用这个词预测这句话的其余词。形式上给人的感觉就是一对多，这里的一句话其实不是一句话，是我们设定的窗口大小，比如一句话”I
miss you very much”，
设置中心词为you，窗口大小为1，那么背景词就是”miss”和”very”。那么对于我们的模型来说，miss和very就是正例，就是我们的预测值(sigmoid后)的值接近于1的，而其余的词就是负例，就是使其值接近于0的。所以负采样就是从这些负例中随机抽取一些负例，不然每次都要计算单词表中所有单词的sigmoid值，这个计算量很大，而使用负采样就大大缩小了计算量。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/window1.png" />
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/window2.png" /></p>
<h5 id="目标函数负似然对数">目标函数：负似然对数</h5>
<p>对于每个位置<span class="math inline">\(t=1,\dots,
T\)</span>，在文本语料库中，Word2Vec在给定中心词的m大小窗口内预测上下文词<span
class="math inline">\(w_t\)</span>:</p>
<p><span class="math display">\[
Likelihood=L(\theta)=\prod_{t=1}^T\prod_{-m\leq j\leq m, j\neq0}
P(w_{t+j} | w_t, \theta)
\]</span></p>
<p>目标函数<span
class="math inline">\(J(\theta)\)</span>为平均负对数似然</p>
<p><span class="math display">\[
Loss = J(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq0}
logP(w_{t+j}|w_t, \theta)
\]</span></p>
<p>对于每个单词w我们都有两个向量：</p>
<ul>
<li><span class="math inline">\(v_w\)</span>当w是中心词</li>
<li><span class="math inline">\(u_w\)</span>当w时背景词
因为word2vec是一个专门训练词向量的神经网络，输入为onehot向量，经过第一层W的计算得到隐藏层，就相当于查表，数值为中心词的词向量，然后再与第二层的W计算得到输出，输出的维度和输入的维度相同，因此输出层的结果在计算上就是<span
class="math inline">\(u_c^Tv_w\)</span>。即输入层对应的中心词的词向量*输出层对应背景词的词向量，再经过softmax就是对应背景词的概率。</li>
</ul>
<p>训练完毕后，我们只使用<span
class="math inline">\(v_w\)</span>，即只使用从输入层到隐藏层的权重。</p>
<p>对于中心词w，上下文词c的概率为：</p>
<p><span class="math display">\[
P(c|w) = \frac{exp(u_c^Tv_w)}{\sum_{o\in V}exp(u_o^Tv_w)}
\]</span></p>
<p>这就是softmax函数。</p>
<h5 id="训练">训练</h5>
<p><span class="math inline">\(\theta^{new} = \theta^{old} - \alpha
\nabla_\theta J(\theta)\)</span></p>
<p>一次进行一次更新，每次更新都是针对一对中心词和其中一个背景词。损失函数：</p>
<p><span class="math display">\[
Loss = J(\theta) = -\frac{1}{T}logL(\theta)=
-\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq0} logP(w_{t+j}|w_t,
\theta)\\\\=\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m,
j\neq0}J_{t,j}(\theta).
\]</span></p>
<p>其中<span class="math inline">\(J_{t,j}(\theta) = -logP(w_{t+j}|w_t,
\theta)\)</span></p>
<p>以”I miss you very
much”这句话为例子，中心词为”you”，其中一个背景词为miss，则损失项为</p>
<p><span class="math display">\[
J_{t,j}(\theta) = -logP(miss|you) =
-log\frac{exp(u_{miss}^Tv_{you})}{\sum_{o\in V}exp(u_o^Tv_{you})} =
\\\\-u_{miss}^Tv_{you}+log\sum_{o\in V} exp(u_o^Tv_{you})
\]</span></p>
<p>这是中心词对应其中一个背景词的损失函数，如果要求总的损失，则将所有背景词的损失相加，然后将所有样本的损失求平均，就是上面的Loss。其实这就是交叉熵损失函数，是个多分类问题，预测的target相当于miss对应的数字，也就是序列值，具体的pytorch代码为</p>
<p><code>loss = (-output_layer[:, Y] + torch.log(torch.sum(torch.exp(output_layer), dim=1))).mean()</code></p>
<p>这里的output_layer就是神经网络的输出层，注意这里的都是对batch操作，这里求出的loss是这个batch上最终的loss，而不是一对中心词与背景词的loss。理解了这个简单的word2vec模型就算理解了。
下面求一下梯度： 注意这里的c为center word, o为上下文词 <span
class="math display">\[
\frac{\partial logP(w_o|w_c)}{\partial v_c} =
\frac{\partial}{\partial
v_c}log\frac{exp(u_0^Tv_c)}{\sum_{i=1}^{|V|}exp(u_i^Tv_c)}\\\\=
\frac{\partial}{\partial v_c}log\, exp(u_o^Tv_c) -
\frac{\partial}{\partial v_c}log\sum_{i=1}^{|V|}exp(u_i^Tv_c)
\]</span></p>
<p>左边的：</p>
<p><span class="math display">\[
\frac{\partial}{\partial v_c}log\,
exp(u_o^Tv_c)=\frac{\partial}{\partial v_c}u_o^Tv_c=u_o
\]</span></p>
<p>第二部分推导</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial {v}_{c}} \log \sum_{i=1}^{|V|} \exp
\left({u}_{i}^{T} {v}_{c}\right) &amp;=\frac{1}{\sum_{i=1}^{|V|} \exp
\left({u}_{i}^{T} {v}_{c}\right)} \cdot \frac{\partial}{\partial
{v}_{c}} \sum_{x=1}^{|V|} \exp \left({u}_{x}^{T} {v}_{c}\right) \\\\
&amp;=\frac{1}{A} \cdot \sum_{x=1}^{|V|} \frac{\partial}{\partial
{v}_{c}} \exp \left({u}_{x}^{T} {v}_{c}\right) \\\\
&amp;=\frac{1}{A} \cdot \sum_{x=1}^{|V|} \exp \left({u}_{x}^{T}
{v}_{c}\right) \frac{\partial}{\partial {v}_{c}} {u}_{x}^{T} {v}_{c}
\\\\
&amp;=\frac{1}{\sum_{i=1}^{|V|} \exp \left({u}_{i}^{T} {v}_{c}\right)}
\sum_{x=1}^{|V|} \exp \left({u}_{x}^{T} {v}_{c}\right) {u}_{x} \\\\
&amp;=\sum_{x=1}^{|V|} \frac{\exp \left({u}_{x}^{T}
{v}_{c}\right)}{\sum_{i=1}^{|V|} \exp \left({u}_{i}^{T} {v}_{c}\right)}
{u}_{x} \\\\
&amp;=\sum_{x=1}^{|V|} P\left(w_{x} \mid w_{c}\right) {u}_{x}
\end{aligned}
\]</span></p>
<p>综上所述</p>
<p><span class="math display">\[
\frac{\partial \log P\left(w_{o} \mid w_{c}\right)}{\partial
{v}_{c}}={u}_{o}-\sum_{j \in V} P\left(w_{j} \mid w_{c}\right) {u}_{j}
\]</span></p>
<p>通过上面计算得到梯度后，我们可以使用随机梯度下降来不断迭代模型参数<span
class="math inline">\(v_c\)</span>。其它模型参数<span
class="math inline">\(u_o\)</span>的迭代方式同理可得。最终，对于词典中任一索引为i的词，我们均得到该词作为中心词和背景词的两组词向量<span
class="math inline">\(v_i\)</span>和<span
class="math inline">\(u_i\)</span></p>
<h4 id="负采样">负采样</h4>
<p>在上面的示例中，对于每对中心词及其上下文词，我们必须更新上下文词的所有向量。这是非常低效的：对于每一步，进行更新所需的时间与词汇量大小成正比。</p>
<p>但是为什么我们必须在每一步都考虑词汇表中的所有上下文向量呢？例如，假设在当前步骤中，我们考虑的不是所有单词的上下文向量，而是当前目标和几个随机选择的单词。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/negative_sampling-min.png" /></p>
<p>以跳字模型为例讨论负采样。词典 <span class="math inline">\(V\)</span>
的大小之所以会在目标函数中出现，是因为中心词 <span
class="math inline">\(w_{c}\)</span> 生成背景词 <span
class="math inline">\(w_{o}\)</span> 的概率 <span
class="math inline">\(P\left(w_{o} \mid w_{c}\right)\)</span> 使用了
softmax，而 softmax 考虑到了背景词可能是词 典中任一词，并体现在了
softmax 的分母上 我们不妨换个角度，假设中心词 <span
class="math inline">\(w_{c}\)</span> 生成背景词 <span
class="math inline">\(w_{o}\)</span>
由以下两个互相独立的联合事件组成来近 似</p>
<ol type="1">
<li><p>中心词 <span class="math inline">\(w_{c}\)</span> 和背景词 <span
class="math inline">\(w_{o}\)</span> 同时出现在该训练数据窗口</p></li>
<li><p>中心词 <span class="math inline">\(w_{c}\)</span>
和噪声词不同时出现在该训练数据窗口</p></li>
</ol>
<ul>
<li>中心词 <span class="math inline">\(w_{c}\)</span> 和第 1 个噪声词
<span class="math inline">\(w_{1}\)</span>
不同时出现在训练数据窗口（噪声词 <span
class="math inline">\(w_{1}\)</span> 按噪声词 分布 <span
class="math inline">\(P(w)\)</span> 随机生成)</li>
<li>….</li>
<li>中心词 <span class="math inline">\(w_{c}\)</span> 和第 <span
class="math inline">\(K\)</span> 个噪声词 <span
class="math inline">\(w_{k}\)</span> 不同时出现在训练数据窗口 (噪声词
<span class="math inline">\(w_{K}\)</span> 按噪声 词分布 <span
class="math inline">\(P(w)\)</span> 随机生成)</li>
</ul>
<p>我们可以使用 <span class="math inline">\(\sigma(x)=\frac{1}{1+\exp
(-x)}\)</span> 函数来表达中心词 <span
class="math inline">\(w_{c}\)</span> 和背景词 <span
class="math inline">\(w_{o}\)</span> 同时出现在训练数据 窗口的概率:</p>
<p><span class="math display">\[
P\left(D=1 \mid w_{o}, w_{c}\right)=\sigma\left({u}_{o}^{T},
{v}_{c}\right)
\]</span></p>
<p>那么，中心词 <span class="math inline">\(w_{c}\)</span> 生成背景词
<span class="math inline">\(w_{o}\)</span> 的对数概率可以近似为</p>
<p><span class="math display">\[
\log P\left(w_{o} \mid w_{c}\right)=\log \left[P\left(D=1 \mid w_{o},
w_{c}\right) \prod_{k=1, w_{k} \sim P(w)}^{K} P\left(D=0 \mid w_{k},
w_{c}\right)\right]
\]</span></p>
<p>其中后面的表示中心词和噪声词不同时出现在训练数据窗口的概率。
假设噪声词 <span class="math inline">\(w_{k}\)</span> 在词典中的索引为
<span class="math inline">\(i_{k}\)</span> ，上式可改写为</p>
<p><span class="math display">\[
\log P\left(w_{o} \mid w_{c}\right)=\log \frac{1}{1+\exp
\left(-{u}_{o}^{T} {v}_{c}\right)}+\sum_{k=1, w_{k} \sim P(w)}^{K} \log
\left[1-\frac{1}{1+\exp \left(-{u}_{i_{k}}^{T} {v}_{c}\right)}\right]
\]</span></p>
<p>因此，有关中心词 <span class="math inline">\(w_{c}\)</span>
生成背景词 <span class="math inline">\(w_{o}\)</span> 的损失函数是(<span
class="math inline">\(1-\sigma(x) = \sigma(-x)\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
-\log P\left(w_{o} \mid w_{c}\right)=-\log \frac{1}{1+\exp
\left(-{u}_{o}^{T} {v}_{c}\right)}-\sum_{k=1, w_{k} \sim P(w)}^{K} \log
\frac{1}{1+\exp \left({u}_{i_{k}}^{T} {v}_{c}\right)}  \\\\
= -\log \sigma(u_o^Tv_c) - \sum_{k=1, w_k\sim P(w)} ^ K \log\sigma
(-u_{ik}^Tv_c)
\end{aligned}
\]</span></p>
<h4 id="与ppmi的关系">与PPMI的关系</h4>
<p>在引入负采样方法后，也就是变成了Skip-Gram with Negative
Sample，简称为SGNS。回顾一下他的目标函数：</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220830151507.png" /></p>
<p>对于word2vec来说，训练完成以后，每一个token都会有两个向量，一个是作为中心词的，一个是作为上下文词的。假设存在一个矩阵<span
class="math inline">\(W=V_{word}*V_{context}\)</span>。</p>
<p>word2vec也可以理解成是对矩阵W的矩阵分解。那么，问题就变成这个matrix
W是什么？</p>
<p>论文<code>NIPS-2014-neural-word-embedding-as-implicit-matrix-factorization-Paper</code>中有将SGNS的目标函数，一步步的变换，变成下面的形式（具体见论文）</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220830151740.png" /></p>
<p>上面的公式，左边的部分就是PMI，PMI是dense
matrix，对模型优化带来很多困难，所以一般会用sparse matrix PPMI（positive
PMI）来替代它。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220830151833.png" /></p>
<p>至此我们找到了矩阵M，即 <img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/Pasted%20image%2020220830152021.png" /></p>
<p>PPMI是基于计数的方法表达词向量，而word2vec是基于推理的方法表达词向量。后面介绍的Glove将基于计数的方法和基于推理的方法结合了起来。
#### 层序softmax</p>
<p>层序softmax利用了二叉树，树的每个节点代表了词典V中的每个词。每个词<span
class="math inline">\(w_i\)</span>对应词向量<span
class="math inline">\(v_i\)</span>。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/softmax.png" /></p>
<p>设 <span class="math inline">\(L(w)\)</span> 为从二叉树根节点到代表词
<span class="math inline">\(w\)</span> 的叶子节点的路径上的节点数，并设
<span class="math inline">\(n(w, i)\)</span> 为该路径上第 <span
class="math inline">\(i\)</span> 个节点，该节点的向量为 <span
class="math inline">\({u}_{n(w, j)}\)</span> 。以上图为例， <span
class="math inline">\(L\left(w_{3}\right)=4\)</span>
。那么，跳字模型和连续词袋模型所需 要计算的任意词 <span
class="math inline">\(w_{i}\)</span> 生成词 <span
class="math inline">\(w\)</span> 的概率为:</p>
<p><span class="math display">\[
P\left(w \mid w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left(\left[n(w,
j+1)=l e f t_{-} \operatorname{child}(n(w, j))\right] \cdot {u}_{n(w,
j)}^{T} {v}_{i}\right)
\]</span></p>
<p>其中，如果 <span class="math inline">\(x\)</span> 为真， <span
class="math inline">\([x]=1\)</span> ；反之 <span
class="math inline">\([x]=-1\)</span> 由于 <span
class="math inline">\(\sigma(x)+\sigma(-x)=1, w_{i}\)</span>
生成词典中任何词的概率之和为 1 :</p>
<p><span class="math display">\[
\sum_{w=1}^{V} P\left(w \mid w_{i}\right)=1
\]</span></p>
<p>上面公式可能比较抽象，下面举个具体的例子，计算 <span
class="math inline">\(w_{i}\)</span> 生成 <span
class="math inline">\(w_{3}\)</span> 的概率，由于在二叉树中由根到 <span
class="math inline">\(w_{3}\)</span>
的路径需要向左、向右、再向左地遍历，所以得到</p>
<p><span class="math display">\[
P\left(w_{3} \mid w_{i}\right)=\sigma\left({u}_{n\left(w_{3},
1\right)}^{T} {v}_{i}\right) \cdot \sigma\left(-{u}_{n\left(w_{3},
2\right)}^{T} {v}_{i}\right) \cdot \sigma\left({u}_{n\left(w_{3},
3\right)}^{T} {v}_{i}\right)
\]</span></p>
<p>由此，我们就可以使用随机梯度下降在跳字模型和连续词袋模型中不断迭代计算词典中所有词向量
<span class="math inline">\({v}\)</span></p>
<p>最后一个问题，层序 softmax 的二叉树是如何建立的?</p>
<p><strong>这里的二叉树 Huffman 树，权重是语料库中 word
出现的频率</strong></p>
<h4 id="标准设置">标准设置</h4>
<ul>
<li>模型: 带负采样的Skip-Gram(SGNS)</li>
<li>负采样的数目: 较小的数据集,15-20;较大的数据集,2-5。</li>
<li>词嵌入维度: 经常用的为300</li>
<li>滑动窗口大小: 5-10</li>
</ul>
<h4 id="代码-1">代码</h4>
<h5 id="简单">简单</h5>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_batch():</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    random_inputs <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    random_labels <span class="op">=</span> []</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    random_index <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(skip_grams)), batch_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> random_index:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        random_inputs.append(np.eye(voc_size)[skip_grams[i][<span class="dv">0</span>]])  <span class="co"># target 相当于onehot编码</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        random_labels.append(skip_grams[i][<span class="dv">1</span>])  <span class="co"># context word</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random_inputs, random_labels</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Word2Vec(nn.Module):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Word2Vec, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># W and WT is not Traspose relationship</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> nn.Linear(voc_size, embedding_size, bias<span class="op">=</span><span class="va">False</span>) <span class="co"># voc_size &gt; embedding_size Weight</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.WT <span class="op">=</span> nn.Linear(embedding_size, voc_size, bias<span class="op">=</span><span class="va">False</span>) <span class="co"># embedding_size &gt; voc_size Weight</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X : [batch_size, voc_size]</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        hidden_layer <span class="op">=</span> <span class="va">self</span>.W(X) <span class="co"># hidden_layer : [batch_size, embedding_size]</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        output_layer <span class="op">=</span> <span class="va">self</span>.WT(hidden_layer) <span class="co"># output_layer : [batch_size, voc_size]</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output_layer</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">2</span> <span class="co"># mini-batch size</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    embedding_size <span class="op">=</span> <span class="dv">2</span> <span class="co"># embedding size</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> [<span class="st">&quot;apple banana fruit&quot;</span>, <span class="st">&quot;banana orange fruit&quot;</span>, <span class="st">&quot;orange banana fruit&quot;</span>,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;dog cat animal&quot;</span>, <span class="st">&quot;cat monkey animal&quot;</span>, <span class="st">&quot;monkey dog animal&quot;</span>]</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    word_sequence <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(sentences).split()</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    word_list <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(sentences).split()</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    word_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(word_list))</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    word_dict <span class="op">=</span> {w: i <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(word_list)}</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    voc_size <span class="op">=</span> <span class="bu">len</span>(word_list)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make skip gram of one size window</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    skip_grams <span class="op">=</span> []</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(word_sequence) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> word_dict[word_sequence[i]]</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> [word_dict[word_sequence[i <span class="op">-</span> <span class="dv">1</span>]], word_dict[word_sequence[i <span class="op">+</span> <span class="dv">1</span>]]]</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> w <span class="kw">in</span> context:</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>            skip_grams.append([target, w])</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Word2Vec()</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5000</span>):</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        input_batch, target_batch <span class="op">=</span> random_batch()</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        input_batch <span class="op">=</span> torch.Tensor(input_batch)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>        target_batch <span class="op">=</span> torch.LongTensor(target_batch)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(input_batch)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target_batch)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&#39;Epoch:&#39;</span>, <span class="st">&#39;</span><span class="sc">%04d</span><span class="st">&#39;</span> <span class="op">%</span> (epoch <span class="op">+</span> <span class="dv">1</span>), <span class="st">&#39;cost =&#39;</span>, <span class="st">&#39;</span><span class="sc">{:.6f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(loss))</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(word_list):</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        W, WT <span class="op">=</span> model.parameters()</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> W[<span class="dv">0</span>][i].item(), W[<span class="dv">1</span>][i].item()</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        plt.scatter(x, y)</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        plt.annotate(label, xy<span class="op">=</span>(x, y), xytext<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">2</span>), textcoords<span class="op">=</span><span class="st">&#39;offset points&#39;</span>, ha<span class="op">=</span><span class="st">&#39;right&#39;</span>, va<span class="op">=</span><span class="st">&#39;bottom&#39;</span>)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code></pre></div>
<p>这是没有任何训练技巧的版本，使用Linear的话要注意输入是独热编码的形式，而且设置bias=False，因为需要的只是权重矩阵，不需要偏置。下面是引入了负采样的技巧的版本，直接使用了nn.Embedding。
##### 复杂</p>
<p>引入了负采样</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.INFO, <span class="bu">format</span><span class="op">=</span><span class="st">&#39;</span><span class="sc">%(asctime)s</span><span class="st"> - </span><span class="sc">%(levelname)s</span><span class="st"> - </span><span class="sc">%(message)s</span><span class="st">&#39;</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="dv">3</span> <span class="co"># window size</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">15</span>  <span class="co"># Number of negitive samples</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>MAX_VOCAB_SIZE <span class="op">=</span> <span class="dv">10000</span> <span class="co"># 单词表的长度</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>EMBEDDING_SIZE <span class="op">=</span> <span class="dv">100</span> <span class="co"># 词嵌入的维数</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span> </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;./text8/text8.train.txt&quot;</span>, <span class="st">&quot;r&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> text.lower().split()</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>vocab_dict <span class="op">=</span> <span class="bu">dict</span>(Counter(text).most_common(MAX_VOCAB_SIZE<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>vocab_dict[<span class="st">&quot;&lt;UNK&gt;&quot;</span>] <span class="op">=</span> <span class="bu">len</span>(text) <span class="op">-</span> np.<span class="bu">sum</span>(<span class="bu">list</span>(vocab_dict.values()))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>word2idx <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab_dict.keys())}</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>idx2word <span class="op">=</span> {i: word <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab_dict.keys())}</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>word_counts <span class="op">=</span> np.array([count <span class="cf">for</span> count <span class="kw">in</span> vocab_dict.values()])</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>word_freqs <span class="op">=</span> word_counts <span class="op">/</span> np.<span class="bu">sum</span>(word_counts)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>word_freqs <span class="op">=</span> word_freqs <span class="op">**</span> (<span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)  <span class="co"># 经验</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WordEmbeddingDataset(Data.Dataset):</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, text, word2idx, word_freqs):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(WordEmbeddingDataset, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_encoded <span class="op">=</span> torch.LongTensor(</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>            [word2idx.get(word, word2idx[<span class="st">&quot;&lt;UNK&gt;&quot;</span>]) <span class="cf">for</span> word <span class="kw">in</span> text])</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2idx <span class="op">=</span> word2idx</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word_freqs <span class="op">=</span> torch.Tensor(word_freqs)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        center_words <span class="op">=</span> <span class="va">self</span>.text_encoded[index]</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        pos_indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(index<span class="op">-</span>C, index)) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            <span class="bu">list</span>(<span class="bu">range</span>(index<span class="op">+</span><span class="dv">1</span>, index<span class="op">+</span>C<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        pos_indices <span class="op">=</span> [i <span class="op">%</span> <span class="bu">len</span>(<span class="va">self</span>.text_encoded) <span class="cf">for</span> i <span class="kw">in</span> pos_indices]</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        pos_words <span class="op">=</span> <span class="va">self</span>.text_encoded[pos_indices]</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        select_weights <span class="op">=</span> copy.deepcopy(<span class="va">self</span>.word_freqs)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        select_weights[center_words] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        select_weights[pos_words] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        neg_words <span class="op">=</span> torch.multinomial(</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            select_weights, K <span class="op">*</span> pos_words.shape[<span class="dv">0</span>], <span class="va">True</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> center_words, pos_words, neg_words</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.text_encoded)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> WordEmbeddingDataset(text, word2idx, word_freqs)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> Data.DataLoader(dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_size):</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_size)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding2 <span class="op">=</span> nn.Embedding(vocab_size, embedding_size)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_lables, pos_labels, neg_labels):</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        input_embedding <span class="op">=</span> <span class="va">self</span>.embedding(input_lables) <span class="co"># [batch_size, embedding_size]</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        pos_embedding <span class="op">=</span> <span class="va">self</span>.embedding2(pos_labels) <span class="co"># [batch_size, window * 2, embedding_size]</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        neg_embedding <span class="op">=</span> <span class="va">self</span>.embedding2(neg_labels) <span class="co"># [batch_size, K * window * 2, embedding_size]</span></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        input_embedding <span class="op">=</span> input_embedding.unsqueeze(<span class="dv">2</span>) <span class="co"># [batch_size, embedding_size, 1]</span></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        pos_dot <span class="op">=</span> torch.bmm(pos_embedding, input_embedding).squeeze(<span class="dv">2</span>) <span class="co"># [batch_size, window * 2]</span></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>        <span class="co">## bmm == batch matrix multiply</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>        neg_dot <span class="op">=</span> torch.bmm(neg_embedding, input_embedding).squeeze(<span class="dv">2</span>) <span class="co"># [batch_size, K * window * 2]</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>        log_pos <span class="op">=</span> F.logsigmoid(pos_dot).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># [batch_size]</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>        log_neg <span class="op">=</span> F.logsigmoid(<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> neg_dot).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># [batch_size]</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>(log_pos <span class="op">+</span> log_neg).mean() <span class="co"># 对应上文推导的公式</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> input_embedding(<span class="va">self</span>):</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding.weight.detach().cpu().numpy()</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(MAX_VOCAB_SIZE, EMBEDDING_SIZE).to(device)</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (input_labels, pos_labels, neg_labels) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        input_labels <span class="op">=</span> input_labels.<span class="bu">long</span>().to(device)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>        pos_labels <span class="op">=</span> pos_labels.<span class="bu">long</span>().to(device)</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>        neg_labels <span class="op">=</span> neg_labels.<span class="bu">long</span>().to(device)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(input_labels, pos_labels, neg_labels)</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            logging.info(<span class="ss">f&quot;epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, batch: </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>embedding_weights <span class="op">=</span> model.input_embedding()</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="ss">f&quot;embedding-</span><span class="sc">{</span>EMBEDDING_SIZE<span class="sc">}</span><span class="ss">.pth&quot;</span>)</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;word2idx.pkl&quot;</span>, <span class="st">&quot;wb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    pickle.dump(word2idx, f)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;idx2word.pkl&quot;</span>, <span class="st">&quot;wb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    pickle.dump(idx2word, f)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;embedding_weights.pkl&quot;</span>, <span class="st">&quot;wb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>    pickle.dump(embedding_weights, f)</span></code></pre></div>
<h3 id="glove">GloVe</h3>
<p>GloVe 模型是基于计数的方法和预测方法（例如 Word2Vec）的组合。模型名称
GloVe 代表“Global Vectors”，体现了它的思想：该方法利用
语料库中的全局信息来学习向量。</p>
<p>正如我们之前看到的，最简单的基于计数的方法使用共现计数来衡量单词 w
和上下文c之间的关联：<span class="math inline">\(N(w,c)\)</span>。 GloVe
也使用这些计数来构建损失函数：</p>
<p><span class="math display">\[
J(\theta) = \sum_{w,c\in V}f(N(w,c))
\cdot(u_c^Tv_w+b_c+\bar{b_w}-logN(w,c)^2)
\]</span></p>
<p>其中： <span class="math display">\[
f(x) = \begin{cases}
        (\frac{x}{x_{max}})^{0.75}, \quad if \;x &lt; x_{max} \\\\
        1, \quad if \; x\geq x_{max}
        \end{cases}
\]</span></p>
<p>具体的推导可以看这个博客：<a
href="https://www.cnblogs.com/Lee-yl/p/11172255.html">https://www.cnblogs.com/Lee-yl/p/11172255.html</a></p>
<p>可以看到GloVe并没有使用神经网络的方法。</p>
<p>与 Word2Vec 类似，我们也有不同的
中心词和上下文词向量——这些是我们的参数。此外，该方法对每个词向量都有一个标量偏置项。</p>
<p>特别有趣的是 GloVe 控制稀有词和频繁词影响的方式：每对 ( w , c )
的损失以如下方式加权</p>
<ul>
<li>罕见事件受到惩罚，</li>
<li>非常频繁的事件不会被过度加权。</li>
</ul>
<h2 id="关于word-embedding">关于word embedding</h2>
<ol type="1">
<li>word embedding只能通过语言模型的训练获取吗？</li>
</ol>
<p>不是的；事实上任何NLP任务都可以在训练过程中获取word
embedding，甚至特定任务下的word
embedding在特定任务中的使用效果还会更好（如基于fasttext的文本分类任务）。之所以我们平时提及的word
embedding均是在语言模型任务中产生，私以为主要有这么几个原因：a).语言模型是无监督任务，存在海量训练语料，无需标注成本；b).语言模型任务本身要求较高，训练过程中可以学习大量的语义知识，进而生成高质的word
representation。</p>
<ol start="2" type="1">
<li>如何评估word embedding好坏？</li>
</ol>
<p>有两种方式：第一种，把word
embedding融入现有系统中，看其对系统性能的提升；第二，从语言学的角度对word
embedding进行分析，如相似度、语义偏移等。更细节的可以参考<a
href="https://www.zhihu.com/question/37489735">这里</a>。 ## 参考</p>
<p>参考：</p>
<blockquote>
<p><a
href="https://blog.csdn.net/malefactor/article/details/83961886">https://blog.csdn.net/malefactor/article/details/83961886</a>
<a
href="https://www.zybuluo.com/Dounm/note/591752">https://www.zybuluo.com/Dounm/note/591752</a>
<a
href="https://lena-voita.github.io/nlp_course/word_embeddings.html">https://lena-voita.github.io/nlp_course/word_embeddings.html</a>
<a
href="https://wmathor.com/index.php/archives/1430/">https://wmathor.com/index.php/archives/1430/</a>
<a
href="https://zhuanlan.zhihu.com/p/27234078">https://zhuanlan.zhihu.com/p/27234078</a><br />
Rong X . word2vec Parameter Learning Explained[J]. Computer Science,
2014.</p>
</blockquote>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-04-20</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/word-embedding/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 X" data-sharer="x" data-url="https://blog.vllbc.top/word-embedding/" data-title="Word Embedding" data-hashtags="NLP,Word Embedding"><i class="fab fa-x-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.vllbc.top/word-embedding/" data-hashtag="NLP"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.vllbc.top/word-embedding/" data-title="Word Embedding"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.vllbc.top/word-embedding/" data-title="Word Embedding"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.vllbc.top/word-embedding/" data-title="Word Embedding"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/word-embedding/">Word Embedding</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/" class="prev" rel="prev" title="三数之和"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>三数之和</a>
            <a href="/svd/" class="next" rel="next" title="SVD">SVD<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article>

    </div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js"></script><script>window.config={"comment":{"valine":{"appId":"Gf5fGIr3qceViiX6xGtzaWwR-gzGzoHsz","appKey":"5FiaGPazjefFXh6wr3CtcX2d","avatar":"hide","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":true,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"如何评价这篇博文？","recordIP":true,"visitor":true}},"lightgallery":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
