# llamaç³»åˆ—

# LLaMAä»‹ç»

LLaMA æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œæ•ˆæœæœ€å¥½çš„å¼€æº LLM ä¹‹ä¸€ã€‚

> **è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ï¼šç›¸æ¯”äº****GPT****ï¼Œæ›´å°çš„æ¨¡å‹+æ›´å¤šçš„****è®­ç»ƒæ•°æ®****ä¹Ÿå¯ä»¥è·å¾—å¯æ¯”çš„æ•ˆæœ

åŸºäºæ›´å¤š tokens çš„è®­ç»ƒé›†ï¼Œåœ¨å„ç§æ¨ç†é¢„ç®—ä¸‹ï¼Œè®­ç»ƒå‡ºæ€§èƒ½æœ€ä½³çš„ä¸€ç³»åˆ—è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸º `LLaMA`ï¼Œå‚æ•°èŒƒå›´ä» 7B åˆ° 65B ä¸ç­‰ï¼Œä¸ç°æœ‰æœ€ä½³ LLM ç›¸æ¯”ï¼Œå…¶æ€§èƒ½æ˜¯æœ‰ç«äº‰åŠ›çš„ã€‚æ¯”å¦‚ï¼ŒLLaMA-13B åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ä¼˜äº GPT-3ï¼Œå°½ç®¡å…¶å°ºå¯¸åªæœ‰ GPT-3 çš„ååˆ†ä¹‹ä¸€ã€‚ä½œè€…ç›¸ä¿¡ï¼ŒLLaMA å°†æœ‰åŠ©äºä½¿ LLM çš„ä½¿ç”¨å’Œç ”ç©¶å¹³æ°‘åŒ–ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œï¼åœ¨è§„æ¨¡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼ŒLLaMA-65B ä¹Ÿå…·æœ‰ä¸æœ€ä½³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Chinchilla æˆ– PaLM-540Bï¼‰ç›¸ç«äº‰çš„èƒ½åŠ›ã€‚

  

LLaMA1ã€2çš„ä¸»è¦å·®åˆ«åœ¨è®­ç»ƒä¸Šä¸‹æ–‡é•¿åº¦ã€è®­ç»ƒtokenæ•°ã€æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠå¯¹é½æ–¹æ³•ä¸Šã€‚

| æ¨¡å‹     | è®­ç»ƒé•¿åº¦  | åˆ†è¯å™¨                 | è¯è¡¨å¤§å° | ä½ç½®ç¼–ç  | æ¿€æ´»å±‚    | æ ‡å‡†åŒ–                   | è®­ç»ƒtokenæ•°                                | é“¾æ¥                               | ç²¾åº¦   | æ³¨æ„åŠ›æœºåˆ¶              | æœ‰æ— chatç‰ˆæœ¬ | Alignment          |
| ------ | ----- | ------------------- | ---- | ---- | ------ | --------------------- | --------------------------------------- | -------------------------------- | ---- | ------------------ | -------- | ------------------ |
| LLaMA  | 2,048 | BPEï¼ˆSentence-Pieceï¼‰ | 32k  | ROPE | SwiGLU | åŸºäº RMSNorm çš„ Pre-Norm | 1ä¸‡äº¿(6.7B,13B)<br><br>1.4ä¸‡äº¿ï¼ˆ32.5B,65.2Bï¼‰ | http://arxiv.org/abs/2302.13971  | fp16 | MHA                | 0        |                    |
| LLaMA2 | 4,096 | åŒä¸Š                  | 32k  | ROPE | åŒä¸Š     | åŒä¸Š                    | 2ä¸‡äº¿                                     | https://arxiv.org/abs/2307.09288 | bf16 | 34B,70B GQA, å…¶ä»–MHA | 1        | SFT+RLHF(æ‹’ç»é‡‡æ ·+PPO) |

ï¼ˆè¡¨æ¥è‡ª[LLaMAå®¶æ—](https://y0y1rx552jl.feishu.cn/docx/PiNOdEhjVoo77jxv3xVcJaUVnFp)ï¼‰

# LLaMA1

## è®­ç»ƒæ•°æ®

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210153528.png)

## è®­ç»ƒå‚æ•°

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210170944.png)


## RMSnorm

ä¸ Layer Norm ç›¸æ¯”ï¼ŒRMS Normçš„ä¸»è¦åŒºåˆ«åœ¨äºå»æ‰äº†å‡å»å‡å€¼çš„éƒ¨åˆ†ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š


$$\overline{a}_{i}=\frac{a_{i}}{RMS(a)}$$

å…¶ä¸­

$$RMS(a)=\sqrt{\frac{1}{n}\Sigma_{i=1}^{n}a_{i}^{2}} \\ $$

  

æ­¤å¤–RMSNorm è¿˜å¯ä»¥å¼•å…¥å¯å­¦ä¹ çš„ç¼©æ”¾å› å­gï¼Œä»è€Œå¾—åˆ°

$$\overline{a}_i=\frac{a_i}{RMS(\boldsymbol{a})}g_i$$

### Pre-normå’ŒPost-norm

æ³¨æ„å…¶ä½¿ç”¨çš„æ˜¯Pre-normç»“æ„ï¼Œä¸Post-normç»“æ„å·®å¼‚å¦‚ä¸‹ï¼š
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171047.png)



å…³äºPre Normçš„æ•ˆæœå’ŒPost Normæ•ˆæœå·®å¼‚ï¼Œç›¸å…³åˆ†æåœ¨è¿™ä¸¤ç¯‡æ–‡ç« ä¸­ï¼š

**[æ¨¡å‹ä¼˜åŒ–æ¼«è°ˆï¼šBERTçš„åˆå§‹æ ‡å‡†å·®ä¸ºä»€ä¹ˆæ˜¯0.02ï¼Ÿ](https://kexue.fm/archives/8747)**

**[ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ](https://kexue.fm/archives/9009)**

æ€»ç»“æ¥è¯´å°±æ˜¯Pre-normåŠ æ·±çš„æ˜¯æ¨¡å‹çš„å®½åº¦ï¼Œè€Œä¸æ˜¯æ·±åº¦ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸å¦‚Post-normï¼Œä½†å¯ä»¥ç¼“è§£Post-normçš„æ¢¯åº¦æ¶ˆå¤±ã€‚

### ä»£ç 

```Python
class LlamaRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        LlamaRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps # eps é˜²æ­¢å–å€’æ•°ä¹‹ååˆ†æ¯ä¸º0
    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) # rsqrt å³sqrtåå–å€’æ•°
        # weight æ˜¯æœ«å°¾ä¹˜çš„å¯è®­ç»ƒå‚æ•°, å³g_i
        return (self.weight * hidden_states).to(input_dtype)
```

  

## RoPE

RoPE çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œé€šè¿‡ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç â€ï¼Œå¯ä»¥è¯´æ˜¯å…·å¤‡äº†ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹ä¾¿æ€§ï¼ŒåŒæ—¶å¯ä»¥è¡¨ç¤ºä¸åŒ token ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚RoPE æ˜¯å°†ä½ç½®ç¼–ç å’Œ queryæˆ–è€…keyè¿›è¡Œç›¸ä¹˜ã€‚

  

$$\begin{bmatrix}\cos m\theta_0&-\sin m\theta_0&0&0&\cdots&0&0\\\sin m\theta_0&\cos m\theta_0&0&0&\cdots&0&0\\0&0&\cos m\theta_1&-\sin m\theta_1&\cdots&0&0\\0&0&\sin m\theta_1&\cos m\theta_1&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&0&0&\cdots&\cos m\theta_{d/2-1}&-\sin m\theta_{d/2-1}\\0&0&0&0&\cdots&\sin m\theta_{d/2-1}&\cos m\theta_{d/2-1}\end{bmatrix}\begin{bmatrix}q_0\\q_1\\q_2\\q_3\\\vdots\\q_{d-2}\\q_{d-1}\end{bmatrix}$$

  

ç”±äºçŸ©é˜µå¤ªç¨€ç–ï¼Œä¼šé€ æˆæµªè´¹ï¼Œå› æ­¤è®¡ç®—æ—¶æ˜¯è¿™ä¹ˆåšçš„ï¼š

  

$$\begin{bmatrix}q_0\\q_1\\q_2\\q_3\\\vdots\\q_{d-2}\\q_{d-1}\end{bmatrix}\otimes\begin{bmatrix}\cos m\theta_0\\\cos m\theta_0\\\cos m\theta_1\\\cos m\theta_1\\\vdots\\\cos m\theta_{d/2-1}\\\cos m\theta_{d/2-1}\end{bmatrix}+\begin{bmatrix}-q_1\\q_0\\-q_3\\q_2\\\vdots\\-q_{d-1}\\q_{d-2}\end{bmatrix}\otimes\begin{bmatrix}\sin m\theta_0\\\sin m\theta_0\\\sin m\theta_1\\\sin m\theta_1\\\vdots\\\sin m\theta_{d/2-1}\\\sin m\theta_{d/2-1}\end{bmatrix}$$

  

æ­¤å¤–ï¼Œè§’åº¦çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

  

$$\theta_j=10000^{-2j/d},j\in[1,2,\dots,d/2]$$

### ä»£ç 

```Python
class LlamaRotaryEmbedding(torch.nn.Module):

    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
        self.register_buffer("inv_freq", inv_freq)
        # Build here to make `torch.jit.trace` work.
        self.max_seq_len_cached = max_position_embeddings
        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device,
        dtype=self.inv_freq.dtype)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation
        # in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        dtype = torch.get_default_dtype()
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :].to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :].to(dtype), persistent=False)
        
    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        # This `if` block is unlikely to be run after we build sin/cos in `__init__`.
        # Keep the logic here just in case.
        if seq_len > self.max_seq_len_cached:
            self.max_seq_len_cached = seq_len
            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
            # Different from paper, but it uses a different permutation
            # in order to obtain the same calculation
            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            self.register_buffer("cos_cached", emb.cos()[None, None, :, :].to(x.dtype),
            persistent=False)
            self.register_buffer("sin_cached", emb.sin()[None, None, :, :].to(x.dtype),
            persistent=False)
    
        return (
        self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        )
    def rotate_half(x):
        """Rotates half the hidden dims of the input."""
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
        # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
        cos = cos.squeeze(1).squeeze(0) # [seq_len, dim]
        sin = sin.squeeze(1).squeeze(0) # [seq_len, dim]
        cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim]
        sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim]
        q_embed = (q * cos) + (rotate_half(q) * sin)
        k_embed = (k * cos) + (rotate_half(k) * sin)
        return q_embed, k_embed
```

## SwiGLU

  

$$\begin{aligned} \mathrm{FFN}_{\mathrm{SwiGLU}}(x,W,V,W_{2})&=\mathrm{SwiGLU}(x,W,V)W_{2}\\\mathrm{SwiGLU}(x,W,V)&=\mathrm{Swish}_{\beta}(xW)\otimes xV\\\mathrm{Swish}_{\beta}(x)&=x\sigma(\beta x) \end{aligned}$$

  

### ä»£ç 

```Python
class LlamaMLP(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
    ):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        # config ä¸­ hidden_act = 'silu'
        # 'silu' å’Œ 'swish' å¯¹åº”çš„æ¿€æ´»å‡½æ•°å‡ä¸ºï¼šSiLUActivation 
        # https://github.com/huggingface/transformers/blob/717dadc6f36be9f50abc66adfd918f9b0e6e3502/src/transformers/activations.py#L229
        self.act_fn = ACT2FN[hidden_act]

    def forward(self, x):
        # å¯¹åº”ä¸Šè¿°å…¬å¼çš„ SwiGLU
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

  

## å®éªŒç»“æœ

### å¸¸è¯†æ¨ç†ä»»åŠ¡
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171109.png)



- LLaMA - 13Bæ¨¡å‹è™½ç„¶æ¯”GPT - 3å°10å€ï¼Œä½†åœ¨å¤§å¤šæ•°åŸºå‡†ä¸Šä¹Ÿä¼˜äºGPT - 3ã€‚
- é™¤BoolQå¤–ï¼ŒLLaMA - 65Båœ¨æ‰€æœ‰æŠ¥å‘Šçš„åŸºå‡†ä¸Šéƒ½ä¼˜äºChinchilla-70Bã€‚
- é™¤äº†åœ¨BoolQå’ŒWinoGrandeä¸Šï¼ŒLLaMA-65Båœ¨æ‰€æœ‰åœ°æ–¹éƒ½è¶…è¿‡äº†PaLM540Bã€‚

### é˜…è¯»ç†è§£ä»»åŠ¡
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171018.png)


- å¯ä»¥çœ‹åˆ°ï¼ŒLLaMA-13Bæ¯”GPT-3é«˜å‡ºäº†å‡ ä¸ªç™¾åˆ†ç‚¹ã€‚
- LLaMA-65Bçš„è¡¨ç°å·²ç»æ¥è¿‘ç”šè‡³è¶…è¶ŠPaLM-540Bçš„è¡¨ç°ã€‚

# LLaMA2

**Llama1åªåšäº†é¢„è®­ç»ƒï¼ŒLlama2åšäº†é¢„è®­ç»ƒ+SFT+RLHF**
![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171118.png)


## KV Cache

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171127.png)

LLMæ¨ç†è¿‡ç¨‹åˆ†ä¸ºPrefillå’ŒDecodeä¸¤ä¸ªé˜¶æ®µã€‚

- Prefillé˜¶æ®µä¼šå¯¹Promptä¸­æ‰€æœ‰çš„tokenåš`å¹¶è¡Œè®¡ç®—`ï¼Œå¾—åˆ°Promptä¸­æ‰€æœ‰Tokensçš„KV Cacheä»¥åŠè®¡ç®—å¾—åˆ°`ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªToken`ã€‚Prompté˜¶æ®µTokenè®¡ç®—å¾—åˆ°çš„KV Cacheä¼šä¿å­˜ä¸‹æ¥ï¼Œç•™ç»™Decodeé˜¶æ®µå¤ç”¨ã€‚
- Decodeé˜¶æ®µæ˜¯ä¸€ä¸ªè‡ªå›å½’è¿‡ç¨‹ï¼Œæ¯decodeä¸€ä¸ªæ–°çš„Tokenï¼Œéƒ½éœ€è¦ç”¨åˆ°æ‰€æœ‰ä¹‹å‰è®¡ç®—å¾—åˆ°çš„KV Cacheæ¥è®¡ç®—å½“å‰query tokençš„Attentionã€‚å› æ­¤ï¼Œå½“è¾“å‡ºé•¿åº¦è¶Šæ¥è¶Šå¤§æˆ–è€…contextå¾ˆé•¿æ—¶ï¼ŒKV Cacheå°†ä¼šå ç”¨å¤§é‡çš„æ˜¾å­˜ã€‚

### ä½¿ç”¨KV cacheæ—¶ä½ç½®ä¿¡æ¯æ€ä¹ˆæ³¨å…¥ï¼Ÿ

> åˆæ¬¡å­¦ä¹ KV cacheæ—¶ï¼Œè™½ç„¶åŸç†æ¯”è¾ƒç®€å•æ˜“æ‡‚ï¼Œä½†æ˜¯å¯¹äºåç»­çš„è¾“å…¥åªæœ‰ä¸€ä¸ªtokenè¿™é‡Œäº§ç”Ÿäº†äº›è®¸å›°æƒ‘ï¼Œåç»­åªè¾“å…¥ä¸€ä¸ªtokençš„è¯ï¼Œä½ç½®ç¼–ç è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿäºæ˜¯æˆ‘æ¯”è¾ƒç®€å•ç²—æš´åœ°çŒœæµ‹ä½ç½®indexéšç€æ¨ç†ä¸æ–­æ›´æ–°ï¼Œå½“æ—¶ç¿»äº†å„ç§èµ„æ–™ä¹Ÿæ²¡æœ‰å¾—åˆ°è§£é‡Šï¼Œåé¢ç¿»äº†ç¿»llamaçš„æºç ï¼Œå‘ç°æˆ‘çš„çŒœæµ‹è¿˜çœŸæ˜¯æ­£ç¡®çš„ã€‚

```Python
def forward(self, tokens: torch.Tensor, start_pos: int):
        """
        Perform a forward pass through the Transformer model.

        Args:
            tokens (torch.Tensor): Input token indices.
            start_pos (int): Starting position for attention caching.

        Returns:
            torch.Tensor: Output logits after applying the Transformer model.

        """
        _bsz, seqlen = tokens.shape
        h = self.tok_embeddings(tokens)
        self.freqs_cis = self.freqs_cis.to(h.device)
        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

        mask = None
        if seqlen > 1:
            mask = torch.full(
                (seqlen, seqlen), float("-inf"), device=tokens.device
            )

            mask = torch.triu(mask, diagonal=1)

            # When performing key-value caching, we compute the attention scores
            # only for the new sequence. Thus, the matrix of scores is of size
            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for
            # j > cache_len + i, since row i corresponds to token cache_len + i.
            mask = torch.hstack([
                torch.zeros((seqlen, start_pos), device=tokens.device),
                mask
            ]).type_as(h)

        for layer in self.layers:
            h = layer(h, start_pos, freqs_cis, mask)
        h = self.norm(h)
        output = self.output(h).float()
        return output
```

å¯ä»¥çœ‹åˆ°forwardå‡½æ•°ä¸­çš„start_poså‚æ•°ä»£è¡¨ç€ä½ç½®ä¿¡æ¯ï¼Œfreqs_cisæ˜¯å®ç°RoPEä½ç½®ç¼–ç éœ€è¦ç”¨åˆ°çš„ã€‚

æ³¨æ„ `freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]`è¿™ä¸€è¡Œï¼Œå³æ˜¯å®ç°äº†ropeç›¸å¯¹ä½ç½®ç¼–ç çš„kv cacheçš„æ ¸å¿ƒã€‚

### ä»£ç 

```Python
class Attention(nn.Module):
        # ...
        self.cache_k = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()
        self.cache_v = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()

    def forward(
            self,
            x: torch.Tensor,
            start_pos: int,
            freqs_cis: torch.Tensor,
            mask: Optional[torch.Tensor],
    ):
        # å‡è®¾å½“å‰xä¸º(1, 1, dim)ï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸€ä¸ªé¢„æµ‹çš„token
        # self-attentionçš„è¾“å…¥ï¼Œæ ‡å‡†çš„(bs, seqlen, hidden_dim)
        bsz, seqlen, _ = x.shape
        # è®¡ç®—å½“å‰tokençš„qkv 
        # q k våˆ†åˆ«è¿›è¡Œæ˜ å°„ï¼Œæ³¨æ„è¿™é‡Œkey, valueä¹Ÿéœ€è¦å…ˆç”±è¾“å…¥è¿›è¡Œæ˜ å°„å†å’Œkv_cacheé‡Œé¢çš„key, valueè¿›è¡Œæ‹¼æ¥
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        
        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        # å¯¹å½“å‰è¾“å…¥çš„queryå’Œkeyè¿›è¡ŒRoPEï¼Œæ³¨æ„kv_cacheé‡Œé¢çš„keyå·²ç»åšè¿‡äº†RoPE
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        # ç¼“å­˜å½“å‰tokençš„kv
        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)
        self.cache_k[:bsz, start_pos: start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos: start_pos + seqlen] = xv

        # å–å‡ºå‰seqlenä¸ªtokençš„kvç¼“å­˜
        # å–å‡ºå…¨éƒ¨ç¼“å­˜çš„keyå’Œvalueï¼ˆåŒ…æ‹¬ä¹‹å‰åœ¨cacheé‡Œé¢çš„å’Œæœ¬æ¬¡è¾“å…¥çš„ï¼‰ï¼Œä½œä¸ºæœ€ç»ˆçš„keyå’Œvalue
        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        # å°†kvé‡å¤å¡«å……ï¼Œä½¿kvå’Œqçš„å¤´æ•°ä¸ªæ•°ç›¸åŒ
        # repeat k/v heads if n_kv_heads < n_headsï¼Œå¯¹é½å¤´çš„æ•°é‡
        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)
        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)
        
        # è®¡ç®—å½“å‰tokençš„attention scoreï¼Œï¼Œæ³¨æ„maskéœ€è¦åŠ ä¸Šï¼Œå¦å¤–ç»´åº¦è¦å¯¹åº”ä¸Š
        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        values = values.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
```

## MQA&GQA

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171158.png)



### ä¸ºä»€ä¹ˆä¸ç»§ç»­ä½¿ç”¨MHAï¼Ÿ

- æ ‡å‡†çš„mhaä¸­ï¼ŒKV headsçš„æ•°é‡å’ŒQuery headsçš„æ•°é‡ç›¸åŒï¼Œæ¯ä¸€ä¸ªq headå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„kv headï¼Œä½†è¿™æ ·çš„å¼€é”€æ¯”è¾ƒå¤§ã€‚
    

### MQA

- æ ‡å‡†çš„MHAä¸­ï¼ŒKV headsçš„æ•°é‡å’ŒQuery headsçš„æ•°é‡ç›¸åŒï¼Œæ¯ä¸€ä¸ªq headå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„kv headï¼Œä½†è¿™æ ·çš„å¼€é”€æ¯”è¾ƒå¤§ã€‚
    
- **MQAæ¯”è¾ƒæç«¯ï¼Œåªä¿ç•™ä¸€ä¸ªKV Headï¼Œå¤šä¸ªQuery Headså…±äº«ç›¸åŒçš„KV Head**ã€‚è¿™ç›¸å½“äºä¸åŒHeadçš„Attentionå·®å¼‚ï¼Œå…¨éƒ¨éƒ½æ”¾åœ¨äº†Queryä¸Šï¼Œéœ€è¦æ¨¡å‹ä»…ä»ä¸åŒçš„Query Headsä¸Šå°±èƒ½å¤Ÿå…³æ³¨åˆ°è¾“å…¥hidden statesä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæå¤§åœ°é™ä½äº†KV Cacheçš„éœ€æ±‚ï¼Œä½†æ˜¯ä¼šå¯¼è‡´æ¨¡å‹æ•ˆæœæœ‰æ‰€ä¸‹é™ã€‚
    

### GQA

- GQAå°±æ˜¯åœ¨MHAå’ŒMQAä¹‹é—´åšäº†ä¸€ä¸ªå¹³è¡¡ã€‚å¯¹query headsè¿›è¡Œåˆ†ç»„ï¼Œåˆ†æˆå‡ ç»„å°±å¯¹åº”å¤šå°‘ä¸ªkv headsï¼Œç„¶åæ¯ä¸€ç»„å†…çš„query Headså…±äº«ç›¸åŒçš„KV headã€‚
- GQAå¯ä»¥åœ¨å‡å°‘è®¡ç®—é‡å’ŒKV CacheåŒæ—¶ç¡®ä¿æ¨¡å‹æ•ˆæœä¸å—åˆ°å¤§çš„å½±å“ã€‚

## SFT

ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰æ˜¯å¯¹å·²ç»é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œä»¥æé«˜å…¶åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸åœ¨å¤§é‡é€šç”¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦åˆ°å¹¿æ³›çš„è¯­è¨€çŸ¥è¯†å’Œç‰¹å¾ã€‚åœ¨SFTè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è°ƒæ•´ï¼Œä½¿å…¶æ›´é€‚åˆè¯¥ä»»åŠ¡ã€‚

SFTæ•°æ®ä¸€èˆ¬å°±æ˜¯<prompt, response>æ•°æ®å¯¹ã€‚åœ¨è®­ç»ƒæ–¹å¼ä¸Šå’Œpretrainæ²¡æœ‰ä»»ä½•åŒºåˆ«ï¼Œå³å¾—åˆ°å½“å‰tokenå¯¹åº”çš„logitï¼Œä»¥next tokenä½œä¸ºæ ‡ç­¾è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚

> pretrain æ˜¯åœ¨èƒŒä¹¦ï¼Œçº¯ç²¹çš„å­¦ä¹ çŸ¥è¯†ï¼›sft åˆ™æ˜¯åœ¨åšé¢˜ï¼Œå­¦ä¹ çš„æ˜¯æŒ‡ä»¤ follow èƒ½åŠ›ã€‚

### ä¸€äº›è¦ç‚¹

- å°‘é‡é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒæ¨¡å‹çš„æ•ˆæœï¼Œè¦å¥½äºå¤§é‡ä½è´¨é‡æ•°æ®é›†çš„è®­ç»ƒæ•ˆæœã€‚åˆ†ææ•°æ®å’Œæ¸…æ´—æ•°æ®å°±æ˜¯ sft é˜¶æ®µ 90% çš„å·¥ä½œé‡ã€‚
    
- sft ä¼šè®©æ¨¡å‹è§åˆ°æœ€é‡è¦çš„ eos_tokenï¼Œpretrain æ¨¡å‹å› ä¸ºæ²¡è§è¿‡è¯¥ token è€Œæ— æ³•åœæ­¢ç”Ÿæˆã€‚
    
- sft çš„ prompt ä¸åš lossï¼Œä½†è¿™å¹¶ä¸æ˜¯è¯´å®ƒä¸èƒ½åš lossã€‚ä¸»è¦åŸå› æ˜¯ prompt çš„åŒè´¨åŒ–æ¯”è¾ƒä¸¥é‡ï¼Œä¸åš loss_mask çš„è¯ï¼ŒåŒæ ·çš„ä¸€å¥è¯ä¼šè¢«ç¿»æ¥è¦†å»çš„å­¦ï¼Œä½†å¦‚æœä½ èƒ½ä¿è¯ä½ çš„æ¯æ¡ prompt éƒ½æ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼Œå°±å®Œå…¨å¯ä»¥çœå» prompt çš„ loss_mask ç¯èŠ‚ã€‚
    
- ä¸ºäº†æé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ï¼Œå°†å¤šç»„æ•°æ®è¿›è¡Œæ‹¼æ¥ï¼Œå°½é‡å¡«æ»¡4096ã€‚ä½†å¯¹äºåˆ†ç±»ä»»åŠ¡ä¼šå‡ºç°é—®é¢˜ï¼Œè¯¦è§https://zhuanlan.zhihu.com/p/809229182ã€‚
    

> ç»è¿‡ä¸€é€šåˆ†æåï¼Œæˆ‘ä»¬å‘ç°ï¼Œæ–°çš„è®­ç»ƒæ–¹å¼æ”¹å˜äº†çŸ­ answer æ•°æ®çš„ loss å æ¯”ï¼Œæ¯•ç«Ÿæ¨¡å‹åœ¨è®¡ç®— loss çš„æ—¶å€™ï¼Œæ˜¯å…ˆç®—ä¸€ä¸ªå¥å­å†…æ¯ä¸ª token çš„ å¹³å‡ lossï¼Œå†ç®—ä¸€ä¸ª batch_size å†…çš„å¹³å‡ lossã€‚
> 
> åˆ†ç±»ä»»åŠ¡çš„ answer é€šå¸¸åªæœ‰ 1 ä¸ª tokenï¼šä¸ concat çš„æ—¶å€™ï¼Œå®ƒçš„ loss è´¡çŒ®å°±æ˜¯ 1 / batch_sizeï¼›concat çš„æ—¶å€™ï¼Œå®ƒå°±éœ€è¦å…ˆå’Œåˆ«çš„ answer çš„ token ç®—å¹³å‡ lossï¼Œå†è´¡çŒ® 1 / batch_sizeã€‚
> 
> è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œé‡‡ç”¨ llama2 æåˆ°çš„ å…ˆ concat è¯­æ–™å†åš sft è®­ç»ƒï¼Œä¼šå¯¹çŸ­ answer æ•°æ®å¾ˆä¸å…¬å¹³ï¼Œä¹Ÿå°±æ›´å®¹æ˜“é€ æˆçŸ­ answer æ•°æ®çš„æ¬ æ‹Ÿåˆï¼Œpretrain ç”±äºæ‰€æœ‰ token éƒ½ç®— loss åˆ™æ²¡æœ‰è¿™ä¸ªç°è±¡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸Šé‡‡æ ·çŸ­ answer æ•°æ®ï¼ŒæˆåŠŸçš„é¿å…äº†åˆ†ç±»ä»»åŠ¡çš„æ•ˆæœä¸‹æ»‘ã€‚

## å®éªŒç»“æœ

![image.png](https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20241210171226.png)
- Llama 2æ¨¡å‹ä¼˜äºLlama 1æ¨¡å‹ã€‚
- Llama 2-70Bæ¯”Llama 1-65Båœ¨MMLUå’ŒBBHä¸Šçš„ç»“æœåˆ†åˆ«æé«˜äº†â‰ˆ5å’Œâ‰ˆ8ä¸ªç‚¹ã€‚
- Llama 2-7Bå’Œ30Bæ¨¡å‹åœ¨é™¤ä»£ç åŸºå‡†ä»¥å¤–çš„æ‰€æœ‰ç±»åˆ«ä¸Šéƒ½ä¼˜äºç›¸åº”å¤§å°çš„MPTæ¨¡å‹ã€‚
- Llama 2-7Bå’Œ34Båœ¨æ‰€æœ‰ç±»åˆ«çš„åŸºå‡†æµ‹è¯•é›†ä¸Šéƒ½ä¼˜äºFalcon-7Bå’Œ40Bæ¨¡å‹ã€‚

  

# å‚è€ƒ

- [[KV Cacheä¼˜åŒ–]ğŸ”¥MQA/GQA/YOCO/CLA/MLKVç¬”è®°: å±‚å†…å’Œå±‚é—´KV Cacheå…±äº« - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/697311739)
- [Transformers KV Caching Explained | by JoÃ£o Lages | Medium](https://medium.com/@joaolages/kv-caching-explained-276520203249)
- https://zhuanlan.zhihu.com/p/679640407
- [LLaMAå®¶æ—](https://y0y1rx552jl.feishu.cn/docx/PiNOdEhjVoo77jxv3xVcJaUVnFp)
- https://zhuanlan.zhihu.com/p/809229182
