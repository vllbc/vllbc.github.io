<!DOCTYPE html>
<html lang="zh-CN">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Layer Norm - vllbc02&#39;s blogs</title><meta name="Description" content="vllbc&#39;s blog"><meta property="og:title" content="Layer Norm" />
<meta property="og:description" content="pre-norm Pre-norm:\(X_t&#43;1=X_{t}&#43;F_{t}(Norm(X_{t}))\) \(先来看Pre-norm^{&#43;},递归展开：\) \[X_{t&#43;1}=X_t&#43;F_t(Norm(X_t))\] \(=X_{0}&#43;F_{1}(Norm(X_{1}))&#43;\ldots&#43;F_{t-1}(Norm(X_{t-1}))&#43;F_{t}(Norm(X_{t}))\) 其中，展开\(^{&#43;}\)后的每一项( \(F_{1}( Norm( X_{1}) ) , \ldots\), \(F_{t- 1}( Norm( X_{t- 1}) )\), \(F_{t}( Norm( X_{t}) )\))之间都" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/layer-norm/" /><meta property="og:image" content="http://localhost:1313/logo.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-29T00:00:00+00:00" /><meta property="og:site_name" content="vllbc02" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/logo.png" /><meta name="twitter:title" content="Layer Norm"/>
<meta name="twitter:description" content="pre-norm Pre-norm:\(X_t&#43;1=X_{t}&#43;F_{t}(Norm(X_{t}))\) \(先来看Pre-norm^{&#43;},递归展开：\) \[X_{t&#43;1}=X_t&#43;F_t(Norm(X_t))\] \(=X_{0}&#43;F_{1}(Norm(X_{1}))&#43;\ldots&#43;F_{t-1}(Norm(X_{t-1}))&#43;F_{t}(Norm(X_{t}))\) 其中，展开\(^{&#43;}\)后的每一项( \(F_{1}( Norm( X_{1}) ) , \ldots\), \(F_{t- 1}( Norm( X_{t- 1}) )\), \(F_{t}( Norm( X_{t}) )\))之间都"/>
<meta name="application-name" content="vllbc02">
<meta name="apple-mobile-web-app-title" content="vllbc02"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/layer-norm/" /><link rel="prev" href="http://localhost:1313/vscode%E9%85%8D%E7%BD%AElatex/" /><link rel="next" href="http://localhost:1313/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Layer Norm",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/layer-norm\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "http:\/\/localhost:1313\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "Deep Learning, 网络正则化, Layer Norm","wordcount":  2059 ,
        "url": "http:\/\/localhost:1313\/layer-norm\/","datePublished": "2023-03-22T00:00:00+00:00","dateModified": "2023-03-29T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "http:\/\/localhost:1313\/images\/avatar.png",
                    "width":  512 ,
                    "height":  512 
                }},"author": {
                "@type": "Person",
                "name": "vllbc"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="vllbc02&#39;s blogs"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="/images/logo.png"
        data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x"
        data-sizes="auto"
        alt="/images/logo.png"
        title="/images/logo.png" width="32" height="32" />vllbc02</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/vllbc/vllbc.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/base16/darcula.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Layer Norm</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://blog.vllbc.top" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>vllbc</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/deep-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Learning</a>&nbsp;<a href="/categories/%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>网络正则化</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-03-22">2023-03-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 2059 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 5 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"></div>
            </div><div class="content" id="content"><h1 id="pre-norm">pre-norm</h1>
<p>Pre-norm:<span
class="math inline">\(X_t+1=X_{t}+F_{t}(Norm(X_{t}))\)</span></p>
<p><span class="math inline">\(先来看Pre-norm^{+},递归展开：\)</span>
<span class="math display">\[X_{t+1}=X_t+F_t(Norm(X_t))\]</span> <span
class="math inline">\(=X_{0}+F_{1}(Norm(X_{1}))+\ldots+F_{t-1}(Norm(X_{t-1}))+F_{t}(Norm(X_{t}))\)</span>
其中，展开<span class="math inline">\(^{+}\)</span>后的每一项( <span
class="math inline">\(F_{1}( Norm( X_{1}) ) , \ldots\)</span>, <span
class="math inline">\(F_{t- 1}( Norm( X_{t- 1}) )\)</span>, <span
class="math inline">\(F_{t}( Norm( X_{t})
)\)</span>)之间都是同一量级的， 所以<span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))+F_t(Norm(X_t))\)</span>和 <span
class="math inline">\(F_1(Norm(X_1))+\ldots
F_{t-1}(Norm(X_{t-1}))\)</span>之间的区别就像t和t-1的区别一样，我们可以将
其记为<span class="math inline">\(X_t+ 1= \mathscr{O} ( t+ 1)\)</span> .
这种特性就导致当t足够大的时候，<span
class="math inline">\(X_{t+1}\)</span>和<span
class="math inline">\(X_t\)</span>之间区别可以忽略不计（直觉上），那么就有：</p>
<p><span class="math display">\[F_t(X_t)+F_{t+1}(X_{t+1})\approx
F_t(X_t)+F_{t+1}(X_t)=(F_t\bigoplus F_{t+1})(X_t)\]</span>
这就是所谓的增加宽度，而没有增加深度。从而导致pre-norm的精度不高。 #
post-norm</p>
<p>Post-norm:<span
class="math inline">\(X_{t+1}=Norm(X_{t}+F_{t}(x_{t}))\)</span>
本来layernorm是为了缓解梯度消失，但是在post-norm这里却成为了梯度消失的罪魁祸首。也导致了收敛较难、需要大量调参。</p>
<p><span
class="math display">\[X_{t+1}=Norm(X_t+F_t(X_t))=\frac{X_t+F_t(X_t)}{\sqrt{2}}\]</span>
<span
class="math display">\[=\frac{X_0}{\sqrt{2}^{t+1}}+\frac{F_0(X_0)}{\sqrt{2}^{t+1}}+\ldots+\frac{F_{t-1}(X_{t-1})}{\sqrt{2}^2}+\frac{F_t(X_t)}{\sqrt{2}}\:(\]</span>
这个结构跟pre-norm比起来充分考虑了所有分支 (残差<span
class="math inline">\(^{+})\)</span>
的输出，做到了真正增加深度，自然精度会相对好一些。</p>
<p>不过它也有它很显然的问题，当t足够大、也就是叠加的attention层足够多以后，底层那些分支(残差)的影响力被衰减掉了，残差有利于解决梯度消失，但是在Post
Norm中，残差这条通道被严重削弱了，越靠近输入，削弱得越严重，残差“名存实亡”，那么势必会有梯度消失的问题，这也就是文章开头所说的postnorm难收敛、参数难调的原因。本来我们做Norm也是为了处理梯度消失，但从分析看来，transformer结构中的layernorm<span
class="math inline">\(^{+}\)</span>并没有完全实现它的作用。那这就意味着transformer原始结构的失败吗？并不是的，因为这种梯度消失的问题在整个结构上来看(配合上adam系优化器和学习率warmup，warmup对于post-norm极为重要)
是并不明显的。</p>
<p>离输入层的残差影响力弱这一特性，也有它的用武之地，比如在<a
href="https://zhida.zhihu.com/search?q=finetune&amp;zhida_source=entity&amp;is_preview=1">finetune</a>的时候，我们就希望不要过多调整靠近输入层的参数、以免破坏预训练的效果。</p>
<h2 id="warmup的重要性">warmup的重要性</h2>
<p><code>Post-LN Transformer</code>在训练的初始阶段，输出层附近的<a
href="https://zhida.zhihu.com/search?q=%E6%9C%9F%E6%9C%9B%E6%A2%AF%E5%BA%A6&amp;zhida_source=entity&amp;is_preview=1">期望梯度</a>非常大，所以，如果没有warm-up，模型优化过程就会炸裂，非常不稳定。
模型对越靠后的层越敏感，也就是越靠后的层学习得越快，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。
很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但
Adam
的更新量是常数量级的，梯度不准，但更新量依然是常数量级，意味着可能就是一个常数量级的<a
href="https://zhida.zhihu.com/search?q=%E9%9A%8F%E6%9C%BA%E5%99%AA%E5%A3%B0&amp;zhida_source=entity&amp;is_preview=1">随机噪声</a>了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。
<img
src="https://cdn.jsdelivr.net/gh/vllbc/img4blog//image/20240913160225.png"
alt="image.png" /></p>
<p>从上图中就可以看出来，post-ln在开始阶段层数越高梯度越大，此时需要小学习率，而当warmup完后，梯度变得很小（绿色部分）。此时可以使用大学习率。</p>
<h2 id="adam如何缓解梯度消失">Adam如何缓解梯度消失</h2>
<p>其实。最关键的原因是，在当前的各种自适应优化技术“下，我们已经不大担心梯度消失问题了。这是因为，当前
NLP 中主流的优化器是 Adam 及其变种。对于 Adam
来说，由于包含了动量和二阶矩校正，所以近似来看，它的更新量大致上为 <span
class="math display">\[\Delta\theta=-\eta\frac{\mathbb{E}_{t}[g_{t}]}{\sqrt{\mathbb{E}_{t}[g_{t}^{2}]}}\]</span>
可以看到，分子分母是都是同量纲的，因此分式结果其实就是
(1)的量级，而更新量就是
(n)量级。也就是说，理论上只要梯度的绝对值大于随机误差，那么对应的参数都会有常数量级的更新量（意思就是参数的更新量与梯度的关系不是很大，因此受梯度消失影响较小）；这跟
SGD 不一样，SGD
的更新量是正比于梯度的，只要梯度小，更新量也会很小，如果梯度
过小，那么参数几乎会没被更新。 所以，Post Norm
的残差虽然被严重削弱，但是在 base、large
级别的模型中，它还不至于削弱到小于随机误差的地步，因此配合 Adam
等优化器，它还是可以得到有效更新的，也就有可能成功训练了。当然，只是有可能，事实上越深的
Post Norm 模型确实越难训练，比如要仔细调节学习率和 Warmup 等。 #
Deep-norm</p>
<p><span
class="math inline">\(最后再提一下DeepNet中结合Post-LN^+的良好性能以及Pre-LN的训练稳定性做出的改良\)</span>。
<span class="math display">\[X_{t+1}=Norm(\alpha
X_t+F_t(X_t))\text{(6)}\]</span> <span class="math inline">\(它在add
norm之前给输入乘了一个up-scale^+的常数系数 α&gt;1\)</span>。</p>
<p>现在 (5) 的展开为： <span
class="math display">\[X_{t+1}=\frac{\alpha^{t+1}X_{0}}{\sqrt{2}^{t+1}}+\frac{\alpha^{t}F_{0}(X_{0})}{\sqrt{2}^{t+1}}+\ldots+\frac{\alpha
F_{t-1}(X_{t-1})}{\sqrt{2}^{2}}+\frac{F_{t}(X_{t})}{\sqrt{2}}\]</span>
因为<span class="math inline">\(\alpha&gt;1\)</span>
,所以它能够在保留post-norm真正增加了深度这优点的同时，一定程度避免了梯度</p>
<p>消失。（本质还是post-norm）</p>
<h1 id="参考">参考</h1>
<p><a
href="https://zhuanlan.zhihu.com/p/662794447">Transformer梳理（一）：Post-Norm
VS Pre-Norm - 知乎 (zhihu.com)</a> <a
href="https://kexue.fm/archives/8747#Warmup%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B5%B7%E4%BD%9C%E7%94%A8%E7%9A%84%EF%BC%9F">模型优化漫谈：BERT的初始标准差为什么是0.02？
- 科学空间|Scientific Spaces (kexue.fm)</a> <a
href="https://kexue.fm/archives/9009">为什么Pre Norm的效果不如Post
Norm？ - 科学空间|Scientific Spaces (kexue.fm)</a> <a
href="https://zhuanlan.zhihu.com/p/84614490">香侬读 |
Transformer中warm-up和LayerNorm的重要性探究 - 知乎 (zhihu.com)</a> <a
href="https://zhuanlan.zhihu.com/p/559495068">Bert/Transformer
被忽视的细节（或许可以用来做面试题） - 知乎 (zhihu.com)</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-03-29</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/layer-norm/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="http://localhost:1313/layer-norm/" data-title="Layer Norm" data-hashtags="Deep Learning,网络正则化,Layer Norm"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="http://localhost:1313/layer-norm/" data-hashtag="Deep Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="http://localhost:1313/layer-norm/" data-title="Layer Norm"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="http://localhost:1313/layer-norm/" data-title="Layer Norm"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="http://localhost:1313/layer-norm/" data-title="Layer Norm"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/deep-learning/">Deep Learning</a>,&nbsp;<a href="/tags/%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/">网络正则化</a>,&nbsp;<a href="/tags/layer-norm/">Layer Norm</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/vscode%E9%85%8D%E7%BD%AElatex/" class="prev" rel="prev" title="vscode配置latex"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>vscode配置latex</a>
            <a href="/l1-l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/" class="next" rel="next" title="L1 L2正则化">L1 L2正则化<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://blog.vllbc.top" target="_blank">vllbc</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
