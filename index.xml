<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>vllbc02</title>
        <link>https://vllbc.top/</link>
        <description>vllbc&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>m18265090197@163.com (vllbc)</managingEditor>
            <webMaster>m18265090197@163.com (vllbc)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate>
            <atom:link href="https://vllbc.top/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>early-stopping</title>
    <link>https://vllbc.top/early-stopping/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/early-stopping/</guid>
    <description><![CDATA[介绍 早停止（Early Stopping）是 当达到某种或某些条件时，认为模型已经收敛，结束模型训练，保存现有模型的一种手段。 如何判断已经收敛？]]></description>
</item>
<item>
    <title>focal loss</title>
    <link>https://vllbc.top/focal-loss/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/focal-loss/</guid>
    <description><![CDATA[Focal Loss Focal Loss主要是为了解决类别不平衡的问题，]]></description>
</item>
<item>
    <title>过拟合的解决方法</title>
    <link>https://vllbc.top/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>最大熵模型</title>
    <link>https://vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>EDA</title>
    <link>https://vllbc.top/eda/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/eda/</guid>
    <description><![CDATA[NLP中的EDA 同义词替换，回译，近音字替换，随机插入，随机交换，随机删除 同义词替换 做法可以是维护一个同义词表，如哈工大的发布的同义词词典。]]></description>
</item>
<item>
    <title>warmup</title>
    <link>https://vllbc.top/warmup/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/warmup/</guid>
    <description><![CDATA[在训练开始的时候，如果学习率太高的话，可能会导致loss来回跳动，会导致无法收敛，因此在训练开始的时候就可以设置一个很小的learning r]]></description>
</item>
<item>
    <title>标签平滑</title>
    <link>https://vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</link>
    <pubDate>Sun, 05 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91/</guid>
    <description><![CDATA[神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。因为oneho]]></description>
</item>
<item>
    <title>Prompt</title>
    <link>https://vllbc.top/prompt/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/prompt/</guid>
    <description><![CDATA[]]></description>
</item>
<item>
    <title>调参技巧</title>
    <link>https://vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/</guid>
    <description><![CDATA[基本原则：快速试错。 小步试错，快速迭代 可以试试无脑的配置 实时打印一些结果 自动调参：网格搜索、random search、贝叶斯优化、 参数初始化]]></description>
</item>
<item>
    <title>对抗训练</title>
    <link>https://vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</link>
    <pubDate>Thu, 02 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://vllbc.top/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</guid>
    <description><![CDATA[]]></description>
</item>
</channel>
</rss>
