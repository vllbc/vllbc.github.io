<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>vllbc02&#39;s blogs</title>
        <link>https://blog.vllbc.top/</link>
        <description>vllbc&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</lastBuildDate>
            <atom:link href="https://blog.vllbc.top/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>shortcode(置顶)</title>
    <link>https://blog.vllbc.top/shortcode%E7%BD%AE%E9%A1%B6/</link>
    <pubDate>Tue, 07 Mar 2023 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/shortcode%E7%BD%AE%E9%A1%B6/</guid>
    <description><![CDATA[<p>贴一下可以玩的shortcode。</p>
<h2 id="音乐播放">音乐播放</h2>
<h3 id="播放列表">播放列表</h3>
<p>夏日口袋专辑：</p>
<meting-js auto="https://music.163.com/album?id=73470837&amp;uct2=U2FsdGVkX18gTMY/Tb1&#43;2PmOZr2G/Q7mOdM/mANJ8xY=" theme="#448aff"></meting-js>
<h3 id="播放单曲">播放单曲</h3>
<p>最爱的一首（我是紬厨）： <meting-js server="netease" type="song" id="1311346841" theme="#448aff"></meting-js></p>
<h2 id="视频播放">视频播放</h2>
<h3 id="bilibili">bilibili</h3>
<p><div class="bilibili"><iframe src="//player.bilibili.com/player.html?bvid=BV1ptXPYREe7&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></div>
 有多P可以选择集数</p>
<h2 id="admonition">admonition</h2>
<p>类型有：note、abstract、info、tip、success、question、warning、failure、danger、bug、example、quote。
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw" aria-hidden="true"></i>技巧<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">一个 <strong>技巧</strong> 横幅</div>
        </div>
    </div></p>]]></description>
</item>
<item>
    <title>MCTS和PRM</title>
    <link>https://blog.vllbc.top/mcts%E5%92%8Cprm/</link>
    <pubDate>Fri, 04 Apr 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/mcts%E5%92%8Cprm/</guid>
    <description><![CDATA[<h2 id="核心总结">核心总结</h2>
<ul>
<li><strong>PRM和MCTS实际上是两种可以独立使用的技术，只不过，往往它们组合使用时往往能产生1+1&gt;2的效果</strong>。例如，
<ul>
<li>单独使用PRM：我们可以让模型对同一个prompt采样多个不同solution，无需MCTS，只需利用模型的temperature等随机参数让每次生成结果不同，然后用PRM对每个solution的每一步打分，最终选择分数最高的路径返回。</li>
<li>单独使用MCTS：使用MCTS生成多个解题路径时，不一定要用PRM来决定哪个节点值得扩展，可以用外部大模型（如GPT-4）来选择，也可以用模型自身的perplexity来判断。本质上，我们需要的是找到最值得扩展的节点，PRM只是挑选的众多方法之一。</li>
</ul></li>
<li><strong>PRM 和 MCTS
既可以应用于优化训练数据，也可以用来预测用</strong>
<ul>
<li>用于得到高质量训练数据：如rStar论文中，可以用PRM和MCTS的方式来迭代地筛选得到质量更好的思维链SFT数据或者RLHF数据，还可以生成更精确的reward
model训练数据。</li>
<li>用于推理：很简单，推理用MCTS的方式把 test-scaling
做上来，再结合PRM的方式从众多路径中挑选最佳答案。</li>
</ul></li>
<li><strong>PRM和MCTS的缺点</strong><br />
这方面 DeepSeek-R1和 kimi1.5的论文已经说得很情况了。</li>
<li>Process Reward Model(PRM) 在实际应用中有三大局限：
<ul>
<li>第一，难以清晰界定一般推理中的细粒度步骤，说白了，怎么定义什么为一个步骤。</li>
<li>第二，判断当前步骤的正误难度大，模型自动化标注不如人意，人工标注又难以拓展。</li>
<li>第三，引入基于模型的PRM易致reward hacking，有时为了训练 policy
model，但反而更多时间去优化 reward model 去了。</li>
</ul></li>
<li>对MCTS的看法：
<ul>
<li>文本的生成搜索空间指数级增长，为应对，给节点设扩展上限，却容易让模型陷入局部最优解困境。</li>
<li>MCTS往往要结合一个精确的PRM来用才能发挥最大效果，但PRM又有上述的问题，陷入一个死循环。
## 参考 https://zhuanlan.zhihu.com/p/27278317894 rStar-Math: Small LLMs
Can Master Math Reasoning with Self-Evolved Deep Thinking</li>
</ul></li>
</ul>]]></description>
</item>
<item>
    <title>LEGB</title>
    <link>https://blog.vllbc.top/legb/</link>
    <pubDate>Mon, 24 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/legb/</guid>
    <description><![CDATA[<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="st">&#39;global&#39;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> outer():</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># def len(in_var):</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     print(&#39;called my len() function: &#39;, end=&quot;&quot;)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     l = 0</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     for i in in_var:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         l += 1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     return l</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> <span class="st">&#39;local&#39;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> inner():</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="kw">nonlocal</span> a</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        a <span class="op">+=</span> <span class="st">&#39; variable&#39;</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    inner()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;a is&#39;</span>, a)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(len(a))</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>outer()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># print(len(a))</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;a is&#39;</span>, a)</span></code></pre></div>
<p>此时为nonlocal
a，会按照local-闭包-global的顺序找到闭包变量a。a的值为local variable</p>]]></description>
</item>
<item>
    <title>debugger</title>
    <link>https://blog.vllbc.top/debugger/</link>
    <pubDate>Sun, 23 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/debugger/</guid>
    <description><![CDATA[<p>python调试工具，类似于vscode的调试工具，使用命令行进行调试。</p>
<h2 id="使用方法">使用方法</h2>
<h3 id="插入式">插入式</h3>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pdb<span class="op">;</span> pdb.set_trace()</span></code></pre></div>
<p>或者</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">breakpoint</span>()</span></code></pre></div>
<h3 id="非插入式">非插入式</h3>
<pre><code>python -m pdb [-c command] (-m module | pyfile) [args ...]</code></pre>
<h2 id="常用命令">常用命令</h2>
<h3 id="h">h</h3>
<p>即help，可用命令如下 </p>]]></description>
</item>
<item>
    <title>generate</title>
    <link>https://blog.vllbc.top/generate/</link>
    <pubDate>Sun, 09 Mar 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/generate/</guid>
    <description><![CDATA[<p>理论部分在这：<a
href="../../NLP/LLM/generate相关.md">generate相关</a> ##
generate参数</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        inputs: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        generation_config: Optional[GenerationConfig] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        logits_processor: Optional[LogitsProcessorList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        stopping_criteria: Optional[StoppingCriteriaList] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        prefix_allowed_tokens_fn: Optional[Callable[[<span class="bu">int</span>, torch.Tensor], List[<span class="bu">int</span>]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        synced_gpus: Optional[<span class="bu">bool</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        assistant_model: Optional[<span class="st">&quot;PreTrainedModel&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        streamer: Optional[<span class="st">&quot;BaseStreamer&quot;</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        negative_prompt_ids: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        negative_prompt_attention_mask: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Union[GenerateOutput, torch.LongTensor]:</span></code></pre></div>
<p>在代码中可以看到在函数入口显式的定义了很多参数。他们的具体含义如下</p>]]></description>
</item>
<item>
    <title>einsum</title>
    <link>https://blog.vllbc.top/einsum/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/einsum/</guid>
    <description><![CDATA[<blockquote>
<p>einops.einsum calls einsum operations with einops-style named axes
indexing, computing tensor products with an arbitrary number of tensors.
Unlike typical einsum syntax, here you must pass tensors first, and then
the pattern.</p>
</blockquote>
<blockquote>
<p>Also, note that rearrange operations such
as <code>"(batch chan) out"</code>, or singleton axes <code>()</code>,
are not currently supported.</p>
</blockquote>
<p>爱因斯坦求和</p>
<p> </p>]]></description>
</item>
<item>
    <title>pack and unpack</title>
    <link>https://blog.vllbc.top/pack-and-unpack/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/pack-and-unpack/</guid>
    <description><![CDATA[<h2 id="pack">pack</h2>
<blockquote>
<p>Packs several tensors into one. See einops tutorial for introduction
into packing (and how it replaces stack and concatenation).</p>
</blockquote>
<p> ## unpack &gt;Unpacks a single tensor into several by
splitting over a selected axes. See einops tutorial for introduction
into packing (and how it replaces stack and concatenation).</p>
<figure>

<figcaption aria-hidden="true">image.png</figcaption>
</figure>]]></description>
</item>
<item>
    <title>rearrange</title>
    <link>https://blog.vllbc.top/rearrange/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/rearrange/</guid>
    <description><![CDATA[<blockquote>
<p>einops.rearrange is a reader-friendly smart element reordering for
multidimensional tensors. This operation includes functionality of
transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack,
concatenate and other operations.</p>
</blockquote>
<p>代替reshape，给维度命名。可以用…代表不想动的维度。  </p>]]></description>
</item>
<item>
    <title>reduce</title>
    <link>https://blog.vllbc.top/reduce/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/reduce/</guid>
    <description><![CDATA[<blockquote>
<p>einops.reduce combines rearrangement and reduction using
reader-friendly notation.</p>
</blockquote>
<p>reduce会使维度减少。  </p>]]></description>
</item>
<item>
    <title>repeat</title>
    <link>https://blog.vllbc.top/repeat/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>vllbc</author>
    <guid>https://blog.vllbc.top/repeat/</guid>
    <description><![CDATA[<blockquote>
<p>einops.repeat allows reordering elements and repeating them in
arbitrary combinations. This operation includes functionality of repeat,
tile, and broadcast functions.</p>
</blockquote>
<p>repeat是使维度增加，与reduce相反。   ## 应用
比如说repeat_kv函数就可以用einops.repeat很方便的实现</p>]]></description>
</item>
</channel>
</rss>
